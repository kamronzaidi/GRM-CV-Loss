The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 48, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 48, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:42:38,388] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.77s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:42:40,212] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Training dataset size: 48, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:42:52,920] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:16,  2.36s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.63s/it] 38%|███▊      | 3/8 [00:07<00:12,  2.51s/it] 50%|█████     | 4/8 [00:09<00:09,  2.26s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.40s/it] 75%|███████▌  | 6/8 [00:14<00:05,  2.57s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.46s/it]100%|██████████| 8/8 [00:19<00:00,  2.55s/it]                                             {'train_runtime': 20.5499, 'train_samples_per_second': 4.672, 'train_steps_per_second': 0.389, 'train_loss': 1.5992658138275146, 'epoch': 2.0}
100%|██████████| 8/8 [00:20<00:00,  2.55s/it]100%|██████████| 8/8 [00:20<00:00,  2.55s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:14,  2.85it/s]  1%|          | 2/212 [00:00<01:04,  3.27it/s]  1%|▏         | 3/212 [00:00<01:01,  3.42it/s]  2%|▏         | 4/212 [00:01<00:56,  3.69it/s]  2%|▏         | 5/212 [00:01<00:55,  3.75it/s]  3%|▎         | 6/212 [00:01<00:56,  3.68it/s]  3%|▎         | 7/212 [00:01<00:55,  3.68it/s]  4%|▍         | 8/212 [00:02<00:54,  3.77it/s]  4%|▍         | 9/212 [00:02<00:53,  3.80it/s]  5%|▍         | 10/212 [00:02<00:54,  3.71it/s]  5%|▌         | 11/212 [00:03<00:53,  3.75it/s]  6%|▌         | 12/212 [00:03<00:53,  3.76it/s]  6%|▌         | 13/212 [00:03<00:52,  3.80it/s]  7%|▋         | 14/212 [00:03<00:53,  3.71it/s]  7%|▋         | 15/212 [00:04<00:52,  3.77it/s]  8%|▊         | 16/212 [00:04<00:52,  3.72it/s]  8%|▊         | 17/212 [00:04<00:51,  3.81it/s]  8%|▊         | 18/212 [00:04<00:52,  3.72it/s]  9%|▉         | 19/212 [00:05<00:51,  3.78it/s]  9%|▉         | 20/212 [00:05<00:51,  3.72it/s] 10%|▉         | 21/212 [00:05<00:50,  3.80it/s] 10%|█         | 22/212 [00:05<00:51,  3.72it/s] 11%|█         | 23/212 [00:06<00:50,  3.78it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.72it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.80it/s] 12%|█▏        | 26/212 [00:07<00:50,  3.72it/s] 13%|█▎        | 27/212 [00:07<00:48,  3.78it/s] 13%|█▎        | 28/212 [00:07<00:49,  3.73it/s] 14%|█▎        | 29/212 [00:07<00:47,  3.82it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.73it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.78it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.74it/s] 16%|█▌        | 33/212 [00:08<00:46,  3.82it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.71it/s] 17%|█▋        | 35/212 [00:09<00:47,  3.77it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.71it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.78it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.70it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.77it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.72it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.79it/s] 20%|█▉        | 42/212 [00:11<00:45,  3.72it/s] 20%|██        | 43/212 [00:11<00:44,  3.77it/s] 21%|██        | 44/212 [00:11<00:45,  3.71it/s] 21%|██        | 45/212 [00:12<00:44,  3.77it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.70it/s] 22%|██▏       | 47/212 [00:12<00:44,  3.69it/s] 23%|██▎       | 48/212 [00:12<00:42,  3.82it/s] 23%|██▎       | 49/212 [00:13<00:41,  3.91it/s] 24%|██▎       | 50/212 [00:13<00:40,  3.98it/s] 24%|██▍       | 51/212 [00:13<00:39,  4.04it/s] 25%|██▍       | 52/212 [00:13<00:39,  4.07it/s] 25%|██▌       | 53/212 [00:14<00:38,  4.10it/s] 25%|██▌       | 54/212 [00:14<00:38,  4.12it/s] 26%|██▌       | 55/212 [00:14<00:38,  4.13it/s] 26%|██▋       | 56/212 [00:14<00:37,  4.13it/s] 27%|██▋       | 57/212 [00:15<00:37,  4.13it/s] 27%|██▋       | 58/212 [00:15<00:37,  4.13it/s] 28%|██▊       | 59/212 [00:15<00:37,  4.13it/s] 28%|██▊       | 60/212 [00:15<00:36,  4.12it/s] 29%|██▉       | 61/212 [00:15<00:36,  4.13it/s] 29%|██▉       | 62/212 [00:16<00:36,  4.14it/s] 30%|██▉       | 63/212 [00:16<00:35,  4.15it/s] 30%|███       | 64/212 [00:16<00:35,  4.15it/s] 31%|███       | 65/212 [00:16<00:35,  4.15it/s] 31%|███       | 66/212 [00:17<00:35,  4.15it/s] 32%|███▏      | 67/212 [00:17<00:34,  4.16it/s] 32%|███▏      | 68/212 [00:17<00:34,  4.16it/s] 33%|███▎      | 69/212 [00:17<00:34,  4.16it/s] 33%|███▎      | 70/212 [00:18<00:34,  4.16it/s] 33%|███▎      | 71/212 [00:18<00:35,  4.02it/s] 34%|███▍      | 72/212 [00:18<00:35,  3.94it/s] 34%|███▍      | 73/212 [00:18<00:36,  3.86it/s] 35%|███▍      | 74/212 [00:19<00:35,  3.92it/s] 35%|███▌      | 75/212 [00:19<00:34,  3.93it/s] 36%|███▌      | 76/212 [00:19<00:35,  3.87it/s] 36%|███▋      | 77/212 [00:20<00:35,  3.80it/s] 37%|███▋      | 78/212 [00:20<00:36,  3.71it/s] 37%|███▋      | 79/212 [00:20<00:34,  3.83it/s] 38%|███▊      | 80/212 [00:20<00:34,  3.85it/s] 38%|███▊      | 81/212 [00:21<00:34,  3.80it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.76it/s] 39%|███▉      | 83/212 [00:21<00:35,  3.68it/s] 40%|███▉      | 84/212 [00:21<00:33,  3.81it/s] 40%|████      | 85/212 [00:22<00:33,  3.83it/s] 41%|████      | 86/212 [00:22<00:33,  3.78it/s] 41%|████      | 87/212 [00:22<00:33,  3.76it/s] 42%|████▏     | 88/212 [00:22<00:33,  3.69it/s] 42%|████▏     | 89/212 [00:23<00:32,  3.82it/s] 42%|████▏     | 90/212 [00:23<00:31,  3.84it/s] 43%|████▎     | 91/212 [00:23<00:32,  3.76it/s] 43%|████▎     | 92/212 [00:23<00:31,  3.77it/s] 44%|████▍     | 93/212 [00:24<00:32,  3.72it/s] 44%|████▍     | 94/212 [00:24<00:30,  3.83it/s] 45%|████▍     | 95/212 [00:24<00:30,  3.81it/s] 45%|████▌     | 96/212 [00:25<00:30,  3.74it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.78it/s] 46%|████▌     | 98/212 [00:25<00:30,  3.72it/s] 47%|████▋     | 99/212 [00:25<00:29,  3.84it/s] 47%|████▋     | 100/212 [00:26<00:29,  3.78it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.75it/s] 48%|████▊     | 102/212 [00:26<00:29,  3.79it/s] 49%|████▊     | 103/212 [00:26<00:29,  3.71it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.81it/s] 50%|████▉     | 105/212 [00:27<00:28,  3.76it/s] 50%|█████     | 106/212 [00:27<00:28,  3.71it/s] 50%|█████     | 107/212 [00:27<00:28,  3.64it/s] 51%|█████     | 108/212 [00:28<00:27,  3.77it/s] 51%|█████▏    | 109/212 [00:28<00:27,  3.80it/s] 52%|█████▏    | 110/212 [00:28<00:27,  3.74it/s] 52%|█████▏    | 111/212 [00:29<00:26,  3.74it/s] 53%|█████▎    | 112/212 [00:29<00:27,  3.67it/s] 53%|█████▎    | 113/212 [00:29<00:26,  3.79it/s] 54%|█████▍    | 114/212 [00:29<00:25,  3.80it/s] 54%|█████▍    | 115/212 [00:30<00:26,  3.73it/s] 55%|█████▍    | 116/212 [00:30<00:25,  3.74it/s] 55%|█████▌    | 117/212 [00:30<00:25,  3.67it/s] 56%|█████▌    | 118/212 [00:30<00:24,  3.79it/s] 56%|█████▌    | 119/212 [00:31<00:24,  3.81it/s] 57%|█████▋    | 120/212 [00:31<00:24,  3.73it/s] 57%|█████▋    | 121/212 [00:31<00:24,  3.76it/s] 58%|█████▊    | 122/212 [00:31<00:24,  3.70it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.79it/s] 58%|█████▊    | 124/212 [00:32<00:23,  3.74it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.71it/s] 59%|█████▉    | 126/212 [00:33<00:23,  3.64it/s] 60%|█████▉    | 127/212 [00:33<00:22,  3.76it/s] 60%|██████    | 128/212 [00:33<00:22,  3.79it/s] 61%|██████    | 129/212 [00:33<00:22,  3.73it/s] 61%|██████▏   | 130/212 [00:34<00:21,  3.74it/s] 62%|██████▏   | 131/212 [00:34<00:22,  3.67it/s] 62%|██████▏   | 132/212 [00:34<00:21,  3.79it/s] 63%|██████▎   | 133/212 [00:34<00:20,  3.80it/s] 63%|██████▎   | 134/212 [00:35<00:20,  3.73it/s] 64%|██████▎   | 135/212 [00:35<00:20,  3.74it/s] 64%|██████▍   | 136/212 [00:35<00:20,  3.68it/s] 65%|██████▍   | 137/212 [00:35<00:19,  3.79it/s] 65%|██████▌   | 138/212 [00:36<00:19,  3.75it/s] 66%|██████▌   | 139/212 [00:36<00:19,  3.71it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.64it/s] 67%|██████▋   | 141/212 [00:37<00:18,  3.78it/s] 67%|██████▋   | 142/212 [00:37<00:18,  3.80it/s] 67%|██████▋   | 143/212 [00:37<00:18,  3.78it/s] 68%|██████▊   | 144/212 [00:37<00:18,  3.73it/s] 68%|██████▊   | 145/212 [00:38<00:18,  3.65it/s] 69%|██████▉   | 146/212 [00:38<00:17,  3.77it/s] 69%|██████▉   | 147/212 [00:38<00:17,  3.80it/s] 70%|██████▉   | 148/212 [00:38<00:17,  3.73it/s] 70%|███████   | 149/212 [00:39<00:16,  3.74it/s] 71%|███████   | 150/212 [00:39<00:16,  3.69it/s] 71%|███████   | 151/212 [00:39<00:16,  3.80it/s] 72%|███████▏  | 152/212 [00:39<00:15,  3.78it/s] 72%|███████▏  | 153/212 [00:40<00:15,  3.71it/s] 73%|███████▎  | 154/212 [00:40<00:15,  3.75it/s] 73%|███████▎  | 155/212 [00:40<00:15,  3.69it/s] 74%|███████▎  | 156/212 [00:41<00:14,  3.78it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.72it/s] 75%|███████▍  | 158/212 [00:41<00:14,  3.73it/s] 75%|███████▌  | 159/212 [00:41<00:14,  3.73it/s] 75%|███████▌  | 160/212 [00:42<00:13,  3.83it/s] 76%|███████▌  | 161/212 [00:42<00:13,  3.90it/s] 76%|███████▋  | 162/212 [00:42<00:12,  3.95it/s] 77%|███████▋  | 163/212 [00:42<00:12,  4.00it/s] 77%|███████▋  | 164/212 [00:43<00:11,  4.03it/s] 78%|███████▊  | 165/212 [00:43<00:11,  4.05it/s] 78%|███████▊  | 166/212 [00:43<00:11,  4.05it/s] 79%|███████▉  | 167/212 [00:43<00:11,  4.06it/s] 79%|███████▉  | 168/212 [00:44<00:10,  4.08it/s] 80%|███████▉  | 169/212 [00:44<00:10,  4.08it/s] 80%|████████  | 170/212 [00:44<00:10,  4.07it/s] 81%|████████  | 171/212 [00:44<00:10,  4.08it/s] 81%|████████  | 172/212 [00:45<00:09,  4.09it/s] 82%|████████▏ | 173/212 [00:45<00:09,  4.09it/s] 82%|████████▏ | 174/212 [00:45<00:09,  4.09it/s] 83%|████████▎ | 175/212 [00:45<00:09,  4.08it/s] 83%|████████▎ | 176/212 [00:46<00:08,  4.09it/s] 83%|████████▎ | 177/212 [00:46<00:08,  4.10it/s] 84%|████████▍ | 178/212 [00:46<00:08,  4.10it/s] 84%|████████▍ | 179/212 [00:46<00:08,  4.08it/s] 85%|████████▍ | 180/212 [00:47<00:07,  4.09it/s] 85%|████████▌ | 181/212 [00:47<00:07,  4.09it/s] 86%|████████▌ | 182/212 [00:47<00:07,  4.09it/s] 86%|████████▋ | 183/212 [00:47<00:07,  4.09it/s] 87%|████████▋ | 184/212 [00:47<00:06,  4.08it/s] 87%|████████▋ | 185/212 [00:48<00:06,  4.09it/s] 88%|████████▊ | 186/212 [00:48<00:06,  4.10it/s] 88%|████████▊ | 187/212 [00:48<00:06,  3.95it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.85it/s] 89%|████████▉ | 189/212 [00:49<00:06,  3.83it/s] 90%|████████▉ | 190/212 [00:49<00:05,  3.87it/s] 90%|█████████ | 191/212 [00:49<00:05,  3.73it/s] 91%|█████████ | 192/212 [00:50<00:05,  3.70it/s] 91%|█████████ | 193/212 [00:50<00:05,  3.73it/s] 92%|█████████▏| 194/212 [00:50<00:04,  3.80it/s] 92%|█████████▏| 195/212 [00:50<00:04,  3.69it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.67it/s] 93%|█████████▎| 197/212 [00:51<00:03,  3.79it/s] 93%|█████████▎| 198/212 [00:51<00:03,  3.87it/s] 94%|█████████▍| 199/212 [00:51<00:03,  3.92it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.97it/s] 95%|█████████▍| 201/212 [00:52<00:02,  4.00it/s] 95%|█████████▌| 202/212 [00:52<00:02,  4.02it/s] 96%|█████████▌| 203/212 [00:52<00:02,  4.03it/s] 96%|█████████▌| 204/212 [00:53<00:01,  4.05it/s] 97%|█████████▋| 205/212 [00:53<00:01,  4.06it/s] 97%|█████████▋| 206/212 [00:53<00:01,  4.06it/s] 98%|█████████▊| 207/212 [00:53<00:01,  4.06it/s] 98%|█████████▊| 208/212 [00:54<00:00,  4.07it/s] 99%|█████████▊| 209/212 [00:54<00:00,  4.07it/s] 99%|█████████▉| 210/212 [00:54<00:00,  4.07it/s]100%|█████████▉| 211/212 [00:54<00:00,  4.07it/s]100%|██████████| 212/212 [00:55<00:00,  4.09it/s]accuracy:  0.6462264150943396
100%|██████████| 212/212 [00:58<00:00,  3.64it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 24644.62 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14753.69 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14804.53 examples/s]
Map (num_proc=10):   0%|          | 0/135 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/135 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/135 [00:00<00:32,  4.07 examples/s]Map (num_proc=10):   1%|          | 1/135 [00:00<00:33,  4.05 examples/s]Map (num_proc=10):   6%|▌         | 8/135 [00:00<00:04, 27.06 examples/s]Map (num_proc=10):   7%|▋         | 9/135 [00:00<00:04, 31.34 examples/s]Map (num_proc=10):  25%|██▌       | 34/135 [00:00<00:00, 104.68 examples/s]Map (num_proc=10):  38%|███▊      | 51/135 [00:00<00:00, 158.08 examples/s]Map (num_proc=10):   0%|          | 0/135 [00:00<?, ? examples/s]Map (num_proc=10):  58%|█████▊    | 78/135 [00:00<00:00, 186.66 examples/s]Map (num_proc=10):  71%|███████   | 96/135 [00:00<00:00, 223.89 examples/s]Map (num_proc=10):  92%|█████████▏| 124/135 [00:00<00:00, 262.64 examples/s]Map (num_proc=10):  96%|█████████▋| 130/135 [00:00<00:00, 252.87 examples/s]Map (num_proc=10):   1%|          | 1/135 [00:00<00:38,  3.45 examples/s]Map (num_proc=10): 100%|██████████| 135/135 [00:00<00:00, 166.09 examples/s]
Map (num_proc=10): 100%|██████████| 135/135 [00:00<00:00, 154.21 examples/s]
Map (num_proc=10):  14%|█▍        | 19/135 [00:00<00:01, 61.50 examples/s]Training dataset size: 48, validation dataset size: 135
Training dataset size: 48, validation dataset size: 135
Map (num_proc=10):  50%|████▉     | 67/135 [00:00<00:00, 191.89 examples/s]Map (num_proc=10):  73%|███████▎  | 99/135 [00:00<00:00, 222.18 examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Map (num_proc=10):  90%|████████▉ | 121/135 [00:00<00:00, 157.29 examples/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 48, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:44:40,463] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:44:40,657] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:44:42,048] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:17,  2.53s/it] 25%|██▌       | 2/8 [00:04<00:13,  2.31s/it] 38%|███▊      | 3/8 [00:07<00:11,  2.40s/it] 50%|█████     | 4/8 [00:09<00:08,  2.19s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:11<00:07,  2.34s/it] 75%|███████▌  | 6/8 [00:13<00:04,  2.25s/it] 88%|████████▊ | 7/8 [00:15<00:02,  2.23s/it]100%|██████████| 8/8 [00:17<00:00,  2.14s/it]                                             {'train_runtime': 18.4607, 'train_samples_per_second': 5.2, 'train_steps_per_second': 0.433, 'train_loss': 0.3479384481906891, 'epoch': 2.0}
100%|██████████| 8/8 [00:18<00:00,  2.14s/it]100%|██████████| 8/8 [00:18<00:00,  2.29s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 23048.08 examples/s]
Map (num_proc=10):   0%|          | 0/163 [00:00<?, ? examples/s]Map (num_proc=10):   2%|▏         | 3/163 [00:00<00:14, 10.94 examples/s]Map (num_proc=10):  25%|██▍       | 40/163 [00:00<00:00, 132.26 examples/s]Map (num_proc=10):  57%|█████▋    | 93/163 [00:00<00:00, 258.16 examples/s]Map (num_proc=10):  84%|████████▍ | 137/163 [00:00<00:00, 312.31 examples/s]Map (num_proc=10): 100%|██████████| 163/163 [00:00<00:00, 215.64 examples/s]
Map:   0%|          | 0/163 [00:00<?, ? examples/s]Map: 100%|██████████| 163/163 [00:00<00:00, 4418.48 examples/s]
Map:   0%|          | 0/163 [00:00<?, ? examples/s]Map: 100%|██████████| 163/163 [00:00<00:00, 3788.64 examples/s]
Map:   0%|          | 0/163 [00:00<?, ? examples/s]Map: 100%|██████████| 163/163 [00:00<00:00, 4518.47 examples/s]
Filter (num_proc=10):   0%|          | 0/163 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 17/163 [00:00<00:02, 68.44 examples/s]Filter (num_proc=10):  51%|█████     | 83/163 [00:00<00:00, 257.50 examples/s]Filter (num_proc=10):  80%|████████  | 131/163 [00:00<00:00, 311.41 examples/s]Filter (num_proc=10): 100%|██████████| 163/163 [00:00<00:00, 260.96 examples/s]
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:53,  3.02it/s]  1%|          | 2/163 [00:00<00:45,  3.50it/s]  2%|▏         | 3/163 [00:00<00:44,  3.59it/s]  2%|▏         | 4/163 [00:01<00:43,  3.63it/s]  3%|▎         | 5/163 [00:01<00:43,  3.60it/s]  4%|▎         | 6/163 [00:01<00:41,  3.76it/s]  4%|▍         | 7/163 [00:01<00:40,  3.82it/s]  5%|▍         | 8/163 [00:02<00:41,  3.78it/s]  6%|▌         | 9/163 [00:02<00:41,  3.75it/s]  6%|▌         | 10/163 [00:02<00:41,  3.68it/s]  7%|▋         | 11/163 [00:02<00:39,  3.81it/s]  7%|▋         | 12/163 [00:03<00:39,  3.85it/s]  8%|▊         | 13/163 [00:03<00:39,  3.81it/s]  9%|▊         | 14/163 [00:03<00:39,  3.77it/s]  9%|▉         | 15/163 [00:04<00:39,  3.71it/s] 10%|▉         | 16/163 [00:04<00:38,  3.82it/s] 10%|█         | 17/163 [00:04<00:37,  3.86it/s] 11%|█         | 18/163 [00:04<00:38,  3.82it/s] 12%|█▏        | 19/163 [00:05<00:38,  3.76it/s] 12%|█▏        | 20/163 [00:05<00:38,  3.69it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.81it/s] 13%|█▎        | 22/163 [00:05<00:36,  3.83it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.80it/s] 15%|█▍        | 24/163 [00:06<00:36,  3.76it/s] 15%|█▌        | 25/163 [00:06<00:37,  3.69it/s] 16%|█▌        | 26/163 [00:06<00:35,  3.81it/s] 17%|█▋        | 27/163 [00:07<00:35,  3.85it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.81it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.77it/s] 18%|█▊        | 30/163 [00:08<00:35,  3.70it/s] 19%|█▉        | 31/163 [00:08<00:34,  3.82it/s] 20%|█▉        | 32/163 [00:08<00:33,  3.86it/s] 20%|██        | 33/163 [00:08<00:34,  3.82it/s] 21%|██        | 34/163 [00:09<00:34,  3.78it/s] 21%|██▏       | 35/163 [00:09<00:34,  3.70it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.82it/s] 23%|██▎       | 37/163 [00:09<00:32,  3.86it/s] 23%|██▎       | 38/163 [00:10<00:32,  3.81it/s] 24%|██▍       | 39/163 [00:10<00:32,  3.77it/s] 25%|██▍       | 40/163 [00:10<00:33,  3.69it/s] 25%|██▌       | 41/163 [00:10<00:31,  3.82it/s] 26%|██▌       | 42/163 [00:11<00:31,  3.86it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.82it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.77it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.69it/s] 28%|██▊       | 46/163 [00:12<00:30,  3.81it/s] 29%|██▉       | 47/163 [00:12<00:30,  3.83it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.80it/s] 30%|███       | 49/163 [00:13<00:30,  3.76it/s] 31%|███       | 50/163 [00:13<00:30,  3.71it/s] 31%|███▏      | 51/163 [00:13<00:29,  3.81it/s] 32%|███▏      | 52/163 [00:13<00:28,  3.87it/s] 33%|███▎      | 53/163 [00:14<00:28,  3.82it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.77it/s] 34%|███▎      | 55/163 [00:14<00:29,  3.71it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.81it/s] 35%|███▍      | 57/163 [00:15<00:27,  3.85it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.80it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.76it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.68it/s] 37%|███▋      | 61/163 [00:16<00:26,  3.81it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.86it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.80it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.77it/s] 40%|███▉      | 65/163 [00:17<00:26,  3.69it/s] 40%|████      | 66/163 [00:17<00:25,  3.82it/s] 41%|████      | 67/163 [00:17<00:24,  3.85it/s] 42%|████▏     | 68/163 [00:18<00:25,  3.77it/s] 42%|████▏     | 69/163 [00:18<00:24,  3.79it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.78it/s] 44%|████▎     | 71/163 [00:18<00:23,  3.87it/s] 44%|████▍     | 72/163 [00:19<00:23,  3.95it/s] 45%|████▍     | 73/163 [00:19<00:22,  3.99it/s] 45%|████▌     | 74/163 [00:19<00:22,  4.02it/s] 46%|████▌     | 75/163 [00:19<00:21,  4.04it/s] 47%|████▋     | 76/163 [00:20<00:21,  4.07it/s] 47%|████▋     | 77/163 [00:20<00:21,  4.08it/s] 48%|████▊     | 78/163 [00:20<00:20,  4.09it/s] 48%|████▊     | 79/163 [00:20<00:20,  4.09it/s] 49%|████▉     | 80/163 [00:20<00:20,  4.09it/s] 50%|████▉     | 81/163 [00:21<00:20,  4.00it/s] 50%|█████     | 82/163 [00:21<00:20,  3.93it/s] 51%|█████     | 83/163 [00:21<00:20,  3.87it/s] 52%|█████▏    | 84/163 [00:22<00:20,  3.85it/s] 52%|█████▏    | 85/163 [00:22<00:19,  3.92it/s] 53%|█████▎    | 86/163 [00:22<00:19,  3.90it/s] 53%|█████▎    | 87/163 [00:22<00:19,  3.80it/s] 54%|█████▍    | 88/163 [00:23<00:19,  3.79it/s] 55%|█████▍    | 89/163 [00:23<00:20,  3.70it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.82it/s] 56%|█████▌    | 91/163 [00:23<00:18,  3.86it/s] 56%|█████▋    | 92/163 [00:24<00:18,  3.82it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.77it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.73it/s] 58%|█████▊    | 95/163 [00:24<00:18,  3.78it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.83it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.80it/s] 60%|██████    | 98/163 [00:25<00:17,  3.75it/s] 61%|██████    | 99/163 [00:25<00:16,  3.78it/s] 61%|██████▏   | 100/163 [00:26<00:16,  3.76it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.84it/s] 63%|██████▎   | 102/163 [00:26<00:16,  3.80it/s] 63%|██████▎   | 103/163 [00:27<00:15,  3.76it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.79it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.71it/s] 65%|██████▌   | 106/163 [00:27<00:14,  3.82it/s] 66%|██████▌   | 107/163 [00:28<00:14,  3.82it/s] 66%|██████▋   | 108/163 [00:28<00:14,  3.74it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.76it/s] 67%|██████▋   | 110/163 [00:28<00:14,  3.70it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.81it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.82it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.75it/s] 70%|██████▉   | 114/163 [00:29<00:12,  3.77it/s] 71%|███████   | 115/163 [00:30<00:12,  3.70it/s] 71%|███████   | 116/163 [00:30<00:12,  3.81it/s] 72%|███████▏  | 117/163 [00:30<00:12,  3.82it/s] 72%|███████▏  | 118/163 [00:31<00:11,  3.75it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.74it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.72it/s] 74%|███████▍  | 121/163 [00:31<00:11,  3.77it/s] 75%|███████▍  | 122/163 [00:32<00:10,  3.80it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.78it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.73it/s] 77%|███████▋  | 125/163 [00:32<00:10,  3.71it/s] 77%|███████▋  | 126/163 [00:33<00:09,  3.77it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.83it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.80it/s] 79%|███████▉  | 129/163 [00:33<00:09,  3.75it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.73it/s] 80%|████████  | 131/163 [00:34<00:08,  3.78it/s] 81%|████████  | 132/163 [00:34<00:08,  3.85it/s] 82%|████████▏ | 133/163 [00:35<00:07,  3.80it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.74it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.73it/s] 83%|████████▎ | 136/163 [00:35<00:07,  3.78it/s] 84%|████████▍ | 137/163 [00:36<00:06,  3.83it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.79it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.73it/s] 86%|████████▌ | 140/163 [00:36<00:06,  3.71it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.77it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.82it/s] 88%|████████▊ | 143/163 [00:37<00:05,  3.78it/s] 88%|████████▊ | 144/163 [00:37<00:05,  3.74it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.71it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.77it/s] 90%|█████████ | 147/163 [00:38<00:04,  3.82it/s] 91%|█████████ | 148/163 [00:38<00:03,  3.78it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.73it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.69it/s] 93%|█████████▎| 151/163 [00:39<00:03,  3.76it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.83it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.79it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.73it/s] 95%|█████████▌| 155/163 [00:40<00:02,  3.71it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.75it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.81it/s] 97%|█████████▋| 158/163 [00:41<00:01,  3.77it/s] 98%|█████████▊| 159/163 [00:41<00:01,  3.71it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.71it/s] 99%|█████████▉| 161/163 [00:42<00:00,  3.75it/s] 99%|█████████▉| 162/163 [00:42<00:00,  3.81it/s]100%|██████████| 163/163 [00:42<00:00,  3.77it/s]accuracy:  0.8466257668711656
100%|██████████| 163/163 [00:45<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16015.92 examples/s]
Map (num_proc=10):   0%|          | 0/198 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/198 [00:00<00:54,  3.60 examples/s]Map (num_proc=10):  22%|██▏       | 43/198 [00:00<00:01, 140.82 examples/s]Map (num_proc=10):  52%|█████▏    | 102/198 [00:00<00:00, 275.49 examples/s]Map (num_proc=10):  79%|███████▉  | 156/198 [00:00<00:00, 349.35 examples/s]Map (num_proc=10): 100%|██████████| 198/198 [00:00<00:00, 237.38 examples/s]
Training dataset size: 48, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 48, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Training dataset size: 48, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:46:18,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:46:21,183] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:46:21,796] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:17,  2.51s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.61s/it] 38%|███▊      | 3/8 [00:07<00:13,  2.64s/it] 50%|█████     | 4/8 [00:10<00:10,  2.57s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.54s/it] 75%|███████▌  | 6/8 [00:15<00:05,  2.52s/it] 88%|████████▊ | 7/8 [00:18<00:02,  2.73s/it]100%|██████████| 8/8 [00:21<00:00,  2.73s/it]                                             {'train_runtime': 21.8477, 'train_samples_per_second': 4.394, 'train_steps_per_second': 0.366, 'train_loss': 0.6906022429466248, 'epoch': 2.0}
100%|██████████| 8/8 [00:21<00:00,  2.73s/it]100%|██████████| 8/8 [00:21<00:00,  2.71s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 20235.53 examples/s]
Map (num_proc=10):   0%|          | 0/232 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/232 [00:00<00:29,  7.87 examples/s]Map (num_proc=10):  12%|█▏        | 27/232 [00:00<00:02, 88.37 examples/s]Map (num_proc=10):  39%|███▉      | 91/232 [00:00<00:00, 258.62 examples/s]Map (num_proc=10):  65%|██████▍   | 150/232 [00:00<00:00, 351.48 examples/s]Map (num_proc=10):  84%|████████▍ | 195/232 [00:00<00:00, 374.16 examples/s]Map (num_proc=10): 100%|██████████| 232/232 [00:00<00:00, 267.18 examples/s]
Map:   0%|          | 0/232 [00:00<?, ? examples/s]Map: 100%|██████████| 232/232 [00:00<00:00, 4157.55 examples/s]
Map:   0%|          | 0/232 [00:00<?, ? examples/s]Map: 100%|██████████| 232/232 [00:00<00:00, 4262.82 examples/s]
Map:   0%|          | 0/232 [00:00<?, ? examples/s]Map: 100%|██████████| 232/232 [00:00<00:00, 4084.24 examples/s]
Filter (num_proc=10):   0%|          | 0/232 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 24/232 [00:00<00:02, 99.54 examples/s]Filter (num_proc=10):  31%|███       | 71/232 [00:00<00:00, 216.06 examples/s]Filter (num_proc=10):  60%|██████    | 140/232 [00:00<00:00, 352.74 examples/s]Filter (num_proc=10): 100%|██████████| 232/232 [00:00<00:00, 426.95 examples/s]Filter (num_proc=10): 100%|██████████| 232/232 [00:00<00:00, 311.12 examples/s]
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:30,  2.57it/s]  1%|          | 2/232 [00:00<01:13,  3.15it/s]  1%|▏         | 3/232 [00:00<01:06,  3.44it/s]  2%|▏         | 4/232 [00:01<01:05,  3.50it/s]  2%|▏         | 5/232 [00:01<01:01,  3.70it/s]  3%|▎         | 6/232 [00:01<01:00,  3.72it/s]  3%|▎         | 7/232 [00:01<01:00,  3.69it/s]  3%|▎         | 8/232 [00:02<00:59,  3.74it/s]  4%|▍         | 9/232 [00:02<01:00,  3.69it/s]  4%|▍         | 10/232 [00:02<00:58,  3.82it/s]  5%|▍         | 11/232 [00:03<00:57,  3.82it/s]  5%|▌         | 12/232 [00:03<00:58,  3.77it/s]  6%|▌         | 13/232 [00:03<00:57,  3.80it/s]  6%|▌         | 14/232 [00:03<00:58,  3.73it/s]  6%|▋         | 15/232 [00:04<00:56,  3.84it/s]  7%|▋         | 16/232 [00:04<00:57,  3.78it/s]  7%|▋         | 17/232 [00:04<00:57,  3.77it/s]  8%|▊         | 18/232 [00:04<00:56,  3.80it/s]  8%|▊         | 19/232 [00:05<00:57,  3.72it/s]  9%|▊         | 20/232 [00:05<00:55,  3.83it/s]  9%|▉         | 21/232 [00:05<00:55,  3.77it/s]  9%|▉         | 22/232 [00:05<00:55,  3.77it/s] 10%|▉         | 23/232 [00:06<00:55,  3.79it/s] 10%|█         | 24/232 [00:06<00:55,  3.73it/s] 11%|█         | 25/232 [00:06<00:53,  3.84it/s] 11%|█         | 26/232 [00:06<00:53,  3.82it/s] 12%|█▏        | 27/232 [00:07<00:54,  3.77it/s] 12%|█▏        | 28/232 [00:07<00:53,  3.79it/s] 12%|█▎        | 29/232 [00:07<00:54,  3.72it/s] 13%|█▎        | 30/232 [00:08<00:52,  3.83it/s] 13%|█▎        | 31/232 [00:08<00:52,  3.80it/s] 14%|█▍        | 32/232 [00:08<00:53,  3.74it/s] 14%|█▍        | 33/232 [00:08<00:52,  3.79it/s] 15%|█▍        | 34/232 [00:09<00:53,  3.73it/s] 15%|█▌        | 35/232 [00:09<00:51,  3.84it/s] 16%|█▌        | 36/232 [00:09<00:51,  3.82it/s] 16%|█▌        | 37/232 [00:09<00:51,  3.76it/s] 16%|█▋        | 38/232 [00:10<00:51,  3.79it/s] 17%|█▋        | 39/232 [00:10<00:51,  3.72it/s] 17%|█▋        | 40/232 [00:10<00:50,  3.83it/s] 18%|█▊        | 41/232 [00:10<00:50,  3.82it/s] 18%|█▊        | 42/232 [00:11<00:50,  3.75it/s] 19%|█▊        | 43/232 [00:11<00:49,  3.79it/s] 19%|█▉        | 44/232 [00:11<00:50,  3.72it/s] 19%|█▉        | 45/232 [00:12<00:48,  3.82it/s] 20%|█▉        | 46/232 [00:12<00:49,  3.77it/s] 20%|██        | 47/232 [00:12<00:49,  3.75it/s] 21%|██        | 48/232 [00:12<00:48,  3.78it/s] 21%|██        | 49/232 [00:13<00:49,  3.71it/s] 22%|██▏       | 50/232 [00:13<00:47,  3.82it/s] 22%|██▏       | 51/232 [00:13<00:47,  3.77it/s] 22%|██▏       | 52/232 [00:13<00:47,  3.76it/s] 23%|██▎       | 53/232 [00:14<00:47,  3.79it/s] 23%|██▎       | 54/232 [00:14<00:47,  3.72it/s] 24%|██▎       | 55/232 [00:14<00:46,  3.83it/s] 24%|██▍       | 56/232 [00:14<00:46,  3.78it/s] 25%|██▍       | 57/232 [00:15<00:46,  3.73it/s] 25%|██▌       | 58/232 [00:15<00:47,  3.66it/s] 25%|██▌       | 59/232 [00:15<00:45,  3.80it/s] 26%|██▌       | 60/232 [00:15<00:44,  3.85it/s] 26%|██▋       | 61/232 [00:16<00:44,  3.80it/s] 27%|██▋       | 62/232 [00:16<00:45,  3.76it/s] 27%|██▋       | 63/232 [00:16<00:45,  3.68it/s] 28%|██▊       | 64/232 [00:17<00:44,  3.80it/s] 28%|██▊       | 65/232 [00:17<00:43,  3.82it/s] 28%|██▊       | 66/232 [00:17<00:43,  3.79it/s] 29%|██▉       | 67/232 [00:17<00:43,  3.75it/s] 29%|██▉       | 68/232 [00:18<00:44,  3.67it/s] 30%|██▉       | 69/232 [00:18<00:42,  3.79it/s] 30%|███       | 70/232 [00:18<00:42,  3.82it/s] 31%|███       | 71/232 [00:18<00:42,  3.78it/s] 31%|███       | 72/232 [00:19<00:42,  3.74it/s] 31%|███▏      | 73/232 [00:19<00:43,  3.66it/s] 32%|███▏      | 74/232 [00:19<00:41,  3.79it/s] 32%|███▏      | 75/232 [00:19<00:41,  3.83it/s] 33%|███▎      | 76/232 [00:20<00:41,  3.76it/s] 33%|███▎      | 77/232 [00:20<00:41,  3.74it/s] 34%|███▎      | 78/232 [00:20<00:41,  3.67it/s] 34%|███▍      | 79/232 [00:21<00:40,  3.79it/s] 34%|███▍      | 80/232 [00:21<00:40,  3.80it/s] 35%|███▍      | 81/232 [00:21<00:40,  3.73it/s] 35%|███▌      | 82/232 [00:21<00:40,  3.73it/s] 36%|███▌      | 83/232 [00:22<00:40,  3.67it/s] 36%|███▌      | 84/232 [00:22<00:39,  3.79it/s] 37%|███▋      | 85/232 [00:22<00:38,  3.80it/s] 37%|███▋      | 86/232 [00:22<00:39,  3.74it/s] 38%|███▊      | 87/232 [00:23<00:38,  3.73it/s] 38%|███▊      | 88/232 [00:23<00:39,  3.68it/s] 38%|███▊      | 89/232 [00:23<00:37,  3.80it/s] 39%|███▉      | 90/232 [00:23<00:37,  3.79it/s] 39%|███▉      | 91/232 [00:24<00:37,  3.73it/s] 40%|███▉      | 92/232 [00:24<00:37,  3.77it/s] 40%|████      | 93/232 [00:24<00:37,  3.72it/s] 41%|████      | 94/232 [00:25<00:36,  3.83it/s] 41%|████      | 95/232 [00:25<00:36,  3.78it/s] 41%|████▏     | 96/232 [00:25<00:36,  3.74it/s] 42%|████▏     | 97/232 [00:25<00:36,  3.72it/s] 42%|████▏     | 98/232 [00:26<00:35,  3.73it/s] 43%|████▎     | 99/232 [00:26<00:34,  3.81it/s] 43%|████▎     | 100/232 [00:26<00:35,  3.74it/s] 44%|████▎     | 101/232 [00:26<00:34,  3.76it/s] 44%|████▍     | 102/232 [00:27<00:34,  3.76it/s] 44%|████▍     | 103/232 [00:27<00:33,  3.86it/s] 45%|████▍     | 104/232 [00:27<00:32,  3.93it/s] 45%|████▌     | 105/232 [00:27<00:31,  3.98it/s] 46%|████▌     | 106/232 [00:28<00:31,  4.03it/s] 46%|████▌     | 107/232 [00:28<00:30,  4.06it/s] 47%|████▋     | 108/232 [00:28<00:30,  4.08it/s] 47%|████▋     | 109/232 [00:28<00:30,  4.10it/s] 47%|████▋     | 110/232 [00:29<00:29,  4.10it/s] 48%|████▊     | 111/232 [00:29<00:29,  4.10it/s] 48%|████▊     | 112/232 [00:29<00:29,  4.10it/s] 49%|████▊     | 113/232 [00:29<00:29,  4.10it/s] 49%|████▉     | 114/232 [00:30<00:28,  4.11it/s] 50%|████▉     | 115/232 [00:30<00:28,  4.11it/s] 50%|█████     | 116/232 [00:30<00:28,  4.11it/s] 50%|█████     | 117/232 [00:30<00:28,  4.10it/s] 51%|█████     | 118/232 [00:31<00:27,  4.10it/s] 51%|█████▏    | 119/232 [00:31<00:27,  4.11it/s] 52%|█████▏    | 120/232 [00:31<00:27,  4.12it/s] 52%|█████▏    | 121/232 [00:31<00:26,  4.12it/s] 53%|█████▎    | 122/232 [00:32<00:26,  4.12it/s] 53%|█████▎    | 123/232 [00:32<00:26,  4.11it/s] 53%|█████▎    | 124/232 [00:32<00:26,  4.11it/s] 54%|█████▍    | 125/232 [00:32<00:26,  4.11it/s] 54%|█████▍    | 126/232 [00:33<00:25,  4.12it/s] 55%|█████▍    | 127/232 [00:33<00:25,  4.13it/s] 55%|█████▌    | 128/232 [00:33<00:25,  4.13it/s] 56%|█████▌    | 129/232 [00:33<00:24,  4.13it/s] 56%|█████▌    | 130/232 [00:33<00:24,  4.12it/s] 56%|█████▋    | 131/232 [00:34<00:24,  4.11it/s] 57%|█████▋    | 132/232 [00:34<00:24,  4.11it/s] 57%|█████▋    | 133/232 [00:34<00:24,  4.11it/s] 58%|█████▊    | 134/232 [00:34<00:23,  4.12it/s] 58%|█████▊    | 135/232 [00:35<00:23,  4.12it/s] 59%|█████▊    | 136/232 [00:35<00:23,  4.11it/s] 59%|█████▉    | 137/232 [00:35<00:23,  4.10it/s] 59%|█████▉    | 138/232 [00:35<00:22,  4.10it/s] 60%|█████▉    | 139/232 [00:36<00:22,  4.11it/s] 60%|██████    | 140/232 [00:36<00:22,  4.12it/s] 61%|██████    | 141/232 [00:36<00:22,  4.12it/s] 61%|██████    | 142/232 [00:36<00:21,  4.12it/s] 62%|██████▏   | 143/232 [00:37<00:21,  4.11it/s] 62%|██████▏   | 144/232 [00:37<00:21,  4.11it/s] 62%|██████▎   | 145/232 [00:37<00:21,  4.11it/s] 63%|██████▎   | 146/232 [00:37<00:20,  4.12it/s] 63%|██████▎   | 147/232 [00:38<00:20,  4.12it/s] 64%|██████▍   | 148/232 [00:38<00:20,  4.12it/s] 64%|██████▍   | 149/232 [00:38<00:20,  4.11it/s] 65%|██████▍   | 150/232 [00:38<00:19,  4.10it/s] 65%|██████▌   | 151/232 [00:39<00:19,  4.11it/s] 66%|██████▌   | 152/232 [00:39<00:19,  4.11it/s] 66%|██████▌   | 153/232 [00:39<00:19,  4.12it/s] 66%|██████▋   | 154/232 [00:39<00:18,  4.12it/s] 67%|██████▋   | 155/232 [00:40<00:18,  4.12it/s] 67%|██████▋   | 156/232 [00:40<00:18,  4.11it/s] 68%|██████▊   | 157/232 [00:40<00:18,  4.11it/s] 68%|██████▊   | 158/232 [00:40<00:17,  4.11it/s] 69%|██████▊   | 159/232 [00:41<00:17,  4.12it/s] 69%|██████▉   | 160/232 [00:41<00:17,  4.13it/s] 69%|██████▉   | 161/232 [00:41<00:17,  4.13it/s] 70%|██████▉   | 162/232 [00:41<00:16,  4.12it/s] 70%|███████   | 163/232 [00:42<00:16,  4.07it/s] 71%|███████   | 164/232 [00:42<00:17,  3.95it/s] 71%|███████   | 165/232 [00:42<00:17,  3.88it/s] 72%|███████▏  | 166/232 [00:42<00:17,  3.85it/s] 72%|███████▏  | 167/232 [00:43<00:16,  3.88it/s] 72%|███████▏  | 168/232 [00:43<00:16,  3.78it/s] 73%|███████▎  | 169/232 [00:43<00:16,  3.78it/s] 73%|███████▎  | 170/232 [00:43<00:16,  3.71it/s] 74%|███████▎  | 171/232 [00:44<00:16,  3.79it/s] 74%|███████▍  | 172/232 [00:44<00:16,  3.73it/s] 75%|███████▍  | 173/232 [00:44<00:15,  3.69it/s] 75%|███████▌  | 174/232 [00:44<00:15,  3.65it/s] 75%|███████▌  | 175/232 [00:45<00:15,  3.77it/s] 76%|███████▌  | 176/232 [00:45<00:15,  3.71it/s] 76%|███████▋  | 177/232 [00:45<00:14,  3.68it/s] 77%|███████▋  | 178/232 [00:46<00:14,  3.62it/s] 77%|███████▋  | 179/232 [00:46<00:14,  3.75it/s] 78%|███████▊  | 180/232 [00:46<00:14,  3.71it/s] 78%|███████▊  | 181/232 [00:46<00:13,  3.68it/s] 78%|███████▊  | 182/232 [00:47<00:13,  3.62it/s] 79%|███████▉  | 183/232 [00:47<00:13,  3.75it/s] 79%|███████▉  | 184/232 [00:47<00:12,  3.75it/s] 80%|███████▉  | 185/232 [00:47<00:12,  3.68it/s] 80%|████████  | 186/232 [00:48<00:12,  3.68it/s] 81%|████████  | 187/232 [00:48<00:12,  3.69it/s] 81%|████████  | 188/232 [00:48<00:11,  3.74it/s] 81%|████████▏ | 189/232 [00:49<00:11,  3.68it/s] 82%|████████▏ | 190/232 [00:49<00:11,  3.72it/s] 82%|████████▏ | 191/232 [00:49<00:11,  3.67it/s] 83%|████████▎ | 192/232 [00:49<00:10,  3.72it/s] 83%|████████▎ | 193/232 [00:50<00:10,  3.66it/s] 84%|████████▎ | 194/232 [00:50<00:10,  3.71it/s] 84%|████████▍ | 195/232 [00:50<00:10,  3.64it/s] 84%|████████▍ | 196/232 [00:50<00:09,  3.71it/s] 85%|████████▍ | 197/232 [00:51<00:09,  3.66it/s] 85%|████████▌ | 198/232 [00:51<00:09,  3.70it/s] 86%|████████▌ | 199/232 [00:51<00:09,  3.65it/s] 86%|████████▌ | 200/232 [00:52<00:08,  3.73it/s] 87%|████████▋ | 201/232 [00:52<00:08,  3.67it/s] 87%|████████▋ | 202/232 [00:52<00:08,  3.71it/s] 88%|████████▊ | 203/232 [00:52<00:07,  3.66it/s] 88%|████████▊ | 204/232 [00:53<00:07,  3.74it/s] 88%|████████▊ | 205/232 [00:53<00:07,  3.68it/s] 89%|████████▉ | 206/232 [00:53<00:06,  3.72it/s] 89%|████████▉ | 207/232 [00:53<00:06,  3.66it/s] 90%|████████▉ | 208/232 [00:54<00:06,  3.78it/s] 90%|█████████ | 209/232 [00:54<00:05,  3.86it/s] 91%|█████████ | 210/232 [00:54<00:05,  3.93it/s] 91%|█████████ | 211/232 [00:54<00:05,  3.83it/s] 91%|█████████▏| 212/232 [00:55<00:05,  3.77it/s] 92%|█████████▏| 213/232 [00:55<00:05,  3.66it/s] 92%|█████████▏| 214/232 [00:55<00:04,  3.78it/s] 93%|█████████▎| 215/232 [00:55<00:04,  3.78it/s] 93%|█████████▎| 216/232 [00:56<00:04,  3.69it/s] 94%|█████████▎| 217/232 [00:56<00:04,  3.70it/s] 94%|█████████▍| 218/232 [00:56<00:03,  3.69it/s] 94%|█████████▍| 219/232 [00:57<00:03,  3.74it/s] 95%|█████████▍| 220/232 [00:57<00:03,  3.69it/s] 95%|█████████▌| 221/232 [00:57<00:02,  3.73it/s] 96%|█████████▌| 222/232 [00:57<00:02,  3.67it/s] 96%|█████████▌| 223/232 [00:58<00:02,  3.73it/s] 97%|█████████▋| 224/232 [00:58<00:02,  3.68it/s] 97%|█████████▋| 225/232 [00:58<00:01,  3.72it/s] 97%|█████████▋| 226/232 [00:58<00:01,  3.66it/s] 98%|█████████▊| 227/232 [00:59<00:01,  3.73it/s] 98%|█████████▊| 228/232 [00:59<00:01,  3.68it/s] 99%|█████████▊| 229/232 [00:59<00:00,  3.71it/s] 99%|█████████▉| 230/232 [01:00<00:00,  3.65it/s]100%|█████████▉| 231/232 [01:00<00:00,  3.73it/s]100%|██████████| 232/232 [01:00<00:00,  3.70it/s]accuracy:  0.8706896551724138
100%|██████████| 232/232 [01:04<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16235.97 examples/s]
Filter (num_proc=10):  70%|███████   | 1804/2575 [00:00<00:00, 17046.06 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14628.87 examples/s]
Map (num_proc=10):   0%|          | 0/164 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/164 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/164 [00:00<00:42,  3.83 examples/s]Map (num_proc=10):  21%|██        | 34/164 [00:00<00:01, 115.50 examples/s]Map (num_proc=10):   1%|          | 2/164 [00:00<00:20,  7.91 examples/s]Map (num_proc=10):  60%|██████    | 99/164 [00:00<00:00, 277.61 examples/s]Map (num_proc=10):  15%|█▍        | 24/164 [00:00<00:01, 83.81 examples/s]Map (num_proc=10):  48%|████▊     | 78/164 [00:00<00:00, 237.86 examples/s]Map (num_proc=10):  84%|████████▍ | 138/164 [00:00<00:00, 285.02 examples/s]Map (num_proc=10):  76%|███████▌  | 124/164 [00:00<00:00, 307.68 examples/s]Map (num_proc=10): 100%|██████████| 164/164 [00:00<00:00, 202.20 examples/s]
Training dataset size: 48, validation dataset size: 164
Map (num_proc=10):  99%|█████████▉| 163/164 [00:00<00:00, 271.23 examples/s]Map (num_proc=10): 100%|██████████| 164/164 [00:00<00:00, 200.43 examples/s]
Training dataset size: 48, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 48, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:48:19,307] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:48:19,707] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 02:48:19,767] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:20,  2.92s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.62s/it] 38%|███▊      | 3/8 [00:07<00:12,  2.41s/it] 50%|█████     | 4/8 [00:10<00:09,  2.47s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.50s/it] 75%|███████▌  | 6/8 [00:15<00:04,  2.50s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.33s/it]100%|██████████| 8/8 [00:19<00:00,  2.31s/it]                                             {'train_runtime': 19.9671, 'train_samples_per_second': 4.808, 'train_steps_per_second': 0.401, 'train_loss': 1.1121432781219482, 'epoch': 2.0}
100%|██████████| 8/8 [00:19<00:00,  2.31s/it]100%|██████████| 8/8 [00:19<00:00,  2.47s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 22466.39 examples/s]
Map (num_proc=10):   0%|          | 0/248 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/248 [00:00<00:31,  7.77 examples/s]Map (num_proc=10):  21%|██        | 52/248 [00:00<00:01, 173.29 examples/s]Map (num_proc=10):  42%|████▏     | 105/248 [00:00<00:00, 287.83 examples/s]Map (num_proc=10):  68%|██████▊   | 169/248 [00:00<00:00, 398.52 examples/s]Map (num_proc=10):  91%|█████████ | 226/248 [00:00<00:00, 436.31 examples/s]Map (num_proc=10): 100%|██████████| 248/248 [00:00<00:00, 291.78 examples/s]
Map:   0%|          | 0/248 [00:00<?, ? examples/s]Map: 100%|██████████| 248/248 [00:00<00:00, 4441.60 examples/s]
Map:   0%|          | 0/248 [00:00<?, ? examples/s]Map: 100%|██████████| 248/248 [00:00<00:00, 4288.30 examples/s]
Map:   0%|          | 0/248 [00:00<?, ? examples/s]Map: 100%|██████████| 248/248 [00:00<00:00, 4172.57 examples/s]
Filter (num_proc=10):   0%|          | 0/248 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 25/248 [00:00<00:01, 116.79 examples/s]Filter (num_proc=10):  40%|████      | 100/248 [00:00<00:00, 341.67 examples/s]Filter (num_proc=10):  71%|███████   | 175/248 [00:00<00:00, 468.03 examples/s]Filter (num_proc=10): 100%|██████████| 248/248 [00:00<00:00, 543.21 examples/s]Filter (num_proc=10): 100%|██████████| 248/248 [00:00<00:00, 398.88 examples/s]
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:30,  2.72it/s]  1%|          | 2/248 [00:00<01:12,  3.41it/s]  1%|          | 3/248 [00:00<01:07,  3.61it/s]  2%|▏         | 4/248 [00:01<01:07,  3.61it/s]  2%|▏         | 5/248 [00:01<01:07,  3.61it/s]  2%|▏         | 6/248 [00:01<01:06,  3.63it/s]  3%|▎         | 7/248 [00:01<01:04,  3.73it/s]  3%|▎         | 8/248 [00:02<01:02,  3.84it/s]  4%|▎         | 9/248 [00:02<01:03,  3.77it/s]  4%|▍         | 10/248 [00:02<01:03,  3.72it/s]  4%|▍         | 11/248 [00:03<01:03,  3.71it/s]  5%|▍         | 12/248 [00:03<01:04,  3.69it/s]  5%|▌         | 13/248 [00:03<01:01,  3.79it/s]  6%|▌         | 14/248 [00:03<01:00,  3.90it/s]  6%|▌         | 15/248 [00:04<01:00,  3.83it/s]  6%|▋         | 16/248 [00:04<01:02,  3.74it/s]  7%|▋         | 17/248 [00:04<01:01,  3.76it/s]  7%|▋         | 18/248 [00:04<01:02,  3.71it/s]  8%|▊         | 19/248 [00:05<01:00,  3.81it/s]  8%|▊         | 20/248 [00:05<00:58,  3.91it/s]  8%|▊         | 21/248 [00:05<00:58,  3.89it/s]  9%|▉         | 22/248 [00:05<01:00,  3.77it/s]  9%|▉         | 23/248 [00:06<00:59,  3.76it/s] 10%|▉         | 24/248 [00:06<00:59,  3.74it/s] 10%|█         | 25/248 [00:06<00:58,  3.80it/s] 10%|█         | 26/248 [00:06<00:56,  3.90it/s] 11%|█         | 27/248 [00:07<00:57,  3.87it/s] 11%|█▏        | 28/248 [00:07<00:58,  3.74it/s] 12%|█▏        | 29/248 [00:07<00:58,  3.76it/s] 12%|█▏        | 30/248 [00:08<00:58,  3.73it/s] 12%|█▎        | 31/248 [00:08<00:57,  3.81it/s] 13%|█▎        | 32/248 [00:08<00:55,  3.90it/s] 13%|█▎        | 33/248 [00:08<00:55,  3.86it/s] 14%|█▎        | 34/248 [00:09<00:57,  3.72it/s] 14%|█▍        | 35/248 [00:09<00:57,  3.73it/s] 15%|█▍        | 36/248 [00:09<00:57,  3.69it/s] 15%|█▍        | 37/248 [00:09<00:55,  3.79it/s] 15%|█▌        | 38/248 [00:10<00:54,  3.89it/s] 16%|█▌        | 39/248 [00:10<00:55,  3.79it/s] 16%|█▌        | 40/248 [00:10<00:55,  3.74it/s] 17%|█▋        | 41/248 [00:10<00:55,  3.75it/s] 17%|█▋        | 42/248 [00:11<00:55,  3.70it/s] 17%|█▋        | 43/248 [00:11<00:54,  3.79it/s] 18%|█▊        | 44/248 [00:11<00:52,  3.89it/s] 18%|█▊        | 45/248 [00:11<00:52,  3.86it/s] 19%|█▊        | 46/248 [00:12<00:53,  3.74it/s] 19%|█▉        | 47/248 [00:12<00:53,  3.73it/s] 19%|█▉        | 48/248 [00:12<00:53,  3.71it/s] 20%|█▉        | 49/248 [00:13<00:52,  3.78it/s] 20%|██        | 50/248 [00:13<00:51,  3.88it/s] 21%|██        | 51/248 [00:13<00:50,  3.86it/s] 21%|██        | 52/248 [00:13<00:52,  3.73it/s] 21%|██▏       | 53/248 [00:14<00:52,  3.74it/s] 22%|██▏       | 54/248 [00:14<00:52,  3.72it/s] 22%|██▏       | 55/248 [00:14<00:51,  3.78it/s] 23%|██▎       | 56/248 [00:14<00:49,  3.88it/s] 23%|██▎       | 57/248 [00:15<00:49,  3.84it/s] 23%|██▎       | 58/248 [00:15<00:51,  3.71it/s] 24%|██▍       | 59/248 [00:15<00:50,  3.73it/s] 24%|██▍       | 60/248 [00:15<00:50,  3.70it/s] 25%|██▍       | 61/248 [00:16<00:49,  3.78it/s] 25%|██▌       | 62/248 [00:16<00:47,  3.89it/s] 25%|██▌       | 63/248 [00:16<00:48,  3.82it/s] 26%|██▌       | 64/248 [00:17<00:49,  3.72it/s] 26%|██▌       | 65/248 [00:17<00:49,  3.73it/s] 27%|██▋       | 66/248 [00:17<00:49,  3.67it/s] 27%|██▋       | 67/248 [00:17<00:47,  3.78it/s] 27%|██▋       | 68/248 [00:18<00:46,  3.88it/s] 28%|██▊       | 69/248 [00:18<00:46,  3.82it/s] 28%|██▊       | 70/248 [00:18<00:47,  3.72it/s] 29%|██▊       | 71/248 [00:18<00:47,  3.73it/s] 29%|██▉       | 72/248 [00:19<00:47,  3.67it/s] 29%|██▉       | 73/248 [00:19<00:46,  3.78it/s] 30%|██▉       | 74/248 [00:19<00:44,  3.88it/s] 30%|███       | 75/248 [00:19<00:45,  3.81it/s] 31%|███       | 76/248 [00:20<00:46,  3.72it/s] 31%|███       | 77/248 [00:20<00:45,  3.73it/s] 31%|███▏      | 78/248 [00:20<00:46,  3.68it/s] 32%|███▏      | 79/248 [00:20<00:44,  3.79it/s] 32%|███▏      | 80/248 [00:21<00:43,  3.88it/s] 33%|███▎      | 81/248 [00:21<00:44,  3.79it/s] 33%|███▎      | 82/248 [00:21<00:44,  3.74it/s] 33%|███▎      | 83/248 [00:22<00:44,  3.72it/s] 34%|███▍      | 84/248 [00:22<00:44,  3.66it/s] 34%|███▍      | 85/248 [00:22<00:43,  3.79it/s] 35%|███▍      | 86/248 [00:22<00:41,  3.88it/s] 35%|███▌      | 87/248 [00:23<00:42,  3.79it/s] 35%|███▌      | 88/248 [00:23<00:42,  3.72it/s] 36%|███▌      | 89/248 [00:23<00:42,  3.72it/s] 36%|███▋      | 90/248 [00:23<00:42,  3.69it/s] 37%|███▋      | 91/248 [00:24<00:41,  3.77it/s] 37%|███▋      | 92/248 [00:24<00:40,  3.86it/s] 38%|███▊      | 93/248 [00:24<00:41,  3.77it/s] 38%|███▊      | 94/248 [00:24<00:41,  3.71it/s] 38%|███▊      | 95/248 [00:25<00:41,  3.72it/s] 39%|███▊      | 96/248 [00:25<00:41,  3.68it/s] 39%|███▉      | 97/248 [00:25<00:39,  3.80it/s] 40%|███▉      | 98/248 [00:26<00:39,  3.83it/s] 40%|███▉      | 99/248 [00:26<00:39,  3.77it/s] 40%|████      | 100/248 [00:26<00:39,  3.71it/s] 41%|████      | 101/248 [00:26<00:39,  3.68it/s] 41%|████      | 102/248 [00:27<00:38,  3.75it/s] 42%|████▏     | 103/248 [00:27<00:37,  3.84it/s] 42%|████▏     | 104/248 [00:27<00:38,  3.74it/s] 42%|████▏     | 105/248 [00:27<00:38,  3.70it/s] 43%|████▎     | 106/248 [00:28<00:38,  3.72it/s] 43%|████▎     | 107/248 [00:28<00:38,  3.65it/s] 44%|████▎     | 108/248 [00:28<00:37,  3.77it/s] 44%|████▍     | 109/248 [00:28<00:35,  3.86it/s] 44%|████▍     | 110/248 [00:29<00:36,  3.79it/s] 45%|████▍     | 111/248 [00:29<00:36,  3.71it/s] 45%|████▌     | 112/248 [00:29<00:36,  3.68it/s] 46%|████▌     | 113/248 [00:30<00:35,  3.80it/s] 46%|████▌     | 114/248 [00:30<00:34,  3.89it/s] 46%|████▋     | 115/248 [00:30<00:33,  3.95it/s] 47%|████▋     | 116/248 [00:30<00:33,  4.00it/s] 47%|████▋     | 117/248 [00:31<00:32,  4.03it/s] 48%|████▊     | 118/248 [00:31<00:32,  4.06it/s] 48%|████▊     | 119/248 [00:31<00:31,  4.07it/s] 48%|████▊     | 120/248 [00:31<00:31,  4.08it/s] 49%|████▉     | 121/248 [00:31<00:31,  4.09it/s] 49%|████▉     | 122/248 [00:32<00:30,  4.11it/s] 50%|████▉     | 123/248 [00:32<00:30,  4.11it/s] 50%|█████     | 124/248 [00:32<00:30,  4.11it/s] 50%|█████     | 125/248 [00:32<00:29,  4.10it/s] 51%|█████     | 126/248 [00:33<00:29,  4.10it/s] 51%|█████     | 127/248 [00:33<00:29,  4.11it/s] 52%|█████▏    | 128/248 [00:33<00:29,  4.12it/s] 52%|█████▏    | 129/248 [00:33<00:28,  4.12it/s] 52%|█████▏    | 130/248 [00:34<00:28,  4.12it/s] 53%|█████▎    | 131/248 [00:34<00:28,  4.11it/s] 53%|█████▎    | 132/248 [00:34<00:28,  4.11it/s] 54%|█████▎    | 133/248 [00:34<00:27,  4.11it/s] 54%|█████▍    | 134/248 [00:35<00:27,  4.12it/s] 54%|█████▍    | 135/248 [00:35<00:27,  4.12it/s] 55%|█████▍    | 136/248 [00:35<00:27,  4.13it/s] 55%|█████▌    | 137/248 [00:35<00:26,  4.12it/s] 56%|█████▌    | 138/248 [00:36<00:26,  4.11it/s] 56%|█████▌    | 139/248 [00:36<00:26,  4.11it/s] 56%|█████▋    | 140/248 [00:36<00:26,  4.04it/s] 57%|█████▋    | 141/248 [00:36<00:27,  3.92it/s] 57%|█████▋    | 142/248 [00:37<00:27,  3.81it/s] 58%|█████▊    | 143/248 [00:37<00:26,  3.90it/s] 58%|█████▊    | 144/248 [00:37<00:26,  3.95it/s] 58%|█████▊    | 145/248 [00:37<00:26,  3.84it/s] 59%|█████▉    | 146/248 [00:38<00:27,  3.77it/s] 59%|█████▉    | 147/248 [00:38<00:27,  3.73it/s] 60%|█████▉    | 148/248 [00:38<00:26,  3.78it/s] 60%|██████    | 149/248 [00:39<00:25,  3.82it/s] 60%|██████    | 150/248 [00:39<00:26,  3.72it/s] 61%|██████    | 151/248 [00:39<00:25,  3.75it/s] 61%|██████▏   | 152/248 [00:39<00:25,  3.69it/s] 62%|██████▏   | 153/248 [00:40<00:24,  3.81it/s] 62%|██████▏   | 154/248 [00:40<00:25,  3.72it/s] 62%|██████▎   | 155/248 [00:40<00:25,  3.70it/s] 63%|██████▎   | 156/248 [00:40<00:24,  3.72it/s] 63%|██████▎   | 157/248 [00:41<00:23,  3.81it/s] 64%|██████▎   | 158/248 [00:41<00:23,  3.89it/s] 64%|██████▍   | 159/248 [00:41<00:22,  3.95it/s] 65%|██████▍   | 160/248 [00:41<00:22,  3.99it/s] 65%|██████▍   | 161/248 [00:42<00:21,  4.01it/s] 65%|██████▌   | 162/248 [00:42<00:21,  4.03it/s] 66%|██████▌   | 163/248 [00:42<00:20,  4.05it/s] 66%|██████▌   | 164/248 [00:42<00:20,  4.06it/s] 67%|██████▋   | 165/248 [00:43<00:20,  4.06it/s] 67%|██████▋   | 166/248 [00:43<00:20,  4.07it/s] 67%|██████▋   | 167/248 [00:43<00:19,  4.08it/s] 68%|██████▊   | 168/248 [00:43<00:19,  4.09it/s] 68%|██████▊   | 169/248 [00:44<00:19,  3.96it/s] 69%|██████▊   | 170/248 [00:44<00:20,  3.86it/s] 69%|██████▉   | 171/248 [00:44<00:19,  3.86it/s] 69%|██████▉   | 172/248 [00:44<00:19,  3.92it/s] 70%|██████▉   | 173/248 [00:45<00:19,  3.86it/s] 70%|███████   | 174/248 [00:45<00:19,  3.81it/s] 71%|███████   | 175/248 [00:45<00:19,  3.76it/s] 71%|███████   | 176/248 [00:45<00:19,  3.76it/s] 71%|███████▏  | 177/248 [00:46<00:18,  3.84it/s] 72%|███████▏  | 178/248 [00:46<00:18,  3.79it/s] 72%|███████▏  | 179/248 [00:46<00:18,  3.76it/s] 73%|███████▎  | 180/248 [00:47<00:18,  3.70it/s] 73%|███████▎  | 181/248 [00:47<00:17,  3.80it/s] 73%|███████▎  | 182/248 [00:47<00:17,  3.84it/s] 74%|███████▍  | 183/248 [00:47<00:17,  3.80it/s] 74%|███████▍  | 184/248 [00:48<00:16,  3.77it/s] 75%|███████▍  | 185/248 [00:48<00:17,  3.70it/s] 75%|███████▌  | 186/248 [00:48<00:16,  3.80it/s] 75%|███████▌  | 187/248 [00:48<00:15,  3.82it/s] 76%|███████▌  | 188/248 [00:49<00:15,  3.79it/s] 76%|███████▌  | 189/248 [00:49<00:15,  3.82it/s] 77%|███████▋  | 190/248 [00:49<00:15,  3.74it/s] 77%|███████▋  | 191/248 [00:49<00:14,  3.84it/s] 77%|███████▋  | 192/248 [00:50<00:14,  3.80it/s] 78%|███████▊  | 193/248 [00:50<00:14,  3.77it/s] 78%|███████▊  | 194/248 [00:50<00:14,  3.75it/s] 79%|███████▊  | 195/248 [00:50<00:14,  3.74it/s] 79%|███████▉  | 196/248 [00:51<00:13,  3.83it/s] 79%|███████▉  | 197/248 [00:51<00:13,  3.78it/s] 80%|███████▉  | 198/248 [00:51<00:13,  3.76it/s] 80%|████████  | 199/248 [00:52<00:13,  3.69it/s] 81%|████████  | 200/248 [00:52<00:12,  3.80it/s] 81%|████████  | 201/248 [00:52<00:12,  3.81it/s] 81%|████████▏ | 202/248 [00:52<00:12,  3.79it/s] 82%|████████▏ | 203/248 [00:53<00:11,  3.81it/s] 82%|████████▏ | 204/248 [00:53<00:11,  3.75it/s] 83%|████████▎ | 205/248 [00:53<00:11,  3.84it/s] 83%|████████▎ | 206/248 [00:53<00:11,  3.81it/s] 83%|████████▎ | 207/248 [00:54<00:10,  3.77it/s] 84%|████████▍ | 208/248 [00:54<00:10,  3.70it/s] 84%|████████▍ | 209/248 [00:54<00:10,  3.76it/s] 85%|████████▍ | 210/248 [00:54<00:09,  3.81it/s] 85%|████████▌ | 211/248 [00:55<00:09,  3.77it/s] 85%|████████▌ | 212/248 [00:55<00:09,  3.75it/s] 86%|████████▌ | 213/248 [00:55<00:09,  3.70it/s] 86%|████████▋ | 214/248 [00:56<00:08,  3.81it/s] 87%|████████▋ | 215/248 [00:56<00:08,  3.82it/s] 87%|████████▋ | 216/248 [00:56<00:08,  3.79it/s] 88%|████████▊ | 217/248 [00:56<00:08,  3.76it/s] 88%|████████▊ | 218/248 [00:57<00:08,  3.69it/s] 88%|████████▊ | 219/248 [00:57<00:07,  3.80it/s] 89%|████████▊ | 220/248 [00:57<00:07,  3.80it/s] 89%|████████▉ | 221/248 [00:57<00:07,  3.78it/s] 90%|████████▉ | 222/248 [00:58<00:06,  3.75it/s] 90%|████████▉ | 223/248 [00:58<00:06,  3.74it/s] 90%|█████████ | 224/248 [00:58<00:06,  3.83it/s] 91%|█████████ | 225/248 [00:58<00:06,  3.80it/s] 91%|█████████ | 226/248 [00:59<00:05,  3.75it/s] 92%|█████████▏| 227/248 [00:59<00:05,  3.67it/s] 92%|█████████▏| 228/248 [00:59<00:05,  3.78it/s] 92%|█████████▏| 229/248 [00:59<00:04,  3.82it/s] 93%|█████████▎| 230/248 [01:00<00:04,  3.78it/s] 93%|█████████▎| 231/248 [01:00<00:04,  3.75it/s] 94%|█████████▎| 232/248 [01:00<00:04,  3.69it/s] 94%|█████████▍| 233/248 [01:01<00:03,  3.79it/s] 94%|█████████▍| 234/248 [01:01<00:03,  3.81it/s] 95%|█████████▍| 235/248 [01:01<00:03,  3.78it/s] 95%|█████████▌| 236/248 [01:01<00:03,  3.76it/s] 96%|█████████▌| 237/248 [01:02<00:02,  3.69it/s] 96%|█████████▌| 238/248 [01:02<00:02,  3.80it/s] 96%|█████████▋| 239/248 [01:02<00:02,  3.81it/s] 97%|█████████▋| 240/248 [01:02<00:02,  3.79it/s] 97%|█████████▋| 241/248 [01:03<00:01,  3.80it/s] 98%|█████████▊| 242/248 [01:03<00:01,  3.72it/s] 98%|█████████▊| 243/248 [01:03<00:01,  3.82it/s] 98%|█████████▊| 244/248 [01:03<00:01,  3.77it/s] 99%|█████████▉| 245/248 [01:04<00:00,  3.74it/s] 99%|█████████▉| 246/248 [01:04<00:00,  3.68it/s]100%|█████████▉| 247/248 [01:04<00:00,  3.79it/s]100%|██████████| 248/248 [01:05<00:00,  3.81it/s]accuracy:  0.5887096774193549
100%|██████████| 248/248 [01:08<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 23593.93 examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15670.71 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14381.00 examples/s]
Filter (num_proc=10):  80%|████████  | 2061/2575 [00:00<00:00, 20441.01 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 13974.27 examples/s]
Map (num_proc=10):   0%|          | 0/236 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/236 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/236 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 1/236 [00:00<01:08,  3.43 examples/s]Map (num_proc=10):   0%|          | 1/236 [00:00<01:10,  3.35 examples/s]Map (num_proc=10):  11%|█         | 25/236 [00:00<00:02, 80.04 examples/s]Map (num_proc=10):   3%|▎         | 8/236 [00:00<00:09, 24.35 examples/s]Map (num_proc=10):   0%|          | 1/236 [00:00<01:08,  3.42 examples/s]Map (num_proc=10):  32%|███▏      | 75/236 [00:00<00:00, 206.63 examples/s]Map (num_proc=10):  30%|███       | 71/236 [00:00<00:00, 212.64 examples/s]Map (num_proc=10):   5%|▌         | 12/236 [00:00<00:06, 36.78 examples/s]Map (num_proc=10):  54%|█████▍    | 128/236 [00:00<00:00, 298.63 examples/s]Map (num_proc=10):  51%|█████     | 120/236 [00:00<00:00, 290.41 examples/s]Map (num_proc=10):  23%|██▎       | 54/236 [00:00<00:01, 151.05 examples/s]Map (num_proc=10):  72%|███████▏  | 170/236 [00:00<00:00, 328.65 examples/s]Map (num_proc=10):  72%|███████▏  | 171/236 [00:00<00:00, 352.42 examples/s]Map (num_proc=10):  56%|█████▋    | 133/236 [00:00<00:00, 341.82 examples/s]Map (num_proc=10):  91%|█████████ | 215/236 [00:00<00:00, 349.10 examples/s]Map (num_proc=10):  91%|█████████ | 215/236 [00:00<00:00, 375.59 examples/s]Map (num_proc=10):  78%|███████▊  | 184/236 [00:00<00:00, 384.45 examples/s]Map (num_proc=10): 100%|██████████| 236/236 [00:00<00:00, 239.86 examples/s]
Map (num_proc=10):  97%|█████████▋| 230/236 [00:00<00:00, 391.41 examples/s]Map (num_proc=10): 100%|██████████| 236/236 [00:00<00:00, 236.35 examples/s]
Training dataset size: 48, validation dataset size: 236
Training dataset size: 48, validation dataset size: 236
Map (num_proc=10): 100%|██████████| 236/236 [00:01<00:00, 231.86 examples/s]
Training dataset size: 48, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.33s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:50:18,821] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 02:50:19,138] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 02:50:19,272] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:17,  2.50s/it] 25%|██▌       | 2/8 [00:04<00:13,  2.32s/it] 38%|███▊      | 3/8 [00:07<00:12,  2.47s/it] 50%|█████     | 4/8 [00:10<00:10,  2.55s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.54s/it] 75%|███████▌  | 6/8 [00:15<00:05,  2.61s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.55s/it]100%|██████████| 8/8 [00:19<00:00,  2.41s/it]                                             {'train_runtime': 20.5024, 'train_samples_per_second': 4.682, 'train_steps_per_second': 0.39, 'train_loss': 0.8797242641448975, 'epoch': 2.0}
100%|██████████| 8/8 [00:20<00:00,  2.41s/it]100%|██████████| 8/8 [00:20<00:00,  2.54s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.95s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 22066.08 examples/s]
Map (num_proc=10):   0%|          | 0/361 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/361 [00:00<00:46,  7.78 examples/s]Map (num_proc=10):   5%|▍         | 18/361 [00:00<00:05, 61.84 examples/s]Map (num_proc=10):  30%|███       | 109/361 [00:00<00:00, 345.77 examples/s]Map (num_proc=10):  58%|█████▊    | 210/361 [00:00<00:00, 540.18 examples/s]Map (num_proc=10):  82%|████████▏ | 295/361 [00:00<00:00, 612.26 examples/s]Map (num_proc=10): 100%|██████████| 361/361 [00:00<00:00, 395.19 examples/s]
Map:   0%|          | 0/361 [00:00<?, ? examples/s]Map: 100%|██████████| 361/361 [00:00<00:00, 4228.48 examples/s]
Map:   0%|          | 0/361 [00:00<?, ? examples/s]Map: 100%|██████████| 361/361 [00:00<00:00, 4410.79 examples/s]
Map:   0%|          | 0/361 [00:00<?, ? examples/s]Map: 100%|██████████| 361/361 [00:00<00:00, 4928.56 examples/s]
Filter (num_proc=10):   0%|          | 0/361 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 37/361 [00:00<00:02, 156.55 examples/s]Filter (num_proc=10):  30%|███       | 109/361 [00:00<00:00, 330.97 examples/s]Filter (num_proc=10):  70%|███████   | 253/361 [00:00<00:00, 563.06 examples/s]Filter (num_proc=10): 100%|██████████| 361/361 [00:00<00:00, 508.01 examples/s]
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<02:09,  2.77it/s]  1%|          | 2/361 [00:00<01:44,  3.43it/s]  1%|          | 3/361 [00:00<01:35,  3.73it/s]  1%|          | 4/361 [00:01<01:31,  3.89it/s]  1%|▏         | 5/361 [00:01<01:29,  3.98it/s]  2%|▏         | 6/361 [00:01<01:27,  4.04it/s]  2%|▏         | 7/361 [00:01<01:26,  4.08it/s]  2%|▏         | 8/361 [00:02<01:26,  4.10it/s]  2%|▏         | 9/361 [00:02<01:25,  4.11it/s]  3%|▎         | 10/361 [00:02<01:25,  4.11it/s]  3%|▎         | 11/361 [00:02<01:24,  4.12it/s]  3%|▎         | 12/361 [00:03<01:24,  4.11it/s]  4%|▎         | 13/361 [00:03<01:24,  4.11it/s]  4%|▍         | 14/361 [00:03<01:24,  4.11it/s]  4%|▍         | 15/361 [00:03<01:23,  4.13it/s]  4%|▍         | 16/361 [00:03<01:23,  4.13it/s]  5%|▍         | 17/361 [00:04<01:23,  4.14it/s]  5%|▍         | 18/361 [00:04<01:23,  4.13it/s]  5%|▌         | 19/361 [00:04<01:22,  4.13it/s]  6%|▌         | 20/361 [00:04<01:22,  4.13it/s]  6%|▌         | 21/361 [00:05<01:22,  4.12it/s]  6%|▌         | 22/361 [00:05<01:22,  4.11it/s]  6%|▋         | 23/361 [00:05<01:22,  4.12it/s]  7%|▋         | 24/361 [00:05<01:21,  4.12it/s]  7%|▋         | 25/361 [00:06<01:22,  4.08it/s]  7%|▋         | 26/361 [00:06<01:24,  3.97it/s]  7%|▋         | 27/361 [00:06<01:25,  3.89it/s]  8%|▊         | 28/361 [00:06<01:26,  3.87it/s]  8%|▊         | 29/361 [00:07<01:24,  3.93it/s]  8%|▊         | 30/361 [00:07<01:24,  3.94it/s]  9%|▊         | 31/361 [00:07<01:25,  3.87it/s]  9%|▉         | 32/361 [00:08<01:26,  3.80it/s]  9%|▉         | 33/361 [00:08<01:28,  3.72it/s]  9%|▉         | 34/361 [00:08<01:25,  3.83it/s] 10%|▉         | 35/361 [00:08<01:23,  3.91it/s] 10%|▉         | 36/361 [00:09<01:24,  3.84it/s] 10%|█         | 37/361 [00:09<01:25,  3.78it/s] 11%|█         | 38/361 [00:09<01:27,  3.69it/s] 11%|█         | 39/361 [00:09<01:24,  3.81it/s] 11%|█         | 40/361 [00:10<01:22,  3.88it/s] 11%|█▏        | 41/361 [00:10<01:23,  3.83it/s] 12%|█▏        | 42/361 [00:10<01:24,  3.79it/s] 12%|█▏        | 43/361 [00:10<01:26,  3.70it/s] 12%|█▏        | 44/361 [00:11<01:22,  3.82it/s] 12%|█▏        | 45/361 [00:11<01:21,  3.88it/s] 13%|█▎        | 46/361 [00:11<01:22,  3.82it/s] 13%|█▎        | 47/361 [00:11<01:23,  3.77it/s] 13%|█▎        | 48/361 [00:12<01:24,  3.68it/s] 14%|█▎        | 49/361 [00:12<01:22,  3.80it/s] 14%|█▍        | 50/361 [00:12<01:20,  3.88it/s] 14%|█▍        | 51/361 [00:13<01:21,  3.82it/s] 14%|█▍        | 52/361 [00:13<01:22,  3.76it/s] 15%|█▍        | 53/361 [00:13<01:23,  3.67it/s] 15%|█▍        | 54/361 [00:13<01:20,  3.80it/s] 15%|█▌        | 55/361 [00:14<01:18,  3.88it/s] 16%|█▌        | 56/361 [00:14<01:20,  3.81it/s] 16%|█▌        | 57/361 [00:14<01:20,  3.79it/s] 16%|█▌        | 58/361 [00:14<01:19,  3.81it/s] 16%|█▋        | 59/361 [00:15<01:20,  3.73it/s] 17%|█▋        | 60/361 [00:15<01:18,  3.84it/s] 17%|█▋        | 61/361 [00:15<01:18,  3.82it/s] 17%|█▋        | 62/361 [00:15<01:19,  3.77it/s] 17%|█▋        | 63/361 [00:16<01:18,  3.79it/s] 18%|█▊        | 64/361 [00:16<01:19,  3.72it/s] 18%|█▊        | 65/361 [00:16<01:17,  3.83it/s] 18%|█▊        | 66/361 [00:16<01:17,  3.82it/s] 19%|█▊        | 67/361 [00:17<01:18,  3.77it/s] 19%|█▉        | 68/361 [00:17<01:17,  3.79it/s] 19%|█▉        | 69/361 [00:17<01:18,  3.72it/s] 19%|█▉        | 70/361 [00:18<01:16,  3.83it/s] 20%|█▉        | 71/361 [00:18<01:16,  3.81it/s] 20%|█▉        | 72/361 [00:18<01:16,  3.76it/s] 20%|██        | 73/361 [00:18<01:16,  3.78it/s] 20%|██        | 74/361 [00:19<01:17,  3.70it/s] 21%|██        | 75/361 [00:19<01:15,  3.80it/s] 21%|██        | 76/361 [00:19<01:15,  3.79it/s] 21%|██▏       | 77/361 [00:19<01:15,  3.75it/s] 22%|██▏       | 78/361 [00:20<01:14,  3.78it/s] 22%|██▏       | 79/361 [00:20<01:16,  3.71it/s] 22%|██▏       | 80/361 [00:20<01:13,  3.82it/s] 22%|██▏       | 81/361 [00:20<01:13,  3.83it/s] 23%|██▎       | 82/361 [00:21<01:13,  3.78it/s] 23%|██▎       | 83/361 [00:21<01:13,  3.78it/s] 23%|██▎       | 84/361 [00:21<01:14,  3.71it/s] 24%|██▎       | 85/361 [00:22<01:12,  3.83it/s] 24%|██▍       | 86/361 [00:22<01:11,  3.83it/s] 24%|██▍       | 87/361 [00:22<01:12,  3.77it/s] 24%|██▍       | 88/361 [00:22<01:12,  3.74it/s] 25%|██▍       | 89/361 [00:23<01:14,  3.66it/s] 25%|██▍       | 90/361 [00:23<01:11,  3.79it/s] 25%|██▌       | 91/361 [00:23<01:09,  3.87it/s] 25%|██▌       | 92/361 [00:23<01:10,  3.81it/s] 26%|██▌       | 93/361 [00:24<01:11,  3.75it/s] 26%|██▌       | 94/361 [00:24<01:12,  3.66it/s] 26%|██▋       | 95/361 [00:24<01:10,  3.79it/s] 27%|██▋       | 96/361 [00:24<01:08,  3.86it/s] 27%|██▋       | 97/361 [00:25<01:09,  3.81it/s] 27%|██▋       | 98/361 [00:25<01:09,  3.76it/s] 27%|██▋       | 99/361 [00:25<01:11,  3.67it/s] 28%|██▊       | 100/361 [00:25<01:08,  3.79it/s] 28%|██▊       | 101/361 [00:26<01:07,  3.84it/s] 28%|██▊       | 102/361 [00:26<01:08,  3.80it/s] 29%|██▊       | 103/361 [00:26<01:08,  3.75it/s] 29%|██▉       | 104/361 [00:27<01:10,  3.66it/s] 29%|██▉       | 105/361 [00:27<01:07,  3.78it/s] 29%|██▉       | 106/361 [00:27<01:06,  3.85it/s] 30%|██▉       | 107/361 [00:27<01:06,  3.81it/s] 30%|██▉       | 108/361 [00:28<01:07,  3.76it/s] 30%|███       | 109/361 [00:28<01:08,  3.67it/s] 30%|███       | 110/361 [00:28<01:06,  3.80it/s] 31%|███       | 111/361 [00:28<01:04,  3.87it/s] 31%|███       | 112/361 [00:29<01:05,  3.82it/s] 31%|███▏      | 113/361 [00:29<01:06,  3.75it/s] 32%|███▏      | 114/361 [00:29<01:07,  3.67it/s] 32%|███▏      | 115/361 [00:29<01:04,  3.79it/s] 32%|███▏      | 116/361 [00:30<01:03,  3.85it/s] 32%|███▏      | 117/361 [00:30<01:04,  3.80it/s] 33%|███▎      | 118/361 [00:30<01:04,  3.75it/s] 33%|███▎      | 119/361 [00:31<01:05,  3.67it/s] 33%|███▎      | 120/361 [00:31<01:03,  3.78it/s] 34%|███▎      | 121/361 [00:31<01:02,  3.84it/s] 34%|███▍      | 122/361 [00:31<01:02,  3.80it/s] 34%|███▍      | 123/361 [00:32<01:03,  3.74it/s] 34%|███▍      | 124/361 [00:32<01:04,  3.66it/s] 35%|███▍      | 125/361 [00:32<01:02,  3.79it/s] 35%|███▍      | 126/361 [00:32<01:01,  3.85it/s] 35%|███▌      | 127/361 [00:33<01:01,  3.80it/s] 35%|███▌      | 128/361 [00:33<01:02,  3.75it/s] 36%|███▌      | 129/361 [00:33<01:03,  3.67it/s] 36%|███▌      | 130/361 [00:33<01:00,  3.80it/s] 36%|███▋      | 131/361 [00:34<00:59,  3.85it/s] 37%|███▋      | 132/361 [00:34<01:00,  3.79it/s] 37%|███▋      | 133/361 [00:34<01:00,  3.75it/s] 37%|███▋      | 134/361 [00:35<01:01,  3.66it/s] 37%|███▋      | 135/361 [00:35<00:59,  3.78it/s] 38%|███▊      | 136/361 [00:35<00:58,  3.84it/s] 38%|███▊      | 137/361 [00:35<00:58,  3.80it/s] 38%|███▊      | 138/361 [00:36<00:59,  3.75it/s] 39%|███▊      | 139/361 [00:36<01:00,  3.67it/s] 39%|███▉      | 140/361 [00:36<00:58,  3.79it/s] 39%|███▉      | 141/361 [00:36<00:56,  3.88it/s] 39%|███▉      | 142/361 [00:37<00:57,  3.82it/s] 40%|███▉      | 143/361 [00:37<00:58,  3.75it/s] 40%|███▉      | 144/361 [00:37<00:59,  3.66it/s] 40%|████      | 145/361 [00:37<00:56,  3.79it/s] 40%|████      | 146/361 [00:38<00:55,  3.88it/s] 41%|████      | 147/361 [00:38<00:56,  3.80it/s] 41%|████      | 148/361 [00:38<00:56,  3.78it/s] 41%|████▏     | 149/361 [00:38<00:56,  3.78it/s] 42%|████▏     | 150/361 [00:39<00:56,  3.71it/s] 42%|████▏     | 151/361 [00:39<00:55,  3.82it/s] 42%|████▏     | 152/361 [00:39<00:54,  3.81it/s] 42%|████▏     | 153/361 [00:40<00:55,  3.76it/s] 43%|████▎     | 154/361 [00:40<00:54,  3.78it/s] 43%|████▎     | 155/361 [00:40<00:55,  3.71it/s] 43%|████▎     | 156/361 [00:40<00:53,  3.82it/s] 43%|████▎     | 157/361 [00:41<00:53,  3.80it/s] 44%|████▍     | 158/361 [00:41<00:54,  3.74it/s] 44%|████▍     | 159/361 [00:41<00:53,  3.77it/s] 44%|████▍     | 160/361 [00:41<00:54,  3.69it/s] 45%|████▍     | 161/361 [00:42<00:52,  3.80it/s] 45%|████▍     | 162/361 [00:42<00:52,  3.78it/s] 45%|████▌     | 163/361 [00:42<00:53,  3.73it/s] 45%|████▌     | 164/361 [00:42<00:52,  3.76it/s] 46%|████▌     | 165/361 [00:43<00:53,  3.69it/s] 46%|████▌     | 166/361 [00:43<00:51,  3.79it/s] 46%|████▋     | 167/361 [00:43<00:51,  3.78it/s] 47%|████▋     | 168/361 [00:44<00:51,  3.73it/s] 47%|████▋     | 169/361 [00:44<00:51,  3.76it/s] 47%|████▋     | 170/361 [00:44<00:51,  3.69it/s] 47%|████▋     | 171/361 [00:44<00:50,  3.79it/s] 48%|████▊     | 172/361 [00:45<00:49,  3.78it/s] 48%|████▊     | 173/361 [00:45<00:50,  3.73it/s] 48%|████▊     | 174/361 [00:45<00:49,  3.75it/s] 48%|████▊     | 175/361 [00:45<00:50,  3.68it/s] 49%|████▉     | 176/361 [00:46<00:48,  3.80it/s] 49%|████▉     | 177/361 [00:46<00:48,  3.79it/s] 49%|████▉     | 178/361 [00:46<00:48,  3.74it/s] 50%|████▉     | 179/361 [00:46<00:48,  3.76it/s] 50%|████▉     | 180/361 [00:47<00:49,  3.69it/s] 50%|█████     | 181/361 [00:47<00:47,  3.79it/s] 50%|█████     | 182/361 [00:47<00:47,  3.79it/s] 51%|█████     | 183/361 [00:48<00:47,  3.74it/s] 51%|█████     | 184/361 [00:48<00:47,  3.76it/s] 51%|█████     | 185/361 [00:48<00:47,  3.69it/s] 52%|█████▏    | 186/361 [00:48<00:46,  3.80it/s] 52%|█████▏    | 187/361 [00:49<00:45,  3.81it/s] 52%|█████▏    | 188/361 [00:49<00:46,  3.72it/s] 52%|█████▏    | 189/361 [00:49<00:46,  3.73it/s] 53%|█████▎    | 190/361 [00:49<00:46,  3.65it/s] 53%|█████▎    | 191/361 [00:50<00:45,  3.76it/s] 53%|█████▎    | 192/361 [00:50<00:44,  3.82it/s] 53%|█████▎    | 193/361 [00:50<00:44,  3.77it/s] 54%|█████▎    | 194/361 [00:50<00:44,  3.72it/s] 54%|█████▍    | 195/361 [00:51<00:45,  3.64it/s] 54%|█████▍    | 196/361 [00:51<00:43,  3.77it/s] 55%|█████▍    | 197/361 [00:51<00:42,  3.84it/s] 55%|█████▍    | 198/361 [00:52<00:43,  3.77it/s] 55%|█████▌    | 199/361 [00:52<00:43,  3.70it/s] 55%|█████▌    | 200/361 [00:52<00:44,  3.63it/s] 56%|█████▌    | 201/361 [00:52<00:42,  3.76it/s] 56%|█████▌    | 202/361 [00:53<00:41,  3.82it/s] 56%|█████▌    | 203/361 [00:53<00:41,  3.76it/s] 57%|█████▋    | 204/361 [00:53<00:42,  3.71it/s] 57%|█████▋    | 205/361 [00:53<00:42,  3.63it/s] 57%|█████▋    | 206/361 [00:54<00:41,  3.76it/s] 57%|█████▋    | 207/361 [00:54<00:40,  3.84it/s] 58%|█████▊    | 208/361 [00:54<00:40,  3.75it/s] 58%|█████▊    | 209/361 [00:54<00:40,  3.72it/s] 58%|█████▊    | 210/361 [00:55<00:40,  3.68it/s] 58%|█████▊    | 211/361 [00:55<00:39,  3.78it/s] 59%|█████▊    | 212/361 [00:55<00:38,  3.86it/s] 59%|█████▉    | 213/361 [00:55<00:37,  3.92it/s] 59%|█████▉    | 214/361 [00:56<00:37,  3.95it/s] 60%|█████▉    | 215/361 [00:56<00:36,  3.98it/s] 60%|█████▉    | 216/361 [00:56<00:36,  4.00it/s] 60%|██████    | 217/361 [00:56<00:35,  4.01it/s] 60%|██████    | 218/361 [00:57<00:35,  4.02it/s] 61%|██████    | 219/361 [00:57<00:35,  4.03it/s] 61%|██████    | 220/361 [00:57<00:34,  4.03it/s] 61%|██████    | 221/361 [00:57<00:34,  4.04it/s] 61%|██████▏   | 222/361 [00:58<00:34,  4.05it/s] 62%|██████▏   | 223/361 [00:58<00:34,  4.04it/s] 62%|██████▏   | 224/361 [00:58<00:33,  4.05it/s] 62%|██████▏   | 225/361 [00:58<00:33,  4.05it/s] 63%|██████▎   | 226/361 [00:59<00:33,  4.05it/s] 63%|██████▎   | 227/361 [00:59<00:33,  4.05it/s] 63%|██████▎   | 228/361 [00:59<00:32,  4.05it/s] 63%|██████▎   | 229/361 [00:59<00:32,  4.05it/s] 64%|██████▎   | 230/361 [01:00<00:32,  4.06it/s] 64%|██████▍   | 231/361 [01:00<00:32,  4.02it/s] 64%|██████▍   | 232/361 [01:00<00:33,  3.89it/s] 65%|██████▍   | 233/361 [01:01<00:33,  3.80it/s] 65%|██████▍   | 234/361 [01:01<00:32,  3.87it/s] 65%|██████▌   | 235/361 [01:01<00:32,  3.82it/s] 65%|██████▌   | 236/361 [01:01<00:33,  3.75it/s] 66%|██████▌   | 237/361 [01:02<00:33,  3.68it/s] 66%|██████▌   | 238/361 [01:02<00:32,  3.78it/s] 66%|██████▌   | 239/361 [01:02<00:32,  3.75it/s] 66%|██████▋   | 240/361 [01:02<00:32,  3.71it/s] 67%|██████▋   | 241/361 [01:03<00:32,  3.66it/s] 67%|██████▋   | 242/361 [01:03<00:31,  3.76it/s] 67%|██████▋   | 243/361 [01:03<00:31,  3.72it/s] 68%|██████▊   | 244/361 [01:03<00:31,  3.70it/s] 68%|██████▊   | 245/361 [01:04<00:31,  3.64it/s] 68%|██████▊   | 246/361 [01:04<00:30,  3.75it/s] 68%|██████▊   | 247/361 [01:04<00:30,  3.72it/s] 69%|██████▊   | 248/361 [01:05<00:30,  3.69it/s] 69%|██████▉   | 249/361 [01:05<00:30,  3.64it/s] 69%|██████▉   | 250/361 [01:05<00:29,  3.75it/s] 70%|██████▉   | 251/361 [01:05<00:29,  3.74it/s] 70%|██████▉   | 252/361 [01:06<00:29,  3.69it/s] 70%|███████   | 253/361 [01:06<00:29,  3.64it/s] 70%|███████   | 254/361 [01:06<00:28,  3.75it/s] 71%|███████   | 255/361 [01:06<00:28,  3.74it/s] 71%|███████   | 256/361 [01:07<00:28,  3.71it/s] 71%|███████   | 257/361 [01:07<00:28,  3.63it/s] 71%|███████▏  | 258/361 [01:07<00:27,  3.75it/s] 72%|███████▏  | 259/361 [01:07<00:27,  3.76it/s] 72%|███████▏  | 260/361 [01:08<00:27,  3.73it/s] 72%|███████▏  | 261/361 [01:08<00:27,  3.63it/s] 73%|███████▎  | 262/361 [01:08<00:26,  3.74it/s] 73%|███████▎  | 263/361 [01:09<00:26,  3.76it/s] 73%|███████▎  | 264/361 [01:09<00:26,  3.73it/s] 73%|███████▎  | 265/361 [01:09<00:26,  3.63it/s] 74%|███████▎  | 266/361 [01:09<00:25,  3.74it/s] 74%|███████▍  | 267/361 [01:10<00:25,  3.75it/s] 74%|███████▍  | 268/361 [01:10<00:25,  3.71it/s] 75%|███████▍  | 269/361 [01:10<00:25,  3.62it/s] 75%|███████▍  | 270/361 [01:10<00:24,  3.73it/s] 75%|███████▌  | 271/361 [01:11<00:24,  3.74it/s] 75%|███████▌  | 272/361 [01:11<00:24,  3.71it/s] 76%|███████▌  | 273/361 [01:11<00:24,  3.62it/s] 76%|███████▌  | 274/361 [01:12<00:23,  3.73it/s] 76%|███████▌  | 275/361 [01:12<00:22,  3.74it/s] 76%|███████▋  | 276/361 [01:12<00:22,  3.71it/s] 77%|███████▋  | 277/361 [01:12<00:23,  3.61it/s] 77%|███████▋  | 278/361 [01:13<00:22,  3.74it/s] 77%|███████▋  | 279/361 [01:13<00:21,  3.75it/s] 78%|███████▊  | 280/361 [01:13<00:21,  3.72it/s] 78%|███████▊  | 281/361 [01:13<00:22,  3.62it/s] 78%|███████▊  | 282/361 [01:14<00:21,  3.74it/s] 78%|███████▊  | 283/361 [01:14<00:20,  3.75it/s] 79%|███████▊  | 284/361 [01:14<00:20,  3.71it/s] 79%|███████▉  | 285/361 [01:15<00:21,  3.62it/s] 79%|███████▉  | 286/361 [01:15<00:20,  3.74it/s] 80%|███████▉  | 287/361 [01:15<00:19,  3.75it/s] 80%|███████▉  | 288/361 [01:15<00:19,  3.72it/s] 80%|████████  | 289/361 [01:16<00:19,  3.63it/s] 80%|████████  | 290/361 [01:16<00:18,  3.74it/s] 81%|████████  | 291/361 [01:16<00:18,  3.76it/s] 81%|████████  | 292/361 [01:16<00:18,  3.72it/s] 81%|████████  | 293/361 [01:17<00:18,  3.63it/s] 81%|████████▏ | 294/361 [01:17<00:17,  3.74it/s] 82%|████████▏ | 295/361 [01:17<00:17,  3.76it/s] 82%|████████▏ | 296/361 [01:17<00:17,  3.72it/s] 82%|████████▏ | 297/361 [01:18<00:17,  3.61it/s] 83%|████████▎ | 298/361 [01:18<00:16,  3.74it/s] 83%|████████▎ | 299/361 [01:18<00:16,  3.76it/s] 83%|████████▎ | 300/361 [01:19<00:16,  3.72it/s] 83%|████████▎ | 301/361 [01:19<00:16,  3.62it/s] 84%|████████▎ | 302/361 [01:19<00:15,  3.73it/s] 84%|████████▍ | 303/361 [01:19<00:15,  3.75it/s] 84%|████████▍ | 304/361 [01:20<00:15,  3.72it/s] 84%|████████▍ | 305/361 [01:20<00:15,  3.62it/s] 85%|████████▍ | 306/361 [01:20<00:14,  3.74it/s] 85%|████████▌ | 307/361 [01:20<00:14,  3.75it/s] 85%|████████▌ | 308/361 [01:21<00:14,  3.71it/s] 86%|████████▌ | 309/361 [01:21<00:14,  3.62it/s] 86%|████████▌ | 310/361 [01:21<00:13,  3.74it/s] 86%|████████▌ | 311/361 [01:22<00:13,  3.74it/s] 86%|████████▋ | 312/361 [01:22<00:13,  3.71it/s] 87%|████████▋ | 313/361 [01:22<00:13,  3.63it/s] 87%|████████▋ | 314/361 [01:22<00:12,  3.74it/s] 87%|████████▋ | 315/361 [01:23<00:12,  3.73it/s] 88%|████████▊ | 316/361 [01:23<00:12,  3.71it/s] 88%|████████▊ | 317/361 [01:23<00:12,  3.63it/s] 88%|████████▊ | 318/361 [01:23<00:11,  3.74it/s] 88%|████████▊ | 319/361 [01:24<00:11,  3.75it/s] 89%|████████▊ | 320/361 [01:24<00:11,  3.71it/s] 89%|████████▉ | 321/361 [01:24<00:11,  3.62it/s] 89%|████████▉ | 322/361 [01:24<00:10,  3.73it/s] 89%|████████▉ | 323/361 [01:25<00:10,  3.75it/s] 90%|████████▉ | 324/361 [01:25<00:09,  3.72it/s] 90%|█████████ | 325/361 [01:25<00:09,  3.62it/s] 90%|█████████ | 326/361 [01:26<00:09,  3.73it/s] 91%|█████████ | 327/361 [01:26<00:09,  3.72it/s] 91%|█████████ | 328/361 [01:26<00:08,  3.71it/s] 91%|█████████ | 329/361 [01:26<00:08,  3.62it/s] 91%|█████████▏| 330/361 [01:27<00:08,  3.74it/s] 92%|█████████▏| 331/361 [01:27<00:07,  3.77it/s] 92%|█████████▏| 332/361 [01:27<00:07,  3.73it/s] 92%|█████████▏| 333/361 [01:27<00:07,  3.66it/s] 93%|█████████▎| 334/361 [01:28<00:07,  3.72it/s] 93%|█████████▎| 335/361 [01:28<00:06,  3.76it/s] 93%|█████████▎| 336/361 [01:28<00:06,  3.72it/s] 93%|█████████▎| 337/361 [01:29<00:06,  3.64it/s] 94%|█████████▎| 338/361 [01:29<00:06,  3.71it/s] 94%|█████████▍| 339/361 [01:29<00:05,  3.75it/s] 94%|█████████▍| 340/361 [01:29<00:05,  3.72it/s] 94%|█████████▍| 341/361 [01:30<00:05,  3.65it/s] 95%|█████████▍| 342/361 [01:30<00:05,  3.71it/s] 95%|█████████▌| 343/361 [01:30<00:04,  3.75it/s] 95%|█████████▌| 344/361 [01:30<00:04,  3.71it/s] 96%|█████████▌| 345/361 [01:31<00:04,  3.65it/s] 96%|█████████▌| 346/361 [01:31<00:04,  3.71it/s] 96%|█████████▌| 347/361 [01:31<00:03,  3.74it/s] 96%|█████████▋| 348/361 [01:32<00:03,  3.71it/s] 97%|█████████▋| 349/361 [01:32<00:03,  3.65it/s] 97%|█████████▋| 350/361 [01:32<00:02,  3.71it/s] 97%|█████████▋| 351/361 [01:32<00:02,  3.74it/s] 98%|█████████▊| 352/361 [01:33<00:02,  3.71it/s] 98%|█████████▊| 353/361 [01:33<00:02,  3.61it/s] 98%|█████████▊| 354/361 [01:33<00:01,  3.73it/s] 98%|█████████▊| 355/361 [01:33<00:01,  3.74it/s] 99%|█████████▊| 356/361 [01:34<00:01,  3.71it/s] 99%|█████████▉| 357/361 [01:34<00:01,  3.62it/s] 99%|█████████▉| 358/361 [01:34<00:00,  3.73it/s] 99%|█████████▉| 359/361 [01:34<00:00,  3.75it/s]100%|█████████▉| 360/361 [01:35<00:00,  3.72it/s]100%|██████████| 361/361 [01:35<00:00,  3.62it/s]accuracy:  0.5734072022160664
100%|██████████| 361/361 [01:41<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 25245.16 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 24461.15 examples/s]Filter (num_proc=10):  70%|███████   | 1804/2575 [00:00<00:00, 17397.91 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15373.59 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15258.16 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 13899.22 examples/s]
Map (num_proc=10):   0%|          | 0/188 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/188 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/188 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/188 [00:00<00:49,  3.79 examples/s]Map (num_proc=10):   1%|          | 1/188 [00:00<00:56,  3.34 examples/s]Map (num_proc=10):   4%|▎         | 7/188 [00:00<00:07, 23.26 examples/s]Map (num_proc=10):   1%|          | 1/188 [00:00<00:56,  3.29 examples/s]Map (num_proc=10):   6%|▋         | 12/188 [00:00<00:04, 37.59 examples/s]Map (num_proc=10):  34%|███▍      | 64/188 [00:00<00:00, 201.58 examples/s]Map (num_proc=10):  16%|█▌        | 30/188 [00:00<00:01, 92.85 examples/s]Map (num_proc=10):  30%|███       | 57/188 [00:00<00:00, 161.82 examples/s]Map (num_proc=10):  37%|███▋      | 69/188 [00:00<00:00, 183.95 examples/s]Map (num_proc=10):  54%|█████▍    | 102/188 [00:00<00:00, 244.95 examples/s]Map (num_proc=10):  60%|█████▉    | 112/188 [00:00<00:00, 268.47 examples/s]Map (num_proc=10):  60%|██████    | 113/188 [00:00<00:00, 255.74 examples/s]Map (num_proc=10):  75%|███████▌  | 141/188 [00:00<00:00, 282.98 examples/s]Map (num_proc=10):  83%|████████▎ | 156/188 [00:00<00:00, 318.76 examples/s]Map (num_proc=10):  86%|████████▌ | 162/188 [00:00<00:00, 305.67 examples/s]Map (num_proc=10):  93%|█████████▎| 175/188 [00:00<00:00, 252.31 examples/s]Map (num_proc=10): 100%|██████████| 188/188 [00:00<00:00, 201.34 examples/s]
Map (num_proc=10): 100%|██████████| 188/188 [00:00<00:00, 190.60 examples/s]
Training dataset size: 48, validation dataset size: 188
Map (num_proc=10): 100%|██████████| 188/188 [00:00<00:00, 200.08 examples/s]
Training dataset size: 48, validation dataset size: 188
Training dataset size: 48, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.93s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:52:53,065] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:52:53,242] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:52:54,658] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:17,  2.57s/it] 25%|██▌       | 2/8 [00:05<00:17,  2.90s/it] 38%|███▊      | 3/8 [00:08<00:13,  2.77s/it] 50%|█████     | 4/8 [00:11<00:11,  2.82s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:14<00:08,  2.94s/it] 75%|███████▌  | 6/8 [00:17<00:06,  3.02s/it] 88%|████████▊ | 7/8 [00:20<00:02,  2.97s/it]100%|██████████| 8/8 [00:23<00:00,  2.97s/it]                                             {'train_runtime': 24.0175, 'train_samples_per_second': 3.997, 'train_steps_per_second': 0.333, 'train_loss': 0.5855574011802673, 'epoch': 2.0}
100%|██████████| 8/8 [00:23<00:00,  2.97s/it]100%|██████████| 8/8 [00:23<00:00,  2.98s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 23170.70 examples/s]
Map (num_proc=10):   0%|          | 0/212 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/212 [00:00<00:26,  8.03 examples/s]Map (num_proc=10):  14%|█▍        | 30/212 [00:00<00:01, 104.89 examples/s]Map (num_proc=10):  50%|█████     | 107/212 [00:00<00:00, 305.99 examples/s]Map (num_proc=10):  70%|███████   | 149/212 [00:00<00:00, 339.50 examples/s]Map (num_proc=10):  91%|█████████ | 192/212 [00:00<00:00, 366.15 examples/s]Map (num_proc=10): 100%|██████████| 212/212 [00:00<00:00, 251.93 examples/s]
Map:   0%|          | 0/212 [00:00<?, ? examples/s]Map: 100%|██████████| 212/212 [00:00<00:00, 4283.06 examples/s]
Map:   0%|          | 0/212 [00:00<?, ? examples/s]Map: 100%|██████████| 212/212 [00:00<00:00, 3994.84 examples/s]
Map:   0%|          | 0/212 [00:00<?, ? examples/s]Map: 100%|██████████| 212/212 [00:00<00:00, 4311.86 examples/s]
Filter (num_proc=10):   0%|          | 0/212 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 22/212 [00:00<00:01, 101.96 examples/s]Filter (num_proc=10):  41%|████      | 86/212 [00:00<00:00, 293.26 examples/s]Filter (num_proc=10):  60%|██████    | 128/212 [00:00<00:00, 329.46 examples/s]Filter (num_proc=10): 100%|██████████| 212/212 [00:00<00:00, 457.17 examples/s]Filter (num_proc=10): 100%|██████████| 212/212 [00:00<00:00, 331.51 examples/s]
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:07,  3.11it/s]  1%|          | 2/212 [00:00<01:01,  3.44it/s]  1%|▏         | 3/212 [00:00<00:59,  3.53it/s]  2%|▏         | 4/212 [00:01<00:58,  3.56it/s]  2%|▏         | 5/212 [00:01<00:55,  3.75it/s]  3%|▎         | 6/212 [00:01<00:54,  3.75it/s]  3%|▎         | 7/212 [00:01<00:55,  3.71it/s]  4%|▍         | 8/212 [00:02<00:55,  3.67it/s]  4%|▍         | 9/212 [00:02<00:53,  3.80it/s]  5%|▍         | 10/212 [00:02<00:53,  3.81it/s]  5%|▌         | 11/212 [00:02<00:53,  3.76it/s]  6%|▌         | 12/212 [00:03<00:52,  3.80it/s]  6%|▌         | 13/212 [00:03<00:53,  3.75it/s]  7%|▋         | 14/212 [00:03<00:51,  3.85it/s]  7%|▋         | 15/212 [00:04<00:51,  3.79it/s]  8%|▊         | 16/212 [00:04<00:52,  3.75it/s]  8%|▊         | 17/212 [00:04<00:52,  3.70it/s]  8%|▊         | 18/212 [00:04<00:50,  3.82it/s]  9%|▉         | 19/212 [00:05<00:50,  3.80it/s]  9%|▉         | 20/212 [00:05<00:50,  3.77it/s] 10%|▉         | 21/212 [00:05<00:50,  3.80it/s] 10%|█         | 22/212 [00:05<00:50,  3.76it/s] 11%|█         | 23/212 [00:06<00:49,  3.83it/s] 11%|█▏        | 24/212 [00:06<00:49,  3.78it/s] 12%|█▏        | 25/212 [00:06<00:48,  3.82it/s] 12%|█▏        | 26/212 [00:06<00:49,  3.74it/s] 13%|█▎        | 27/212 [00:07<00:48,  3.83it/s] 13%|█▎        | 28/212 [00:07<00:48,  3.78it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.74it/s] 14%|█▍        | 30/212 [00:08<00:49,  3.70it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.81it/s] 15%|█▌        | 32/212 [00:08<00:47,  3.78it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.76it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.78it/s] 17%|█▋        | 35/212 [00:09<00:47,  3.76it/s] 17%|█▋        | 36/212 [00:09<00:46,  3.82it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.76it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.78it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.74it/s] 19%|█▉        | 40/212 [00:10<00:45,  3.81it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.75it/s] 20%|█▉        | 42/212 [00:11<00:44,  3.79it/s] 20%|██        | 43/212 [00:11<00:45,  3.72it/s] 21%|██        | 44/212 [00:11<00:44,  3.78it/s] 21%|██        | 45/212 [00:11<00:44,  3.74it/s] 22%|██▏       | 46/212 [00:12<00:43,  3.78it/s] 22%|██▏       | 47/212 [00:12<00:44,  3.72it/s] 23%|██▎       | 48/212 [00:12<00:43,  3.80it/s] 23%|██▎       | 49/212 [00:13<00:43,  3.75it/s] 24%|██▎       | 50/212 [00:13<00:42,  3.79it/s] 24%|██▍       | 51/212 [00:13<00:43,  3.71it/s] 25%|██▍       | 52/212 [00:13<00:42,  3.80it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.75it/s] 25%|██▌       | 54/212 [00:14<00:41,  3.79it/s] 26%|██▌       | 55/212 [00:14<00:42,  3.72it/s] 26%|██▋       | 56/212 [00:14<00:40,  3.81it/s] 27%|██▋       | 57/212 [00:15<00:41,  3.76it/s] 27%|██▋       | 58/212 [00:15<00:40,  3.77it/s] 28%|██▊       | 59/212 [00:15<00:41,  3.70it/s] 28%|██▊       | 60/212 [00:15<00:39,  3.81it/s] 29%|██▉       | 61/212 [00:16<00:40,  3.76it/s] 29%|██▉       | 62/212 [00:16<00:40,  3.73it/s] 30%|██▉       | 63/212 [00:16<00:40,  3.68it/s] 30%|███       | 64/212 [00:17<00:38,  3.81it/s] 31%|███       | 65/212 [00:17<00:38,  3.79it/s] 31%|███       | 66/212 [00:17<00:38,  3.76it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.75it/s] 32%|███▏      | 68/212 [00:18<00:38,  3.74it/s] 33%|███▎      | 69/212 [00:18<00:37,  3.80it/s] 33%|███▎      | 70/212 [00:18<00:37,  3.74it/s] 33%|███▎      | 71/212 [00:18<00:37,  3.78it/s] 34%|███▍      | 72/212 [00:19<00:37,  3.71it/s] 34%|███▍      | 73/212 [00:19<00:36,  3.77it/s] 35%|███▍      | 74/212 [00:19<00:36,  3.73it/s] 35%|███▌      | 75/212 [00:19<00:36,  3.77it/s] 36%|███▌      | 76/212 [00:20<00:36,  3.70it/s] 36%|███▋      | 77/212 [00:20<00:35,  3.78it/s] 37%|███▋      | 78/212 [00:20<00:35,  3.73it/s] 37%|███▋      | 79/212 [00:21<00:35,  3.77it/s] 38%|███▊      | 80/212 [00:21<00:35,  3.71it/s] 38%|███▊      | 81/212 [00:21<00:34,  3.78it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.73it/s] 39%|███▉      | 83/212 [00:22<00:34,  3.76it/s] 40%|███▉      | 84/212 [00:22<00:34,  3.69it/s] 40%|████      | 85/212 [00:22<00:33,  3.79it/s] 41%|████      | 86/212 [00:22<00:33,  3.74it/s] 41%|████      | 87/212 [00:23<00:33,  3.73it/s] 42%|████▏     | 88/212 [00:23<00:33,  3.67it/s] 42%|████▏     | 89/212 [00:23<00:32,  3.79it/s] 42%|████▏     | 90/212 [00:23<00:32,  3.77it/s] 43%|████▎     | 91/212 [00:24<00:32,  3.73it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.66it/s] 44%|████▍     | 93/212 [00:24<00:31,  3.79it/s] 44%|████▍     | 94/212 [00:25<00:31,  3.79it/s] 45%|████▍     | 95/212 [00:25<00:31,  3.74it/s] 45%|████▌     | 96/212 [00:25<00:30,  3.75it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.72it/s] 46%|████▌     | 98/212 [00:26<00:30,  3.77it/s] 47%|████▋     | 99/212 [00:26<00:30,  3.72it/s] 47%|████▋     | 100/212 [00:26<00:29,  3.77it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.70it/s] 48%|████▊     | 102/212 [00:27<00:29,  3.78it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.72it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.75it/s] 50%|████▉     | 105/212 [00:28<00:29,  3.68it/s] 50%|█████     | 106/212 [00:28<00:27,  3.79it/s] 50%|█████     | 107/212 [00:28<00:28,  3.74it/s] 51%|█████     | 108/212 [00:28<00:28,  3.71it/s] 51%|█████▏    | 109/212 [00:29<00:28,  3.66it/s] 52%|█████▏    | 110/212 [00:29<00:26,  3.78it/s] 52%|█████▏    | 111/212 [00:29<00:26,  3.80it/s] 53%|█████▎    | 112/212 [00:29<00:26,  3.75it/s] 53%|█████▎    | 113/212 [00:30<00:26,  3.76it/s] 54%|█████▍    | 114/212 [00:30<00:26,  3.72it/s] 54%|█████▍    | 115/212 [00:30<00:25,  3.79it/s] 55%|█████▍    | 116/212 [00:30<00:25,  3.74it/s] 55%|█████▌    | 117/212 [00:31<00:25,  3.78it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.71it/s] 56%|█████▌    | 119/212 [00:31<00:24,  3.78it/s] 57%|█████▋    | 120/212 [00:32<00:24,  3.72it/s] 57%|█████▋    | 121/212 [00:32<00:24,  3.76it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.69it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.77it/s] 58%|█████▊    | 124/212 [00:33<00:23,  3.72it/s] 59%|█████▉    | 125/212 [00:33<00:23,  3.71it/s] 59%|█████▉    | 126/212 [00:33<00:23,  3.67it/s] 60%|█████▉    | 127/212 [00:33<00:22,  3.79it/s] 60%|██████    | 128/212 [00:34<00:22,  3.77it/s] 61%|██████    | 129/212 [00:34<00:22,  3.73it/s] 61%|██████▏   | 130/212 [00:34<00:21,  3.73it/s] 62%|██████▏   | 131/212 [00:34<00:21,  3.83it/s] 62%|██████▏   | 132/212 [00:35<00:20,  3.90it/s] 63%|██████▎   | 133/212 [00:35<00:20,  3.95it/s] 63%|██████▎   | 134/212 [00:35<00:19,  3.98it/s] 64%|██████▎   | 135/212 [00:35<00:19,  4.02it/s] 64%|██████▍   | 136/212 [00:36<00:18,  4.04it/s] 65%|██████▍   | 137/212 [00:36<00:18,  4.05it/s] 65%|██████▌   | 138/212 [00:36<00:18,  4.05it/s] 66%|██████▌   | 139/212 [00:36<00:17,  4.06it/s] 66%|██████▌   | 140/212 [00:37<00:17,  4.08it/s] 67%|██████▋   | 141/212 [00:37<00:17,  4.07it/s] 67%|██████▋   | 142/212 [00:37<00:17,  4.07it/s] 67%|██████▋   | 143/212 [00:37<00:16,  4.08it/s] 68%|██████▊   | 144/212 [00:38<00:16,  4.08it/s] 68%|██████▊   | 145/212 [00:38<00:16,  4.08it/s] 69%|██████▉   | 146/212 [00:38<00:16,  4.07it/s] 69%|██████▉   | 147/212 [00:38<00:15,  4.08it/s] 70%|██████▉   | 148/212 [00:39<00:15,  4.09it/s] 70%|███████   | 149/212 [00:39<00:15,  4.08it/s] 71%|███████   | 150/212 [00:39<00:15,  4.08it/s] 71%|███████   | 151/212 [00:39<00:14,  4.08it/s] 72%|███████▏  | 152/212 [00:40<00:14,  4.08it/s] 72%|███████▏  | 153/212 [00:40<00:14,  4.07it/s] 73%|███████▎  | 154/212 [00:40<00:14,  4.07it/s] 73%|███████▎  | 155/212 [00:40<00:13,  4.07it/s] 74%|███████▎  | 156/212 [00:41<00:13,  4.06it/s] 74%|███████▍  | 157/212 [00:41<00:13,  4.00it/s] 75%|███████▍  | 158/212 [00:41<00:13,  3.87it/s] 75%|███████▌  | 159/212 [00:41<00:14,  3.77it/s] 75%|███████▌  | 160/212 [00:42<00:13,  3.85it/s] 76%|███████▌  | 161/212 [00:42<00:13,  3.84it/s] 76%|███████▋  | 162/212 [00:42<00:13,  3.73it/s] 77%|███████▋  | 163/212 [00:42<00:13,  3.66it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.77it/s] 78%|███████▊  | 165/212 [00:43<00:12,  3.79it/s] 78%|███████▊  | 166/212 [00:43<00:12,  3.70it/s] 79%|███████▉  | 167/212 [00:44<00:12,  3.74it/s] 79%|███████▉  | 168/212 [00:44<00:11,  3.67it/s] 80%|███████▉  | 169/212 [00:44<00:11,  3.74it/s] 80%|████████  | 170/212 [00:44<00:11,  3.66it/s] 81%|████████  | 171/212 [00:45<00:11,  3.69it/s] 81%|████████  | 172/212 [00:45<00:10,  3.65it/s] 82%|████████▏ | 173/212 [00:45<00:10,  3.74it/s] 82%|████████▏ | 174/212 [00:45<00:10,  3.65it/s] 83%|████████▎ | 175/212 [00:46<00:09,  3.71it/s] 83%|████████▎ | 176/212 [00:46<00:09,  3.66it/s] 83%|████████▎ | 177/212 [00:46<00:09,  3.73it/s] 84%|████████▍ | 178/212 [00:47<00:09,  3.66it/s] 84%|████████▍ | 179/212 [00:47<00:08,  3.70it/s] 85%|████████▍ | 180/212 [00:47<00:08,  3.64it/s] 85%|████████▌ | 181/212 [00:47<00:08,  3.73it/s] 86%|████████▌ | 182/212 [00:48<00:08,  3.65it/s] 86%|████████▋ | 183/212 [00:48<00:07,  3.69it/s] 87%|████████▋ | 184/212 [00:48<00:07,  3.63it/s] 87%|████████▋ | 185/212 [00:48<00:07,  3.73it/s] 88%|████████▊ | 186/212 [00:49<00:07,  3.64it/s] 88%|████████▊ | 187/212 [00:49<00:06,  3.70it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.64it/s] 89%|████████▉ | 189/212 [00:49<00:06,  3.72it/s] 90%|████████▉ | 190/212 [00:50<00:06,  3.63it/s] 90%|█████████ | 191/212 [00:50<00:05,  3.68it/s] 91%|█████████ | 192/212 [00:50<00:05,  3.64it/s] 91%|█████████ | 193/212 [00:51<00:05,  3.72it/s] 92%|█████████▏| 194/212 [00:51<00:04,  3.64it/s] 92%|█████████▏| 195/212 [00:51<00:04,  3.62it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.74it/s] 93%|█████████▎| 197/212 [00:52<00:03,  3.83it/s] 93%|█████████▎| 198/212 [00:52<00:03,  3.90it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.94it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.97it/s] 95%|█████████▍| 201/212 [00:53<00:02,  3.99it/s] 95%|█████████▌| 202/212 [00:53<00:02,  4.00it/s] 96%|█████████▌| 203/212 [00:53<00:02,  4.02it/s] 96%|█████████▌| 204/212 [00:53<00:01,  4.03it/s] 97%|█████████▋| 205/212 [00:54<00:01,  4.03it/s] 97%|█████████▋| 206/212 [00:54<00:01,  4.04it/s] 98%|█████████▊| 207/212 [00:54<00:01,  4.04it/s] 98%|█████████▊| 208/212 [00:54<00:00,  4.04it/s] 99%|█████████▊| 209/212 [00:55<00:00,  4.05it/s] 99%|█████████▉| 210/212 [00:55<00:00,  4.05it/s]100%|█████████▉| 211/212 [00:55<00:00,  4.05it/s]100%|██████████| 212/212 [00:55<00:00,  4.06it/s]accuracy:  0.8254716981132075
100%|██████████| 212/212 [00:59<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):  40%|████      | 1031/2575 [00:00<00:00, 9979.13 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14223.20 examples/s]
Filter (num_proc=10):  70%|███████   | 1804/2575 [00:00<00:00, 17933.98 examples/s]Filter (num_proc=10):  80%|████████  | 2061/2575 [00:00<00:00, 19802.72 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14151.06 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 13787.77 examples/s]
Map (num_proc=10):   0%|          | 0/134 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/134 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/134 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/134 [00:00<00:38,  3.42 examples/s]Map (num_proc=10):   1%|          | 1/134 [00:00<00:39,  3.35 examples/s]Map (num_proc=10):   1%|          | 1/134 [00:00<00:39,  3.40 examples/s]Map (num_proc=10):  17%|█▋        | 23/134 [00:00<00:01, 70.25 examples/s]Map (num_proc=10):  10%|█         | 14/134 [00:00<00:02, 42.56 examples/s]Map (num_proc=10):  43%|████▎     | 57/134 [00:00<00:00, 151.44 examples/s]Map (num_proc=10):  10%|█         | 14/134 [00:00<00:02, 41.15 examples/s]Map (num_proc=10):  61%|██████    | 82/134 [00:00<00:00, 179.91 examples/s]Map (num_proc=10):  37%|███▋      | 49/134 [00:00<00:00, 128.42 examples/s]Map (num_proc=10):  37%|███▋      | 50/134 [00:00<00:00, 135.04 examples/s]Map (num_proc=10):  74%|███████▍  | 99/134 [00:00<00:00, 230.95 examples/s]Map (num_proc=10):  57%|█████▋    | 77/134 [00:00<00:00, 173.61 examples/s]Map (num_proc=10):  81%|████████▏ | 109/134 [00:00<00:00, 182.40 examples/s]Map (num_proc=10):  84%|████████▎ | 112/134 [00:00<00:00, 218.01 examples/s]Map (num_proc=10):  96%|█████████▋| 129/134 [00:00<00:00, 210.09 examples/s]Map (num_proc=10): 100%|██████████| 134/134 [00:00<00:00, 178.14 examples/s]Map (num_proc=10): 100%|██████████| 134/134 [00:00<00:00, 135.24 examples/s]
Map (num_proc=10): 100%|██████████| 134/134 [00:00<00:00, 141.20 examples/s]
Training dataset size: 48, validation dataset size: 134
Map (num_proc=10): 100%|██████████| 134/134 [00:00<00:00, 141.03 examples/s]
Training dataset size: 48, validation dataset size: 134
Training dataset size: 48, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.97s/it][2025-03-12 02:54:46,821] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 02:54:46,946] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:54:48,095] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:20,  2.90s/it] 25%|██▌       | 2/8 [00:05<00:17,  2.84s/it] 38%|███▊      | 3/8 [00:08<00:13,  2.70s/it] 50%|█████     | 4/8 [00:11<00:11,  2.79s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:13<00:08,  2.72s/it] 75%|███████▌  | 6/8 [00:16<00:05,  2.67s/it] 88%|████████▊ | 7/8 [00:19<00:02,  2.87s/it]100%|██████████| 8/8 [00:22<00:00,  2.84s/it]                                             {'train_runtime': 23.0569, 'train_samples_per_second': 4.164, 'train_steps_per_second': 0.347, 'train_loss': 0.6260664463043213, 'epoch': 2.0}
100%|██████████| 8/8 [00:22<00:00,  2.84s/it]100%|██████████| 8/8 [00:22<00:00,  2.86s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 22252.62 examples/s]
Map (num_proc=10):   0%|          | 0/163 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/163 [00:00<00:20,  7.80 examples/s]Map (num_proc=10):  21%|██▏       | 35/163 [00:00<00:01, 113.12 examples/s]Map (num_proc=10):  52%|█████▏    | 85/163 [00:00<00:00, 230.90 examples/s]Map (num_proc=10):  72%|███████▏  | 118/163 [00:00<00:00, 247.52 examples/s]Map (num_proc=10):  92%|█████████▏| 150/163 [00:00<00:00, 265.75 examples/s]Map (num_proc=10): 100%|██████████| 163/163 [00:00<00:00, 193.37 examples/s]
Map:   0%|          | 0/163 [00:00<?, ? examples/s]Map: 100%|██████████| 163/163 [00:00<00:00, 4058.55 examples/s]
Map:   0%|          | 0/163 [00:00<?, ? examples/s]Map: 100%|██████████| 163/163 [00:00<00:00, 4248.47 examples/s]
Map:   0%|          | 0/163 [00:00<?, ? examples/s]Map: 100%|██████████| 163/163 [00:00<00:00, 4233.81 examples/s]
Filter (num_proc=10):   0%|          | 0/163 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 17/163 [00:00<00:01, 75.92 examples/s]Filter (num_proc=10):  41%|████      | 67/163 [00:00<00:00, 226.55 examples/s]Filter (num_proc=10):  71%|███████   | 115/163 [00:00<00:00, 256.04 examples/s]Filter (num_proc=10): 100%|██████████| 163/163 [00:00<00:00, 257.65 examples/s]
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:54,  2.96it/s]  1%|          | 2/163 [00:00<00:48,  3.35it/s]  2%|▏         | 3/163 [00:00<00:45,  3.52it/s]  2%|▏         | 4/163 [00:01<00:44,  3.56it/s]  3%|▎         | 5/163 [00:01<00:42,  3.75it/s]  4%|▎         | 6/163 [00:01<00:41,  3.77it/s]  4%|▍         | 7/163 [00:01<00:41,  3.75it/s]  5%|▍         | 8/163 [00:02<00:42,  3.67it/s]  6%|▌         | 9/163 [00:02<00:40,  3.81it/s]  6%|▌         | 10/163 [00:02<00:39,  3.83it/s]  7%|▋         | 11/163 [00:02<00:40,  3.79it/s]  7%|▋         | 12/163 [00:03<00:40,  3.69it/s]  8%|▊         | 13/163 [00:03<00:39,  3.81it/s]  9%|▊         | 14/163 [00:03<00:38,  3.84it/s]  9%|▉         | 15/163 [00:04<00:38,  3.80it/s] 10%|▉         | 16/163 [00:04<00:38,  3.77it/s] 10%|█         | 17/163 [00:04<00:38,  3.79it/s] 11%|█         | 18/163 [00:04<00:37,  3.83it/s] 12%|█▏        | 19/163 [00:05<00:38,  3.79it/s] 12%|█▏        | 20/163 [00:05<00:37,  3.78it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.75it/s] 13%|█▎        | 22/163 [00:05<00:36,  3.82it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.78it/s] 15%|█▍        | 24/163 [00:06<00:36,  3.78it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.75it/s] 16%|█▌        | 26/163 [00:06<00:35,  3.81it/s] 17%|█▋        | 27/163 [00:07<00:36,  3.78it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.76it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.74it/s] 18%|█▊        | 30/163 [00:08<00:34,  3.80it/s] 19%|█▉        | 31/163 [00:08<00:35,  3.77it/s] 20%|█▉        | 32/163 [00:08<00:34,  3.80it/s] 20%|██        | 33/163 [00:08<00:34,  3.73it/s] 21%|██        | 34/163 [00:09<00:33,  3.81it/s] 21%|██▏       | 35/163 [00:09<00:33,  3.78it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.80it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.73it/s] 23%|██▎       | 38/163 [00:10<00:32,  3.81it/s] 24%|██▍       | 39/163 [00:10<00:32,  3.77it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.80it/s] 25%|██▌       | 41/163 [00:10<00:32,  3.73it/s] 26%|██▌       | 42/163 [00:11<00:31,  3.82it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.78it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.81it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.74it/s] 28%|██▊       | 46/163 [00:12<00:30,  3.80it/s] 29%|██▉       | 47/163 [00:12<00:30,  3.77it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.78it/s] 30%|███       | 49/163 [00:13<00:30,  3.75it/s] 31%|███       | 50/163 [00:13<00:29,  3.81it/s] 31%|███▏      | 51/163 [00:13<00:29,  3.77it/s] 32%|███▏      | 52/163 [00:13<00:29,  3.78it/s] 33%|███▎      | 53/163 [00:14<00:29,  3.74it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.81it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.77it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.79it/s] 35%|███▍      | 57/163 [00:15<00:28,  3.75it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.82it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.77it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.81it/s] 37%|███▋      | 61/163 [00:16<00:27,  3.74it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.81it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.77it/s] 39%|███▉      | 64/163 [00:16<00:25,  3.81it/s] 40%|███▉      | 65/163 [00:17<00:26,  3.74it/s] 40%|████      | 66/163 [00:17<00:25,  3.83it/s] 41%|████      | 67/163 [00:17<00:25,  3.78it/s] 42%|████▏     | 68/163 [00:18<00:24,  3.81it/s] 42%|████▏     | 69/163 [00:18<00:25,  3.74it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.82it/s] 44%|████▎     | 71/163 [00:18<00:24,  3.78it/s] 44%|████▍     | 72/163 [00:19<00:24,  3.77it/s] 45%|████▍     | 73/163 [00:19<00:24,  3.71it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.83it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.80it/s] 47%|████▋     | 76/163 [00:20<00:23,  3.78it/s] 47%|████▋     | 77/163 [00:20<00:22,  3.78it/s] 48%|████▊     | 78/163 [00:20<00:21,  3.88it/s] 48%|████▊     | 79/163 [00:20<00:21,  3.94it/s] 49%|████▉     | 80/163 [00:21<00:20,  3.99it/s] 50%|████▉     | 81/163 [00:21<00:20,  4.03it/s] 50%|█████     | 82/163 [00:21<00:19,  4.06it/s] 51%|█████     | 83/163 [00:21<00:19,  4.08it/s] 52%|█████▏    | 84/163 [00:22<00:19,  4.09it/s] 52%|█████▏    | 85/163 [00:22<00:19,  4.09it/s] 53%|█████▎    | 86/163 [00:22<00:18,  4.09it/s] 53%|█████▎    | 87/163 [00:22<00:18,  4.10it/s] 54%|█████▍    | 88/163 [00:23<00:18,  4.11it/s] 55%|█████▍    | 89/163 [00:23<00:17,  4.11it/s] 55%|█████▌    | 90/163 [00:23<00:17,  4.12it/s] 56%|█████▌    | 91/163 [00:23<00:17,  4.12it/s] 56%|█████▋    | 92/163 [00:24<00:17,  4.11it/s] 57%|█████▋    | 93/163 [00:24<00:17,  4.10it/s] 58%|█████▊    | 94/163 [00:24<00:16,  4.11it/s] 58%|█████▊    | 95/163 [00:24<00:16,  4.12it/s] 59%|█████▉    | 96/163 [00:25<00:16,  4.12it/s] 60%|█████▉    | 97/163 [00:25<00:16,  4.12it/s] 60%|██████    | 98/163 [00:25<00:15,  4.12it/s] 61%|██████    | 99/163 [00:25<00:15,  4.11it/s] 61%|██████▏   | 100/163 [00:26<00:15,  4.11it/s] 62%|██████▏   | 101/163 [00:26<00:15,  4.11it/s] 63%|██████▎   | 102/163 [00:26<00:14,  4.12it/s] 63%|██████▎   | 103/163 [00:26<00:14,  4.12it/s] 64%|██████▍   | 104/163 [00:26<00:14,  4.12it/s] 64%|██████▍   | 105/163 [00:27<00:14,  4.11it/s] 65%|██████▌   | 106/163 [00:27<00:13,  4.11it/s] 66%|██████▌   | 107/163 [00:27<00:13,  4.11it/s] 66%|██████▋   | 108/163 [00:27<00:13,  4.12it/s] 67%|██████▋   | 109/163 [00:28<00:13,  4.12it/s] 67%|██████▋   | 110/163 [00:28<00:12,  4.12it/s] 68%|██████▊   | 111/163 [00:28<00:12,  4.12it/s] 69%|██████▊   | 112/163 [00:28<00:12,  4.11it/s] 69%|██████▉   | 113/163 [00:29<00:12,  4.11it/s] 70%|██████▉   | 114/163 [00:29<00:11,  4.11it/s] 71%|███████   | 115/163 [00:29<00:11,  4.04it/s] 71%|███████   | 116/163 [00:29<00:11,  3.93it/s] 72%|███████▏  | 117/163 [00:30<00:11,  3.87it/s] 72%|███████▏  | 118/163 [00:30<00:11,  3.88it/s] 73%|███████▎  | 119/163 [00:30<00:11,  3.87it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.75it/s] 74%|███████▍  | 121/163 [00:31<00:11,  3.77it/s] 75%|███████▍  | 122/163 [00:31<00:11,  3.71it/s] 75%|███████▌  | 123/163 [00:31<00:10,  3.78it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.71it/s] 77%|███████▋  | 125/163 [00:32<00:10,  3.74it/s] 77%|███████▋  | 126/163 [00:32<00:10,  3.69it/s] 78%|███████▊  | 127/163 [00:32<00:09,  3.77it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.70it/s] 79%|███████▉  | 129/163 [00:33<00:09,  3.74it/s] 80%|███████▉  | 130/163 [00:33<00:08,  3.68it/s] 80%|████████  | 131/163 [00:33<00:08,  3.77it/s] 81%|████████  | 132/163 [00:34<00:08,  3.73it/s] 82%|████████▏ | 133/163 [00:34<00:08,  3.69it/s] 82%|████████▏ | 134/163 [00:34<00:07,  3.65it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.78it/s] 83%|████████▎ | 136/163 [00:35<00:07,  3.73it/s] 84%|████████▍ | 137/163 [00:35<00:07,  3.70it/s] 85%|████████▍ | 138/163 [00:35<00:06,  3.76it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.70it/s] 86%|████████▌ | 140/163 [00:36<00:06,  3.74it/s] 87%|████████▋ | 141/163 [00:36<00:05,  3.68it/s] 87%|████████▋ | 142/163 [00:36<00:05,  3.73it/s] 88%|████████▊ | 143/163 [00:37<00:05,  3.68it/s] 88%|████████▊ | 144/163 [00:37<00:05,  3.75it/s] 89%|████████▉ | 145/163 [00:37<00:04,  3.70it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.72it/s] 90%|█████████ | 147/163 [00:38<00:04,  3.67it/s] 91%|█████████ | 148/163 [00:38<00:03,  3.77it/s] 91%|█████████▏| 149/163 [00:38<00:03,  3.71it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.68it/s] 93%|█████████▎| 151/163 [00:39<00:03,  3.63it/s] 93%|█████████▎| 152/163 [00:39<00:02,  3.76it/s] 94%|█████████▍| 153/163 [00:39<00:02,  3.75it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.69it/s] 95%|█████████▌| 155/163 [00:40<00:02,  3.74it/s] 96%|█████████▌| 156/163 [00:40<00:01,  3.68it/s] 96%|█████████▋| 157/163 [00:40<00:01,  3.74it/s] 97%|█████████▋| 158/163 [00:41<00:01,  3.67it/s] 98%|█████████▊| 159/163 [00:41<00:01,  3.71it/s] 98%|█████████▊| 160/163 [00:41<00:00,  3.67it/s] 99%|█████████▉| 161/163 [00:42<00:00,  3.76it/s] 99%|█████████▉| 162/163 [00:42<00:00,  3.72it/s]100%|██████████| 163/163 [00:42<00:00,  3.70it/s]accuracy:  0.7300613496932515
100%|██████████| 163/163 [00:45<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):  70%|███████   | 1804/2575 [00:00<00:00, 17500.44 examples/s]Filter (num_proc=10):  90%|█████████ | 2318/2575 [00:00<00:00, 22560.96 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14411.76 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15052.85 examples/s]
Map (num_proc=10):   0%|          | 0/171 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/171 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/171 [00:00<00:48,  3.53 examples/s]Map (num_proc=10):   1%|          | 1/171 [00:00<00:46,  3.67 examples/s]Map (num_proc=10):  14%|█▍        | 24/171 [00:00<00:01, 75.74 examples/s]Map (num_proc=10):   9%|▉         | 16/171 [00:00<00:03, 50.95 examples/s]Map (num_proc=10):  50%|█████     | 86/171 [00:00<00:00, 239.80 examples/s]Map (num_proc=10):  38%|███▊      | 65/171 [00:00<00:00, 188.78 examples/s]Map (num_proc=10):  73%|███████▎  | 125/171 [00:00<00:00, 279.42 examples/s]Map (num_proc=10):  67%|██████▋   | 114/171 [00:00<00:00, 271.43 examples/s]Map (num_proc=10):  96%|█████████▌| 164/171 [00:00<00:00, 302.43 examples/s]Map (num_proc=10):  86%|████████▌ | 147/171 [00:00<00:00, 268.02 examples/s]Map (num_proc=10): 100%|██████████| 171/171 [00:00<00:00, 199.27 examples/s]
Training dataset size: 48, validation dataset size: 171
Map (num_proc=10): 100%|██████████| 171/171 [00:00<00:00, 181.20 examples/s]
Training dataset size: 48, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 48, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 02:56:26,035] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 02:56:26,145] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:56:30,031] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:17,  2.50s/it] 25%|██▌       | 2/8 [00:04<00:13,  2.30s/it] 38%|███▊      | 3/8 [00:06<00:11,  2.24s/it] 50%|█████     | 4/8 [00:09<00:09,  2.33s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:11<00:07,  2.43s/it] 75%|███████▌  | 6/8 [00:14<00:05,  2.55s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.52s/it]100%|██████████| 8/8 [00:19<00:00,  2.53s/it]                                             {'train_runtime': 20.3556, 'train_samples_per_second': 4.716, 'train_steps_per_second': 0.393, 'train_loss': 0.5285858511924744, 'epoch': 2.0}
100%|██████████| 8/8 [00:20<00:00,  2.53s/it]100%|██████████| 8/8 [00:20<00:00,  2.52s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 21304.58 examples/s]
Map (num_proc=10):   0%|          | 0/217 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/217 [00:00<00:27,  7.72 examples/s]Map (num_proc=10):  20%|█▉        | 43/217 [00:00<00:01, 146.85 examples/s]Map (num_proc=10):  54%|█████▍    | 118/217 [00:00<00:00, 318.29 examples/s]Map (num_proc=10):  77%|███████▋  | 168/217 [00:00<00:00, 352.21 examples/s]Map (num_proc=10):  96%|█████████▋| 209/217 [00:00<00:00, 361.61 examples/s]Map (num_proc=10): 100%|██████████| 217/217 [00:00<00:00, 261.06 examples/s]
Map:   0%|          | 0/217 [00:00<?, ? examples/s]Map: 100%|██████████| 217/217 [00:00<00:00, 4109.22 examples/s]
Map:   0%|          | 0/217 [00:00<?, ? examples/s]Map: 100%|██████████| 217/217 [00:00<00:00, 4425.45 examples/s]
Map:   0%|          | 0/217 [00:00<?, ? examples/s]Map: 100%|██████████| 217/217 [00:00<00:00, 3868.66 examples/s]
Filter (num_proc=10):   0%|          | 0/217 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 22/217 [00:00<00:02, 96.84 examples/s]Filter (num_proc=10):  30%|███       | 66/217 [00:00<00:00, 226.76 examples/s]Filter (num_proc=10):  71%|███████   | 154/217 [00:00<00:00, 421.06 examples/s]Filter (num_proc=10): 100%|██████████| 217/217 [00:00<00:00, 467.71 examples/s]Filter (num_proc=10): 100%|██████████| 217/217 [00:00<00:00, 335.69 examples/s]
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:20,  2.70it/s]  1%|          | 2/217 [00:00<01:03,  3.39it/s]  1%|▏         | 3/217 [00:00<00:58,  3.69it/s]  2%|▏         | 4/217 [00:01<00:55,  3.84it/s]  2%|▏         | 5/217 [00:01<00:53,  3.94it/s]  3%|▎         | 6/217 [00:01<00:52,  4.00it/s]  3%|▎         | 7/217 [00:01<00:51,  4.06it/s]  4%|▎         | 8/217 [00:02<00:51,  4.09it/s]  4%|▍         | 9/217 [00:02<00:50,  4.12it/s]  5%|▍         | 10/217 [00:02<00:50,  4.13it/s]  5%|▌         | 11/217 [00:02<00:49,  4.14it/s]  6%|▌         | 12/217 [00:03<00:49,  4.14it/s]  6%|▌         | 13/217 [00:03<00:49,  4.15it/s]  6%|▋         | 14/217 [00:03<00:48,  4.15it/s]  7%|▋         | 15/217 [00:03<00:48,  4.16it/s]  7%|▋         | 16/217 [00:04<00:49,  4.06it/s]  8%|▊         | 17/217 [00:04<00:50,  3.95it/s]  8%|▊         | 18/217 [00:04<00:51,  3.88it/s]  9%|▉         | 19/217 [00:04<00:50,  3.96it/s]  9%|▉         | 20/217 [00:05<00:50,  3.89it/s] 10%|▉         | 21/217 [00:05<00:52,  3.75it/s] 10%|█         | 22/217 [00:05<00:51,  3.78it/s] 11%|█         | 23/217 [00:05<00:52,  3.72it/s] 11%|█         | 24/217 [00:06<00:50,  3.83it/s] 12%|█▏        | 25/217 [00:06<00:51,  3.73it/s] 12%|█▏        | 26/217 [00:06<00:51,  3.68it/s] 12%|█▏        | 27/217 [00:06<00:52,  3.63it/s] 13%|█▎        | 28/217 [00:07<00:50,  3.77it/s] 13%|█▎        | 29/217 [00:07<00:49,  3.79it/s] 14%|█▍        | 30/217 [00:07<00:51,  3.66it/s] 14%|█▍        | 31/217 [00:08<00:49,  3.73it/s] 15%|█▍        | 32/217 [00:08<00:50,  3.69it/s] 15%|█▌        | 33/217 [00:08<00:48,  3.79it/s] 16%|█▌        | 34/217 [00:08<00:49,  3.73it/s] 16%|█▌        | 35/217 [00:09<00:49,  3.67it/s] 17%|█▋        | 36/217 [00:09<00:50,  3.62it/s] 17%|█▋        | 37/217 [00:09<00:47,  3.75it/s] 18%|█▊        | 38/217 [00:09<00:47,  3.78it/s] 18%|█▊        | 39/217 [00:10<00:48,  3.66it/s] 18%|█▊        | 40/217 [00:10<00:47,  3.71it/s] 19%|█▉        | 41/217 [00:10<00:48,  3.66it/s] 19%|█▉        | 42/217 [00:10<00:46,  3.77it/s] 20%|█▉        | 43/217 [00:11<00:47,  3.69it/s] 20%|██        | 44/217 [00:11<00:47,  3.64it/s] 21%|██        | 45/217 [00:11<00:47,  3.60it/s] 21%|██        | 46/217 [00:12<00:45,  3.74it/s] 22%|██▏       | 47/217 [00:12<00:45,  3.77it/s] 22%|██▏       | 48/217 [00:12<00:46,  3.66it/s] 23%|██▎       | 49/217 [00:12<00:45,  3.69it/s] 23%|██▎       | 50/217 [00:13<00:45,  3.65it/s] 24%|██▎       | 51/217 [00:13<00:44,  3.76it/s] 24%|██▍       | 52/217 [00:13<00:44,  3.70it/s] 24%|██▍       | 53/217 [00:13<00:44,  3.65it/s] 25%|██▍       | 54/217 [00:14<00:45,  3.61it/s] 25%|██▌       | 55/217 [00:14<00:43,  3.74it/s] 26%|██▌       | 56/217 [00:14<00:42,  3.77it/s] 26%|██▋       | 57/217 [00:15<00:43,  3.66it/s] 27%|██▋       | 58/217 [00:15<00:42,  3.71it/s] 27%|██▋       | 59/217 [00:15<00:43,  3.65it/s] 28%|██▊       | 60/217 [00:15<00:41,  3.76it/s] 28%|██▊       | 61/217 [00:16<00:42,  3.69it/s] 29%|██▊       | 62/217 [00:16<00:42,  3.64it/s] 29%|██▉       | 63/217 [00:16<00:42,  3.60it/s] 29%|██▉       | 64/217 [00:16<00:40,  3.73it/s] 30%|██▉       | 65/217 [00:17<00:40,  3.77it/s] 30%|███       | 66/217 [00:17<00:41,  3.65it/s] 31%|███       | 67/217 [00:17<00:40,  3.71it/s] 31%|███▏      | 68/217 [00:18<00:40,  3.66it/s] 32%|███▏      | 69/217 [00:18<00:39,  3.75it/s] 32%|███▏      | 70/217 [00:18<00:39,  3.69it/s] 33%|███▎      | 71/217 [00:18<00:40,  3.64it/s] 33%|███▎      | 72/217 [00:19<00:40,  3.59it/s] 34%|███▎      | 73/217 [00:19<00:38,  3.74it/s] 34%|███▍      | 74/217 [00:19<00:38,  3.76it/s] 35%|███▍      | 75/217 [00:19<00:38,  3.65it/s] 35%|███▌      | 76/217 [00:20<00:38,  3.69it/s] 35%|███▌      | 77/217 [00:20<00:38,  3.65it/s] 36%|███▌      | 78/217 [00:20<00:36,  3.77it/s] 36%|███▋      | 79/217 [00:21<00:37,  3.70it/s] 37%|███▋      | 80/217 [00:21<00:37,  3.64it/s] 37%|███▋      | 81/217 [00:21<00:37,  3.60it/s] 38%|███▊      | 82/217 [00:21<00:36,  3.74it/s] 38%|███▊      | 83/217 [00:22<00:35,  3.77it/s] 39%|███▊      | 84/217 [00:22<00:36,  3.65it/s] 39%|███▉      | 85/217 [00:22<00:35,  3.67it/s] 40%|███▉      | 86/217 [00:22<00:36,  3.62it/s] 40%|████      | 87/217 [00:23<00:34,  3.75it/s] 41%|████      | 88/217 [00:23<00:34,  3.76it/s] 41%|████      | 89/217 [00:23<00:35,  3.65it/s] 41%|████▏     | 90/217 [00:24<00:34,  3.69it/s] 42%|████▏     | 91/217 [00:24<00:34,  3.64it/s] 42%|████▏     | 92/217 [00:24<00:33,  3.76it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.69it/s] 43%|████▎     | 94/217 [00:25<00:33,  3.63it/s] 44%|████▍     | 95/217 [00:25<00:34,  3.59it/s] 44%|████▍     | 96/217 [00:25<00:32,  3.73it/s] 45%|████▍     | 97/217 [00:25<00:31,  3.77it/s] 45%|████▌     | 98/217 [00:26<00:32,  3.65it/s] 46%|████▌     | 99/217 [00:26<00:32,  3.67it/s] 46%|████▌     | 100/217 [00:26<00:32,  3.64it/s] 47%|████▋     | 101/217 [00:26<00:30,  3.77it/s] 47%|████▋     | 102/217 [00:27<00:31,  3.70it/s] 47%|████▋     | 103/217 [00:27<00:31,  3.66it/s] 48%|████▊     | 104/217 [00:27<00:30,  3.67it/s] 48%|████▊     | 105/217 [00:28<00:30,  3.68it/s] 49%|████▉     | 106/217 [00:28<00:29,  3.75it/s] 49%|████▉     | 107/217 [00:28<00:29,  3.68it/s] 50%|████▉     | 108/217 [00:28<00:30,  3.63it/s] 50%|█████     | 109/217 [00:29<00:30,  3.58it/s] 51%|█████     | 110/217 [00:29<00:28,  3.73it/s] 51%|█████     | 111/217 [00:29<00:28,  3.76it/s] 52%|█████▏    | 112/217 [00:29<00:28,  3.64it/s] 52%|█████▏    | 113/217 [00:30<00:28,  3.69it/s] 53%|█████▎    | 114/217 [00:30<00:28,  3.66it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.76it/s] 53%|█████▎    | 116/217 [00:31<00:27,  3.68it/s] 54%|█████▍    | 117/217 [00:31<00:27,  3.63it/s] 54%|█████▍    | 118/217 [00:31<00:27,  3.59it/s] 55%|█████▍    | 119/217 [00:31<00:26,  3.73it/s] 55%|█████▌    | 120/217 [00:32<00:25,  3.74it/s] 56%|█████▌    | 121/217 [00:32<00:26,  3.63it/s] 56%|█████▌    | 122/217 [00:32<00:25,  3.67it/s] 57%|█████▋    | 123/217 [00:32<00:25,  3.62it/s] 57%|█████▋    | 124/217 [00:33<00:24,  3.75it/s] 58%|█████▊    | 125/217 [00:33<00:25,  3.67it/s] 58%|█████▊    | 126/217 [00:33<00:25,  3.62it/s] 59%|█████▊    | 127/217 [00:34<00:25,  3.58it/s] 59%|█████▉    | 128/217 [00:34<00:23,  3.72it/s] 59%|█████▉    | 129/217 [00:34<00:23,  3.73it/s] 60%|█████▉    | 130/217 [00:34<00:24,  3.62it/s] 60%|██████    | 131/217 [00:35<00:23,  3.68it/s] 61%|██████    | 132/217 [00:35<00:23,  3.63it/s] 61%|██████▏   | 133/217 [00:35<00:22,  3.73it/s] 62%|██████▏   | 134/217 [00:35<00:22,  3.67it/s] 62%|██████▏   | 135/217 [00:36<00:22,  3.62it/s] 63%|██████▎   | 136/217 [00:36<00:22,  3.58it/s] 63%|██████▎   | 137/217 [00:36<00:21,  3.72it/s] 64%|██████▎   | 138/217 [00:37<00:21,  3.74it/s] 64%|██████▍   | 139/217 [00:37<00:21,  3.62it/s] 65%|██████▍   | 140/217 [00:37<00:20,  3.69it/s] 65%|██████▍   | 141/217 [00:37<00:20,  3.63it/s] 65%|██████▌   | 142/217 [00:38<00:20,  3.72it/s] 66%|██████▌   | 143/217 [00:38<00:20,  3.65it/s] 66%|██████▋   | 144/217 [00:38<00:20,  3.61it/s] 67%|██████▋   | 145/217 [00:39<00:20,  3.56it/s] 67%|██████▋   | 146/217 [00:39<00:19,  3.70it/s] 68%|██████▊   | 147/217 [00:39<00:18,  3.73it/s] 68%|██████▊   | 148/217 [00:39<00:19,  3.62it/s] 69%|██████▊   | 149/217 [00:40<00:18,  3.66it/s] 69%|██████▉   | 150/217 [00:40<00:18,  3.62it/s] 70%|██████▉   | 151/217 [00:40<00:17,  3.74it/s] 70%|███████   | 152/217 [00:40<00:17,  3.67it/s] 71%|███████   | 153/217 [00:41<00:17,  3.62it/s] 71%|███████   | 154/217 [00:41<00:17,  3.57it/s] 71%|███████▏  | 155/217 [00:41<00:16,  3.71it/s] 72%|███████▏  | 156/217 [00:41<00:16,  3.73it/s] 72%|███████▏  | 157/217 [00:42<00:16,  3.61it/s] 73%|███████▎  | 158/217 [00:42<00:16,  3.65it/s] 73%|███████▎  | 159/217 [00:42<00:16,  3.62it/s] 74%|███████▎  | 160/217 [00:43<00:15,  3.74it/s] 74%|███████▍  | 161/217 [00:43<00:15,  3.66it/s] 75%|███████▍  | 162/217 [00:43<00:15,  3.61it/s] 75%|███████▌  | 163/217 [00:43<00:15,  3.56it/s] 76%|███████▌  | 164/217 [00:44<00:14,  3.70it/s] 76%|███████▌  | 165/217 [00:44<00:13,  3.72it/s] 76%|███████▋  | 166/217 [00:44<00:14,  3.61it/s] 77%|███████▋  | 167/217 [00:45<00:13,  3.66it/s] 77%|███████▋  | 168/217 [00:45<00:13,  3.63it/s] 78%|███████▊  | 169/217 [00:45<00:12,  3.71it/s] 78%|███████▊  | 170/217 [00:45<00:12,  3.63it/s] 79%|███████▉  | 171/217 [00:46<00:12,  3.59it/s] 79%|███████▉  | 172/217 [00:46<00:12,  3.55it/s] 80%|███████▉  | 173/217 [00:46<00:11,  3.71it/s] 80%|████████  | 174/217 [00:46<00:11,  3.73it/s] 81%|████████  | 175/217 [00:47<00:11,  3.62it/s] 81%|████████  | 176/217 [00:47<00:11,  3.67it/s] 82%|████████▏ | 177/217 [00:47<00:11,  3.63it/s] 82%|████████▏ | 178/217 [00:48<00:10,  3.73it/s] 82%|████████▏ | 179/217 [00:48<00:10,  3.66it/s] 83%|████████▎ | 180/217 [00:48<00:10,  3.61it/s] 83%|████████▎ | 181/217 [00:48<00:10,  3.57it/s] 84%|████████▍ | 182/217 [00:49<00:09,  3.71it/s] 84%|████████▍ | 183/217 [00:49<00:09,  3.73it/s] 85%|████████▍ | 184/217 [00:49<00:09,  3.61it/s] 85%|████████▌ | 185/217 [00:49<00:08,  3.66it/s] 86%|████████▌ | 186/217 [00:50<00:08,  3.62it/s] 86%|████████▌ | 187/217 [00:50<00:08,  3.71it/s] 87%|████████▋ | 188/217 [00:50<00:07,  3.64it/s] 87%|████████▋ | 189/217 [00:51<00:07,  3.59it/s] 88%|████████▊ | 190/217 [00:51<00:07,  3.55it/s] 88%|████████▊ | 191/217 [00:51<00:07,  3.69it/s] 88%|████████▊ | 192/217 [00:51<00:06,  3.71it/s] 89%|████████▉ | 193/217 [00:52<00:06,  3.60it/s] 89%|████████▉ | 194/217 [00:52<00:06,  3.66it/s] 90%|████████▉ | 195/217 [00:52<00:06,  3.62it/s] 90%|█████████ | 196/217 [00:52<00:05,  3.73it/s] 91%|█████████ | 197/217 [00:53<00:05,  3.65it/s] 91%|█████████ | 198/217 [00:53<00:05,  3.60it/s] 92%|█████████▏| 199/217 [00:53<00:05,  3.56it/s] 92%|█████████▏| 200/217 [00:54<00:04,  3.69it/s] 93%|█████████▎| 201/217 [00:54<00:04,  3.71it/s] 93%|█████████▎| 202/217 [00:54<00:04,  3.59it/s] 94%|█████████▎| 203/217 [00:54<00:03,  3.61it/s] 94%|█████████▍| 204/217 [00:55<00:03,  3.63it/s] 94%|█████████▍| 205/217 [00:55<00:03,  3.72it/s] 95%|█████████▍| 206/217 [00:55<00:03,  3.64it/s] 95%|█████████▌| 207/217 [00:55<00:02,  3.60it/s] 96%|█████████▌| 208/217 [00:56<00:02,  3.56it/s] 96%|█████████▋| 209/217 [00:56<00:02,  3.69it/s] 97%|█████████▋| 210/217 [00:56<00:01,  3.71it/s] 97%|█████████▋| 211/217 [00:57<00:01,  3.60it/s] 98%|█████████▊| 212/217 [00:57<00:01,  3.63it/s] 98%|█████████▊| 213/217 [00:57<00:01,  3.62it/s] 99%|█████████▊| 214/217 [00:57<00:00,  3.72it/s] 99%|█████████▉| 215/217 [00:58<00:00,  3.65it/s]100%|█████████▉| 216/217 [00:58<00:00,  3.60it/s]100%|██████████| 217/217 [00:58<00:00,  3.56it/s]accuracy:  0.8202764976958525
100%|██████████| 217/217 [01:02<00:00,  3.50it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15754.31 examples/s]
Filter (num_proc=10):  20%|█▉        | 514/2575 [00:00<00:00, 3938.01 examples/s]
Map (num_proc=10):   0%|          | 0/103 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/103 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/103 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/103 [00:00<00:26,  3.89 examples/s]Map (num_proc=10):   2%|▏         | 2/103 [00:00<00:13,  7.75 examples/s]Map (num_proc=10):   1%|          | 1/103 [00:00<00:26,  3.81 examples/s]Map (num_proc=10):  17%|█▋        | 17/103 [00:00<00:01, 57.53 examples/s]Map (num_proc=10):  22%|██▏       | 23/103 [00:00<00:01, 78.08 examples/s]Map (num_proc=10):  23%|██▎       | 24/103 [00:00<00:01, 78.92 examples/s]Map (num_proc=10):  49%|████▊     | 50/103 [00:00<00:00, 141.89 examples/s]Map (num_proc=10):  44%|████▎     | 45/103 [00:00<00:00, 115.20 examples/s]Map (num_proc=10):  53%|█████▎    | 55/103 [00:00<00:00, 148.00 examples/s]Map (num_proc=10):  72%|███████▏  | 74/103 [00:00<00:00, 170.12 examples/s]Map (num_proc=10):  77%|███████▋  | 79/103 [00:00<00:00, 183.48 examples/s]Map (num_proc=10):  83%|████████▎ | 86/103 [00:00<00:00, 193.77 examples/s]Map (num_proc=10): 100%|██████████| 103/103 [00:00<00:00, 139.57 examples/s]
Map (num_proc=10): 100%|██████████| 103/103 [00:00<00:00, 137.90 examples/s]
Training dataset size: 48, validation dataset size: 103
Map (num_proc=10): 100%|██████████| 103/103 [00:00<00:00, 141.25 examples/s]
Training dataset size: 48, validation dataset size: 103
Training dataset size: 48, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:58:26,133] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:58:26,358] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:58:26,576] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:17,  2.51s/it] 25%|██▌       | 2/8 [00:04<00:13,  2.33s/it] 38%|███▊      | 3/8 [00:07<00:12,  2.46s/it] 50%|█████     | 4/8 [00:09<00:09,  2.44s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.49s/it] 75%|███████▌  | 6/8 [00:14<00:04,  2.40s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.46s/it]100%|██████████| 8/8 [00:18<00:00,  2.27s/it]                                             {'train_runtime': 19.6142, 'train_samples_per_second': 4.894, 'train_steps_per_second': 0.408, 'train_loss': 0.8533219695091248, 'epoch': 2.0}
100%|██████████| 8/8 [00:19<00:00,  2.27s/it]100%|██████████| 8/8 [00:19<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 22136.44 examples/s]
Map (num_proc=10):   0%|          | 0/150 [00:00<?, ? examples/s]Map (num_proc=10):   1%|▏         | 2/150 [00:00<00:18,  7.81 examples/s]Map (num_proc=10):  30%|███       | 45/150 [00:00<00:00, 152.15 examples/s]Map (num_proc=10):  61%|██████    | 91/150 [00:00<00:00, 239.16 examples/s]Map (num_proc=10):  87%|████████▋ | 131/150 [00:00<00:00, 273.58 examples/s]Map (num_proc=10): 100%|██████████| 150/150 [00:00<00:00, 203.34 examples/s]
Map:   0%|          | 0/150 [00:00<?, ? examples/s]Map: 100%|██████████| 150/150 [00:00<00:00, 4167.24 examples/s]
Map:   0%|          | 0/150 [00:00<?, ? examples/s]Map: 100%|██████████| 150/150 [00:00<00:00, 4451.67 examples/s]
Map:   0%|          | 0/150 [00:00<?, ? examples/s]Map: 100%|██████████| 150/150 [00:00<00:00, 2961.51 examples/s]
Filter (num_proc=10):   0%|          | 0/150 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 15/150 [00:00<00:01, 72.03 examples/s]Filter (num_proc=10):  40%|████      | 60/150 [00:00<00:00, 201.70 examples/s]Filter (num_proc=10):  70%|███████   | 105/150 [00:00<00:00, 248.45 examples/s]Filter (num_proc=10): 100%|██████████| 150/150 [00:00<00:00, 253.53 examples/s]
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:50,  2.94it/s]  1%|▏         | 2/150 [00:00<00:41,  3.54it/s]  2%|▏         | 3/150 [00:00<00:38,  3.78it/s]  3%|▎         | 4/150 [00:01<00:37,  3.91it/s]  3%|▎         | 5/150 [00:01<00:36,  3.99it/s]  4%|▍         | 6/150 [00:01<00:35,  4.04it/s]  5%|▍         | 7/150 [00:01<00:35,  4.08it/s]  5%|▌         | 8/150 [00:02<00:34,  4.09it/s]  6%|▌         | 9/150 [00:02<00:34,  4.11it/s]  7%|▋         | 10/150 [00:02<00:34,  4.12it/s]  7%|▋         | 11/150 [00:02<00:33,  4.12it/s]  8%|▊         | 12/150 [00:03<00:33,  4.12it/s]  9%|▊         | 13/150 [00:03<00:33,  4.11it/s]  9%|▉         | 14/150 [00:03<00:33,  4.12it/s] 10%|█         | 15/150 [00:03<00:32,  4.13it/s] 11%|█         | 16/150 [00:03<00:32,  4.13it/s] 11%|█▏        | 17/150 [00:04<00:32,  4.14it/s] 12%|█▏        | 18/150 [00:04<00:31,  4.14it/s] 13%|█▎        | 19/150 [00:04<00:32,  4.07it/s] 13%|█▎        | 20/150 [00:04<00:32,  3.97it/s] 14%|█▍        | 21/150 [00:05<00:33,  3.90it/s] 15%|█▍        | 22/150 [00:05<00:32,  3.88it/s] 15%|█▌        | 23/150 [00:05<00:32,  3.94it/s] 16%|█▌        | 24/150 [00:05<00:31,  3.96it/s] 17%|█▋        | 25/150 [00:06<00:32,  3.90it/s] 17%|█▋        | 26/150 [00:06<00:32,  3.83it/s] 18%|█▊        | 27/150 [00:06<00:32,  3.73it/s] 19%|█▊        | 28/150 [00:07<00:31,  3.84it/s] 19%|█▉        | 29/150 [00:07<00:31,  3.88it/s] 20%|██        | 30/150 [00:07<00:31,  3.83it/s] 21%|██        | 31/150 [00:07<00:31,  3.78it/s] 21%|██▏       | 32/150 [00:08<00:31,  3.69it/s] 22%|██▏       | 33/150 [00:08<00:30,  3.82it/s] 23%|██▎       | 34/150 [00:08<00:29,  3.89it/s] 23%|██▎       | 35/150 [00:08<00:30,  3.83it/s] 24%|██▍       | 36/150 [00:09<00:30,  3.78it/s] 25%|██▍       | 37/150 [00:09<00:30,  3.68it/s] 25%|██▌       | 38/150 [00:09<00:29,  3.81it/s] 26%|██▌       | 39/150 [00:09<00:28,  3.87it/s] 27%|██▋       | 40/150 [00:10<00:28,  3.83it/s] 27%|██▋       | 41/150 [00:10<00:28,  3.78it/s] 28%|██▊       | 42/150 [00:10<00:29,  3.70it/s] 29%|██▊       | 43/150 [00:11<00:27,  3.83it/s] 29%|██▉       | 44/150 [00:11<00:27,  3.92it/s] 30%|███       | 45/150 [00:11<00:27,  3.85it/s] 31%|███       | 46/150 [00:11<00:27,  3.80it/s] 31%|███▏      | 47/150 [00:12<00:26,  3.82it/s] 32%|███▏      | 48/150 [00:12<00:27,  3.75it/s] 33%|███▎      | 49/150 [00:12<00:26,  3.86it/s] 33%|███▎      | 50/150 [00:12<00:26,  3.84it/s] 34%|███▍      | 51/150 [00:13<00:26,  3.80it/s] 35%|███▍      | 52/150 [00:13<00:25,  3.81it/s] 35%|███▌      | 53/150 [00:13<00:26,  3.73it/s] 36%|███▌      | 54/150 [00:13<00:25,  3.84it/s] 37%|███▋      | 55/150 [00:14<00:24,  3.83it/s] 37%|███▋      | 56/150 [00:14<00:24,  3.79it/s] 38%|███▊      | 57/150 [00:14<00:24,  3.79it/s] 39%|███▊      | 58/150 [00:14<00:24,  3.72it/s] 39%|███▉      | 59/150 [00:15<00:23,  3.83it/s] 40%|████      | 60/150 [00:15<00:23,  3.82it/s] 41%|████      | 61/150 [00:15<00:23,  3.79it/s] 41%|████▏     | 62/150 [00:16<00:23,  3.79it/s] 42%|████▏     | 63/150 [00:16<00:23,  3.72it/s] 43%|████▎     | 64/150 [00:16<00:22,  3.83it/s] 43%|████▎     | 65/150 [00:16<00:22,  3.84it/s] 44%|████▍     | 66/150 [00:17<00:22,  3.79it/s] 45%|████▍     | 67/150 [00:17<00:22,  3.76it/s] 45%|████▌     | 68/150 [00:17<00:22,  3.69it/s] 46%|████▌     | 69/150 [00:17<00:21,  3.80it/s] 47%|████▋     | 70/150 [00:18<00:20,  3.84it/s] 47%|████▋     | 71/150 [00:18<00:20,  3.80it/s] 48%|████▊     | 72/150 [00:18<00:20,  3.76it/s] 49%|████▊     | 73/150 [00:18<00:20,  3.67it/s] 49%|████▉     | 74/150 [00:19<00:20,  3.79it/s] 50%|█████     | 75/150 [00:19<00:19,  3.87it/s] 51%|█████     | 76/150 [00:19<00:19,  3.82it/s] 51%|█████▏    | 77/150 [00:19<00:19,  3.76it/s] 52%|█████▏    | 78/150 [00:20<00:19,  3.70it/s] 53%|█████▎    | 79/150 [00:20<00:18,  3.79it/s] 53%|█████▎    | 80/150 [00:20<00:18,  3.87it/s] 54%|█████▍    | 81/150 [00:21<00:18,  3.82it/s] 55%|█████▍    | 82/150 [00:21<00:18,  3.75it/s] 55%|█████▌    | 83/150 [00:21<00:18,  3.67it/s] 56%|█████▌    | 84/150 [00:21<00:17,  3.79it/s] 57%|█████▋    | 85/150 [00:22<00:16,  3.87it/s] 57%|█████▋    | 86/150 [00:22<00:16,  3.82it/s] 58%|█████▊    | 87/150 [00:22<00:16,  3.77it/s] 59%|█████▊    | 88/150 [00:22<00:16,  3.68it/s] 59%|█████▉    | 89/150 [00:23<00:16,  3.81it/s] 60%|██████    | 90/150 [00:23<00:15,  3.89it/s] 61%|██████    | 91/150 [00:23<00:15,  3.84it/s] 61%|██████▏   | 92/150 [00:23<00:15,  3.77it/s] 62%|██████▏   | 93/150 [00:24<00:15,  3.76it/s] 63%|██████▎   | 94/150 [00:24<00:14,  3.74it/s] 63%|██████▎   | 95/150 [00:24<00:14,  3.83it/s] 64%|██████▍   | 96/150 [00:24<00:14,  3.82it/s] 65%|██████▍   | 97/150 [00:25<00:14,  3.78it/s] 65%|██████▌   | 98/150 [00:25<00:13,  3.78it/s] 66%|██████▌   | 99/150 [00:25<00:13,  3.71it/s] 67%|██████▋   | 100/150 [00:26<00:13,  3.82it/s] 67%|██████▋   | 101/150 [00:26<00:12,  3.83it/s] 68%|██████▊   | 102/150 [00:26<00:12,  3.78it/s] 69%|██████▊   | 103/150 [00:26<00:12,  3.75it/s] 69%|██████▉   | 104/150 [00:27<00:12,  3.68it/s] 70%|███████   | 105/150 [00:27<00:11,  3.80it/s] 71%|███████   | 106/150 [00:27<00:11,  3.84it/s] 71%|███████▏  | 107/150 [00:27<00:11,  3.80it/s] 72%|███████▏  | 108/150 [00:28<00:11,  3.77it/s] 73%|███████▎  | 109/150 [00:28<00:10,  3.75it/s] 73%|███████▎  | 110/150 [00:28<00:10,  3.85it/s] 74%|███████▍  | 111/150 [00:28<00:09,  3.92it/s] 75%|███████▍  | 112/150 [00:29<00:09,  3.98it/s] 75%|███████▌  | 113/150 [00:29<00:09,  4.03it/s] 76%|███████▌  | 114/150 [00:29<00:08,  4.05it/s] 77%|███████▋  | 115/150 [00:29<00:08,  4.07it/s] 77%|███████▋  | 116/150 [00:30<00:08,  4.08it/s] 78%|███████▊  | 117/150 [00:30<00:08,  4.09it/s] 79%|███████▊  | 118/150 [00:30<00:07,  4.10it/s] 79%|███████▉  | 119/150 [00:30<00:07,  4.12it/s] 80%|████████  | 120/150 [00:31<00:07,  4.12it/s] 81%|████████  | 121/150 [00:31<00:07,  4.12it/s] 81%|████████▏ | 122/150 [00:31<00:06,  4.11it/s] 82%|████████▏ | 123/150 [00:31<00:06,  4.11it/s] 83%|████████▎ | 124/150 [00:32<00:06,  4.11it/s] 83%|████████▎ | 125/150 [00:32<00:06,  4.12it/s] 84%|████████▍ | 126/150 [00:32<00:05,  4.12it/s] 85%|████████▍ | 127/150 [00:32<00:05,  4.13it/s] 85%|████████▌ | 128/150 [00:33<00:05,  4.13it/s] 86%|████████▌ | 129/150 [00:33<00:05,  4.12it/s] 87%|████████▋ | 130/150 [00:33<00:04,  4.12it/s] 87%|████████▋ | 131/150 [00:33<00:04,  4.11it/s] 88%|████████▊ | 132/150 [00:34<00:04,  4.11it/s] 89%|████████▊ | 133/150 [00:34<00:04,  4.12it/s] 89%|████████▉ | 134/150 [00:34<00:03,  4.12it/s] 90%|█████████ | 135/150 [00:34<00:03,  4.11it/s] 91%|█████████ | 136/150 [00:34<00:03,  4.10it/s] 91%|█████████▏| 137/150 [00:35<00:03,  4.10it/s] 92%|█████████▏| 138/150 [00:35<00:02,  4.11it/s] 93%|█████████▎| 139/150 [00:35<00:02,  4.12it/s] 93%|█████████▎| 140/150 [00:35<00:02,  4.12it/s] 94%|█████████▍| 141/150 [00:36<00:02,  3.98it/s] 95%|█████████▍| 142/150 [00:36<00:02,  3.88it/s] 95%|█████████▌| 143/150 [00:36<00:01,  3.88it/s] 96%|█████████▌| 144/150 [00:37<00:01,  3.87it/s] 97%|█████████▋| 145/150 [00:37<00:01,  3.78it/s] 97%|█████████▋| 146/150 [00:37<00:01,  3.83it/s] 98%|█████████▊| 147/150 [00:37<00:00,  3.75it/s] 99%|█████████▊| 148/150 [00:38<00:00,  3.79it/s] 99%|█████████▉| 149/150 [00:38<00:00,  3.73it/s]100%|██████████| 150/150 [00:38<00:00,  3.78it/s]accuracy:  0.6066666666666667
100%|██████████| 150/150 [00:40<00:00,  3.66it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16798.17 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15899.72 examples/s]
Map (num_proc=10):   0%|          | 0/145 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/145 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/145 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/145 [00:00<00:41,  3.50 examples/s]Map (num_proc=10):   1%|          | 1/145 [00:00<00:42,  3.43 examples/s]Map (num_proc=10):  14%|█▍        | 20/145 [00:00<00:02, 62.33 examples/s]Map (num_proc=10):   1%|▏         | 2/145 [00:00<00:22,  6.48 examples/s]Map (num_proc=10):  14%|█▍        | 21/145 [00:00<00:01, 65.12 examples/s]Map (num_proc=10):  49%|████▉     | 71/145 [00:00<00:00, 200.58 examples/s]Map (num_proc=10):  32%|███▏      | 47/145 [00:00<00:00, 125.52 examples/s]Map (num_proc=10):  12%|█▏        | 17/145 [00:00<00:02, 47.44 examples/s]Map (num_proc=10):  74%|███████▍  | 107/145 [00:00<00:00, 240.65 examples/s]Map (num_proc=10):  63%|██████▎   | 91/145 [00:00<00:00, 206.50 examples/s]Map (num_proc=10):  54%|█████▍    | 78/145 [00:00<00:00, 192.83 examples/s]Map (num_proc=10):  97%|█████████▋| 140/145 [00:00<00:00, 226.04 examples/s]Map (num_proc=10):  79%|███████▊  | 114/145 [00:00<00:00, 234.89 examples/s]Map (num_proc=10):  83%|████████▎ | 120/145 [00:00<00:00, 208.06 examples/s]Map (num_proc=10): 100%|██████████| 145/145 [00:00<00:00, 157.50 examples/s]
Map (num_proc=10):  99%|█████████▉| 144/145 [00:00<00:00, 193.27 examples/s]Map (num_proc=10): 100%|██████████| 145/145 [00:00<00:00, 214.93 examples/s]Training dataset size: 48, validation dataset size: 145
Map (num_proc=10): 100%|██████████| 145/145 [00:00<00:00, 157.61 examples/s]
Map (num_proc=10): 100%|██████████| 145/145 [00:01<00:00, 142.93 examples/s]
Training dataset size: 48, validation dataset size: 145
Training dataset size: 48, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:59:57,212] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.24s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.48s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:59:58,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 02:59:59,040] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:20,  2.96s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.63s/it] 38%|███▊      | 3/8 [00:07<00:11,  2.27s/it] 50%|█████     | 4/8 [00:09<00:09,  2.43s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.43s/it] 75%|███████▌  | 6/8 [00:14<00:04,  2.37s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.48s/it]100%|██████████| 8/8 [00:19<00:00,  2.43s/it]                                             {'train_runtime': 20.2472, 'train_samples_per_second': 4.741, 'train_steps_per_second': 0.395, 'train_loss': 1.3460296392440796, 'epoch': 2.0}
100%|██████████| 8/8 [00:20<00:00,  2.43s/it]100%|██████████| 8/8 [00:20<00:00,  2.51s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 21042.36 examples/s]
Map (num_proc=10):   0%|          | 0/200 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 2/200 [00:00<00:29,  6.77 examples/s]Map (num_proc=10):  26%|██▋       | 53/200 [00:00<00:00, 167.21 examples/s]Map (num_proc=10):  56%|█████▋    | 113/200 [00:00<00:00, 288.70 examples/s]Map (num_proc=10):  79%|███████▉  | 158/200 [00:00<00:00, 336.33 examples/s]Map (num_proc=10): 100%|██████████| 200/200 [00:00<00:00, 287.72 examples/s]Map (num_proc=10): 100%|██████████| 200/200 [00:00<00:00, 226.29 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map: 100%|██████████| 200/200 [00:00<00:00, 4296.89 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map: 100%|██████████| 200/200 [00:00<00:00, 4194.28 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map: 100%|██████████| 200/200 [00:00<00:00, 3796.01 examples/s]
Filter (num_proc=10):   0%|          | 0/200 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 20/200 [00:00<00:02, 82.03 examples/s]Filter (num_proc=10):  40%|████      | 80/200 [00:00<00:00, 259.84 examples/s]Filter (num_proc=10):  60%|██████    | 120/200 [00:00<00:00, 295.56 examples/s]Filter (num_proc=10): 100%|██████████| 200/200 [00:00<00:00, 394.70 examples/s]Filter (num_proc=10): 100%|██████████| 200/200 [00:00<00:00, 287.71 examples/s]
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:16,  2.59it/s]  1%|          | 2/200 [00:00<00:59,  3.32it/s]  2%|▏         | 3/200 [00:00<00:55,  3.52it/s]  2%|▏         | 4/200 [00:01<00:54,  3.58it/s]  2%|▎         | 5/200 [00:01<00:52,  3.69it/s]  3%|▎         | 6/200 [00:01<00:52,  3.66it/s]  4%|▎         | 7/200 [00:01<00:50,  3.81it/s]  4%|▍         | 8/200 [00:02<00:50,  3.81it/s]  4%|▍         | 9/200 [00:02<00:50,  3.77it/s]  5%|▌         | 10/200 [00:02<00:50,  3.79it/s]  6%|▌         | 11/200 [00:03<00:50,  3.72it/s]  6%|▌         | 12/200 [00:03<00:48,  3.84it/s]  6%|▋         | 13/200 [00:03<00:49,  3.80it/s]  7%|▋         | 14/200 [00:03<00:49,  3.77it/s]  8%|▊         | 15/200 [00:04<00:48,  3.78it/s]  8%|▊         | 16/200 [00:04<00:49,  3.74it/s]  8%|▊         | 17/200 [00:04<00:47,  3.84it/s]  9%|▉         | 18/200 [00:04<00:48,  3.78it/s] 10%|▉         | 19/200 [00:05<00:48,  3.76it/s] 10%|█         | 20/200 [00:05<00:47,  3.79it/s] 10%|█         | 21/200 [00:05<00:48,  3.73it/s] 11%|█         | 22/200 [00:05<00:46,  3.84it/s] 12%|█▏        | 23/200 [00:06<00:46,  3.82it/s] 12%|█▏        | 24/200 [00:06<00:46,  3.77it/s] 12%|█▎        | 25/200 [00:06<00:46,  3.80it/s] 13%|█▎        | 26/200 [00:06<00:46,  3.73it/s] 14%|█▎        | 27/200 [00:07<00:44,  3.84it/s] 14%|█▍        | 28/200 [00:07<00:45,  3.78it/s] 14%|█▍        | 29/200 [00:07<00:45,  3.74it/s] 15%|█▌        | 30/200 [00:08<00:45,  3.73it/s] 16%|█▌        | 31/200 [00:08<00:44,  3.84it/s] 16%|█▌        | 32/200 [00:08<00:42,  3.92it/s] 16%|█▋        | 33/200 [00:08<00:41,  3.98it/s] 17%|█▋        | 34/200 [00:09<00:41,  4.02it/s] 18%|█▊        | 35/200 [00:09<00:40,  4.04it/s] 18%|█▊        | 36/200 [00:09<00:40,  4.07it/s] 18%|█▊        | 37/200 [00:09<00:39,  4.09it/s] 19%|█▉        | 38/200 [00:09<00:39,  4.10it/s] 20%|█▉        | 39/200 [00:10<00:39,  4.11it/s] 20%|██        | 40/200 [00:10<00:38,  4.12it/s] 20%|██        | 41/200 [00:10<00:38,  4.11it/s] 21%|██        | 42/200 [00:10<00:38,  4.11it/s] 22%|██▏       | 43/200 [00:11<00:38,  4.11it/s] 22%|██▏       | 44/200 [00:11<00:37,  4.11it/s] 22%|██▎       | 45/200 [00:11<00:37,  4.12it/s] 23%|██▎       | 46/200 [00:11<00:37,  4.12it/s] 24%|██▎       | 47/200 [00:12<00:37,  4.13it/s] 24%|██▍       | 48/200 [00:12<00:36,  4.13it/s] 24%|██▍       | 49/200 [00:12<00:36,  4.12it/s] 25%|██▌       | 50/200 [00:12<00:36,  4.11it/s] 26%|██▌       | 51/200 [00:13<00:36,  4.11it/s] 26%|██▌       | 52/200 [00:13<00:35,  4.12it/s] 26%|██▋       | 53/200 [00:13<00:35,  4.12it/s] 27%|██▋       | 54/200 [00:13<00:35,  4.13it/s] 28%|██▊       | 55/200 [00:14<00:35,  4.09it/s] 28%|██▊       | 56/200 [00:14<00:36,  3.96it/s] 28%|██▊       | 57/200 [00:14<00:36,  3.88it/s] 29%|██▉       | 58/200 [00:14<00:36,  3.87it/s] 30%|██▉       | 59/200 [00:15<00:35,  3.94it/s] 30%|███       | 60/200 [00:15<00:35,  3.91it/s] 30%|███       | 61/200 [00:15<00:36,  3.80it/s] 31%|███       | 62/200 [00:15<00:36,  3.76it/s] 32%|███▏      | 63/200 [00:16<00:36,  3.77it/s] 32%|███▏      | 64/200 [00:16<00:35,  3.83it/s] 32%|███▎      | 65/200 [00:16<00:36,  3.74it/s] 33%|███▎      | 66/200 [00:17<00:35,  3.74it/s] 34%|███▎      | 67/200 [00:17<00:35,  3.70it/s] 34%|███▍      | 68/200 [00:17<00:34,  3.81it/s] 34%|███▍      | 69/200 [00:17<00:34,  3.81it/s] 35%|███▌      | 70/200 [00:18<00:34,  3.73it/s] 36%|███▌      | 71/200 [00:18<00:34,  3.69it/s] 36%|███▌      | 72/200 [00:18<00:34,  3.75it/s] 36%|███▋      | 73/200 [00:18<00:33,  3.80it/s] 37%|███▋      | 74/200 [00:19<00:34,  3.71it/s] 38%|███▊      | 75/200 [00:19<00:33,  3.73it/s] 38%|███▊      | 76/200 [00:19<00:33,  3.68it/s] 38%|███▊      | 77/200 [00:19<00:32,  3.80it/s] 39%|███▉      | 78/200 [00:20<00:32,  3.74it/s] 40%|███▉      | 79/200 [00:20<00:32,  3.71it/s] 40%|████      | 80/200 [00:20<00:32,  3.66it/s] 40%|████      | 81/200 [00:21<00:31,  3.78it/s] 41%|████      | 82/200 [00:21<00:31,  3.80it/s] 42%|████▏     | 83/200 [00:21<00:31,  3.73it/s] 42%|████▏     | 84/200 [00:21<00:30,  3.76it/s] 42%|████▎     | 85/200 [00:22<00:31,  3.70it/s] 43%|████▎     | 86/200 [00:22<00:29,  3.81it/s] 44%|████▎     | 87/200 [00:22<00:30,  3.74it/s] 44%|████▍     | 88/200 [00:22<00:30,  3.68it/s] 44%|████▍     | 89/200 [00:23<00:30,  3.63it/s] 45%|████▌     | 90/200 [00:23<00:29,  3.75it/s] 46%|████▌     | 91/200 [00:23<00:28,  3.79it/s] 46%|████▌     | 92/200 [00:23<00:29,  3.71it/s] 46%|████▋     | 93/200 [00:24<00:28,  3.73it/s] 47%|████▋     | 94/200 [00:24<00:28,  3.71it/s] 48%|████▊     | 95/200 [00:24<00:27,  3.82it/s] 48%|████▊     | 96/200 [00:25<00:27,  3.76it/s] 48%|████▊     | 97/200 [00:25<00:27,  3.71it/s] 49%|████▉     | 98/200 [00:25<00:28,  3.64it/s] 50%|████▉     | 99/200 [00:25<00:26,  3.77it/s] 50%|█████     | 100/200 [00:26<00:26,  3.79it/s] 50%|█████     | 101/200 [00:26<00:26,  3.70it/s] 51%|█████     | 102/200 [00:26<00:26,  3.73it/s] 52%|█████▏    | 103/200 [00:26<00:26,  3.68it/s] 52%|█████▏    | 104/200 [00:27<00:25,  3.78it/s] 52%|█████▎    | 105/200 [00:27<00:25,  3.72it/s] 53%|█████▎    | 106/200 [00:27<00:25,  3.69it/s] 54%|█████▎    | 107/200 [00:28<00:25,  3.64it/s] 54%|█████▍    | 108/200 [00:28<00:24,  3.77it/s] 55%|█████▍    | 109/200 [00:28<00:24,  3.79it/s] 55%|█████▌    | 110/200 [00:28<00:24,  3.70it/s] 56%|█████▌    | 111/200 [00:29<00:24,  3.66it/s] 56%|█████▌    | 112/200 [00:29<00:23,  3.71it/s] 56%|█████▋    | 113/200 [00:29<00:23,  3.77it/s] 57%|█████▋    | 114/200 [00:29<00:23,  3.69it/s] 57%|█████▊    | 115/200 [00:30<00:23,  3.70it/s] 58%|█████▊    | 116/200 [00:30<00:22,  3.66it/s] 58%|█████▊    | 117/200 [00:30<00:21,  3.78it/s] 59%|█████▉    | 118/200 [00:30<00:21,  3.77it/s] 60%|█████▉    | 119/200 [00:31<00:21,  3.69it/s] 60%|██████    | 120/200 [00:31<00:22,  3.60it/s] 60%|██████    | 121/200 [00:31<00:21,  3.74it/s] 61%|██████    | 122/200 [00:32<00:20,  3.77it/s] 62%|██████▏   | 123/200 [00:32<00:20,  3.69it/s] 62%|██████▏   | 124/200 [00:32<00:20,  3.72it/s] 62%|██████▎   | 125/200 [00:32<00:20,  3.67it/s] 63%|██████▎   | 126/200 [00:33<00:19,  3.77it/s] 64%|██████▎   | 127/200 [00:33<00:19,  3.71it/s] 64%|██████▍   | 128/200 [00:33<00:19,  3.68it/s] 64%|██████▍   | 129/200 [00:33<00:19,  3.64it/s] 65%|██████▌   | 130/200 [00:34<00:18,  3.75it/s] 66%|██████▌   | 131/200 [00:34<00:18,  3.78it/s] 66%|██████▌   | 132/200 [00:34<00:18,  3.69it/s] 66%|██████▋   | 133/200 [00:35<00:18,  3.70it/s] 67%|██████▋   | 134/200 [00:35<00:17,  3.70it/s] 68%|██████▊   | 135/200 [00:35<00:17,  3.79it/s] 68%|██████▊   | 136/200 [00:35<00:17,  3.73it/s] 68%|██████▊   | 137/200 [00:36<00:17,  3.69it/s] 69%|██████▉   | 138/200 [00:36<00:17,  3.64it/s] 70%|██████▉   | 139/200 [00:36<00:16,  3.76it/s] 70%|███████   | 140/200 [00:36<00:15,  3.77it/s] 70%|███████   | 141/200 [00:37<00:15,  3.69it/s] 71%|███████   | 142/200 [00:37<00:16,  3.60it/s] 72%|███████▏  | 143/200 [00:37<00:15,  3.74it/s] 72%|███████▏  | 144/200 [00:37<00:14,  3.78it/s] 72%|███████▎  | 145/200 [00:38<00:14,  3.71it/s] 73%|███████▎  | 146/200 [00:38<00:14,  3.74it/s] 74%|███████▎  | 147/200 [00:38<00:14,  3.68it/s] 74%|███████▍  | 148/200 [00:39<00:13,  3.78it/s] 74%|███████▍  | 149/200 [00:39<00:13,  3.71it/s] 75%|███████▌  | 150/200 [00:39<00:13,  3.67it/s] 76%|███████▌  | 151/200 [00:39<00:13,  3.63it/s] 76%|███████▌  | 152/200 [00:40<00:12,  3.75it/s] 76%|███████▋  | 153/200 [00:40<00:12,  3.75it/s] 77%|███████▋  | 154/200 [00:40<00:12,  3.68it/s] 78%|███████▊  | 155/200 [00:40<00:12,  3.65it/s] 78%|███████▊  | 156/200 [00:41<00:11,  3.71it/s] 78%|███████▊  | 157/200 [00:41<00:11,  3.76it/s] 79%|███████▉  | 158/200 [00:41<00:11,  3.68it/s] 80%|███████▉  | 159/200 [00:42<00:11,  3.71it/s] 80%|████████  | 160/200 [00:42<00:10,  3.66it/s] 80%|████████  | 161/200 [00:42<00:10,  3.75it/s] 81%|████████  | 162/200 [00:42<00:10,  3.70it/s] 82%|████████▏ | 163/200 [00:43<00:10,  3.66it/s] 82%|████████▏ | 164/200 [00:43<00:09,  3.62it/s] 82%|████████▎ | 165/200 [00:43<00:09,  3.74it/s] 83%|████████▎ | 166/200 [00:43<00:09,  3.76it/s] 84%|████████▎ | 167/200 [00:44<00:08,  3.68it/s] 84%|████████▍ | 168/200 [00:44<00:08,  3.59it/s] 84%|████████▍ | 169/200 [00:44<00:08,  3.74it/s] 85%|████████▌ | 170/200 [00:44<00:07,  3.78it/s] 86%|████████▌ | 171/200 [00:45<00:07,  3.69it/s] 86%|████████▌ | 172/200 [00:45<00:07,  3.65it/s] 86%|████████▋ | 173/200 [00:45<00:07,  3.77it/s] 87%|████████▋ | 174/200 [00:46<00:06,  3.86it/s] 88%|████████▊ | 175/200 [00:46<00:06,  3.92it/s] 88%|████████▊ | 176/200 [00:46<00:06,  3.96it/s] 88%|████████▊ | 177/200 [00:46<00:05,  3.99it/s] 89%|████████▉ | 178/200 [00:47<00:05,  4.02it/s] 90%|████████▉ | 179/200 [00:47<00:05,  4.03it/s] 90%|█████████ | 180/200 [00:47<00:04,  4.03it/s] 90%|█████████ | 181/200 [00:47<00:04,  4.05it/s] 91%|█████████ | 182/200 [00:48<00:04,  4.06it/s] 92%|█████████▏| 183/200 [00:48<00:04,  4.06it/s] 92%|█████████▏| 184/200 [00:48<00:03,  4.06it/s] 92%|█████████▎| 185/200 [00:48<00:03,  4.07it/s] 93%|█████████▎| 186/200 [00:48<00:03,  4.06it/s] 94%|█████████▎| 187/200 [00:49<00:03,  4.06it/s] 94%|█████████▍| 188/200 [00:49<00:02,  4.06it/s] 94%|█████████▍| 189/200 [00:49<00:02,  4.06it/s] 95%|█████████▌| 190/200 [00:49<00:02,  4.05it/s] 96%|█████████▌| 191/200 [00:50<00:02,  4.06it/s] 96%|█████████▌| 192/200 [00:50<00:01,  4.05it/s] 96%|█████████▋| 193/200 [00:50<00:01,  4.05it/s] 97%|█████████▋| 194/200 [00:50<00:01,  4.05it/s] 98%|█████████▊| 195/200 [00:51<00:01,  4.05it/s] 98%|█████████▊| 196/200 [00:51<00:00,  4.05it/s] 98%|█████████▊| 197/200 [00:51<00:00,  4.06it/s] 99%|█████████▉| 198/200 [00:51<00:00,  4.05it/s]100%|█████████▉| 199/200 [00:52<00:00,  4.05it/s]100%|██████████| 200/200 [00:52<00:00,  4.07it/s]accuracy:  0.565
100%|██████████| 200/200 [00:55<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16327.65 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16132.32 examples/s]
Map (num_proc=10):   0%|          | 0/191 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/191 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/191 [00:00<00:51,  3.66 examples/s]Map (num_proc=10):  12%|█▏        | 23/191 [00:00<00:02, 70.95 examples/s]Map (num_proc=10):   1%|          | 1/191 [00:00<00:53,  3.53 examples/s]Map (num_proc=10):  53%|█████▎    | 101/191 [00:00<00:00, 279.29 examples/s]Map (num_proc=10):   8%|▊         | 15/191 [00:00<00:03, 46.95 examples/s]Map (num_proc=10):  39%|███▉      | 75/191 [00:00<00:00, 217.71 examples/s]Map (num_proc=10):  73%|███████▎  | 139/191 [00:00<00:00, 278.46 examples/s]Map (num_proc=10):  65%|██████▍   | 124/191 [00:00<00:00, 285.74 examples/s]Map (num_proc=10):  91%|█████████ | 173/191 [00:00<00:00, 261.70 examples/s]Map (num_proc=10):  83%|████████▎ | 159/191 [00:00<00:00, 300.26 examples/s]Map (num_proc=10): 100%|██████████| 191/191 [00:00<00:00, 192.79 examples/s]
Training dataset size: 48, validation dataset size: 191
Map (num_proc=10): 100%|██████████| 191/191 [00:00<00:00, 199.71 examples/s]
Training dataset size: 48, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:01:45,288] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.43s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Training dataset size: 48, validation dataset size: 191
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:01:46,831] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:01:50,271] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:19,  2.74s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.61s/it] 38%|███▊      | 3/8 [00:08<00:13,  2.77s/it] 50%|█████     | 4/8 [00:10<00:10,  2.64s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:13<00:08,  2.67s/it] 75%|███████▌  | 6/8 [00:16<00:05,  2.73s/it] 88%|████████▊ | 7/8 [00:18<00:02,  2.66s/it]100%|██████████| 8/8 [00:21<00:00,  2.56s/it]                                             {'train_runtime': 21.7391, 'train_samples_per_second': 4.416, 'train_steps_per_second': 0.368, 'train_loss': 1.6751608848571777, 'epoch': 2.0}
100%|██████████| 8/8 [00:21<00:00,  2.56s/it]100%|██████████| 8/8 [00:21<00:00,  2.69s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 22618.74 examples/s]
Map (num_proc=10):   0%|          | 0/279 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 3/279 [00:00<00:24, 11.40 examples/s]Map (num_proc=10):  17%|█▋        | 48/279 [00:00<00:01, 156.26 examples/s]Map (num_proc=10):  49%|████▉     | 138/279 [00:00<00:00, 371.27 examples/s]Map (num_proc=10):  67%|██████▋   | 187/279 [00:00<00:00, 407.03 examples/s]Map (num_proc=10):  89%|████████▉ | 248/279 [00:00<00:00, 465.15 examples/s]Map (num_proc=10): 100%|██████████| 279/279 [00:00<00:00, 309.97 examples/s]
Map:   0%|          | 0/279 [00:00<?, ? examples/s]Map: 100%|██████████| 279/279 [00:00<00:00, 3971.84 examples/s]
Map:   0%|          | 0/279 [00:00<?, ? examples/s]Map: 100%|██████████| 279/279 [00:00<00:00, 3911.37 examples/s]
Map:   0%|          | 0/279 [00:00<?, ? examples/s]Map: 100%|██████████| 279/279 [00:00<00:00, 4215.26 examples/s]
Filter (num_proc=10):   0%|          | 0/279 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 28/279 [00:00<00:02, 114.84 examples/s]Filter (num_proc=10):  50%|█████     | 140/279 [00:00<00:00, 427.70 examples/s]Filter (num_proc=10):  80%|████████  | 224/279 [00:00<00:00, 539.31 examples/s]Filter (num_proc=10): 100%|██████████| 279/279 [00:00<00:00, 440.24 examples/s]
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:48,  2.56it/s]  1%|          | 2/279 [00:00<01:28,  3.14it/s]  1%|          | 3/279 [00:00<01:20,  3.41it/s]  1%|▏         | 4/279 [00:01<01:15,  3.67it/s]  2%|▏         | 5/279 [00:01<01:14,  3.69it/s]  2%|▏         | 6/279 [00:01<01:13,  3.70it/s]  3%|▎         | 7/279 [00:01<01:13,  3.70it/s]  3%|▎         | 8/279 [00:02<01:12,  3.74it/s]  3%|▎         | 9/279 [00:02<01:10,  3.84it/s]  4%|▎         | 10/279 [00:02<01:10,  3.80it/s]  4%|▍         | 11/279 [00:03<01:11,  3.77it/s]  4%|▍         | 12/279 [00:03<01:12,  3.70it/s]  5%|▍         | 13/279 [00:03<01:09,  3.83it/s]  5%|▌         | 14/279 [00:03<01:08,  3.86it/s]  5%|▌         | 15/279 [00:04<01:09,  3.81it/s]  6%|▌         | 16/279 [00:04<01:09,  3.78it/s]  6%|▌         | 17/279 [00:04<01:10,  3.71it/s]  6%|▋         | 18/279 [00:04<01:08,  3.82it/s]  7%|▋         | 19/279 [00:05<01:07,  3.88it/s]  7%|▋         | 20/279 [00:05<01:07,  3.83it/s]  8%|▊         | 21/279 [00:05<01:08,  3.79it/s]  8%|▊         | 22/279 [00:05<01:08,  3.73it/s]  8%|▊         | 23/279 [00:06<01:06,  3.84it/s]  9%|▊         | 24/279 [00:06<01:06,  3.84it/s]  9%|▉         | 25/279 [00:06<01:06,  3.80it/s]  9%|▉         | 26/279 [00:06<01:07,  3.75it/s] 10%|▉         | 27/279 [00:07<01:06,  3.78it/s] 10%|█         | 28/279 [00:07<01:04,  3.87it/s] 10%|█         | 29/279 [00:07<01:05,  3.82it/s] 11%|█         | 30/279 [00:08<01:05,  3.78it/s] 11%|█         | 31/279 [00:08<01:06,  3.70it/s] 11%|█▏        | 32/279 [00:08<01:04,  3.81it/s] 12%|█▏        | 33/279 [00:08<01:03,  3.86it/s] 12%|█▏        | 34/279 [00:09<01:04,  3.81it/s] 13%|█▎        | 35/279 [00:09<01:04,  3.79it/s] 13%|█▎        | 36/279 [00:09<01:05,  3.70it/s] 13%|█▎        | 37/279 [00:09<01:03,  3.82it/s] 14%|█▎        | 38/279 [00:10<01:02,  3.86it/s] 14%|█▍        | 39/279 [00:10<01:03,  3.81it/s] 14%|█▍        | 40/279 [00:10<01:03,  3.77it/s] 15%|█▍        | 41/279 [00:10<01:04,  3.70it/s] 15%|█▌        | 42/279 [00:11<01:01,  3.83it/s] 15%|█▌        | 43/279 [00:11<01:01,  3.87it/s] 16%|█▌        | 44/279 [00:11<01:01,  3.81it/s] 16%|█▌        | 45/279 [00:11<01:01,  3.78it/s] 16%|█▋        | 46/279 [00:12<01:02,  3.71it/s] 17%|█▋        | 47/279 [00:12<01:00,  3.83it/s] 17%|█▋        | 48/279 [00:12<00:59,  3.86it/s] 18%|█▊        | 49/279 [00:13<01:00,  3.81it/s] 18%|█▊        | 50/279 [00:13<01:00,  3.77it/s] 18%|█▊        | 51/279 [00:13<01:01,  3.69it/s] 19%|█▊        | 52/279 [00:13<00:59,  3.81it/s] 19%|█▉        | 53/279 [00:14<00:58,  3.85it/s] 19%|█▉        | 54/279 [00:14<00:59,  3.80it/s] 20%|█▉        | 55/279 [00:14<00:59,  3.77it/s] 20%|██        | 56/279 [00:14<01:00,  3.69it/s] 20%|██        | 57/279 [00:15<00:58,  3.81it/s] 21%|██        | 58/279 [00:15<00:57,  3.85it/s] 21%|██        | 59/279 [00:15<00:58,  3.79it/s] 22%|██▏       | 60/279 [00:15<00:58,  3.75it/s] 22%|██▏       | 61/279 [00:16<00:59,  3.68it/s] 22%|██▏       | 62/279 [00:16<00:57,  3.80it/s] 23%|██▎       | 63/279 [00:16<00:56,  3.82it/s] 23%|██▎       | 64/279 [00:16<00:56,  3.79it/s] 23%|██▎       | 65/279 [00:17<00:56,  3.76it/s] 24%|██▎       | 66/279 [00:17<00:57,  3.68it/s] 24%|██▍       | 67/279 [00:17<00:55,  3.79it/s] 24%|██▍       | 68/279 [00:18<00:54,  3.84it/s] 25%|██▍       | 69/279 [00:18<00:55,  3.78it/s] 25%|██▌       | 70/279 [00:18<00:54,  3.81it/s] 25%|██▌       | 71/279 [00:18<00:54,  3.79it/s] 26%|██▌       | 72/279 [00:19<00:53,  3.87it/s] 26%|██▌       | 73/279 [00:19<00:52,  3.94it/s] 27%|██▋       | 74/279 [00:19<00:51,  3.99it/s] 27%|██▋       | 75/279 [00:19<00:50,  4.03it/s] 27%|██▋       | 76/279 [00:20<00:50,  4.05it/s] 28%|██▊       | 77/279 [00:20<00:49,  4.06it/s] 28%|██▊       | 78/279 [00:20<00:49,  4.07it/s] 28%|██▊       | 79/279 [00:20<00:49,  4.08it/s] 29%|██▊       | 80/279 [00:21<00:48,  4.09it/s] 29%|██▉       | 81/279 [00:21<00:49,  4.02it/s] 29%|██▉       | 82/279 [00:21<00:50,  3.91it/s] 30%|██▉       | 83/279 [00:21<00:50,  3.84it/s] 30%|███       | 84/279 [00:22<00:49,  3.91it/s] 30%|███       | 85/279 [00:22<00:50,  3.87it/s] 31%|███       | 86/279 [00:22<00:50,  3.81it/s] 31%|███       | 87/279 [00:22<00:50,  3.79it/s] 32%|███▏      | 88/279 [00:23<00:50,  3.75it/s] 32%|███▏      | 89/279 [00:23<00:49,  3.84it/s] 32%|███▏      | 90/279 [00:23<00:50,  3.77it/s] 33%|███▎      | 91/279 [00:23<00:50,  3.75it/s] 33%|███▎      | 92/279 [00:24<00:50,  3.70it/s] 33%|███▎      | 93/279 [00:24<00:48,  3.81it/s] 34%|███▎      | 94/279 [00:24<00:48,  3.82it/s] 34%|███▍      | 95/279 [00:25<00:48,  3.79it/s] 34%|███▍      | 96/279 [00:25<00:48,  3.80it/s] 35%|███▍      | 97/279 [00:25<00:48,  3.72it/s] 35%|███▌      | 98/279 [00:25<00:47,  3.81it/s] 35%|███▌      | 99/279 [00:26<00:47,  3.77it/s] 36%|███▌      | 100/279 [00:26<00:47,  3.73it/s] 36%|███▌      | 101/279 [00:26<00:48,  3.67it/s] 37%|███▋      | 102/279 [00:26<00:46,  3.78it/s] 37%|███▋      | 103/279 [00:27<00:46,  3.80it/s] 37%|███▋      | 104/279 [00:27<00:46,  3.76it/s] 38%|███▊      | 105/279 [00:27<00:46,  3.73it/s] 38%|███▊      | 106/279 [00:27<00:45,  3.82it/s] 38%|███▊      | 107/279 [00:28<00:44,  3.89it/s] 39%|███▊      | 108/279 [00:28<00:43,  3.95it/s] 39%|███▉      | 109/279 [00:28<00:42,  3.99it/s] 39%|███▉      | 110/279 [00:28<00:42,  4.01it/s] 40%|███▉      | 111/279 [00:29<00:41,  4.03it/s] 40%|████      | 112/279 [00:29<00:41,  4.05it/s] 41%|████      | 113/279 [00:29<00:40,  4.06it/s] 41%|████      | 114/279 [00:29<00:40,  4.06it/s] 41%|████      | 115/279 [00:30<00:40,  4.06it/s] 42%|████▏     | 116/279 [00:30<00:39,  4.08it/s] 42%|████▏     | 117/279 [00:30<00:39,  4.08it/s] 42%|████▏     | 118/279 [00:30<00:39,  4.07it/s] 43%|████▎     | 119/279 [00:31<00:39,  4.07it/s] 43%|████▎     | 120/279 [00:31<00:39,  4.08it/s] 43%|████▎     | 121/279 [00:31<00:38,  4.07it/s] 44%|████▎     | 122/279 [00:31<00:38,  4.07it/s] 44%|████▍     | 123/279 [00:32<00:38,  4.08it/s] 44%|████▍     | 124/279 [00:32<00:37,  4.09it/s] 45%|████▍     | 125/279 [00:32<00:37,  4.08it/s] 45%|████▌     | 126/279 [00:32<00:38,  3.96it/s] 46%|████▌     | 127/279 [00:33<00:39,  3.87it/s] 46%|████▌     | 128/279 [00:33<00:39,  3.80it/s] 46%|████▌     | 129/279 [00:33<00:38,  3.85it/s] 47%|████▋     | 130/279 [00:33<00:37,  3.92it/s] 47%|████▋     | 131/279 [00:34<00:38,  3.88it/s] 47%|████▋     | 132/279 [00:34<00:38,  3.80it/s] 48%|████▊     | 133/279 [00:34<00:38,  3.81it/s] 48%|████▊     | 134/279 [00:34<00:38,  3.73it/s] 48%|████▊     | 135/279 [00:35<00:37,  3.82it/s] 49%|████▊     | 136/279 [00:35<00:37,  3.82it/s] 49%|████▉     | 137/279 [00:35<00:37,  3.76it/s] 49%|████▉     | 138/279 [00:36<00:37,  3.71it/s] 50%|████▉     | 139/279 [00:36<00:38,  3.65it/s] 50%|█████     | 140/279 [00:36<00:36,  3.76it/s] 51%|█████     | 141/279 [00:36<00:35,  3.84it/s] 51%|█████     | 142/279 [00:37<00:36,  3.79it/s] 51%|█████▏    | 143/279 [00:37<00:36,  3.73it/s] 52%|█████▏    | 144/279 [00:37<00:36,  3.68it/s] 52%|█████▏    | 145/279 [00:37<00:35,  3.76it/s] 52%|█████▏    | 146/279 [00:38<00:34,  3.83it/s] 53%|█████▎    | 147/279 [00:38<00:35,  3.77it/s] 53%|█████▎    | 148/279 [00:38<00:35,  3.72it/s] 53%|█████▎    | 149/279 [00:38<00:35,  3.64it/s] 54%|█████▍    | 150/279 [00:39<00:34,  3.76it/s] 54%|█████▍    | 151/279 [00:39<00:33,  3.82it/s] 54%|█████▍    | 152/279 [00:39<00:33,  3.76it/s] 55%|█████▍    | 153/279 [00:40<00:33,  3.71it/s] 55%|█████▌    | 154/279 [00:40<00:33,  3.68it/s] 56%|█████▌    | 155/279 [00:40<00:33,  3.75it/s] 56%|█████▌    | 156/279 [00:40<00:32,  3.82it/s] 56%|█████▋    | 157/279 [00:41<00:32,  3.77it/s] 57%|█████▋    | 158/279 [00:41<00:32,  3.72it/s] 57%|█████▋    | 159/279 [00:41<00:32,  3.66it/s] 57%|█████▋    | 160/279 [00:41<00:31,  3.76it/s] 58%|█████▊    | 161/279 [00:42<00:30,  3.85it/s] 58%|█████▊    | 162/279 [00:42<00:30,  3.78it/s] 58%|█████▊    | 163/279 [00:42<00:30,  3.75it/s] 59%|█████▉    | 164/279 [00:42<00:30,  3.76it/s] 59%|█████▉    | 165/279 [00:43<00:30,  3.69it/s] 59%|█████▉    | 166/279 [00:43<00:29,  3.80it/s] 60%|█████▉    | 167/279 [00:43<00:28,  3.88it/s] 60%|██████    | 168/279 [00:43<00:28,  3.88it/s] 61%|██████    | 169/279 [00:44<00:28,  3.82it/s] 61%|██████    | 170/279 [00:44<00:29,  3.76it/s] 61%|██████▏   | 171/279 [00:44<00:29,  3.69it/s] 62%|██████▏   | 172/279 [00:45<00:28,  3.77it/s] 62%|██████▏   | 173/279 [00:45<00:27,  3.81it/s] 62%|██████▏   | 174/279 [00:45<00:27,  3.77it/s] 63%|██████▎   | 175/279 [00:45<00:27,  3.72it/s] 63%|██████▎   | 176/279 [00:46<00:28,  3.67it/s] 63%|██████▎   | 177/279 [00:46<00:27,  3.76it/s] 64%|██████▍   | 178/279 [00:46<00:26,  3.85it/s] 64%|██████▍   | 179/279 [00:46<00:26,  3.81it/s] 65%|██████▍   | 180/279 [00:47<00:26,  3.75it/s] 65%|██████▍   | 181/279 [00:47<00:26,  3.77it/s] 65%|██████▌   | 182/279 [00:47<00:26,  3.70it/s] 66%|██████▌   | 183/279 [00:47<00:25,  3.81it/s] 66%|██████▌   | 184/279 [00:48<00:24,  3.81it/s] 66%|██████▋   | 185/279 [00:48<00:25,  3.73it/s] 67%|██████▋   | 186/279 [00:48<00:24,  3.73it/s] 67%|██████▋   | 187/279 [00:49<00:25,  3.65it/s] 67%|██████▋   | 188/279 [00:49<00:24,  3.78it/s] 68%|██████▊   | 189/279 [00:49<00:23,  3.81it/s] 68%|██████▊   | 190/279 [00:49<00:23,  3.77it/s] 68%|██████▊   | 191/279 [00:50<00:23,  3.72it/s] 69%|██████▉   | 192/279 [00:50<00:23,  3.64it/s] 69%|██████▉   | 193/279 [00:50<00:22,  3.77it/s] 70%|██████▉   | 194/279 [00:50<00:22,  3.83it/s] 70%|██████▉   | 195/279 [00:51<00:22,  3.77it/s] 70%|███████   | 196/279 [00:51<00:22,  3.72it/s] 71%|███████   | 197/279 [00:51<00:22,  3.67it/s] 71%|███████   | 198/279 [00:52<00:21,  3.76it/s] 71%|███████▏  | 199/279 [00:52<00:20,  3.84it/s] 72%|███████▏  | 200/279 [00:52<00:20,  3.78it/s] 72%|███████▏  | 201/279 [00:52<00:20,  3.73it/s] 72%|███████▏  | 202/279 [00:53<00:20,  3.69it/s] 73%|███████▎  | 203/279 [00:53<00:20,  3.74it/s] 73%|███████▎  | 204/279 [00:53<00:19,  3.84it/s] 73%|███████▎  | 205/279 [00:53<00:19,  3.77it/s] 74%|███████▍  | 206/279 [00:54<00:19,  3.74it/s] 74%|███████▍  | 207/279 [00:54<00:19,  3.76it/s] 75%|███████▍  | 208/279 [00:54<00:18,  3.74it/s] 75%|███████▍  | 209/279 [00:54<00:18,  3.83it/s] 75%|███████▌  | 210/279 [00:55<00:17,  3.89it/s] 76%|███████▌  | 211/279 [00:55<00:17,  3.95it/s] 76%|███████▌  | 212/279 [00:55<00:16,  3.98it/s] 76%|███████▋  | 213/279 [00:55<00:16,  4.00it/s] 77%|███████▋  | 214/279 [00:56<00:16,  4.02it/s] 77%|███████▋  | 215/279 [00:56<00:15,  4.04it/s] 77%|███████▋  | 216/279 [00:56<00:15,  4.04it/s] 78%|███████▊  | 217/279 [00:56<00:15,  4.05it/s] 78%|███████▊  | 218/279 [00:57<00:15,  4.05it/s] 78%|███████▊  | 219/279 [00:57<00:14,  4.05it/s] 79%|███████▉  | 220/279 [00:57<00:14,  4.05it/s] 79%|███████▉  | 221/279 [00:57<00:14,  4.06it/s] 80%|███████▉  | 222/279 [00:58<00:14,  4.05it/s] 80%|███████▉  | 223/279 [00:58<00:13,  4.05it/s] 80%|████████  | 224/279 [00:58<00:13,  4.06it/s] 81%|████████  | 225/279 [00:58<00:13,  4.05it/s] 81%|████████  | 226/279 [00:59<00:13,  4.05it/s] 81%|████████▏ | 227/279 [00:59<00:13,  3.95it/s] 82%|████████▏ | 228/279 [00:59<00:13,  3.85it/s] 82%|████████▏ | 229/279 [00:59<00:13,  3.83it/s] 82%|████████▏ | 230/279 [01:00<00:12,  3.89it/s] 83%|████████▎ | 231/279 [01:00<00:12,  3.83it/s] 83%|████████▎ | 232/279 [01:00<00:12,  3.77it/s] 84%|████████▎ | 233/279 [01:00<00:12,  3.70it/s] 84%|████████▍ | 234/279 [01:01<00:11,  3.80it/s] 84%|████████▍ | 235/279 [01:01<00:11,  3.79it/s] 85%|████████▍ | 236/279 [01:01<00:11,  3.75it/s] 85%|████████▍ | 237/279 [01:02<00:11,  3.69it/s] 85%|████████▌ | 238/279 [01:02<00:10,  3.74it/s] 86%|████████▌ | 239/279 [01:02<00:10,  3.79it/s] 86%|████████▌ | 240/279 [01:02<00:10,  3.74it/s] 86%|████████▋ | 241/279 [01:03<00:10,  3.78it/s] 87%|████████▋ | 242/279 [01:03<00:09,  3.71it/s] 87%|████████▋ | 243/279 [01:03<00:09,  3.78it/s] 87%|████████▋ | 244/279 [01:03<00:09,  3.73it/s] 88%|████████▊ | 245/279 [01:04<00:09,  3.75it/s] 88%|████████▊ | 246/279 [01:04<00:08,  3.68it/s] 89%|████████▊ | 247/279 [01:04<00:08,  3.78it/s] 89%|████████▉ | 248/279 [01:04<00:08,  3.74it/s] 89%|████████▉ | 249/279 [01:05<00:08,  3.72it/s] 90%|████████▉ | 250/279 [01:05<00:07,  3.66it/s] 90%|████████▉ | 251/279 [01:05<00:07,  3.78it/s] 90%|█████████ | 252/279 [01:06<00:06,  3.86it/s] 91%|█████████ | 253/279 [01:06<00:06,  3.92it/s] 91%|█████████ | 254/279 [01:06<00:06,  3.88it/s] 91%|█████████▏| 255/279 [01:06<00:06,  3.80it/s] 92%|█████████▏| 256/279 [01:07<00:06,  3.73it/s] 92%|█████████▏| 257/279 [01:07<00:05,  3.76it/s] 92%|█████████▏| 258/279 [01:07<00:05,  3.79it/s] 93%|█████████▎| 259/279 [01:07<00:05,  3.74it/s] 93%|█████████▎| 260/279 [01:08<00:05,  3.77it/s] 94%|█████████▎| 261/279 [01:08<00:04,  3.69it/s] 94%|█████████▍| 262/279 [01:08<00:04,  3.75it/s] 94%|█████████▍| 263/279 [01:08<00:04,  3.71it/s] 95%|█████████▍| 264/279 [01:09<00:03,  3.76it/s] 95%|█████████▍| 265/279 [01:09<00:03,  3.68it/s] 95%|█████████▌| 266/279 [01:09<00:03,  3.77it/s] 96%|█████████▌| 267/279 [01:10<00:03,  3.73it/s] 96%|█████████▌| 268/279 [01:10<00:02,  3.71it/s] 96%|█████████▋| 269/279 [01:10<00:02,  3.64it/s] 97%|█████████▋| 270/279 [01:10<00:02,  3.75it/s] 97%|█████████▋| 271/279 [01:11<00:02,  3.74it/s] 97%|█████████▋| 272/279 [01:11<00:01,  3.72it/s] 98%|█████████▊| 273/279 [01:11<00:01,  3.65it/s] 98%|█████████▊| 274/279 [01:11<00:01,  3.75it/s] 99%|█████████▊| 275/279 [01:12<00:01,  3.78it/s] 99%|█████████▉| 276/279 [01:12<00:00,  3.74it/s] 99%|█████████▉| 277/279 [01:12<00:00,  3.77it/s]100%|█████████▉| 278/279 [01:12<00:00,  3.69it/s]100%|██████████| 279/279 [01:13<00:00,  3.77it/s]accuracy:  0.5304659498207885
100%|██████████| 279/279 [01:17<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16046.11 examples/s]
Map (num_proc=10):   0%|          | 0/241 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 1/241 [00:00<01:14,  3.22 examples/s]Map (num_proc=10):  13%|█▎        | 31/241 [00:00<00:02, 95.67 examples/s]Map (num_proc=10):  51%|█████     | 123/241 [00:00<00:00, 347.32 examples/s]Map (num_proc=10):   0%|          | 0/241 [00:00<?, ? examples/s]Map (num_proc=10):  73%|███████▎  | 177/241 [00:00<00:00, 395.24 examples/s]Map (num_proc=10):  97%|█████████▋| 233/241 [00:00<00:00, 400.79 examples/s]Map (num_proc=10):   0%|          | 1/241 [00:00<01:07,  3.57 examples/s]Map (num_proc=10): 100%|██████████| 241/241 [00:00<00:00, 260.87 examples/s]
Map (num_proc=10):  15%|█▍        | 35/241 [00:00<00:01, 116.58 examples/s]Training dataset size: 48, validation dataset size: 241
Map (num_proc=10):  50%|████▉     | 120/241 [00:00<00:00, 333.87 examples/s]Map (num_proc=10):  70%|██████▉   | 168/241 [00:00<00:00, 349.33 examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Map (num_proc=10):  91%|█████████▏| 220/241 [00:00<00:00, 394.24 examples/s]Map (num_proc=10): 100%|██████████| 241/241 [00:00<00:00, 245.86 examples/s]
Training dataset size: 48, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 48, validation dataset size: 241
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:03:58,597] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:04:01,936] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:04:02,346] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:16,  2.35s/it] 25%|██▌       | 2/8 [00:04<00:14,  2.43s/it] 38%|███▊      | 3/8 [00:07<00:12,  2.43s/it] 50%|█████     | 4/8 [00:09<00:09,  2.29s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:11<00:07,  2.40s/it] 75%|███████▌  | 6/8 [00:14<00:04,  2.40s/it] 88%|████████▊ | 7/8 [00:16<00:02,  2.37s/it]100%|██████████| 8/8 [00:18<00:00,  2.33s/it]                                             {'train_runtime': 19.5223, 'train_samples_per_second': 4.917, 'train_steps_per_second': 0.41, 'train_loss': 0.2876121997833252, 'epoch': 2.0}
100%|██████████| 8/8 [00:19<00:00,  2.33s/it]100%|██████████| 8/8 [00:19<00:00,  2.42s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 20011.36 examples/s]
Map (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Map (num_proc=10):   1%|▏         | 4/272 [00:00<00:16, 16.02 examples/s]Map (num_proc=10):  13%|█▎        | 35/272 [00:00<00:02, 116.49 examples/s]Map (num_proc=10):  42%|████▏     | 114/272 [00:00<00:00, 335.11 examples/s]Map (num_proc=10):  62%|██████▏   | 169/272 [00:00<00:00, 403.18 examples/s]Map (num_proc=10):  83%|████████▎ | 226/272 [00:00<00:00, 447.21 examples/s]Map (num_proc=10): 100%|██████████| 272/272 [00:00<00:00, 309.10 examples/s]
Map:   0%|          | 0/272 [00:00<?, ? examples/s]Map: 100%|██████████| 272/272 [00:00<00:00, 4178.45 examples/s]
Map:   0%|          | 0/272 [00:00<?, ? examples/s]Map: 100%|██████████| 272/272 [00:00<00:00, 4902.75 examples/s]
Map:   0%|          | 0/272 [00:00<?, ? examples/s]Map: 100%|██████████| 272/272 [00:00<00:00, 4538.35 examples/s]
Filter (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 28/272 [00:00<00:02, 101.88 examples/s]Filter (num_proc=10):  31%|███       | 83/272 [00:00<00:00, 247.62 examples/s]Filter (num_proc=10):  70%|███████   | 191/272 [00:00<00:00, 431.06 examples/s]Filter (num_proc=10): 100%|██████████| 272/272 [00:00<00:00, 408.99 examples/s]
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:37,  2.77it/s]  1%|          | 2/272 [00:00<01:23,  3.24it/s]  1%|          | 3/272 [00:00<01:18,  3.41it/s]  1%|▏         | 4/272 [00:01<01:13,  3.66it/s]  2%|▏         | 5/272 [00:01<01:12,  3.66it/s]  2%|▏         | 6/272 [00:01<01:12,  3.66it/s]  3%|▎         | 7/272 [00:01<01:12,  3.65it/s]  3%|▎         | 8/272 [00:02<01:09,  3.80it/s]  3%|▎         | 9/272 [00:02<01:10,  3.76it/s]  4%|▎         | 10/272 [00:02<01:10,  3.72it/s]  4%|▍         | 11/272 [00:03<01:10,  3.69it/s]  4%|▍         | 12/272 [00:03<01:08,  3.80it/s]  5%|▍         | 13/272 [00:03<01:09,  3.75it/s]  5%|▌         | 14/272 [00:03<01:09,  3.74it/s]  6%|▌         | 15/272 [00:04<01:09,  3.70it/s]  6%|▌         | 16/272 [00:04<01:07,  3.80it/s]  6%|▋         | 17/272 [00:04<01:08,  3.74it/s]  7%|▋         | 18/272 [00:04<01:08,  3.69it/s]  7%|▋         | 19/272 [00:05<01:06,  3.82it/s]  7%|▋         | 20/272 [00:05<01:04,  3.91it/s]  8%|▊         | 21/272 [00:05<01:03,  3.98it/s]  8%|▊         | 22/272 [00:05<01:01,  4.03it/s]  8%|▊         | 23/272 [00:06<01:01,  4.07it/s]  9%|▉         | 24/272 [00:06<01:00,  4.10it/s]  9%|▉         | 25/272 [00:06<00:59,  4.12it/s] 10%|▉         | 26/272 [00:06<00:59,  4.13it/s] 10%|▉         | 27/272 [00:07<00:59,  4.14it/s] 10%|█         | 28/272 [00:07<00:58,  4.15it/s] 11%|█         | 29/272 [00:07<00:58,  4.15it/s] 11%|█         | 30/272 [00:07<00:58,  4.15it/s] 11%|█▏        | 31/272 [00:08<00:58,  4.15it/s] 12%|█▏        | 32/272 [00:08<00:57,  4.16it/s] 12%|█▏        | 33/272 [00:08<00:57,  4.16it/s] 12%|█▎        | 34/272 [00:08<00:57,  4.16it/s] 13%|█▎        | 35/272 [00:08<00:57,  4.16it/s] 13%|█▎        | 36/272 [00:09<00:56,  4.15it/s] 14%|█▎        | 37/272 [00:09<00:56,  4.15it/s] 14%|█▍        | 38/272 [00:09<00:56,  4.15it/s] 14%|█▍        | 39/272 [00:09<00:56,  4.13it/s] 15%|█▍        | 40/272 [00:10<00:56,  4.12it/s] 15%|█▌        | 41/272 [00:10<00:55,  4.13it/s] 15%|█▌        | 42/272 [00:10<00:55,  4.13it/s] 16%|█▌        | 43/272 [00:10<00:55,  4.14it/s] 16%|█▌        | 44/272 [00:11<00:54,  4.15it/s] 17%|█▋        | 45/272 [00:11<00:54,  4.15it/s] 17%|█▋        | 46/272 [00:11<00:54,  4.15it/s] 17%|█▋        | 47/272 [00:11<00:54,  4.15it/s] 18%|█▊        | 48/272 [00:12<00:54,  4.14it/s] 18%|█▊        | 49/272 [00:12<00:53,  4.14it/s] 18%|█▊        | 50/272 [00:12<00:55,  3.97it/s] 19%|█▉        | 51/272 [00:12<00:57,  3.83it/s] 19%|█▉        | 52/272 [00:13<00:56,  3.92it/s] 19%|█▉        | 53/272 [00:13<00:56,  3.89it/s] 20%|█▉        | 54/272 [00:13<00:57,  3.82it/s] 20%|██        | 55/272 [00:13<00:58,  3.72it/s] 21%|██        | 56/272 [00:14<00:56,  3.83it/s] 21%|██        | 57/272 [00:14<00:56,  3.78it/s] 21%|██▏       | 58/272 [00:14<00:57,  3.75it/s] 22%|██▏       | 59/272 [00:15<00:57,  3.70it/s] 22%|██▏       | 60/272 [00:15<00:56,  3.77it/s] 22%|██▏       | 61/272 [00:15<00:56,  3.73it/s] 23%|██▎       | 62/272 [00:15<00:57,  3.67it/s] 23%|██▎       | 63/272 [00:16<00:55,  3.75it/s] 24%|██▎       | 64/272 [00:16<00:55,  3.77it/s] 24%|██▍       | 65/272 [00:16<00:55,  3.73it/s] 24%|██▍       | 66/272 [00:16<00:56,  3.66it/s] 25%|██▍       | 67/272 [00:17<00:54,  3.79it/s] 25%|██▌       | 68/272 [00:17<00:54,  3.75it/s] 25%|██▌       | 69/272 [00:17<00:54,  3.73it/s] 26%|██▌       | 70/272 [00:18<00:54,  3.69it/s] 26%|██▌       | 71/272 [00:18<00:53,  3.78it/s] 26%|██▋       | 72/272 [00:18<00:53,  3.72it/s] 27%|██▋       | 73/272 [00:18<00:54,  3.67it/s] 27%|██▋       | 74/272 [00:19<00:52,  3.74it/s] 28%|██▊       | 75/272 [00:19<00:52,  3.76it/s] 28%|██▊       | 76/272 [00:19<00:52,  3.71it/s] 28%|██▊       | 77/272 [00:19<00:53,  3.65it/s] 29%|██▊       | 78/272 [00:20<00:51,  3.77it/s] 29%|██▉       | 79/272 [00:20<00:51,  3.74it/s] 29%|██▉       | 80/272 [00:20<00:51,  3.72it/s] 30%|██▉       | 81/272 [00:20<00:51,  3.68it/s] 30%|███       | 82/272 [00:21<00:50,  3.78it/s] 31%|███       | 83/272 [00:21<00:50,  3.73it/s] 31%|███       | 84/272 [00:21<00:50,  3.69it/s] 31%|███▏      | 85/272 [00:22<00:49,  3.75it/s] 32%|███▏      | 86/272 [00:22<00:49,  3.77it/s] 32%|███▏      | 87/272 [00:22<00:49,  3.73it/s] 32%|███▏      | 88/272 [00:22<00:50,  3.66it/s] 33%|███▎      | 89/272 [00:23<00:48,  3.78it/s] 33%|███▎      | 90/272 [00:23<00:48,  3.75it/s] 33%|███▎      | 91/272 [00:23<00:48,  3.73it/s] 34%|███▍      | 92/272 [00:23<00:48,  3.68it/s] 34%|███▍      | 93/272 [00:24<00:47,  3.77it/s] 35%|███▍      | 94/272 [00:24<00:47,  3.73it/s] 35%|███▍      | 95/272 [00:24<00:48,  3.68it/s] 35%|███▌      | 96/272 [00:24<00:47,  3.74it/s] 36%|███▌      | 97/272 [00:25<00:46,  3.77it/s] 36%|███▌      | 98/272 [00:25<00:46,  3.73it/s] 36%|███▋      | 99/272 [00:25<00:47,  3.66it/s] 37%|███▋      | 100/272 [00:26<00:45,  3.79it/s] 37%|███▋      | 101/272 [00:26<00:45,  3.75it/s] 38%|███▊      | 102/272 [00:26<00:45,  3.73it/s] 38%|███▊      | 103/272 [00:26<00:45,  3.69it/s] 38%|███▊      | 104/272 [00:27<00:44,  3.78it/s] 39%|███▊      | 105/272 [00:27<00:44,  3.74it/s] 39%|███▉      | 106/272 [00:27<00:44,  3.69it/s] 39%|███▉      | 107/272 [00:27<00:43,  3.75it/s] 40%|███▉      | 108/272 [00:28<00:43,  3.78it/s] 40%|████      | 109/272 [00:28<00:43,  3.73it/s] 40%|████      | 110/272 [00:28<00:44,  3.66it/s] 41%|████      | 111/272 [00:28<00:42,  3.79it/s] 41%|████      | 112/272 [00:29<00:42,  3.75it/s] 42%|████▏     | 113/272 [00:29<00:42,  3.74it/s] 42%|████▏     | 114/272 [00:29<00:42,  3.69it/s] 42%|████▏     | 115/272 [00:30<00:41,  3.77it/s] 43%|████▎     | 116/272 [00:30<00:41,  3.73it/s] 43%|████▎     | 117/272 [00:30<00:42,  3.67it/s] 43%|████▎     | 118/272 [00:30<00:41,  3.74it/s] 44%|████▍     | 119/272 [00:31<00:40,  3.77it/s] 44%|████▍     | 120/272 [00:31<00:40,  3.73it/s] 44%|████▍     | 121/272 [00:31<00:41,  3.65it/s] 45%|████▍     | 122/272 [00:31<00:39,  3.78it/s] 45%|████▌     | 123/272 [00:32<00:39,  3.74it/s] 46%|████▌     | 124/272 [00:32<00:39,  3.73it/s] 46%|████▌     | 125/272 [00:32<00:39,  3.69it/s] 46%|████▋     | 126/272 [00:33<00:38,  3.79it/s] 47%|████▋     | 127/272 [00:33<00:38,  3.74it/s] 47%|████▋     | 128/272 [00:33<00:38,  3.70it/s] 47%|████▋     | 129/272 [00:33<00:38,  3.75it/s] 48%|████▊     | 130/272 [00:34<00:37,  3.77it/s] 48%|████▊     | 131/272 [00:34<00:37,  3.73it/s] 49%|████▊     | 132/272 [00:34<00:38,  3.66it/s] 49%|████▉     | 133/272 [00:34<00:36,  3.79it/s] 49%|████▉     | 134/272 [00:35<00:36,  3.75it/s] 50%|████▉     | 135/272 [00:35<00:36,  3.72it/s] 50%|█████     | 136/272 [00:35<00:37,  3.67it/s] 50%|█████     | 137/272 [00:35<00:35,  3.76it/s] 51%|█████     | 138/272 [00:36<00:36,  3.72it/s] 51%|█████     | 139/272 [00:36<00:36,  3.66it/s] 51%|█████▏    | 140/272 [00:36<00:35,  3.73it/s] 52%|█████▏    | 141/272 [00:37<00:34,  3.75it/s] 52%|█████▏    | 142/272 [00:37<00:35,  3.71it/s] 53%|█████▎    | 143/272 [00:37<00:35,  3.65it/s] 53%|█████▎    | 144/272 [00:37<00:33,  3.78it/s] 53%|█████▎    | 145/272 [00:38<00:33,  3.74it/s] 54%|█████▎    | 146/272 [00:38<00:33,  3.72it/s] 54%|█████▍    | 147/272 [00:38<00:34,  3.67it/s] 54%|█████▍    | 148/272 [00:38<00:32,  3.76it/s] 55%|█████▍    | 149/272 [00:39<00:33,  3.72it/s] 55%|█████▌    | 150/272 [00:39<00:33,  3.67it/s] 56%|█████▌    | 151/272 [00:39<00:32,  3.73it/s] 56%|█████▌    | 152/272 [00:39<00:31,  3.75it/s] 56%|█████▋    | 153/272 [00:40<00:32,  3.72it/s] 57%|█████▋    | 154/272 [00:40<00:32,  3.65it/s] 57%|█████▋    | 155/272 [00:40<00:30,  3.78it/s] 57%|█████▋    | 156/272 [00:41<00:31,  3.74it/s] 58%|█████▊    | 157/272 [00:41<00:30,  3.72it/s] 58%|█████▊    | 158/272 [00:41<00:31,  3.67it/s] 58%|█████▊    | 159/272 [00:41<00:30,  3.75it/s] 59%|█████▉    | 160/272 [00:42<00:30,  3.71it/s] 59%|█████▉    | 161/272 [00:42<00:30,  3.66it/s] 60%|█████▉    | 162/272 [00:42<00:29,  3.73it/s] 60%|█████▉    | 163/272 [00:42<00:29,  3.75it/s] 60%|██████    | 164/272 [00:43<00:29,  3.72it/s] 61%|██████    | 165/272 [00:43<00:29,  3.65it/s] 61%|██████    | 166/272 [00:43<00:28,  3.78it/s] 61%|██████▏   | 167/272 [00:44<00:28,  3.75it/s] 62%|██████▏   | 168/272 [00:44<00:27,  3.72it/s] 62%|██████▏   | 169/272 [00:44<00:28,  3.68it/s] 62%|██████▎   | 170/272 [00:44<00:27,  3.77it/s] 63%|██████▎   | 171/272 [00:45<00:27,  3.72it/s] 63%|██████▎   | 172/272 [00:45<00:27,  3.67it/s] 64%|██████▎   | 173/272 [00:45<00:26,  3.73it/s] 64%|██████▍   | 174/272 [00:45<00:26,  3.75it/s] 64%|██████▍   | 175/272 [00:46<00:26,  3.71it/s] 65%|██████▍   | 176/272 [00:46<00:26,  3.64it/s] 65%|██████▌   | 177/272 [00:46<00:25,  3.77it/s] 65%|██████▌   | 178/272 [00:46<00:25,  3.74it/s] 66%|██████▌   | 179/272 [00:47<00:25,  3.72it/s] 66%|██████▌   | 180/272 [00:47<00:25,  3.67it/s] 67%|██████▋   | 181/272 [00:47<00:24,  3.76it/s] 67%|██████▋   | 182/272 [00:48<00:24,  3.71it/s] 67%|██████▋   | 183/272 [00:48<00:24,  3.66it/s] 68%|██████▊   | 184/272 [00:48<00:23,  3.73it/s] 68%|██████▊   | 185/272 [00:48<00:23,  3.75it/s] 68%|██████▊   | 186/272 [00:49<00:23,  3.71it/s] 69%|██████▉   | 187/272 [00:49<00:23,  3.65it/s] 69%|██████▉   | 188/272 [00:49<00:22,  3.78it/s] 69%|██████▉   | 189/272 [00:49<00:22,  3.75it/s] 70%|██████▉   | 190/272 [00:50<00:22,  3.73it/s] 70%|███████   | 191/272 [00:50<00:22,  3.67it/s] 71%|███████   | 192/272 [00:50<00:21,  3.75it/s] 71%|███████   | 193/272 [00:51<00:21,  3.70it/s] 71%|███████▏  | 194/272 [00:51<00:21,  3.65it/s] 72%|███████▏  | 195/272 [00:51<00:20,  3.72it/s] 72%|███████▏  | 196/272 [00:51<00:20,  3.74it/s] 72%|███████▏  | 197/272 [00:52<00:20,  3.70it/s] 73%|███████▎  | 198/272 [00:52<00:20,  3.64it/s] 73%|███████▎  | 199/272 [00:52<00:19,  3.76it/s] 74%|███████▎  | 200/272 [00:52<00:19,  3.71it/s] 74%|███████▍  | 201/272 [00:53<00:19,  3.71it/s] 74%|███████▍  | 202/272 [00:53<00:19,  3.66it/s] 75%|███████▍  | 203/272 [00:53<00:18,  3.76it/s] 75%|███████▌  | 204/272 [00:54<00:18,  3.71it/s] 75%|███████▌  | 205/272 [00:54<00:18,  3.67it/s] 76%|███████▌  | 206/272 [00:54<00:17,  3.72it/s] 76%|███████▌  | 207/272 [00:54<00:17,  3.74it/s] 76%|███████▋  | 208/272 [00:55<00:17,  3.70it/s] 77%|███████▋  | 209/272 [00:55<00:17,  3.63it/s] 77%|███████▋  | 210/272 [00:55<00:16,  3.75it/s] 78%|███████▊  | 211/272 [00:55<00:16,  3.73it/s] 78%|███████▊  | 212/272 [00:56<00:16,  3.71it/s] 78%|███████▊  | 213/272 [00:56<00:16,  3.66it/s] 79%|███████▊  | 214/272 [00:56<00:15,  3.75it/s] 79%|███████▉  | 215/272 [00:56<00:15,  3.71it/s] 79%|███████▉  | 216/272 [00:57<00:15,  3.66it/s] 80%|███████▉  | 217/272 [00:57<00:14,  3.72it/s] 80%|████████  | 218/272 [00:57<00:14,  3.75it/s] 81%|████████  | 219/272 [00:58<00:14,  3.71it/s] 81%|████████  | 220/272 [00:58<00:14,  3.64it/s] 81%|████████▏ | 221/272 [00:58<00:13,  3.76it/s] 82%|████████▏ | 222/272 [00:58<00:13,  3.74it/s] 82%|████████▏ | 223/272 [00:59<00:13,  3.72it/s] 82%|████████▏ | 224/272 [00:59<00:13,  3.67it/s] 83%|████████▎ | 225/272 [00:59<00:12,  3.74it/s] 83%|████████▎ | 226/272 [00:59<00:12,  3.70it/s] 83%|████████▎ | 227/272 [01:00<00:12,  3.64it/s] 84%|████████▍ | 228/272 [01:00<00:11,  3.71it/s] 84%|████████▍ | 229/272 [01:00<00:11,  3.73it/s] 85%|████████▍ | 230/272 [01:01<00:11,  3.69it/s] 85%|████████▍ | 231/272 [01:01<00:11,  3.62it/s] 85%|████████▌ | 232/272 [01:01<00:10,  3.75it/s] 86%|████████▌ | 233/272 [01:01<00:10,  3.71it/s] 86%|████████▌ | 234/272 [01:02<00:10,  3.71it/s] 86%|████████▋ | 235/272 [01:02<00:10,  3.65it/s] 87%|████████▋ | 236/272 [01:02<00:09,  3.74it/s] 87%|████████▋ | 237/272 [01:02<00:09,  3.70it/s] 88%|████████▊ | 238/272 [01:03<00:09,  3.65it/s] 88%|████████▊ | 239/272 [01:03<00:08,  3.72it/s] 88%|████████▊ | 240/272 [01:03<00:08,  3.74it/s] 89%|████████▊ | 241/272 [01:03<00:08,  3.69it/s] 89%|████████▉ | 242/272 [01:04<00:08,  3.62it/s] 89%|████████▉ | 243/272 [01:04<00:07,  3.75it/s] 90%|████████▉ | 244/272 [01:04<00:07,  3.71it/s] 90%|█████████ | 245/272 [01:05<00:07,  3.70it/s] 90%|█████████ | 246/272 [01:05<00:07,  3.65it/s] 91%|█████████ | 247/272 [01:05<00:06,  3.75it/s] 91%|█████████ | 248/272 [01:05<00:06,  3.71it/s] 92%|█████████▏| 249/272 [01:06<00:06,  3.71it/s] 92%|█████████▏| 250/272 [01:06<00:06,  3.66it/s] 92%|█████████▏| 251/272 [01:06<00:05,  3.74it/s] 93%|█████████▎| 252/272 [01:06<00:05,  3.70it/s] 93%|█████████▎| 253/272 [01:07<00:05,  3.65it/s] 93%|█████████▎| 254/272 [01:07<00:04,  3.72it/s] 94%|█████████▍| 255/272 [01:07<00:04,  3.74it/s] 94%|█████████▍| 256/272 [01:08<00:04,  3.72it/s] 94%|█████████▍| 257/272 [01:08<00:04,  3.71it/s] 95%|█████████▍| 258/272 [01:08<00:03,  3.81it/s] 95%|█████████▌| 259/272 [01:08<00:03,  3.88it/s] 96%|█████████▌| 260/272 [01:09<00:03,  3.94it/s] 96%|█████████▌| 261/272 [01:09<00:02,  3.99it/s] 96%|█████████▋| 262/272 [01:09<00:02,  4.01it/s] 97%|█████████▋| 263/272 [01:09<00:02,  4.03it/s] 97%|█████████▋| 264/272 [01:10<00:01,  4.05it/s] 97%|█████████▋| 265/272 [01:10<00:01,  4.06it/s] 98%|█████████▊| 266/272 [01:10<00:01,  4.06it/s] 98%|█████████▊| 267/272 [01:10<00:01,  4.07it/s] 99%|█████████▊| 268/272 [01:11<00:00,  4.07it/s] 99%|█████████▉| 269/272 [01:11<00:00,  4.08it/s] 99%|█████████▉| 270/272 [01:11<00:00,  4.07it/s]100%|█████████▉| 271/272 [01:11<00:00,  4.08it/s]100%|██████████| 272/272 [01:11<00:00,  4.09it/s]accuracy:  0.8492647058823529
100%|██████████| 272/272 [01:16<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 16848.61 examples/s]
Map (num_proc=10):   0%|          | 0/149 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/149 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/149 [00:00<00:41,  3.57 examples/s]Map (num_proc=10):  19%|█▉        | 28/149 [00:00<00:01, 93.16 examples/s]Map (num_proc=10):   1%|          | 1/149 [00:00<00:50,  2.91 examples/s]Map (num_proc=10):  50%|████▉     | 74/149 [00:00<00:00, 204.75 examples/s]Map (num_proc=10):  19%|█▉        | 29/149 [00:00<00:01, 81.82 examples/s]Map (num_proc=10):  76%|███████▌  | 113/149 [00:00<00:00, 241.48 examples/s]Map (num_proc=10):   0%|          | 0/149 [00:00<?, ? examples/s]Map (num_proc=10):  56%|█████▋    | 84/149 [00:00<00:00, 206.76 examples/s]Map (num_proc=10):  97%|█████████▋| 145/149 [00:00<00:00, 245.83 examples/s]Map (num_proc=10):  86%|████████▌ | 128/149 [00:00<00:00, 243.18 examples/s]Map (num_proc=10): 100%|██████████| 149/149 [00:00<00:00, 172.45 examples/s]
Map (num_proc=10):   1%|          | 1/149 [00:00<00:40,  3.64 examples/s]Training dataset size: 48, validation dataset size: 149
Map (num_proc=10): 100%|██████████| 149/149 [00:00<00:00, 161.14 examples/s]
Map (num_proc=10):  15%|█▌        | 23/149 [00:00<00:01, 75.47 examples/s]Training dataset size: 48, validation dataset size: 149
Map (num_proc=10):  30%|██▉       | 44/149 [00:00<00:00, 111.89 examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Map (num_proc=10):  30%|██▉       | 44/149 [00:00<00:01, 76.75 examples/s] 
Training dataset size: 48, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:06:09,023] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:06:09,437] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:06:09,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:20,  2.93s/it] 25%|██▌       | 2/8 [00:05<00:15,  2.64s/it] 38%|███▊      | 3/8 [00:07<00:12,  2.57s/it] 50%|█████     | 4/8 [00:10<00:09,  2.49s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:12<00:07,  2.53s/it] 75%|███████▌  | 6/8 [00:14<00:04,  2.40s/it] 88%|████████▊ | 7/8 [00:17<00:02,  2.51s/it]100%|██████████| 8/8 [00:20<00:00,  2.57s/it]                                             {'train_runtime': 21.0625, 'train_samples_per_second': 4.558, 'train_steps_per_second': 0.38, 'train_loss': 1.0699361562728882, 'epoch': 2.0}
100%|██████████| 8/8 [00:20<00:00,  2.57s/it]100%|██████████| 8/8 [00:20<00:00,  2.61s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 23238.93 examples/s]
Map (num_proc=10):   0%|          | 0/175 [00:00<?, ? examples/s]Map (num_proc=10):   3%|▎         | 5/175 [00:00<00:08, 20.47 examples/s]Map (num_proc=10):  22%|██▏       | 39/175 [00:00<00:00, 137.00 examples/s]Map (num_proc=10):  57%|█████▋    | 99/175 [00:00<00:00, 287.95 examples/s]Map (num_proc=10):  83%|████████▎ | 145/175 [00:00<00:00, 314.68 examples/s]Map (num_proc=10): 100%|██████████| 175/175 [00:00<00:00, 221.18 examples/s]
Map:   0%|          | 0/175 [00:00<?, ? examples/s]Map: 100%|██████████| 175/175 [00:00<00:00, 4331.07 examples/s]
Map:   0%|          | 0/175 [00:00<?, ? examples/s]Map: 100%|██████████| 175/175 [00:00<00:00, 4428.11 examples/s]
Map:   0%|          | 0/175 [00:00<?, ? examples/s]Map: 100%|██████████| 175/175 [00:00<00:00, 4057.69 examples/s]
Filter (num_proc=10):   0%|          | 0/175 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 18/175 [00:00<00:01, 81.95 examples/s]Filter (num_proc=10):  41%|████      | 72/175 [00:00<00:00, 222.21 examples/s]Filter (num_proc=10):  81%|████████  | 141/175 [00:00<00:00, 359.82 examples/s]Filter (num_proc=10): 100%|██████████| 175/175 [00:00<00:00, 273.65 examples/s]
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<01:02,  2.77it/s]  1%|          | 2/175 [00:00<00:50,  3.44it/s]  2%|▏         | 3/175 [00:00<00:45,  3.74it/s]  2%|▏         | 4/175 [00:01<00:43,  3.90it/s]  3%|▎         | 5/175 [00:01<00:42,  3.99it/s]  3%|▎         | 6/175 [00:01<00:41,  4.04it/s]  4%|▍         | 7/175 [00:01<00:41,  4.08it/s]  5%|▍         | 8/175 [00:02<00:40,  4.11it/s]  5%|▌         | 9/175 [00:02<00:40,  4.12it/s]  6%|▌         | 10/175 [00:02<00:39,  4.13it/s]  6%|▋         | 11/175 [00:02<00:39,  4.15it/s]  7%|▋         | 12/175 [00:03<00:39,  4.15it/s]  7%|▋         | 13/175 [00:03<00:38,  4.15it/s]  8%|▊         | 14/175 [00:03<00:38,  4.16it/s]  9%|▊         | 15/175 [00:03<00:38,  4.16it/s]  9%|▉         | 16/175 [00:03<00:38,  4.16it/s] 10%|▉         | 17/175 [00:04<00:38,  4.16it/s] 10%|█         | 18/175 [00:04<00:38,  4.06it/s] 11%|█         | 19/175 [00:04<00:39,  3.95it/s] 11%|█▏        | 20/175 [00:04<00:39,  3.90it/s] 12%|█▏        | 21/175 [00:05<00:38,  3.97it/s] 13%|█▎        | 22/175 [00:05<00:39,  3.90it/s] 13%|█▎        | 23/175 [00:05<00:39,  3.83it/s] 14%|█▎        | 24/175 [00:06<00:40,  3.76it/s] 14%|█▍        | 25/175 [00:06<00:38,  3.86it/s] 15%|█▍        | 26/175 [00:06<00:38,  3.86it/s] 15%|█▌        | 27/175 [00:06<00:38,  3.82it/s] 16%|█▌        | 28/175 [00:07<00:38,  3.84it/s] 17%|█▋        | 29/175 [00:07<00:38,  3.78it/s] 17%|█▋        | 30/175 [00:07<00:37,  3.84it/s] 18%|█▊        | 31/175 [00:07<00:37,  3.80it/s] 18%|█▊        | 32/175 [00:08<00:37,  3.81it/s] 19%|█▉        | 33/175 [00:08<00:38,  3.74it/s] 19%|█▉        | 34/175 [00:08<00:36,  3.85it/s] 20%|██        | 35/175 [00:08<00:36,  3.81it/s] 21%|██        | 36/175 [00:09<00:36,  3.78it/s] 21%|██        | 37/175 [00:09<00:37,  3.72it/s] 22%|██▏       | 38/175 [00:09<00:35,  3.83it/s] 22%|██▏       | 39/175 [00:09<00:35,  3.84it/s] 23%|██▎       | 40/175 [00:10<00:35,  3.80it/s] 23%|██▎       | 41/175 [00:10<00:35,  3.81it/s] 24%|██▍       | 42/175 [00:10<00:35,  3.77it/s] 25%|██▍       | 43/175 [00:11<00:34,  3.84it/s] 25%|██▌       | 44/175 [00:11<00:34,  3.80it/s] 26%|██▌       | 45/175 [00:11<00:33,  3.82it/s] 26%|██▋       | 46/175 [00:11<00:34,  3.75it/s] 27%|██▋       | 47/175 [00:12<00:33,  3.84it/s] 27%|██▋       | 48/175 [00:12<00:33,  3.81it/s] 28%|██▊       | 49/175 [00:12<00:32,  3.82it/s] 29%|██▊       | 50/175 [00:12<00:32,  3.79it/s] 29%|██▉       | 51/175 [00:13<00:31,  3.89it/s] 30%|██▉       | 52/175 [00:13<00:31,  3.96it/s] 30%|███       | 53/175 [00:13<00:30,  4.00it/s] 31%|███       | 54/175 [00:13<00:30,  4.02it/s] 31%|███▏      | 55/175 [00:14<00:29,  4.04it/s] 32%|███▏      | 56/175 [00:14<00:29,  4.06it/s] 33%|███▎      | 57/175 [00:14<00:28,  4.08it/s] 33%|███▎      | 58/175 [00:14<00:28,  4.08it/s] 34%|███▎      | 59/175 [00:15<00:28,  4.08it/s] 34%|███▍      | 60/175 [00:15<00:28,  4.09it/s] 35%|███▍      | 61/175 [00:15<00:28,  4.01it/s] 35%|███▌      | 62/175 [00:15<00:28,  3.92it/s] 36%|███▌      | 63/175 [00:16<00:29,  3.86it/s] 37%|███▋      | 64/175 [00:16<00:28,  3.85it/s] 37%|███▋      | 65/175 [00:16<00:28,  3.92it/s] 38%|███▊      | 66/175 [00:16<00:27,  3.93it/s] 38%|███▊      | 67/175 [00:17<00:27,  3.86it/s] 39%|███▉      | 68/175 [00:17<00:28,  3.80it/s] 39%|███▉      | 69/175 [00:17<00:28,  3.74it/s] 40%|████      | 70/175 [00:17<00:27,  3.80it/s] 41%|████      | 71/175 [00:18<00:26,  3.88it/s] 41%|████      | 72/175 [00:18<00:27,  3.80it/s] 42%|████▏     | 73/175 [00:18<00:26,  3.79it/s] 42%|████▏     | 74/175 [00:18<00:26,  3.80it/s] 43%|████▎     | 75/175 [00:19<00:26,  3.73it/s] 43%|████▎     | 76/175 [00:19<00:25,  3.84it/s] 44%|████▍     | 77/175 [00:19<00:25,  3.85it/s] 45%|████▍     | 78/175 [00:20<00:25,  3.81it/s] 45%|████▌     | 79/175 [00:20<00:25,  3.76it/s] 46%|████▌     | 80/175 [00:20<00:25,  3.70it/s] 46%|████▋     | 81/175 [00:20<00:24,  3.79it/s] 47%|████▋     | 82/175 [00:21<00:24,  3.87it/s] 47%|████▋     | 83/175 [00:21<00:24,  3.79it/s] 48%|████▊     | 84/175 [00:21<00:24,  3.77it/s] 49%|████▊     | 85/175 [00:21<00:23,  3.79it/s] 49%|████▉     | 86/175 [00:22<00:23,  3.72it/s] 50%|████▉     | 87/175 [00:22<00:22,  3.84it/s] 50%|█████     | 88/175 [00:22<00:22,  3.84it/s] 51%|█████     | 89/175 [00:22<00:22,  3.79it/s] 51%|█████▏    | 90/175 [00:23<00:22,  3.75it/s] 52%|█████▏    | 91/175 [00:23<00:22,  3.68it/s] 53%|█████▎    | 92/175 [00:23<00:21,  3.79it/s] 53%|█████▎    | 93/175 [00:24<00:21,  3.84it/s] 54%|█████▎    | 94/175 [00:24<00:21,  3.80it/s] 54%|█████▍    | 95/175 [00:24<00:21,  3.76it/s] 55%|█████▍    | 96/175 [00:24<00:21,  3.69it/s] 55%|█████▌    | 97/175 [00:25<00:20,  3.79it/s] 56%|█████▌    | 98/175 [00:25<00:19,  3.85it/s] 57%|█████▋    | 99/175 [00:25<00:20,  3.80it/s] 57%|█████▋    | 100/175 [00:25<00:19,  3.75it/s] 58%|█████▊    | 101/175 [00:26<00:20,  3.68it/s] 58%|█████▊    | 102/175 [00:26<00:19,  3.78it/s] 59%|█████▉    | 103/175 [00:26<00:18,  3.86it/s] 59%|█████▉    | 104/175 [00:26<00:18,  3.80it/s] 60%|██████    | 105/175 [00:27<00:18,  3.74it/s] 61%|██████    | 106/175 [00:27<00:18,  3.79it/s] 61%|██████    | 107/175 [00:27<00:18,  3.72it/s] 62%|██████▏   | 108/175 [00:27<00:17,  3.84it/s] 62%|██████▏   | 109/175 [00:28<00:17,  3.83it/s] 63%|██████▎   | 110/175 [00:28<00:17,  3.79it/s] 63%|██████▎   | 111/175 [00:28<00:16,  3.79it/s] 64%|██████▍   | 112/175 [00:29<00:16,  3.72it/s] 65%|██████▍   | 113/175 [00:29<00:16,  3.83it/s] 65%|██████▌   | 114/175 [00:29<00:15,  3.84it/s] 66%|██████▌   | 115/175 [00:29<00:15,  3.80it/s] 66%|██████▋   | 116/175 [00:30<00:15,  3.75it/s] 67%|██████▋   | 117/175 [00:30<00:15,  3.68it/s] 67%|██████▋   | 118/175 [00:30<00:15,  3.79it/s] 68%|██████▊   | 119/175 [00:30<00:14,  3.87it/s] 69%|██████▊   | 120/175 [00:31<00:14,  3.82it/s] 69%|██████▉   | 121/175 [00:31<00:14,  3.76it/s] 70%|██████▉   | 122/175 [00:31<00:14,  3.69it/s] 70%|███████   | 123/175 [00:31<00:13,  3.80it/s] 71%|███████   | 124/175 [00:32<00:13,  3.86it/s] 71%|███████▏  | 125/175 [00:32<00:13,  3.82it/s] 72%|███████▏  | 126/175 [00:32<00:13,  3.76it/s] 73%|███████▎  | 127/175 [00:33<00:13,  3.69it/s] 73%|███████▎  | 128/175 [00:33<00:12,  3.80it/s] 74%|███████▎  | 129/175 [00:33<00:11,  3.85it/s] 74%|███████▍  | 130/175 [00:33<00:11,  3.80it/s] 75%|███████▍  | 131/175 [00:34<00:11,  3.76it/s] 75%|███████▌  | 132/175 [00:34<00:11,  3.68it/s] 76%|███████▌  | 133/175 [00:34<00:11,  3.81it/s] 77%|███████▋  | 134/175 [00:34<00:10,  3.89it/s] 77%|███████▋  | 135/175 [00:35<00:10,  3.83it/s] 78%|███████▊  | 136/175 [00:35<00:10,  3.78it/s] 78%|███████▊  | 137/175 [00:35<00:09,  3.81it/s] 79%|███████▉  | 138/175 [00:35<00:09,  3.73it/s] 79%|███████▉  | 139/175 [00:36<00:09,  3.83it/s] 80%|████████  | 140/175 [00:36<00:09,  3.81it/s] 81%|████████  | 141/175 [00:36<00:09,  3.76it/s] 81%|████████  | 142/175 [00:36<00:08,  3.77it/s] 82%|████████▏ | 143/175 [00:37<00:08,  3.70it/s] 82%|████████▏ | 144/175 [00:37<00:08,  3.82it/s] 83%|████████▎ | 145/175 [00:37<00:07,  3.83it/s] 83%|████████▎ | 146/175 [00:38<00:07,  3.79it/s] 84%|████████▍ | 147/175 [00:38<00:07,  3.75it/s] 85%|████████▍ | 148/175 [00:38<00:07,  3.67it/s] 85%|████████▌ | 149/175 [00:38<00:06,  3.79it/s] 86%|████████▌ | 150/175 [00:39<00:06,  3.85it/s] 86%|████████▋ | 151/175 [00:39<00:06,  3.80it/s] 87%|████████▋ | 152/175 [00:39<00:06,  3.75it/s] 87%|████████▋ | 153/175 [00:39<00:05,  3.67it/s] 88%|████████▊ | 154/175 [00:40<00:05,  3.80it/s] 89%|████████▊ | 155/175 [00:40<00:05,  3.87it/s] 89%|████████▉ | 156/175 [00:40<00:04,  3.82it/s] 90%|████████▉ | 157/175 [00:40<00:04,  3.76it/s] 90%|█████████ | 158/175 [00:41<00:04,  3.67it/s] 91%|█████████ | 159/175 [00:41<00:04,  3.80it/s] 91%|█████████▏| 160/175 [00:41<00:03,  3.89it/s] 92%|█████████▏| 161/175 [00:41<00:03,  3.81it/s] 93%|█████████▎| 162/175 [00:42<00:03,  3.79it/s] 93%|█████████▎| 163/175 [00:42<00:03,  3.80it/s] 94%|█████████▎| 164/175 [00:42<00:02,  3.72it/s] 94%|█████████▍| 165/175 [00:43<00:02,  3.81it/s] 95%|█████████▍| 166/175 [00:43<00:02,  3.79it/s] 95%|█████████▌| 167/175 [00:43<00:02,  3.74it/s] 96%|█████████▌| 168/175 [00:43<00:01,  3.77it/s] 97%|█████████▋| 169/175 [00:44<00:01,  3.70it/s] 97%|█████████▋| 170/175 [00:44<00:01,  3.82it/s] 98%|█████████▊| 171/175 [00:44<00:01,  3.82it/s] 98%|█████████▊| 172/175 [00:44<00:00,  3.74it/s] 99%|█████████▉| 173/175 [00:45<00:00,  3.74it/s] 99%|█████████▉| 174/175 [00:45<00:00,  3.66it/s]100%|██████████| 175/175 [00:45<00:00,  3.80it/s]accuracy:  0.8
100%|██████████| 175/175 [00:48<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10):   0%|          | 0/2575 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 15281.86 examples/s]
Filter (num_proc=10):  80%|████████  | 2061/2575 [00:00<00:00, 20323.03 examples/s]Filter (num_proc=10):  90%|█████████ | 2318/2575 [00:00<00:00, 21768.40 examples/s]Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 14505.70 examples/s]
Filter (num_proc=10): 100%|██████████| 2575/2575 [00:00<00:00, 13391.03 examples/s]
Map (num_proc=10):   0%|          | 0/119 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/119 [00:00<?, ? examples/s]Map (num_proc=10):   0%|          | 0/119 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/119 [00:00<00:32,  3.64 examples/s]Map (num_proc=10):  21%|██        | 25/119 [00:00<00:01, 76.56 examples/s]Map (num_proc=10):   1%|          | 1/119 [00:00<00:37,  3.18 examples/s]Map (num_proc=10):   2%|▏         | 2/119 [00:00<00:20,  5.79 examples/s]Map (num_proc=10):  50%|████▉     | 59/119 [00:00<00:00, 146.90 examples/s]Map (num_proc=10):  27%|██▋       | 32/119 [00:00<00:01, 84.82 examples/s]Map (num_proc=10):  18%|█▊        | 21/119 [00:00<00:01, 58.04 examples/s]Map (num_proc=10):  76%|███████▋  | 91/119 [00:00<00:00, 194.51 examples/s]Map (num_proc=10):  52%|█████▏    | 62/119 [00:00<00:00, 140.63 examples/s]Map (num_proc=10):  97%|█████████▋| 115/119 [00:00<00:00, 201.55 examples/s]Map (num_proc=10):  58%|█████▊    | 69/119 [00:00<00:00, 163.07 examples/s]Map (num_proc=10):  80%|███████▉  | 95/119 [00:00<00:00, 185.16 examples/s]Map (num_proc=10): 100%|██████████| 119/119 [00:00<00:00, 137.24 examples/s]
Map (num_proc=10):  93%|█████████▎| 111/119 [00:00<00:00, 216.12 examples/s]Training dataset size: 48, validation dataset size: 119
Map (num_proc=10): 100%|██████████| 119/119 [00:00<00:00, 139.06 examples/s]
Map (num_proc=10): 100%|██████████| 119/119 [00:00<00:00, 141.14 examples/s]
Training dataset size: 48, validation dataset size: 119
Training dataset size: 48, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.33s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:07:49,755] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:07:50,300] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:07:51,497] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/8 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
 12%|█▎        | 1/8 [00:02<00:18,  2.66s/it] 25%|██▌       | 2/8 [00:04<00:14,  2.38s/it] 38%|███▊      | 3/8 [00:07<00:11,  2.32s/it] 50%|█████     | 4/8 [00:09<00:09,  2.26s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 62%|██████▎   | 5/8 [00:11<00:07,  2.35s/it] 75%|███████▌  | 6/8 [00:14<00:05,  2.55s/it] 88%|████████▊ | 7/8 [00:16<00:02,  2.32s/it]100%|██████████| 8/8 [00:18<00:00,  2.35s/it]                                             {'train_runtime': 19.6581, 'train_samples_per_second': 4.883, 'train_steps_per_second': 0.407, 'train_loss': 0.8009598255157471, 'epoch': 2.0}
100%|██████████| 8/8 [00:19<00:00,  2.35s/it]100%|██████████| 8/8 [00:19<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Filter (num_proc=10):   0%|          | 0/3355 [00:00<?, ? examples/s]Filter (num_proc=10): 100%|██████████| 3355/3355 [00:00<00:00, 23012.57 examples/s]
Map (num_proc=10):   0%|          | 0/153 [00:00<?, ? examples/s]Map (num_proc=10):   1%|          | 1/153 [00:00<00:41,  3.70 examples/s]Map (num_proc=10):  33%|███▎      | 50/153 [00:00<00:00, 159.54 examples/s]Map (num_proc=10):  63%|██████▎   | 97/153 [00:00<00:00, 251.52 examples/s]Map (num_proc=10):  92%|█████████▏| 141/153 [00:00<00:00, 284.20 examples/s]Map (num_proc=10): 100%|██████████| 153/153 [00:00<00:00, 204.46 examples/s]
Map:   0%|          | 0/153 [00:00<?, ? examples/s]Map: 100%|██████████| 153/153 [00:00<00:00, 3977.20 examples/s]
Map:   0%|          | 0/153 [00:00<?, ? examples/s]Map: 100%|██████████| 153/153 [00:00<00:00, 4228.74 examples/s]
Map:   0%|          | 0/153 [00:00<?, ? examples/s]Map: 100%|██████████| 153/153 [00:00<00:00, 4354.66 examples/s]
Filter (num_proc=10):   0%|          | 0/153 [00:00<?, ? examples/s]Filter (num_proc=10):  10%|█         | 16/153 [00:00<00:01, 74.22 examples/s]Filter (num_proc=10):  41%|████      | 63/153 [00:00<00:00, 219.66 examples/s]Filter (num_proc=10):  71%|███████   | 108/153 [00:00<00:00, 287.65 examples/s]Filter (num_proc=10): 100%|██████████| 153/153 [00:00<00:00, 325.82 examples/s]Filter (num_proc=10): 100%|██████████| 153/153 [00:00<00:00, 243.20 examples/s]
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:49,  3.09it/s]  1%|▏         | 2/153 [00:00<00:41,  3.62it/s]  2%|▏         | 3/153 [00:00<00:39,  3.83it/s]  3%|▎         | 4/153 [00:01<00:37,  3.94it/s]  3%|▎         | 5/153 [00:01<00:36,  4.01it/s]  4%|▍         | 6/153 [00:01<00:36,  4.06it/s]  5%|▍         | 7/153 [00:01<00:35,  4.09it/s]  5%|▌         | 8/153 [00:02<00:35,  4.11it/s]  6%|▌         | 9/153 [00:02<00:34,  4.12it/s]  7%|▋         | 10/153 [00:02<00:34,  4.11it/s]  7%|▋         | 11/153 [00:02<00:34,  4.11it/s]  8%|▊         | 12/153 [00:02<00:34,  4.11it/s]  8%|▊         | 13/153 [00:03<00:34,  4.11it/s]  9%|▉         | 14/153 [00:03<00:33,  4.12it/s] 10%|▉         | 15/153 [00:03<00:33,  4.13it/s] 10%|█         | 16/153 [00:03<00:33,  4.13it/s] 11%|█         | 17/153 [00:04<00:32,  4.14it/s] 12%|█▏        | 18/153 [00:04<00:32,  4.14it/s] 12%|█▏        | 19/153 [00:04<00:32,  4.14it/s] 13%|█▎        | 20/153 [00:04<00:32,  4.13it/s] 14%|█▎        | 21/153 [00:05<00:32,  4.12it/s] 14%|█▍        | 22/153 [00:05<00:31,  4.11it/s] 15%|█▌        | 23/153 [00:05<00:31,  4.11it/s] 16%|█▌        | 24/153 [00:05<00:31,  4.12it/s] 16%|█▋        | 25/153 [00:06<00:31,  4.11it/s] 17%|█▋        | 26/153 [00:06<00:31,  4.09it/s] 18%|█▊        | 27/153 [00:06<00:31,  4.05it/s] 18%|█▊        | 28/153 [00:06<00:31,  3.92it/s] 19%|█▉        | 29/153 [00:07<00:32,  3.82it/s] 20%|█▉        | 30/153 [00:07<00:31,  3.87it/s] 20%|██        | 31/153 [00:07<00:31,  3.85it/s] 21%|██        | 32/153 [00:07<00:32,  3.74it/s] 22%|██▏       | 33/153 [00:08<00:32,  3.66it/s] 22%|██▏       | 34/153 [00:08<00:31,  3.78it/s] 23%|██▎       | 35/153 [00:08<00:31,  3.77it/s] 24%|██▎       | 36/153 [00:09<00:31,  3.69it/s] 24%|██▍       | 37/153 [00:09<00:31,  3.63it/s] 25%|██▍       | 38/153 [00:09<00:30,  3.75it/s] 25%|██▌       | 39/153 [00:09<00:30,  3.76it/s] 26%|██▌       | 40/153 [00:10<00:30,  3.68it/s] 27%|██▋       | 41/153 [00:10<00:30,  3.63it/s] 27%|██▋       | 42/153 [00:10<00:29,  3.76it/s] 28%|██▊       | 43/153 [00:10<00:29,  3.73it/s] 29%|██▉       | 44/153 [00:11<00:29,  3.69it/s] 29%|██▉       | 45/153 [00:11<00:29,  3.63it/s] 30%|███       | 46/153 [00:11<00:28,  3.77it/s] 31%|███       | 47/153 [00:12<00:28,  3.72it/s] 31%|███▏      | 48/153 [00:12<00:28,  3.69it/s] 32%|███▏      | 49/153 [00:12<00:28,  3.66it/s] 33%|███▎      | 50/153 [00:12<00:27,  3.80it/s] 33%|███▎      | 51/153 [00:13<00:27,  3.73it/s] 34%|███▍      | 52/153 [00:13<00:27,  3.70it/s] 35%|███▍      | 53/153 [00:13<00:27,  3.67it/s] 35%|███▌      | 54/153 [00:13<00:26,  3.80it/s] 36%|███▌      | 55/153 [00:14<00:26,  3.72it/s] 37%|███▋      | 56/153 [00:14<00:26,  3.70it/s] 37%|███▋      | 57/153 [00:14<00:26,  3.67it/s] 38%|███▊      | 58/153 [00:14<00:25,  3.78it/s] 39%|███▊      | 59/153 [00:15<00:25,  3.73it/s] 39%|███▉      | 60/153 [00:15<00:25,  3.69it/s] 40%|███▉      | 61/153 [00:15<00:25,  3.67it/s] 41%|████      | 62/153 [00:16<00:24,  3.77it/s] 41%|████      | 63/153 [00:16<00:24,  3.67it/s] 42%|████▏     | 64/153 [00:16<00:23,  3.73it/s] 42%|████▏     | 65/153 [00:16<00:23,  3.69it/s] 43%|████▎     | 66/153 [00:17<00:23,  3.74it/s] 44%|████▍     | 67/153 [00:17<00:23,  3.67it/s] 44%|████▍     | 68/153 [00:17<00:23,  3.61it/s] 45%|████▌     | 69/153 [00:17<00:22,  3.74it/s] 46%|████▌     | 70/153 [00:18<00:22,  3.75it/s] 46%|████▋     | 71/153 [00:18<00:22,  3.67it/s] 47%|████▋     | 72/153 [00:18<00:22,  3.61it/s] 48%|████▊     | 73/153 [00:19<00:21,  3.74it/s] 48%|████▊     | 74/153 [00:19<00:21,  3.74it/s] 49%|████▉     | 75/153 [00:19<00:21,  3.67it/s] 50%|████▉     | 76/153 [00:19<00:21,  3.61it/s] 50%|█████     | 77/153 [00:20<00:20,  3.75it/s] 51%|█████     | 78/153 [00:20<00:19,  3.75it/s] 52%|█████▏    | 79/153 [00:20<00:20,  3.67it/s] 52%|█████▏    | 80/153 [00:20<00:20,  3.62it/s] 53%|█████▎    | 81/153 [00:21<00:19,  3.76it/s] 54%|█████▎    | 82/153 [00:21<00:18,  3.76it/s] 54%|█████▍    | 83/153 [00:21<00:19,  3.68it/s] 55%|█████▍    | 84/153 [00:22<00:19,  3.63it/s] 56%|█████▌    | 85/153 [00:22<00:18,  3.76it/s] 56%|█████▌    | 86/153 [00:22<00:17,  3.77it/s] 57%|█████▋    | 87/153 [00:22<00:17,  3.68it/s] 58%|█████▊    | 88/153 [00:23<00:17,  3.62it/s] 58%|█████▊    | 89/153 [00:23<00:17,  3.76it/s] 59%|█████▉    | 90/153 [00:23<00:16,  3.72it/s] 59%|█████▉    | 91/153 [00:23<00:16,  3.68it/s] 60%|██████    | 92/153 [00:24<00:16,  3.63it/s] 61%|██████    | 93/153 [00:24<00:15,  3.77it/s] 61%|██████▏   | 94/153 [00:24<00:15,  3.76it/s] 62%|██████▏   | 95/153 [00:24<00:15,  3.68it/s] 63%|██████▎   | 96/153 [00:25<00:15,  3.62it/s] 63%|██████▎   | 97/153 [00:25<00:14,  3.76it/s] 64%|██████▍   | 98/153 [00:25<00:14,  3.74it/s] 65%|██████▍   | 99/153 [00:26<00:14,  3.68it/s] 65%|██████▌   | 100/153 [00:26<00:14,  3.62it/s] 66%|██████▌   | 101/153 [00:26<00:13,  3.76it/s] 67%|██████▋   | 102/153 [00:26<00:13,  3.76it/s] 67%|██████▋   | 103/153 [00:27<00:13,  3.67it/s] 68%|██████▊   | 104/153 [00:27<00:13,  3.62it/s] 69%|██████▊   | 105/153 [00:27<00:12,  3.76it/s] 69%|██████▉   | 106/153 [00:27<00:12,  3.72it/s] 70%|██████▉   | 107/153 [00:28<00:12,  3.68it/s] 71%|███████   | 108/153 [00:28<00:12,  3.62it/s] 71%|███████   | 109/153 [00:28<00:11,  3.76it/s] 72%|███████▏  | 110/153 [00:29<00:11,  3.75it/s] 73%|███████▎  | 111/153 [00:29<00:11,  3.66it/s] 73%|███████▎  | 112/153 [00:29<00:11,  3.60it/s] 74%|███████▍  | 113/153 [00:29<00:10,  3.74it/s] 75%|███████▍  | 114/153 [00:30<00:10,  3.71it/s] 75%|███████▌  | 115/153 [00:30<00:10,  3.66it/s] 76%|███████▌  | 116/153 [00:30<00:10,  3.61it/s] 76%|███████▋  | 117/153 [00:30<00:09,  3.75it/s] 77%|███████▋  | 118/153 [00:31<00:09,  3.75it/s] 78%|███████▊  | 119/153 [00:31<00:09,  3.67it/s] 78%|███████▊  | 120/153 [00:31<00:09,  3.61it/s] 79%|███████▉  | 121/153 [00:32<00:08,  3.75it/s] 80%|███████▉  | 122/153 [00:32<00:08,  3.73it/s] 80%|████████  | 123/153 [00:32<00:08,  3.67it/s] 81%|████████  | 124/153 [00:32<00:08,  3.61it/s] 82%|████████▏ | 125/153 [00:33<00:07,  3.76it/s] 82%|████████▏ | 126/153 [00:33<00:07,  3.77it/s] 83%|████████▎ | 127/153 [00:33<00:07,  3.69it/s] 84%|████████▎ | 128/153 [00:33<00:06,  3.62it/s] 84%|████████▍ | 129/153 [00:34<00:06,  3.76it/s] 85%|████████▍ | 130/153 [00:34<00:06,  3.77it/s] 86%|████████▌ | 131/153 [00:34<00:05,  3.68it/s] 86%|████████▋ | 132/153 [00:35<00:05,  3.69it/s] 87%|████████▋ | 133/153 [00:35<00:05,  3.80it/s] 88%|████████▊ | 134/153 [00:35<00:04,  3.87it/s] 88%|████████▊ | 135/153 [00:35<00:04,  3.94it/s] 89%|████████▉ | 136/153 [00:35<00:04,  3.99it/s] 90%|████████▉ | 137/153 [00:36<00:03,  4.02it/s] 90%|█████████ | 138/153 [00:36<00:03,  4.04it/s] 91%|█████████ | 139/153 [00:36<00:03,  4.05it/s] 92%|█████████▏| 140/153 [00:36<00:03,  4.07it/s] 92%|█████████▏| 141/153 [00:37<00:02,  4.09it/s] 93%|█████████▎| 142/153 [00:37<00:02,  4.08it/s] 93%|█████████▎| 143/153 [00:37<00:02,  4.07it/s] 94%|█████████▍| 144/153 [00:37<00:02,  4.08it/s] 95%|█████████▍| 145/153 [00:38<00:01,  4.09it/s] 95%|█████████▌| 146/153 [00:38<00:01,  4.10it/s] 96%|█████████▌| 147/153 [00:38<00:01,  4.09it/s] 97%|█████████▋| 148/153 [00:38<00:01,  4.08it/s] 97%|█████████▋| 149/153 [00:39<00:00,  4.09it/s] 98%|█████████▊| 150/153 [00:39<00:00,  4.10it/s] 99%|█████████▊| 151/153 [00:39<00:00,  4.11it/s] 99%|█████████▉| 152/153 [00:39<00:00,  4.09it/s]100%|██████████| 153/153 [00:40<00:00,  4.09it/s]accuracy:  0.6535947712418301
100%|██████████| 153/153 [00:42<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 152
Training dataset size: 96, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.26s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:09:21,565] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:09:21,768] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:09:23,814] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:33,  2.24s/it] 12%|█▎        | 2/16 [00:04<00:32,  2.34s/it] 19%|█▉        | 3/16 [00:07<00:30,  2.36s/it] 25%|██▌       | 4/16 [00:08<00:26,  2.17s/it] 31%|███▏      | 5/16 [00:11<00:24,  2.27s/it] 38%|███▊      | 6/16 [00:13<00:22,  2.21s/it] 44%|████▍     | 7/16 [00:15<00:19,  2.15s/it] 50%|█████     | 8/16 [00:17<00:16,  2.07s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:19<00:15,  2.16s/it] 62%|██████▎   | 10/16 [00:21<00:12,  2.16s/it]                                               {'loss': 1.1088, 'grad_norm': 18.3978214263916, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:21<00:12,  2.16s/it] 69%|██████▉   | 11/16 [00:23<00:10,  2.10s/it] 75%|███████▌  | 12/16 [00:26<00:09,  2.34s/it] 81%|████████▏ | 13/16 [00:29<00:07,  2.39s/it] 88%|████████▊ | 14/16 [00:31<00:04,  2.38s/it] 94%|█████████▍| 15/16 [00:33<00:02,  2.33s/it]100%|██████████| 16/16 [00:36<00:00,  2.36s/it]                                               {'train_runtime': 36.9024, 'train_samples_per_second': 5.203, 'train_steps_per_second': 0.434, 'train_loss': 0.9990070462226868, 'epoch': 2.0}
100%|██████████| 16/16 [00:36<00:00,  2.36s/it]100%|██████████| 16/16 [00:36<00:00,  2.29s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:17,  2.71it/s]  1%|          | 2/212 [00:00<01:01,  3.41it/s]  1%|▏         | 3/212 [00:00<00:59,  3.53it/s]  2%|▏         | 4/212 [00:01<00:57,  3.61it/s]  2%|▏         | 5/212 [00:01<00:57,  3.61it/s]  3%|▎         | 6/212 [00:01<00:54,  3.77it/s]  3%|▎         | 7/212 [00:01<00:54,  3.76it/s]  4%|▍         | 8/212 [00:02<00:54,  3.74it/s]  4%|▍         | 9/212 [00:02<00:54,  3.70it/s]  5%|▍         | 10/212 [00:02<00:52,  3.82it/s]  5%|▌         | 11/212 [00:02<00:53,  3.79it/s]  6%|▌         | 12/212 [00:03<00:53,  3.77it/s]  6%|▌         | 13/212 [00:03<00:53,  3.71it/s]  7%|▋         | 14/212 [00:03<00:51,  3.83it/s]  7%|▋         | 15/212 [00:04<00:51,  3.81it/s]  8%|▊         | 16/212 [00:04<00:51,  3.78it/s]  8%|▊         | 17/212 [00:04<00:52,  3.72it/s]  8%|▊         | 18/212 [00:04<00:50,  3.83it/s]  9%|▉         | 19/212 [00:05<00:50,  3.79it/s]  9%|▉         | 20/212 [00:05<00:50,  3.77it/s] 10%|▉         | 21/212 [00:05<00:51,  3.72it/s] 10%|█         | 22/212 [00:05<00:49,  3.82it/s] 11%|█         | 23/212 [00:06<00:50,  3.77it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.76it/s] 12%|█▏        | 25/212 [00:06<00:50,  3.71it/s] 12%|█▏        | 26/212 [00:06<00:48,  3.83it/s] 13%|█▎        | 27/212 [00:07<00:48,  3.78it/s] 13%|█▎        | 28/212 [00:07<00:48,  3.77it/s] 14%|█▎        | 29/212 [00:07<00:49,  3.71it/s] 14%|█▍        | 30/212 [00:08<00:47,  3.83it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.79it/s] 15%|█▌        | 32/212 [00:08<00:47,  3.77it/s] 16%|█▌        | 33/212 [00:08<00:48,  3.72it/s] 16%|█▌        | 34/212 [00:09<00:46,  3.83it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.78it/s] 17%|█▋        | 36/212 [00:09<00:46,  3.76it/s] 17%|█▋        | 37/212 [00:09<00:47,  3.72it/s] 18%|█▊        | 38/212 [00:10<00:45,  3.83it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.78it/s] 19%|█▉        | 40/212 [00:10<00:45,  3.76it/s] 19%|█▉        | 41/212 [00:10<00:46,  3.71it/s] 20%|█▉        | 42/212 [00:11<00:44,  3.83it/s] 20%|██        | 43/212 [00:11<00:44,  3.77it/s] 21%|██        | 44/212 [00:11<00:44,  3.76it/s] 21%|██        | 45/212 [00:12<00:45,  3.71it/s] 22%|██▏       | 46/212 [00:12<00:43,  3.83it/s] 22%|██▏       | 47/212 [00:12<00:43,  3.78it/s] 23%|██▎       | 48/212 [00:12<00:43,  3.75it/s] 23%|██▎       | 49/212 [00:13<00:44,  3.70it/s] 24%|██▎       | 50/212 [00:13<00:42,  3.82it/s] 24%|██▍       | 51/212 [00:13<00:42,  3.77it/s] 25%|██▍       | 52/212 [00:13<00:42,  3.76it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.70it/s] 25%|██▌       | 54/212 [00:14<00:41,  3.81it/s] 26%|██▌       | 55/212 [00:14<00:41,  3.75it/s] 26%|██▋       | 56/212 [00:14<00:41,  3.74it/s] 27%|██▋       | 57/212 [00:15<00:41,  3.69it/s] 27%|██▋       | 58/212 [00:15<00:40,  3.81it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.76it/s] 28%|██▊       | 60/212 [00:16<00:41,  3.69it/s] 29%|██▉       | 61/212 [00:16<00:39,  3.82it/s] 29%|██▉       | 62/212 [00:16<00:38,  3.91it/s] 30%|██▉       | 63/212 [00:16<00:37,  3.98it/s] 30%|███       | 64/212 [00:16<00:36,  4.02it/s] 31%|███       | 65/212 [00:17<00:36,  4.05it/s] 31%|███       | 66/212 [00:17<00:35,  4.06it/s] 32%|███▏      | 67/212 [00:17<00:35,  4.07it/s] 32%|███▏      | 68/212 [00:17<00:35,  4.08it/s] 33%|███▎      | 69/212 [00:18<00:34,  4.10it/s] 33%|███▎      | 70/212 [00:18<00:34,  4.11it/s] 33%|███▎      | 71/212 [00:18<00:34,  4.11it/s] 34%|███▍      | 72/212 [00:18<00:34,  4.10it/s] 34%|███▍      | 73/212 [00:19<00:33,  4.10it/s] 35%|███▍      | 74/212 [00:19<00:33,  4.10it/s] 35%|███▌      | 75/212 [00:19<00:33,  4.11it/s] 36%|███▌      | 76/212 [00:19<00:33,  4.12it/s] 36%|███▋      | 77/212 [00:20<00:32,  4.12it/s] 37%|███▋      | 78/212 [00:20<00:32,  4.11it/s] 37%|███▋      | 79/212 [00:20<00:32,  4.10it/s] 38%|███▊      | 80/212 [00:20<00:32,  4.10it/s] 38%|███▊      | 81/212 [00:21<00:31,  4.10it/s] 39%|███▊      | 82/212 [00:21<00:31,  4.10it/s] 39%|███▉      | 83/212 [00:21<00:31,  4.09it/s] 40%|███▉      | 84/212 [00:21<00:31,  4.09it/s] 40%|████      | 85/212 [00:22<00:30,  4.10it/s] 41%|████      | 86/212 [00:22<00:30,  4.10it/s] 41%|████      | 87/212 [00:22<00:30,  4.10it/s] 42%|████▏     | 88/212 [00:22<00:30,  4.09it/s] 42%|████▏     | 89/212 [00:23<00:30,  4.09it/s] 42%|████▏     | 90/212 [00:23<00:29,  4.10it/s] 43%|████▎     | 91/212 [00:23<00:29,  4.10it/s] 43%|████▎     | 92/212 [00:23<00:29,  4.09it/s] 44%|████▍     | 93/212 [00:24<00:29,  4.04it/s] 44%|████▍     | 94/212 [00:24<00:30,  3.93it/s] 45%|████▍     | 95/212 [00:24<00:30,  3.85it/s] 45%|████▌     | 96/212 [00:24<00:30,  3.84it/s] 46%|████▌     | 97/212 [00:25<00:29,  3.85it/s] 46%|████▌     | 98/212 [00:25<00:30,  3.73it/s] 47%|████▋     | 99/212 [00:25<00:30,  3.70it/s] 47%|████▋     | 100/212 [00:25<00:29,  3.81it/s] 48%|████▊     | 101/212 [00:26<00:28,  3.90it/s] 48%|████▊     | 102/212 [00:26<00:27,  3.96it/s] 49%|████▊     | 103/212 [00:26<00:27,  4.00it/s] 49%|████▉     | 104/212 [00:26<00:26,  4.02it/s] 50%|████▉     | 105/212 [00:27<00:26,  4.04it/s] 50%|█████     | 106/212 [00:27<00:26,  4.06it/s] 50%|█████     | 107/212 [00:27<00:25,  4.06it/s] 51%|█████     | 108/212 [00:27<00:25,  4.06it/s] 51%|█████▏    | 109/212 [00:28<00:25,  4.07it/s] 52%|█████▏    | 110/212 [00:28<00:25,  4.01it/s] 52%|█████▏    | 111/212 [00:28<00:25,  3.91it/s] 53%|█████▎    | 112/212 [00:28<00:26,  3.81it/s] 53%|█████▎    | 113/212 [00:29<00:25,  3.89it/s] 54%|█████▍    | 114/212 [00:29<00:24,  3.94it/s] 54%|█████▍    | 115/212 [00:29<00:25,  3.87it/s] 55%|█████▍    | 116/212 [00:29<00:25,  3.83it/s] 55%|█████▌    | 117/212 [00:30<00:24,  3.84it/s] 56%|█████▌    | 118/212 [00:30<00:25,  3.74it/s] 56%|█████▌    | 119/212 [00:30<00:24,  3.84it/s] 57%|█████▋    | 120/212 [00:31<00:24,  3.80it/s] 57%|█████▋    | 121/212 [00:31<00:23,  3.79it/s] 58%|█████▊    | 122/212 [00:31<00:23,  3.81it/s] 58%|█████▊    | 123/212 [00:31<00:23,  3.72it/s] 58%|█████▊    | 124/212 [00:32<00:23,  3.83it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.77it/s] 59%|█████▉    | 126/212 [00:32<00:22,  3.75it/s] 60%|█████▉    | 127/212 [00:32<00:22,  3.72it/s] 60%|██████    | 128/212 [00:33<00:21,  3.83it/s] 61%|██████    | 129/212 [00:33<00:21,  3.90it/s] 61%|██████▏   | 130/212 [00:33<00:20,  3.95it/s] 62%|██████▏   | 131/212 [00:33<00:20,  3.99it/s] 62%|██████▏   | 132/212 [00:34<00:19,  4.03it/s] 63%|██████▎   | 133/212 [00:34<00:19,  4.05it/s] 63%|██████▎   | 134/212 [00:34<00:19,  4.05it/s] 64%|██████▎   | 135/212 [00:34<00:18,  4.08it/s] 64%|██████▍   | 136/212 [00:35<00:18,  4.09it/s] 65%|██████▍   | 137/212 [00:35<00:18,  4.11it/s] 65%|██████▌   | 138/212 [00:35<00:18,  4.11it/s] 66%|██████▌   | 139/212 [00:35<00:17,  4.11it/s] 66%|██████▌   | 140/212 [00:36<00:17,  4.11it/s] 67%|██████▋   | 141/212 [00:36<00:17,  4.11it/s] 67%|██████▋   | 142/212 [00:36<00:17,  4.11it/s] 67%|██████▋   | 143/212 [00:36<00:16,  4.12it/s] 68%|██████▊   | 144/212 [00:37<00:16,  4.12it/s] 68%|██████▊   | 145/212 [00:37<00:16,  4.13it/s] 69%|██████▉   | 146/212 [00:37<00:16,  4.12it/s] 69%|██████▉   | 147/212 [00:37<00:15,  4.11it/s] 70%|██████▉   | 148/212 [00:38<00:15,  4.09it/s] 70%|███████   | 149/212 [00:38<00:15,  4.10it/s] 71%|███████   | 150/212 [00:38<00:15,  4.11it/s] 71%|███████   | 151/212 [00:38<00:14,  4.10it/s] 72%|███████▏  | 152/212 [00:38<00:14,  4.09it/s] 72%|███████▏  | 153/212 [00:39<00:14,  4.09it/s] 73%|███████▎  | 154/212 [00:39<00:14,  4.10it/s] 73%|███████▎  | 155/212 [00:39<00:13,  4.11it/s] 74%|███████▎  | 156/212 [00:39<00:13,  4.11it/s] 74%|███████▍  | 157/212 [00:40<00:13,  4.10it/s] 75%|███████▍  | 158/212 [00:40<00:13,  4.10it/s] 75%|███████▌  | 159/212 [00:40<00:12,  4.11it/s] 75%|███████▌  | 160/212 [00:40<00:12,  4.11it/s] 76%|███████▌  | 161/212 [00:41<00:12,  4.12it/s] 76%|███████▋  | 162/212 [00:41<00:12,  4.11it/s] 77%|███████▋  | 163/212 [00:41<00:11,  4.10it/s] 77%|███████▋  | 164/212 [00:41<00:11,  4.10it/s] 78%|███████▊  | 165/212 [00:42<00:11,  4.10it/s] 78%|███████▊  | 166/212 [00:42<00:11,  4.11it/s] 79%|███████▉  | 167/212 [00:42<00:10,  4.10it/s] 79%|███████▉  | 168/212 [00:42<00:10,  4.09it/s] 80%|███████▉  | 169/212 [00:43<00:10,  4.09it/s] 80%|████████  | 170/212 [00:43<00:10,  4.07it/s] 81%|████████  | 171/212 [00:43<00:10,  3.96it/s] 81%|████████  | 172/212 [00:43<00:10,  3.88it/s] 82%|████████▏ | 173/212 [00:44<00:10,  3.85it/s] 82%|████████▏ | 174/212 [00:44<00:09,  3.87it/s] 83%|████████▎ | 175/212 [00:44<00:09,  3.78it/s] 83%|████████▎ | 176/212 [00:44<00:09,  3.80it/s] 83%|████████▎ | 177/212 [00:45<00:09,  3.73it/s] 84%|████████▍ | 178/212 [00:45<00:08,  3.79it/s] 84%|████████▍ | 179/212 [00:45<00:08,  3.72it/s] 85%|████████▍ | 180/212 [00:46<00:08,  3.76it/s] 85%|████████▌ | 181/212 [00:46<00:08,  3.70it/s] 86%|████████▌ | 182/212 [00:46<00:07,  3.78it/s] 86%|████████▋ | 183/212 [00:46<00:07,  3.71it/s] 87%|████████▋ | 184/212 [00:47<00:07,  3.74it/s] 87%|████████▋ | 185/212 [00:47<00:07,  3.69it/s] 88%|████████▊ | 186/212 [00:47<00:06,  3.78it/s] 88%|████████▊ | 187/212 [00:47<00:06,  3.73it/s] 89%|████████▊ | 188/212 [00:48<00:06,  3.70it/s] 89%|████████▉ | 189/212 [00:48<00:06,  3.66it/s] 90%|████████▉ | 190/212 [00:48<00:05,  3.78it/s] 90%|█████████ | 191/212 [00:49<00:05,  3.74it/s] 91%|█████████ | 192/212 [00:49<00:05,  3.71it/s] 91%|█████████ | 193/212 [00:49<00:05,  3.76it/s] 92%|█████████▏| 194/212 [00:49<00:04,  3.72it/s] 92%|█████████▏| 195/212 [00:50<00:04,  3.76it/s] 92%|█████████▏| 196/212 [00:50<00:04,  3.70it/s] 93%|█████████▎| 197/212 [00:50<00:04,  3.74it/s] 93%|█████████▎| 198/212 [00:50<00:03,  3.68it/s] 94%|█████████▍| 199/212 [00:51<00:03,  3.74it/s] 94%|█████████▍| 200/212 [00:51<00:03,  3.69it/s] 95%|█████████▍| 201/212 [00:51<00:02,  3.73it/s] 95%|█████████▌| 202/212 [00:51<00:02,  3.68it/s] 96%|█████████▌| 203/212 [00:52<00:02,  3.75it/s] 96%|█████████▌| 204/212 [00:52<00:02,  3.69it/s] 97%|█████████▋| 205/212 [00:52<00:01,  3.69it/s] 97%|█████████▋| 206/212 [00:53<00:01,  3.64it/s] 98%|█████████▊| 207/212 [00:53<00:01,  3.76it/s] 98%|█████████▊| 208/212 [00:53<00:01,  3.72it/s] 99%|█████████▊| 209/212 [00:53<00:00,  3.68it/s] 99%|█████████▉| 210/212 [00:54<00:00,  3.64it/s]100%|█████████▉| 211/212 [00:54<00:00,  3.76it/s]100%|██████████| 212/212 [00:54<00:00,  3.78it/s]accuracy:  0.6415094339622641
100%|██████████| 212/212 [00:57<00:00,  3.66it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 135
Training dataset size: 96, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:11:24,501] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:11:25,055] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:11:26,296] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:40,  2.67s/it] 12%|█▎        | 2/16 [00:05<00:35,  2.56s/it] 19%|█▉        | 3/16 [00:07<00:33,  2.58s/it] 25%|██▌       | 4/16 [00:10<00:30,  2.54s/it] 31%|███▏      | 5/16 [00:12<00:27,  2.51s/it] 38%|███▊      | 6/16 [00:15<00:27,  2.73s/it] 44%|████▍     | 7/16 [00:17<00:22,  2.51s/it] 50%|█████     | 8/16 [00:20<00:19,  2.47s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:22<00:17,  2.49s/it] 62%|██████▎   | 10/16 [00:25<00:15,  2.51s/it]                                               {'loss': 0.5456, 'grad_norm': 2.2716972827911377, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:25<00:15,  2.51s/it] 69%|██████▉   | 11/16 [00:27<00:12,  2.49s/it] 75%|███████▌  | 12/16 [00:30<00:09,  2.42s/it] 81%|████████▏ | 13/16 [00:32<00:07,  2.46s/it] 88%|████████▊ | 14/16 [00:35<00:04,  2.47s/it] 94%|█████████▍| 15/16 [00:37<00:02,  2.41s/it]100%|██████████| 16/16 [00:39<00:00,  2.37s/it]                                               {'train_runtime': 40.3185, 'train_samples_per_second': 4.762, 'train_steps_per_second': 0.397, 'train_loss': 0.47802385687828064, 'epoch': 2.0}
100%|██████████| 16/16 [00:40<00:00,  2.37s/it]100%|██████████| 16/16 [00:40<00:00,  2.51s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:50,  3.23it/s]  1%|          | 2/163 [00:00<00:43,  3.70it/s]  2%|▏         | 3/163 [00:00<00:41,  3.89it/s]  2%|▏         | 4/163 [00:01<00:39,  3.99it/s]  3%|▎         | 5/163 [00:01<00:39,  4.05it/s]  4%|▎         | 6/163 [00:01<00:38,  4.08it/s]  4%|▍         | 7/163 [00:01<00:38,  4.10it/s]  5%|▍         | 8/163 [00:01<00:37,  4.12it/s]  6%|▌         | 9/163 [00:02<00:37,  4.14it/s]  6%|▌         | 10/163 [00:02<00:36,  4.14it/s]  7%|▋         | 11/163 [00:02<00:36,  4.15it/s]  7%|▋         | 12/163 [00:02<00:36,  4.16it/s]  8%|▊         | 13/163 [00:03<00:36,  4.16it/s]  9%|▊         | 14/163 [00:03<00:35,  4.16it/s]  9%|▉         | 15/163 [00:03<00:35,  4.17it/s] 10%|▉         | 16/163 [00:03<00:35,  4.16it/s] 10%|█         | 17/163 [00:04<00:35,  4.17it/s] 11%|█         | 18/163 [00:04<00:34,  4.17it/s] 12%|█▏        | 19/163 [00:04<00:34,  4.17it/s] 12%|█▏        | 20/163 [00:04<00:34,  4.17it/s] 13%|█▎        | 21/163 [00:05<00:34,  4.16it/s] 13%|█▎        | 22/163 [00:05<00:33,  4.16it/s] 14%|█▍        | 23/163 [00:05<00:33,  4.16it/s] 15%|█▍        | 24/163 [00:05<00:33,  4.15it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.14it/s] 16%|█▌        | 26/163 [00:06<00:33,  4.13it/s] 17%|█▋        | 27/163 [00:06<00:32,  4.12it/s] 17%|█▋        | 28/163 [00:06<00:32,  4.13it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.13it/s] 18%|█▊        | 30/163 [00:07<00:32,  4.13it/s] 19%|█▉        | 31/163 [00:07<00:31,  4.14it/s] 20%|█▉        | 32/163 [00:07<00:31,  4.14it/s] 20%|██        | 33/163 [00:08<00:31,  4.14it/s] 21%|██        | 34/163 [00:08<00:31,  4.14it/s] 21%|██▏       | 35/163 [00:08<00:30,  4.14it/s] 22%|██▏       | 36/163 [00:08<00:30,  4.12it/s] 23%|██▎       | 37/163 [00:08<00:30,  4.11it/s] 23%|██▎       | 38/163 [00:09<00:30,  4.12it/s] 24%|██▍       | 39/163 [00:09<00:30,  4.12it/s] 25%|██▍       | 40/163 [00:09<00:29,  4.13it/s] 25%|██▌       | 41/163 [00:09<00:29,  4.14it/s] 26%|██▌       | 42/163 [00:10<00:29,  4.14it/s] 26%|██▋       | 43/163 [00:10<00:29,  4.13it/s] 27%|██▋       | 44/163 [00:10<00:28,  4.11it/s] 28%|██▊       | 45/163 [00:10<00:28,  4.11it/s] 28%|██▊       | 46/163 [00:11<00:28,  4.12it/s] 29%|██▉       | 47/163 [00:11<00:28,  4.12it/s] 29%|██▉       | 48/163 [00:11<00:27,  4.12it/s] 30%|███       | 49/163 [00:11<00:27,  4.12it/s] 31%|███       | 50/163 [00:12<00:27,  4.11it/s] 31%|███▏      | 51/163 [00:12<00:28,  3.98it/s] 32%|███▏      | 52/163 [00:12<00:28,  3.90it/s] 33%|███▎      | 53/163 [00:12<00:28,  3.87it/s] 33%|███▎      | 54/163 [00:13<00:27,  3.94it/s] 34%|███▎      | 55/163 [00:13<00:27,  3.93it/s] 34%|███▍      | 56/163 [00:13<00:27,  3.87it/s] 35%|███▍      | 57/163 [00:13<00:27,  3.82it/s] 36%|███▌      | 58/163 [00:14<00:28,  3.71it/s] 36%|███▌      | 59/163 [00:14<00:27,  3.83it/s] 37%|███▋      | 60/163 [00:14<00:26,  3.89it/s] 37%|███▋      | 61/163 [00:15<00:26,  3.84it/s] 38%|███▊      | 62/163 [00:15<00:26,  3.79it/s] 39%|███▊      | 63/163 [00:15<00:27,  3.70it/s] 39%|███▉      | 64/163 [00:15<00:25,  3.81it/s] 40%|███▉      | 65/163 [00:16<00:25,  3.86it/s] 40%|████      | 66/163 [00:16<00:25,  3.84it/s] 41%|████      | 67/163 [00:16<00:25,  3.82it/s] 42%|████▏     | 68/163 [00:16<00:25,  3.80it/s] 42%|████▏     | 69/163 [00:17<00:24,  3.88it/s] 43%|████▎     | 70/163 [00:17<00:23,  3.94it/s] 44%|████▎     | 71/163 [00:17<00:23,  3.98it/s] 44%|████▍     | 72/163 [00:17<00:22,  4.02it/s] 45%|████▍     | 73/163 [00:18<00:22,  4.05it/s] 45%|████▌     | 74/163 [00:18<00:21,  4.07it/s] 46%|████▌     | 75/163 [00:18<00:21,  4.08it/s] 47%|████▋     | 76/163 [00:18<00:21,  4.08it/s] 47%|████▋     | 77/163 [00:19<00:21,  4.09it/s] 48%|████▊     | 78/163 [00:19<00:20,  4.10it/s] 48%|████▊     | 79/163 [00:19<00:20,  4.10it/s] 49%|████▉     | 80/163 [00:19<00:20,  4.10it/s] 50%|████▉     | 81/163 [00:20<00:20,  4.09it/s] 50%|█████     | 82/163 [00:20<00:19,  4.10it/s] 51%|█████     | 83/163 [00:20<00:19,  4.10it/s] 52%|█████▏    | 84/163 [00:20<00:19,  4.11it/s] 52%|█████▏    | 85/163 [00:21<00:19,  4.10it/s] 53%|█████▎    | 86/163 [00:21<00:18,  4.09it/s] 53%|█████▎    | 87/163 [00:21<00:18,  4.10it/s] 54%|█████▍    | 88/163 [00:21<00:18,  4.02it/s] 55%|█████▍    | 89/163 [00:22<00:18,  3.95it/s] 55%|█████▌    | 90/163 [00:22<00:18,  3.88it/s] 56%|█████▌    | 91/163 [00:22<00:18,  3.86it/s] 56%|█████▋    | 92/163 [00:22<00:18,  3.93it/s] 57%|█████▋    | 93/163 [00:23<00:17,  3.92it/s] 58%|█████▊    | 94/163 [00:23<00:18,  3.83it/s] 58%|█████▊    | 95/163 [00:23<00:18,  3.75it/s] 59%|█████▉    | 96/163 [00:23<00:17,  3.79it/s] 60%|█████▉    | 97/163 [00:24<00:17,  3.71it/s] 60%|██████    | 98/163 [00:24<00:17,  3.81it/s] 61%|██████    | 99/163 [00:24<00:16,  3.81it/s] 61%|██████▏   | 100/163 [00:24<00:17,  3.68it/s] 62%|██████▏   | 101/163 [00:25<00:16,  3.72it/s] 63%|██████▎   | 102/163 [00:25<00:16,  3.72it/s] 63%|██████▎   | 103/163 [00:25<00:15,  3.83it/s] 64%|██████▍   | 104/163 [00:25<00:15,  3.90it/s] 64%|██████▍   | 105/163 [00:26<00:14,  3.95it/s] 65%|██████▌   | 106/163 [00:26<00:14,  4.00it/s] 66%|██████▌   | 107/163 [00:26<00:13,  4.04it/s] 66%|██████▋   | 108/163 [00:26<00:13,  4.06it/s] 67%|██████▋   | 109/163 [00:27<00:13,  4.06it/s] 67%|██████▋   | 110/163 [00:27<00:13,  4.06it/s] 68%|██████▊   | 111/163 [00:27<00:12,  4.07it/s] 69%|██████▊   | 112/163 [00:27<00:12,  4.09it/s] 69%|██████▉   | 113/163 [00:28<00:12,  4.08it/s] 70%|██████▉   | 114/163 [00:28<00:12,  4.07it/s] 71%|███████   | 115/163 [00:28<00:11,  4.08it/s] 71%|███████   | 116/163 [00:28<00:11,  4.09it/s] 72%|███████▏  | 117/163 [00:29<00:11,  4.09it/s] 72%|███████▏  | 118/163 [00:29<00:11,  4.08it/s] 73%|███████▎  | 119/163 [00:29<00:10,  4.09it/s] 74%|███████▎  | 120/163 [00:29<00:10,  4.10it/s] 74%|███████▍  | 121/163 [00:30<00:10,  4.10it/s] 75%|███████▍  | 122/163 [00:30<00:10,  4.09it/s] 75%|███████▌  | 123/163 [00:30<00:09,  4.09it/s] 76%|███████▌  | 124/163 [00:30<00:09,  4.09it/s] 77%|███████▋  | 125/163 [00:31<00:09,  4.10it/s] 77%|███████▋  | 126/163 [00:31<00:09,  4.09it/s] 78%|███████▊  | 127/163 [00:31<00:08,  4.08it/s] 79%|███████▊  | 128/163 [00:31<00:08,  4.08it/s] 79%|███████▉  | 129/163 [00:32<00:08,  4.09it/s] 80%|███████▉  | 130/163 [00:32<00:08,  4.08it/s] 80%|████████  | 131/163 [00:32<00:07,  4.08it/s] 81%|████████  | 132/163 [00:32<00:07,  4.08it/s] 82%|████████▏ | 133/163 [00:33<00:07,  4.09it/s] 82%|████████▏ | 134/163 [00:33<00:07,  3.97it/s] 83%|████████▎ | 135/163 [00:33<00:07,  3.87it/s] 83%|████████▎ | 136/163 [00:33<00:07,  3.79it/s] 84%|████████▍ | 137/163 [00:34<00:06,  3.87it/s] 85%|████████▍ | 138/163 [00:34<00:06,  3.83it/s] 85%|████████▌ | 139/163 [00:34<00:06,  3.71it/s] 86%|████████▌ | 140/163 [00:34<00:06,  3.63it/s] 87%|████████▋ | 141/163 [00:35<00:05,  3.76it/s] 87%|████████▋ | 142/163 [00:35<00:05,  3.73it/s] 88%|████████▊ | 143/163 [00:35<00:05,  3.67it/s] 88%|████████▊ | 144/163 [00:36<00:05,  3.61it/s] 89%|████████▉ | 145/163 [00:36<00:04,  3.73it/s] 90%|████████▉ | 146/163 [00:36<00:04,  3.68it/s] 90%|█████████ | 147/163 [00:36<00:04,  3.65it/s] 91%|█████████ | 148/163 [00:37<00:04,  3.60it/s] 91%|█████████▏| 149/163 [00:37<00:03,  3.73it/s] 92%|█████████▏| 150/163 [00:37<00:03,  3.70it/s] 93%|█████████▎| 151/163 [00:37<00:03,  3.65it/s] 93%|█████████▎| 152/163 [00:38<00:03,  3.59it/s] 94%|█████████▍| 153/163 [00:38<00:02,  3.72it/s] 94%|█████████▍| 154/163 [00:38<00:02,  3.68it/s] 95%|█████████▌| 155/163 [00:39<00:02,  3.65it/s] 96%|█████████▌| 156/163 [00:39<00:01,  3.62it/s] 96%|█████████▋| 157/163 [00:39<00:01,  3.70it/s] 97%|█████████▋| 158/163 [00:39<00:01,  3.71it/s] 98%|█████████▊| 159/163 [00:40<00:01,  3.64it/s] 98%|█████████▊| 160/163 [00:40<00:00,  3.58it/s] 99%|█████████▉| 161/163 [00:40<00:00,  3.72it/s] 99%|█████████▉| 162/163 [00:40<00:00,  3.73it/s]100%|██████████| 163/163 [00:41<00:00,  3.65it/s]accuracy:  0.8773006134969326
100%|██████████| 163/163 [00:43<00:00,  3.72it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 198
Training dataset size: 96, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.80s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:13:17,277] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:13:17,583] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:13:17,980] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:38,  2.60s/it] 12%|█▎        | 2/16 [00:05<00:36,  2.62s/it] 19%|█▉        | 3/16 [00:07<00:33,  2.56s/it] 25%|██▌       | 4/16 [00:09<00:29,  2.43s/it] 31%|███▏      | 5/16 [00:12<00:27,  2.47s/it] 38%|███▊      | 6/16 [00:15<00:25,  2.55s/it] 44%|████▍     | 7/16 [00:17<00:23,  2.59s/it] 50%|█████     | 8/16 [00:20<00:20,  2.61s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:23<00:18,  2.59s/it] 62%|██████▎   | 10/16 [00:25<00:16,  2.69s/it]                                               {'loss': 0.5672, 'grad_norm': 2.171492099761963, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:25<00:16,  2.69s/it] 69%|██████▉   | 11/16 [00:28<00:13,  2.67s/it] 75%|███████▌  | 12/16 [00:30<00:10,  2.58s/it] 81%|████████▏ | 13/16 [00:33<00:08,  2.71s/it] 88%|████████▊ | 14/16 [00:36<00:05,  2.68s/it] 94%|█████████▍| 15/16 [00:39<00:02,  2.69s/it]100%|██████████| 16/16 [00:42<00:00,  2.79s/it]                                               {'train_runtime': 42.9769, 'train_samples_per_second': 4.468, 'train_steps_per_second': 0.372, 'train_loss': 0.6117255687713623, 'epoch': 2.0}
100%|██████████| 16/16 [00:42<00:00,  2.79s/it]100%|██████████| 16/16 [00:42<00:00,  2.67s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:19,  2.92it/s]  1%|          | 2/232 [00:00<01:08,  3.35it/s]  1%|▏         | 3/232 [00:00<01:04,  3.53it/s]  2%|▏         | 4/232 [00:01<01:02,  3.67it/s]  2%|▏         | 5/232 [00:01<01:02,  3.65it/s]  3%|▎         | 6/232 [00:01<01:00,  3.72it/s]  3%|▎         | 7/232 [00:01<01:01,  3.67it/s]  3%|▎         | 8/232 [00:02<00:59,  3.74it/s]  4%|▍         | 9/232 [00:02<01:00,  3.68it/s]  4%|▍         | 10/232 [00:02<00:59,  3.74it/s]  5%|▍         | 11/232 [00:03<01:00,  3.68it/s]  5%|▌         | 12/232 [00:03<00:58,  3.75it/s]  6%|▌         | 13/232 [00:03<00:59,  3.70it/s]  6%|▌         | 14/232 [00:03<00:58,  3.75it/s]  6%|▋         | 15/232 [00:04<00:58,  3.70it/s]  7%|▋         | 16/232 [00:04<00:57,  3.78it/s]  7%|▋         | 17/232 [00:04<00:57,  3.72it/s]  8%|▊         | 18/232 [00:04<00:56,  3.76it/s]  8%|▊         | 19/232 [00:05<00:57,  3.70it/s]  9%|▊         | 20/232 [00:05<00:56,  3.76it/s]  9%|▉         | 21/232 [00:05<00:57,  3.70it/s]  9%|▉         | 22/232 [00:05<00:56,  3.75it/s] 10%|▉         | 23/232 [00:06<00:56,  3.69it/s] 10%|█         | 24/232 [00:06<00:55,  3.74it/s] 11%|█         | 25/232 [00:06<00:56,  3.69it/s] 11%|█         | 26/232 [00:07<00:55,  3.74it/s] 12%|█▏        | 27/232 [00:07<00:55,  3.69it/s] 12%|█▏        | 28/232 [00:07<00:54,  3.75it/s] 12%|█▎        | 29/232 [00:07<00:54,  3.70it/s] 13%|█▎        | 30/232 [00:08<00:53,  3.75it/s] 13%|█▎        | 31/232 [00:08<00:54,  3.69it/s] 14%|█▍        | 32/232 [00:08<00:53,  3.76it/s] 14%|█▍        | 33/232 [00:08<00:53,  3.69it/s] 15%|█▍        | 34/232 [00:09<00:52,  3.74it/s] 15%|█▌        | 35/232 [00:09<00:53,  3.69it/s] 16%|█▌        | 36/232 [00:09<00:52,  3.75it/s] 16%|█▌        | 37/232 [00:10<00:52,  3.69it/s] 16%|█▋        | 38/232 [00:10<00:51,  3.73it/s] 17%|█▋        | 39/232 [00:10<00:52,  3.67it/s] 17%|█▋        | 40/232 [00:10<00:51,  3.74it/s] 18%|█▊        | 41/232 [00:11<00:51,  3.68it/s] 18%|█▊        | 42/232 [00:11<00:50,  3.73it/s] 19%|█▊        | 43/232 [00:11<00:51,  3.68it/s] 19%|█▉        | 44/232 [00:11<00:50,  3.75it/s] 19%|█▉        | 45/232 [00:12<00:50,  3.68it/s] 20%|█▉        | 46/232 [00:12<00:49,  3.73it/s] 20%|██        | 47/232 [00:12<00:50,  3.68it/s] 21%|██        | 48/232 [00:12<00:49,  3.74it/s] 21%|██        | 49/232 [00:13<00:49,  3.68it/s] 22%|██▏       | 50/232 [00:13<00:48,  3.73it/s] 22%|██▏       | 51/232 [00:13<00:49,  3.67it/s] 22%|██▏       | 52/232 [00:14<00:48,  3.74it/s] 23%|██▎       | 53/232 [00:14<00:48,  3.68it/s] 23%|██▎       | 54/232 [00:14<00:47,  3.72it/s] 24%|██▎       | 55/232 [00:14<00:48,  3.67it/s] 24%|██▍       | 56/232 [00:15<00:47,  3.73it/s] 25%|██▍       | 57/232 [00:15<00:47,  3.67it/s] 25%|██▌       | 58/232 [00:15<00:46,  3.72it/s] 25%|██▌       | 59/232 [00:15<00:47,  3.67it/s] 26%|██▌       | 60/232 [00:16<00:45,  3.74it/s] 26%|██▋       | 61/232 [00:16<00:46,  3.67it/s] 27%|██▋       | 62/232 [00:16<00:45,  3.73it/s] 27%|██▋       | 63/232 [00:17<00:45,  3.67it/s] 28%|██▊       | 64/232 [00:17<00:44,  3.75it/s] 28%|██▊       | 65/232 [00:17<00:45,  3.69it/s] 28%|██▊       | 66/232 [00:17<00:44,  3.73it/s] 29%|██▉       | 67/232 [00:18<00:44,  3.67it/s] 29%|██▉       | 68/232 [00:18<00:43,  3.73it/s] 30%|██▉       | 69/232 [00:18<00:44,  3.67it/s] 30%|███       | 70/232 [00:18<00:43,  3.72it/s] 31%|███       | 71/232 [00:19<00:43,  3.67it/s] 31%|███       | 72/232 [00:19<00:42,  3.75it/s] 31%|███▏      | 73/232 [00:19<00:43,  3.68it/s] 32%|███▏      | 74/232 [00:19<00:42,  3.72it/s] 32%|███▏      | 75/232 [00:20<00:42,  3.67it/s] 33%|███▎      | 76/232 [00:20<00:41,  3.74it/s] 33%|███▎      | 77/232 [00:20<00:42,  3.68it/s] 34%|███▎      | 78/232 [00:21<00:41,  3.72it/s] 34%|███▍      | 79/232 [00:21<00:41,  3.67it/s] 34%|███▍      | 80/232 [00:21<00:40,  3.73it/s] 35%|███▍      | 81/232 [00:21<00:41,  3.67it/s] 35%|███▌      | 82/232 [00:22<00:40,  3.72it/s] 36%|███▌      | 83/232 [00:22<00:40,  3.68it/s] 36%|███▌      | 84/232 [00:22<00:39,  3.74it/s] 37%|███▋      | 85/232 [00:22<00:39,  3.68it/s] 37%|███▋      | 86/232 [00:23<00:39,  3.73it/s] 38%|███▊      | 87/232 [00:23<00:39,  3.68it/s] 38%|███▊      | 88/232 [00:23<00:38,  3.74it/s] 38%|███▊      | 89/232 [00:24<00:38,  3.68it/s] 39%|███▉      | 90/232 [00:24<00:38,  3.73it/s] 39%|███▉      | 91/232 [00:24<00:38,  3.67it/s] 40%|███▉      | 92/232 [00:24<00:37,  3.74it/s] 40%|████      | 93/232 [00:25<00:37,  3.67it/s] 41%|████      | 94/232 [00:25<00:37,  3.72it/s] 41%|████      | 95/232 [00:25<00:37,  3.67it/s] 41%|████▏     | 96/232 [00:25<00:36,  3.75it/s] 42%|████▏     | 97/232 [00:26<00:36,  3.69it/s] 42%|████▏     | 98/232 [00:26<00:35,  3.74it/s] 43%|████▎     | 99/232 [00:26<00:35,  3.74it/s] 43%|████▎     | 100/232 [00:26<00:34,  3.84it/s] 44%|████▎     | 101/232 [00:27<00:33,  3.91it/s] 44%|████▍     | 102/232 [00:27<00:32,  3.96it/s] 44%|████▍     | 103/232 [00:27<00:32,  4.00it/s] 45%|████▍     | 104/232 [00:27<00:31,  4.04it/s] 45%|████▌     | 105/232 [00:28<00:31,  4.06it/s] 46%|████▌     | 106/232 [00:28<00:30,  4.07it/s] 46%|████▌     | 107/232 [00:28<00:30,  4.07it/s] 47%|████▋     | 108/232 [00:28<00:30,  4.07it/s] 47%|████▋     | 109/232 [00:29<00:30,  4.08it/s] 47%|████▋     | 110/232 [00:29<00:29,  4.08it/s] 48%|████▊     | 111/232 [00:29<00:29,  4.07it/s] 48%|████▊     | 112/232 [00:29<00:29,  4.08it/s] 49%|████▊     | 113/232 [00:30<00:29,  4.09it/s] 49%|████▉     | 114/232 [00:30<00:28,  4.09it/s] 50%|████▉     | 115/232 [00:30<00:28,  4.08it/s] 50%|█████     | 116/232 [00:30<00:28,  4.08it/s] 50%|█████     | 117/232 [00:31<00:28,  4.09it/s] 51%|█████     | 118/232 [00:31<00:27,  4.09it/s] 51%|█████▏    | 119/232 [00:31<00:28,  3.97it/s] 52%|█████▏    | 120/232 [00:31<00:28,  3.86it/s] 52%|█████▏    | 121/232 [00:32<00:28,  3.85it/s] 53%|█████▎    | 122/232 [00:32<00:28,  3.92it/s] 53%|█████▎    | 123/232 [00:32<00:28,  3.87it/s] 53%|█████▎    | 124/232 [00:32<00:28,  3.82it/s] 54%|█████▍    | 125/232 [00:33<00:28,  3.69it/s] 54%|█████▍    | 126/232 [00:33<00:27,  3.81it/s] 55%|█████▍    | 127/232 [00:33<00:27,  3.82it/s] 55%|█████▌    | 128/232 [00:34<00:27,  3.78it/s] 56%|█████▌    | 129/232 [00:34<00:27,  3.69it/s] 56%|█████▌    | 130/232 [00:34<00:27,  3.77it/s] 56%|█████▋    | 131/232 [00:34<00:26,  3.82it/s] 57%|█████▋    | 132/232 [00:35<00:26,  3.77it/s] 57%|█████▋    | 133/232 [00:35<00:26,  3.71it/s] 58%|█████▊    | 134/232 [00:35<00:26,  3.76it/s] 58%|█████▊    | 135/232 [00:35<00:25,  3.81it/s] 59%|█████▊    | 136/232 [00:36<00:25,  3.77it/s] 59%|█████▉    | 137/232 [00:36<00:25,  3.70it/s] 59%|█████▉    | 138/232 [00:36<00:25,  3.75it/s] 60%|█████▉    | 139/232 [00:36<00:24,  3.80it/s] 60%|██████    | 140/232 [00:37<00:24,  3.76it/s] 61%|██████    | 141/232 [00:37<00:24,  3.70it/s] 61%|██████    | 142/232 [00:37<00:24,  3.75it/s] 62%|██████▏   | 143/232 [00:38<00:23,  3.78it/s] 62%|██████▏   | 144/232 [00:38<00:23,  3.75it/s] 62%|██████▎   | 145/232 [00:38<00:23,  3.69it/s] 63%|██████▎   | 146/232 [00:38<00:22,  3.75it/s] 63%|██████▎   | 147/232 [00:39<00:22,  3.78it/s] 64%|██████▍   | 148/232 [00:39<00:22,  3.75it/s] 64%|██████▍   | 149/232 [00:39<00:22,  3.69it/s] 65%|██████▍   | 150/232 [00:39<00:21,  3.74it/s] 65%|██████▌   | 151/232 [00:40<00:21,  3.81it/s] 66%|██████▌   | 152/232 [00:40<00:21,  3.77it/s] 66%|██████▌   | 153/232 [00:40<00:21,  3.71it/s] 66%|██████▋   | 154/232 [00:40<00:20,  3.75it/s] 67%|██████▋   | 155/232 [00:41<00:20,  3.81it/s] 67%|██████▋   | 156/232 [00:41<00:20,  3.77it/s] 68%|██████▊   | 157/232 [00:41<00:20,  3.75it/s] 68%|██████▊   | 158/232 [00:42<00:19,  3.74it/s] 69%|██████▊   | 159/232 [00:42<00:19,  3.81it/s] 69%|██████▉   | 160/232 [00:42<00:19,  3.76it/s] 69%|██████▉   | 161/232 [00:42<00:18,  3.79it/s] 70%|██████▉   | 162/232 [00:43<00:18,  3.72it/s] 70%|███████   | 163/232 [00:43<00:18,  3.79it/s] 71%|███████   | 164/232 [00:43<00:18,  3.75it/s] 71%|███████   | 165/232 [00:43<00:17,  3.76it/s] 72%|███████▏  | 166/232 [00:44<00:17,  3.69it/s] 72%|███████▏  | 167/232 [00:44<00:17,  3.79it/s] 72%|███████▏  | 168/232 [00:44<00:17,  3.76it/s] 73%|███████▎  | 169/232 [00:44<00:16,  3.73it/s] 73%|███████▎  | 170/232 [00:45<00:16,  3.67it/s] 74%|███████▎  | 171/232 [00:45<00:16,  3.78it/s] 74%|███████▍  | 172/232 [00:45<00:15,  3.79it/s] 75%|███████▍  | 173/232 [00:46<00:15,  3.75it/s] 75%|███████▌  | 174/232 [00:46<00:15,  3.68it/s] 75%|███████▌  | 175/232 [00:46<00:15,  3.76it/s] 76%|███████▌  | 176/232 [00:46<00:14,  3.80it/s] 76%|███████▋  | 177/232 [00:47<00:14,  3.76it/s] 77%|███████▋  | 178/232 [00:47<00:14,  3.79it/s] 77%|███████▋  | 179/232 [00:47<00:14,  3.72it/s] 78%|███████▊  | 180/232 [00:47<00:13,  3.82it/s] 78%|███████▊  | 181/232 [00:48<00:13,  3.77it/s] 78%|███████▊  | 182/232 [00:48<00:13,  3.74it/s] 79%|███████▉  | 183/232 [00:48<00:13,  3.68it/s] 79%|███████▉  | 184/232 [00:48<00:12,  3.79it/s] 80%|███████▉  | 185/232 [00:49<00:12,  3.80it/s] 80%|████████  | 186/232 [00:49<00:12,  3.76it/s] 81%|████████  | 187/232 [00:49<00:12,  3.71it/s] 81%|████████  | 188/232 [00:50<00:11,  3.75it/s] 81%|████████▏ | 189/232 [00:50<00:11,  3.79it/s] 82%|████████▏ | 190/232 [00:50<00:11,  3.75it/s] 82%|████████▏ | 191/232 [00:50<00:10,  3.79it/s] 83%|████████▎ | 192/232 [00:51<00:10,  3.71it/s] 83%|████████▎ | 193/232 [00:51<00:10,  3.80it/s] 84%|████████▎ | 194/232 [00:51<00:10,  3.75it/s] 84%|████████▍ | 195/232 [00:51<00:09,  3.77it/s] 84%|████████▍ | 196/232 [00:52<00:09,  3.69it/s] 85%|████████▍ | 197/232 [00:52<00:09,  3.77it/s] 85%|████████▌ | 198/232 [00:52<00:09,  3.74it/s] 86%|████████▌ | 199/232 [00:52<00:08,  3.74it/s] 86%|████████▌ | 200/232 [00:53<00:08,  3.68it/s] 87%|████████▋ | 201/232 [00:53<00:08,  3.78it/s] 87%|████████▋ | 202/232 [00:53<00:07,  3.78it/s] 88%|████████▊ | 203/232 [00:54<00:07,  3.75it/s] 88%|████████▊ | 204/232 [00:54<00:07,  3.67it/s] 88%|████████▊ | 205/232 [00:54<00:07,  3.77it/s] 89%|████████▉ | 206/232 [00:54<00:06,  3.79it/s] 89%|████████▉ | 207/232 [00:55<00:06,  3.76it/s] 90%|████████▉ | 208/232 [00:55<00:06,  3.79it/s] 90%|█████████ | 209/232 [00:55<00:06,  3.71it/s] 91%|█████████ | 210/232 [00:55<00:05,  3.79it/s] 91%|█████████ | 211/232 [00:56<00:05,  3.75it/s] 91%|█████████▏| 212/232 [00:56<00:05,  3.79it/s] 92%|█████████▏| 213/232 [00:56<00:05,  3.72it/s] 92%|█████████▏| 214/232 [00:56<00:04,  3.82it/s] 93%|█████████▎| 215/232 [00:57<00:04,  3.76it/s] 93%|█████████▎| 216/232 [00:57<00:04,  3.74it/s] 94%|█████████▎| 217/232 [00:57<00:04,  3.68it/s] 94%|█████████▍| 218/232 [00:57<00:03,  3.79it/s] 94%|█████████▍| 219/232 [00:58<00:03,  3.78it/s] 95%|█████████▍| 220/232 [00:58<00:03,  3.74it/s] 95%|█████████▌| 221/232 [00:58<00:03,  3.65it/s] 96%|█████████▌| 222/232 [00:59<00:02,  3.77it/s] 96%|█████████▌| 223/232 [00:59<00:02,  3.79it/s] 97%|█████████▋| 224/232 [00:59<00:02,  3.75it/s] 97%|█████████▋| 225/232 [00:59<00:01,  3.79it/s] 97%|█████████▋| 226/232 [01:00<00:01,  3.70it/s] 98%|█████████▊| 227/232 [01:00<00:01,  3.78it/s] 98%|█████████▊| 228/232 [01:00<00:01,  3.74it/s] 99%|█████████▊| 229/232 [01:00<00:00,  3.76it/s] 99%|█████████▉| 230/232 [01:01<00:00,  3.71it/s]100%|█████████▉| 231/232 [01:01<00:00,  3.80it/s]100%|██████████| 232/232 [01:01<00:00,  3.76it/s]accuracy:  0.8706896551724138
100%|██████████| 232/232 [01:05<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 164
Training dataset size: 96, validation dataset size: 164
Training dataset size: 96, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.36s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:15:32,667] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:15:32,859] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:15:33,050] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:38,  2.59s/it] 12%|█▎        | 2/16 [00:05<00:37,  2.67s/it] 19%|█▉        | 3/16 [00:07<00:31,  2.44s/it] 25%|██▌       | 4/16 [00:09<00:27,  2.30s/it] 31%|███▏      | 5/16 [00:12<00:26,  2.44s/it] 38%|███▊      | 6/16 [00:14<00:24,  2.43s/it] 44%|████▍     | 7/16 [00:16<00:21,  2.34s/it] 50%|█████     | 8/16 [00:19<00:19,  2.39s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:21<00:16,  2.40s/it] 62%|██████▎   | 10/16 [00:24<00:14,  2.43s/it]                                               {'loss': 1.0843, 'grad_norm': 10.159090995788574, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:24<00:14,  2.43s/it] 69%|██████▉   | 11/16 [00:26<00:12,  2.42s/it] 75%|███████▌  | 12/16 [00:29<00:09,  2.40s/it] 81%|████████▏ | 13/16 [00:31<00:07,  2.38s/it] 88%|████████▊ | 14/16 [00:33<00:04,  2.34s/it] 94%|█████████▍| 15/16 [00:36<00:02,  2.47s/it]100%|██████████| 16/16 [00:38<00:00,  2.50s/it]                                               {'train_runtime': 39.5737, 'train_samples_per_second': 4.852, 'train_steps_per_second': 0.404, 'train_loss': 1.0978579223155975, 'epoch': 2.0}
100%|██████████| 16/16 [00:39<00:00,  2.50s/it]100%|██████████| 16/16 [00:39<00:00,  2.46s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:16,  3.22it/s]  1%|          | 2/248 [00:00<01:10,  3.51it/s]  1%|          | 3/248 [00:00<01:08,  3.60it/s]  2%|▏         | 4/248 [00:01<01:07,  3.60it/s]  2%|▏         | 5/248 [00:01<01:04,  3.78it/s]  2%|▏         | 6/248 [00:01<01:03,  3.78it/s]  3%|▎         | 7/248 [00:01<01:03,  3.77it/s]  3%|▎         | 8/248 [00:02<01:05,  3.68it/s]  4%|▎         | 9/248 [00:02<01:02,  3.81it/s]  4%|▍         | 10/248 [00:02<01:01,  3.84it/s]  4%|▍         | 11/248 [00:02<01:02,  3.80it/s]  5%|▍         | 12/248 [00:03<01:03,  3.72it/s]  5%|▌         | 13/248 [00:03<01:01,  3.80it/s]  6%|▌         | 14/248 [00:03<01:01,  3.83it/s]  6%|▌         | 15/248 [00:04<01:01,  3.79it/s]  6%|▋         | 16/248 [00:04<01:01,  3.77it/s]  7%|▋         | 17/248 [00:04<01:01,  3.77it/s]  7%|▋         | 18/248 [00:04<01:00,  3.82it/s]  8%|▊         | 19/248 [00:05<01:00,  3.79it/s]  8%|▊         | 20/248 [00:05<01:01,  3.72it/s]  8%|▊         | 21/248 [00:05<01:00,  3.78it/s]  9%|▉         | 22/248 [00:05<00:59,  3.82it/s]  9%|▉         | 23/248 [00:06<00:59,  3.79it/s] 10%|▉         | 24/248 [00:06<00:59,  3.77it/s] 10%|█         | 25/248 [00:06<00:59,  3.77it/s] 10%|█         | 26/248 [00:06<00:57,  3.83it/s] 11%|█         | 27/248 [00:07<00:58,  3.79it/s] 11%|█▏        | 28/248 [00:07<00:59,  3.73it/s] 12%|█▏        | 29/248 [00:07<00:57,  3.78it/s] 12%|█▏        | 30/248 [00:07<00:57,  3.81it/s] 12%|█▎        | 31/248 [00:08<00:57,  3.78it/s] 13%|█▎        | 32/248 [00:08<00:57,  3.73it/s] 13%|█▎        | 33/248 [00:08<00:56,  3.77it/s] 14%|█▎        | 34/248 [00:09<00:56,  3.81it/s] 14%|█▍        | 35/248 [00:09<00:56,  3.77it/s] 15%|█▍        | 36/248 [00:09<00:56,  3.75it/s] 15%|█▍        | 37/248 [00:09<00:54,  3.85it/s] 15%|█▌        | 38/248 [00:10<00:53,  3.92it/s] 16%|█▌        | 39/248 [00:10<00:52,  3.98it/s] 16%|█▌        | 40/248 [00:10<00:51,  4.03it/s] 17%|█▋        | 41/248 [00:10<00:50,  4.06it/s] 17%|█▋        | 42/248 [00:11<00:50,  4.09it/s] 17%|█▋        | 43/248 [00:11<00:49,  4.11it/s] 18%|█▊        | 44/248 [00:11<00:49,  4.12it/s] 18%|█▊        | 45/248 [00:11<00:49,  4.12it/s] 19%|█▊        | 46/248 [00:11<00:49,  4.12it/s] 19%|█▉        | 47/248 [00:12<00:48,  4.11it/s] 19%|█▉        | 48/248 [00:12<00:48,  4.11it/s] 20%|█▉        | 49/248 [00:12<00:48,  4.11it/s] 20%|██        | 50/248 [00:12<00:48,  4.05it/s] 21%|██        | 51/248 [00:13<00:50,  3.94it/s] 21%|██        | 52/248 [00:13<00:50,  3.84it/s] 21%|██▏       | 53/248 [00:13<00:49,  3.93it/s] 22%|██▏       | 54/248 [00:14<00:49,  3.93it/s] 22%|██▏       | 55/248 [00:14<00:49,  3.87it/s] 23%|██▎       | 56/248 [00:14<00:49,  3.84it/s] 23%|██▎       | 57/248 [00:14<00:50,  3.76it/s] 23%|██▎       | 58/248 [00:15<00:49,  3.85it/s] 24%|██▍       | 59/248 [00:15<00:49,  3.81it/s] 24%|██▍       | 60/248 [00:15<00:49,  3.77it/s] 25%|██▍       | 61/248 [00:15<00:50,  3.69it/s] 25%|██▌       | 62/248 [00:16<00:48,  3.81it/s] 25%|██▌       | 63/248 [00:16<00:48,  3.84it/s] 26%|██▌       | 64/248 [00:16<00:48,  3.80it/s] 26%|██▌       | 65/248 [00:16<00:48,  3.78it/s] 27%|██▋       | 66/248 [00:17<00:48,  3.72it/s] 27%|██▋       | 67/248 [00:17<00:47,  3.83it/s] 27%|██▋       | 68/248 [00:17<00:47,  3.82it/s] 28%|██▊       | 69/248 [00:17<00:47,  3.79it/s] 28%|██▊       | 70/248 [00:18<00:47,  3.73it/s] 29%|██▊       | 71/248 [00:18<00:46,  3.77it/s] 29%|██▉       | 72/248 [00:18<00:45,  3.83it/s] 29%|██▉       | 73/248 [00:19<00:46,  3.78it/s] 30%|██▉       | 74/248 [00:19<00:46,  3.78it/s] 30%|███       | 75/248 [00:19<00:45,  3.77it/s] 31%|███       | 76/248 [00:19<00:44,  3.87it/s] 31%|███       | 77/248 [00:20<00:43,  3.95it/s] 31%|███▏      | 78/248 [00:20<00:42,  3.99it/s] 32%|███▏      | 79/248 [00:20<00:42,  4.02it/s] 32%|███▏      | 80/248 [00:20<00:41,  4.05it/s] 33%|███▎      | 81/248 [00:21<00:41,  4.07it/s] 33%|███▎      | 82/248 [00:21<00:40,  4.09it/s] 33%|███▎      | 83/248 [00:21<00:40,  4.10it/s] 34%|███▍      | 84/248 [00:21<00:39,  4.10it/s] 34%|███▍      | 85/248 [00:22<00:39,  4.10it/s] 35%|███▍      | 86/248 [00:22<00:39,  4.09it/s] 35%|███▌      | 87/248 [00:22<00:39,  4.10it/s] 35%|███▌      | 88/248 [00:22<00:38,  4.11it/s] 36%|███▌      | 89/248 [00:22<00:38,  4.11it/s] 36%|███▋      | 90/248 [00:23<00:38,  4.11it/s] 37%|███▋      | 91/248 [00:23<00:38,  4.10it/s] 37%|███▋      | 92/248 [00:23<00:38,  4.10it/s] 38%|███▊      | 93/248 [00:23<00:37,  4.11it/s] 38%|███▊      | 94/248 [00:24<00:37,  4.12it/s] 38%|███▊      | 95/248 [00:24<00:37,  4.12it/s] 39%|███▊      | 96/248 [00:24<00:36,  4.11it/s] 39%|███▉      | 97/248 [00:24<00:36,  4.10it/s] 40%|███▉      | 98/248 [00:25<00:36,  4.10it/s] 40%|███▉      | 99/248 [00:25<00:36,  4.10it/s] 40%|████      | 100/248 [00:25<00:36,  4.10it/s] 41%|████      | 101/248 [00:25<00:35,  4.09it/s] 41%|████      | 102/248 [00:26<00:35,  4.09it/s] 42%|████▏     | 103/248 [00:26<00:35,  4.10it/s] 42%|████▏     | 104/248 [00:26<00:35,  4.10it/s] 42%|████▏     | 105/248 [00:26<00:34,  4.10it/s] 43%|████▎     | 106/248 [00:27<00:34,  4.09it/s] 43%|████▎     | 107/248 [00:27<00:34,  4.09it/s] 44%|████▎     | 108/248 [00:27<00:34,  4.10it/s] 44%|████▍     | 109/248 [00:27<00:33,  4.10it/s] 44%|████▍     | 110/248 [00:28<00:33,  4.09it/s] 45%|████▍     | 111/248 [00:28<00:33,  4.09it/s] 45%|████▌     | 112/248 [00:28<00:33,  4.09it/s] 46%|████▌     | 113/248 [00:28<00:32,  4.10it/s] 46%|████▌     | 114/248 [00:29<00:32,  4.10it/s] 46%|████▋     | 115/248 [00:29<00:32,  4.09it/s] 47%|████▋     | 116/248 [00:29<00:32,  4.08it/s] 47%|████▋     | 117/248 [00:29<00:31,  4.09it/s] 48%|████▊     | 118/248 [00:30<00:31,  4.10it/s] 48%|████▊     | 119/248 [00:30<00:31,  4.09it/s] 48%|████▊     | 120/248 [00:30<00:31,  4.08it/s] 49%|████▉     | 121/248 [00:30<00:31,  4.09it/s] 49%|████▉     | 122/248 [00:31<00:30,  4.09it/s] 50%|████▉     | 123/248 [00:31<00:30,  4.09it/s] 50%|█████     | 124/248 [00:31<00:30,  4.08it/s] 50%|█████     | 125/248 [00:31<00:30,  4.09it/s] 51%|█████     | 126/248 [00:32<00:29,  4.10it/s] 51%|█████     | 127/248 [00:32<00:29,  4.10it/s] 52%|█████▏    | 128/248 [00:32<00:29,  4.09it/s] 52%|█████▏    | 129/248 [00:32<00:29,  4.09it/s] 52%|█████▏    | 130/248 [00:32<00:28,  4.09it/s] 53%|█████▎    | 131/248 [00:33<00:28,  4.10it/s] 53%|█████▎    | 132/248 [00:33<00:28,  4.10it/s] 54%|█████▎    | 133/248 [00:33<00:28,  4.09it/s] 54%|█████▍    | 134/248 [00:33<00:27,  4.09it/s] 54%|█████▍    | 135/248 [00:34<00:27,  4.09it/s] 55%|█████▍    | 136/248 [00:34<00:27,  4.09it/s] 55%|█████▌    | 137/248 [00:34<00:27,  4.09it/s] 56%|█████▌    | 138/248 [00:34<00:26,  4.08it/s] 56%|█████▌    | 139/248 [00:35<00:26,  4.09it/s] 56%|█████▋    | 140/248 [00:35<00:26,  4.09it/s] 57%|█████▋    | 141/248 [00:35<00:26,  4.09it/s] 57%|█████▋    | 142/248 [00:35<00:25,  4.08it/s] 58%|█████▊    | 143/248 [00:36<00:25,  4.08it/s] 58%|█████▊    | 144/248 [00:36<00:25,  4.09it/s] 58%|█████▊    | 145/248 [00:36<00:25,  4.09it/s] 59%|█████▉    | 146/248 [00:36<00:24,  4.08it/s] 59%|█████▉    | 147/248 [00:37<00:24,  4.08it/s] 60%|█████▉    | 148/248 [00:37<00:24,  4.09it/s] 60%|██████    | 149/248 [00:37<00:24,  4.08it/s] 60%|██████    | 150/248 [00:37<00:24,  4.08it/s] 61%|██████    | 151/248 [00:38<00:23,  4.08it/s] 61%|██████▏   | 152/248 [00:38<00:23,  4.08it/s] 62%|██████▏   | 153/248 [00:38<00:23,  3.99it/s] 62%|██████▏   | 154/248 [00:38<00:24,  3.88it/s] 62%|██████▎   | 155/248 [00:39<00:24,  3.84it/s] 63%|██████▎   | 156/248 [00:39<00:23,  3.90it/s] 63%|██████▎   | 157/248 [00:39<00:23,  3.84it/s] 64%|██████▎   | 158/248 [00:39<00:23,  3.79it/s] 64%|██████▍   | 159/248 [00:40<00:23,  3.71it/s] 65%|██████▍   | 160/248 [00:40<00:23,  3.77it/s] 65%|██████▍   | 161/248 [00:40<00:22,  3.79it/s] 65%|██████▌   | 162/248 [00:41<00:22,  3.74it/s] 66%|██████▌   | 163/248 [00:41<00:23,  3.68it/s] 66%|██████▌   | 164/248 [00:41<00:22,  3.73it/s] 67%|██████▋   | 165/248 [00:41<00:22,  3.77it/s] 67%|██████▋   | 166/248 [00:42<00:21,  3.73it/s] 67%|██████▋   | 167/248 [00:42<00:22,  3.67it/s] 68%|██████▊   | 168/248 [00:42<00:21,  3.73it/s] 68%|██████▊   | 169/248 [00:42<00:21,  3.75it/s] 69%|██████▊   | 170/248 [00:43<00:21,  3.71it/s] 69%|██████▉   | 171/248 [00:43<00:21,  3.66it/s] 69%|██████▉   | 172/248 [00:43<00:20,  3.72it/s] 70%|██████▉   | 173/248 [00:44<00:19,  3.75it/s] 70%|███████   | 174/248 [00:44<00:19,  3.72it/s] 71%|███████   | 175/248 [00:44<00:19,  3.71it/s] 71%|███████   | 176/248 [00:44<00:19,  3.70it/s] 71%|███████▏  | 177/248 [00:45<00:18,  3.76it/s] 72%|███████▏  | 178/248 [00:45<00:18,  3.72it/s] 72%|███████▏  | 179/248 [00:45<00:18,  3.75it/s] 73%|███████▎  | 180/248 [00:45<00:18,  3.68it/s] 73%|███████▎  | 181/248 [00:46<00:17,  3.76it/s] 73%|███████▎  | 182/248 [00:46<00:17,  3.71it/s] 74%|███████▍  | 183/248 [00:46<00:17,  3.70it/s] 74%|███████▍  | 184/248 [00:46<00:17,  3.66it/s] 75%|███████▍  | 185/248 [00:47<00:16,  3.77it/s] 75%|███████▌  | 186/248 [00:47<00:16,  3.74it/s] 75%|███████▌  | 187/248 [00:47<00:16,  3.70it/s] 76%|███████▌  | 188/248 [00:48<00:16,  3.64it/s] 76%|███████▌  | 189/248 [00:48<00:15,  3.75it/s] 77%|███████▋  | 190/248 [00:48<00:15,  3.74it/s] 77%|███████▋  | 191/248 [00:48<00:15,  3.70it/s] 77%|███████▋  | 192/248 [00:49<00:15,  3.62it/s] 78%|███████▊  | 193/248 [00:49<00:14,  3.75it/s] 78%|███████▊  | 194/248 [00:49<00:14,  3.77it/s] 79%|███████▊  | 195/248 [00:49<00:14,  3.73it/s] 79%|███████▉  | 196/248 [00:50<00:14,  3.67it/s] 79%|███████▉  | 197/248 [00:50<00:13,  3.73it/s] 80%|███████▉  | 198/248 [00:50<00:13,  3.77it/s] 80%|████████  | 199/248 [00:51<00:13,  3.73it/s] 81%|████████  | 200/248 [00:51<00:13,  3.68it/s] 81%|████████  | 201/248 [00:51<00:12,  3.71it/s] 81%|████████▏ | 202/248 [00:51<00:12,  3.76it/s] 82%|████████▏ | 203/248 [00:52<00:12,  3.72it/s] 82%|████████▏ | 204/248 [00:52<00:11,  3.67it/s] 83%|████████▎ | 205/248 [00:52<00:11,  3.70it/s] 83%|████████▎ | 206/248 [00:52<00:11,  3.76it/s] 83%|████████▎ | 207/248 [00:53<00:11,  3.71it/s] 84%|████████▍ | 208/248 [00:53<00:10,  3.74it/s] 84%|████████▍ | 209/248 [00:53<00:10,  3.67it/s] 85%|████████▍ | 210/248 [00:53<00:10,  3.75it/s] 85%|████████▌ | 211/248 [00:54<00:10,  3.69it/s] 85%|████████▌ | 212/248 [00:54<00:09,  3.69it/s] 86%|████████▌ | 213/248 [00:54<00:09,  3.65it/s] 86%|████████▋ | 214/248 [00:55<00:09,  3.76it/s] 87%|████████▋ | 215/248 [00:55<00:08,  3.73it/s] 87%|████████▋ | 216/248 [00:55<00:08,  3.70it/s] 88%|████████▊ | 217/248 [00:55<00:08,  3.64it/s] 88%|████████▊ | 218/248 [00:56<00:08,  3.75it/s] 88%|████████▊ | 219/248 [00:56<00:07,  3.75it/s] 89%|████████▊ | 220/248 [00:56<00:07,  3.72it/s] 89%|████████▉ | 221/248 [00:56<00:07,  3.67it/s] 90%|████████▉ | 222/248 [00:57<00:06,  3.72it/s] 90%|████████▉ | 223/248 [00:57<00:06,  3.76it/s] 90%|█████████ | 224/248 [00:57<00:06,  3.72it/s] 91%|█████████ | 225/248 [00:58<00:06,  3.72it/s] 91%|█████████ | 226/248 [00:58<00:05,  3.67it/s] 92%|█████████▏| 227/248 [00:58<00:05,  3.75it/s] 92%|█████████▏| 228/248 [00:58<00:05,  3.70it/s] 92%|█████████▏| 229/248 [00:59<00:05,  3.69it/s] 93%|█████████▎| 230/248 [00:59<00:04,  3.65it/s] 93%|█████████▎| 231/248 [00:59<00:04,  3.76it/s] 94%|█████████▎| 232/248 [00:59<00:04,  3.72it/s] 94%|█████████▍| 233/248 [01:00<00:04,  3.69it/s] 94%|█████████▍| 234/248 [01:00<00:03,  3.65it/s] 95%|█████████▍| 235/248 [01:00<00:03,  3.76it/s] 95%|█████████▌| 236/248 [01:00<00:03,  3.73it/s] 96%|█████████▌| 237/248 [01:01<00:02,  3.69it/s] 96%|█████████▌| 238/248 [01:01<00:02,  3.63it/s] 96%|█████████▋| 239/248 [01:01<00:02,  3.75it/s] 97%|█████████▋| 240/248 [01:02<00:02,  3.73it/s] 97%|█████████▋| 241/248 [01:02<00:01,  3.68it/s] 98%|█████████▊| 242/248 [01:02<00:01,  3.63it/s] 98%|█████████▊| 243/248 [01:02<00:01,  3.75it/s] 98%|█████████▊| 244/248 [01:03<00:01,  3.74it/s] 99%|█████████▉| 245/248 [01:03<00:00,  3.70it/s] 99%|█████████▉| 246/248 [01:03<00:00,  3.66it/s]100%|█████████▉| 247/248 [01:03<00:00,  3.71it/s]100%|██████████| 248/248 [01:04<00:00,  3.74it/s]accuracy:  0.5806451612903226
100%|██████████| 248/248 [01:07<00:00,  3.65it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 236
Training dataset size: 96, validation dataset size: 236
Training dataset size: 96, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.79s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:17:46,175] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 03:17:46,246] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:17:47,017] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:37,  2.50s/it] 12%|█▎        | 2/16 [00:05<00:36,  2.57s/it] 19%|█▉        | 3/16 [00:07<00:31,  2.41s/it] 25%|██▌       | 4/16 [00:09<00:26,  2.20s/it] 31%|███▏      | 5/16 [00:11<00:25,  2.33s/it] 38%|███▊      | 6/16 [00:14<00:23,  2.31s/it] 44%|████▍     | 7/16 [00:16<00:20,  2.32s/it] 50%|█████     | 8/16 [00:18<00:17,  2.22s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:20<00:15,  2.25s/it] 62%|██████▎   | 10/16 [00:22<00:13,  2.25s/it]                                               {'loss': 1.0414, 'grad_norm': 9.431879043579102, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:22<00:13,  2.25s/it] 69%|██████▉   | 11/16 [00:25<00:11,  2.31s/it] 75%|███████▌  | 12/16 [00:28<00:09,  2.42s/it] 81%|████████▏ | 13/16 [00:30<00:07,  2.40s/it] 88%|████████▊ | 14/16 [00:32<00:04,  2.33s/it] 94%|█████████▍| 15/16 [00:34<00:02,  2.30s/it]100%|██████████| 16/16 [00:36<00:00,  2.23s/it]                                               {'train_runtime': 37.546, 'train_samples_per_second': 5.114, 'train_steps_per_second': 0.426, 'train_loss': 0.9585944414138794, 'epoch': 2.0}
100%|██████████| 16/16 [00:37<00:00,  2.23s/it]100%|██████████| 16/16 [00:37<00:00,  2.33s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<01:58,  3.03it/s]  1%|          | 2/361 [00:00<01:39,  3.60it/s]  1%|          | 3/361 [00:00<01:33,  3.82it/s]  1%|          | 4/361 [00:01<01:30,  3.94it/s]  1%|▏         | 5/361 [00:01<01:28,  4.00it/s]  2%|▏         | 6/361 [00:01<01:27,  4.05it/s]  2%|▏         | 7/361 [00:01<01:26,  4.09it/s]  2%|▏         | 8/361 [00:02<01:25,  4.11it/s]  2%|▏         | 9/361 [00:02<01:25,  4.13it/s]  3%|▎         | 10/361 [00:02<01:24,  4.14it/s]  3%|▎         | 11/361 [00:02<01:24,  4.14it/s]  3%|▎         | 12/361 [00:02<01:24,  4.15it/s]  4%|▎         | 13/361 [00:03<01:23,  4.15it/s]  4%|▍         | 14/361 [00:03<01:23,  4.16it/s]  4%|▍         | 15/361 [00:03<01:23,  4.16it/s]  4%|▍         | 16/361 [00:03<01:22,  4.16it/s]  5%|▍         | 17/361 [00:04<01:22,  4.16it/s]  5%|▍         | 18/361 [00:04<01:22,  4.16it/s]  5%|▌         | 19/361 [00:04<01:22,  4.16it/s]  6%|▌         | 20/361 [00:04<01:22,  4.15it/s]  6%|▌         | 21/361 [00:05<01:21,  4.15it/s]  6%|▌         | 22/361 [00:05<01:21,  4.14it/s]  6%|▋         | 23/361 [00:05<01:21,  4.13it/s]  7%|▋         | 24/361 [00:05<01:21,  4.12it/s]  7%|▋         | 25/361 [00:06<01:24,  3.99it/s]  7%|▋         | 26/361 [00:06<01:25,  3.94it/s]  7%|▋         | 27/361 [00:06<01:25,  3.90it/s]  8%|▊         | 28/361 [00:06<01:25,  3.91it/s]  8%|▊         | 29/361 [00:07<01:25,  3.89it/s]  8%|▊         | 30/361 [00:07<01:27,  3.79it/s]  9%|▊         | 31/361 [00:07<01:26,  3.81it/s]  9%|▉         | 32/361 [00:08<01:27,  3.74it/s]  9%|▉         | 33/361 [00:08<01:25,  3.82it/s]  9%|▉         | 34/361 [00:08<01:26,  3.78it/s] 10%|▉         | 35/361 [00:08<01:27,  3.74it/s] 10%|▉         | 36/361 [00:09<01:27,  3.72it/s] 10%|█         | 37/361 [00:09<01:25,  3.79it/s] 11%|█         | 38/361 [00:09<01:24,  3.82it/s] 11%|█         | 39/361 [00:09<01:26,  3.74it/s] 11%|█         | 40/361 [00:10<01:24,  3.78it/s] 11%|█▏        | 41/361 [00:10<01:26,  3.72it/s] 12%|█▏        | 42/361 [00:10<01:24,  3.77it/s] 12%|█▏        | 43/361 [00:10<01:26,  3.70it/s] 12%|█▏        | 44/361 [00:11<01:24,  3.74it/s] 12%|█▏        | 45/361 [00:11<01:25,  3.69it/s] 13%|█▎        | 46/361 [00:11<01:23,  3.78it/s] 13%|█▎        | 47/361 [00:11<01:23,  3.75it/s] 13%|█▎        | 48/361 [00:12<01:24,  3.72it/s] 14%|█▎        | 49/361 [00:12<01:25,  3.66it/s] 14%|█▍        | 50/361 [00:12<01:22,  3.78it/s] 14%|█▍        | 51/361 [00:13<01:22,  3.74it/s] 14%|█▍        | 52/361 [00:13<01:23,  3.71it/s] 15%|█▍        | 53/361 [00:13<01:22,  3.75it/s] 15%|█▍        | 54/361 [00:13<01:22,  3.72it/s] 15%|█▌        | 55/361 [00:14<01:21,  3.77it/s] 16%|█▌        | 56/361 [00:14<01:22,  3.71it/s] 16%|█▌        | 57/361 [00:14<01:21,  3.75it/s] 16%|█▌        | 58/361 [00:14<01:21,  3.70it/s] 16%|█▋        | 59/361 [00:15<01:19,  3.78it/s] 17%|█▋        | 60/361 [00:15<01:20,  3.74it/s] 17%|█▋        | 61/361 [00:15<01:20,  3.72it/s] 17%|█▋        | 62/361 [00:16<01:21,  3.67it/s] 17%|█▋        | 63/361 [00:16<01:18,  3.77it/s] 18%|█▊        | 64/361 [00:16<01:18,  3.78it/s] 18%|█▊        | 65/361 [00:16<01:19,  3.70it/s] 18%|█▊        | 66/361 [00:17<01:18,  3.75it/s] 19%|█▊        | 67/361 [00:17<01:19,  3.68it/s] 19%|█▉        | 68/361 [00:17<01:18,  3.74it/s] 19%|█▉        | 69/361 [00:17<01:19,  3.68it/s] 19%|█▉        | 70/361 [00:18<01:18,  3.72it/s] 20%|█▉        | 71/361 [00:18<01:19,  3.67it/s] 20%|█▉        | 72/361 [00:18<01:17,  3.75it/s] 20%|██        | 73/361 [00:18<01:18,  3.69it/s] 20%|██        | 74/361 [00:19<01:17,  3.72it/s] 21%|██        | 75/361 [00:19<01:17,  3.67it/s] 21%|██        | 76/361 [00:19<01:16,  3.74it/s] 21%|██▏       | 77/361 [00:20<01:17,  3.67it/s] 22%|██▏       | 78/361 [00:20<01:16,  3.71it/s] 22%|██▏       | 79/361 [00:20<01:16,  3.67it/s] 22%|██▏       | 80/361 [00:20<01:15,  3.74it/s] 22%|██▏       | 81/361 [00:21<01:16,  3.68it/s] 23%|██▎       | 82/361 [00:21<01:15,  3.72it/s] 23%|██▎       | 83/361 [00:21<01:15,  3.67it/s] 23%|██▎       | 84/361 [00:21<01:13,  3.76it/s] 24%|██▎       | 85/361 [00:22<01:14,  3.69it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.72it/s] 24%|██▍       | 87/361 [00:22<01:14,  3.67it/s] 24%|██▍       | 88/361 [00:23<01:13,  3.73it/s] 25%|██▍       | 89/361 [00:23<01:14,  3.67it/s] 25%|██▍       | 90/361 [00:23<01:12,  3.72it/s] 25%|██▌       | 91/361 [00:23<01:13,  3.67it/s] 25%|██▌       | 92/361 [00:24<01:12,  3.72it/s] 26%|██▌       | 93/361 [00:24<01:12,  3.68it/s] 26%|██▌       | 94/361 [00:24<01:11,  3.71it/s] 26%|██▋       | 95/361 [00:24<01:12,  3.67it/s] 27%|██▋       | 96/361 [00:25<01:10,  3.74it/s] 27%|██▋       | 97/361 [00:25<01:11,  3.68it/s] 27%|██▋       | 98/361 [00:25<01:10,  3.73it/s] 27%|██▋       | 99/361 [00:26<01:11,  3.68it/s] 28%|██▊       | 100/361 [00:26<01:09,  3.73it/s] 28%|██▊       | 101/361 [00:26<01:10,  3.68it/s] 28%|██▊       | 102/361 [00:26<01:09,  3.72it/s] 29%|██▊       | 103/361 [00:27<01:10,  3.68it/s] 29%|██▉       | 104/361 [00:27<01:08,  3.75it/s] 29%|██▉       | 105/361 [00:27<01:09,  3.69it/s] 29%|██▉       | 106/361 [00:27<01:08,  3.73it/s] 30%|██▉       | 107/361 [00:28<01:09,  3.67it/s] 30%|██▉       | 108/361 [00:28<01:07,  3.73it/s] 30%|███       | 109/361 [00:28<01:08,  3.68it/s] 30%|███       | 110/361 [00:28<01:07,  3.72it/s] 31%|███       | 111/361 [00:29<01:08,  3.68it/s] 31%|███       | 112/361 [00:29<01:06,  3.75it/s] 31%|███▏      | 113/361 [00:29<01:07,  3.69it/s] 32%|███▏      | 114/361 [00:30<01:06,  3.70it/s] 32%|███▏      | 115/361 [00:30<01:07,  3.66it/s] 32%|███▏      | 116/361 [00:30<01:05,  3.77it/s] 32%|███▏      | 117/361 [00:30<01:05,  3.71it/s] 33%|███▎      | 118/361 [00:31<01:06,  3.67it/s] 33%|███▎      | 119/361 [00:31<01:06,  3.62it/s] 33%|███▎      | 120/361 [00:31<01:04,  3.75it/s] 34%|███▎      | 121/361 [00:31<01:04,  3.72it/s] 34%|███▍      | 122/361 [00:32<01:04,  3.69it/s] 34%|███▍      | 123/361 [00:32<01:04,  3.70it/s] 34%|███▍      | 124/361 [00:32<01:04,  3.69it/s] 35%|███▍      | 125/361 [00:33<01:03,  3.73it/s] 35%|███▍      | 126/361 [00:33<01:04,  3.66it/s] 35%|███▌      | 127/361 [00:33<01:03,  3.71it/s] 35%|███▌      | 128/361 [00:33<01:03,  3.67it/s] 36%|███▌      | 129/361 [00:34<01:02,  3.73it/s] 36%|███▌      | 130/361 [00:34<01:02,  3.69it/s] 36%|███▋      | 131/361 [00:34<01:01,  3.72it/s] 37%|███▋      | 132/361 [00:34<01:02,  3.67it/s] 37%|███▋      | 133/361 [00:35<01:00,  3.75it/s] 37%|███▋      | 134/361 [00:35<01:01,  3.69it/s] 37%|███▋      | 135/361 [00:35<01:01,  3.70it/s] 38%|███▊      | 136/361 [00:36<01:01,  3.65it/s] 38%|███▊      | 137/361 [00:36<00:59,  3.76it/s] 38%|███▊      | 138/361 [00:36<01:00,  3.70it/s] 39%|███▊      | 139/361 [00:36<01:00,  3.68it/s] 39%|███▉      | 140/361 [00:37<01:00,  3.67it/s] 39%|███▉      | 141/361 [00:37<00:57,  3.79it/s] 39%|███▉      | 142/361 [00:37<00:56,  3.87it/s] 40%|███▉      | 143/361 [00:37<00:55,  3.93it/s] 40%|███▉      | 144/361 [00:38<00:54,  3.99it/s] 40%|████      | 145/361 [00:38<00:53,  4.03it/s] 40%|████      | 146/361 [00:38<00:53,  4.05it/s] 41%|████      | 147/361 [00:38<00:52,  4.05it/s] 41%|████      | 148/361 [00:39<00:52,  4.07it/s] 41%|████▏     | 149/361 [00:39<00:51,  4.09it/s] 42%|████▏     | 150/361 [00:39<00:51,  4.11it/s] 42%|████▏     | 151/361 [00:39<00:51,  4.11it/s] 42%|████▏     | 152/361 [00:40<00:52,  3.98it/s] 42%|████▏     | 153/361 [00:40<00:53,  3.89it/s] 43%|████▎     | 154/361 [00:40<00:53,  3.87it/s] 43%|████▎     | 155/361 [00:40<00:52,  3.95it/s] 43%|████▎     | 156/361 [00:41<00:52,  3.93it/s] 43%|████▎     | 157/361 [00:41<00:53,  3.83it/s] 44%|████▍     | 158/361 [00:41<00:54,  3.75it/s] 44%|████▍     | 159/361 [00:41<00:55,  3.65it/s] 44%|████▍     | 160/361 [00:42<00:53,  3.78it/s] 45%|████▍     | 161/361 [00:42<00:51,  3.87it/s] 45%|████▍     | 162/361 [00:42<00:52,  3.77it/s] 45%|████▌     | 163/361 [00:42<00:53,  3.71it/s] 45%|████▌     | 164/361 [00:43<00:53,  3.71it/s] 46%|████▌     | 165/361 [00:43<00:53,  3.63it/s] 46%|████▌     | 166/361 [00:43<00:51,  3.77it/s] 46%|████▋     | 167/361 [00:44<00:50,  3.85it/s] 47%|████▋     | 168/361 [00:44<00:51,  3.77it/s] 47%|████▋     | 169/361 [00:44<00:51,  3.70it/s] 47%|████▋     | 170/361 [00:44<00:52,  3.66it/s] 47%|████▋     | 171/361 [00:45<00:50,  3.75it/s] 48%|████▊     | 172/361 [00:45<00:49,  3.84it/s] 48%|████▊     | 173/361 [00:45<00:49,  3.76it/s] 48%|████▊     | 174/361 [00:45<00:50,  3.69it/s] 48%|████▊     | 175/361 [00:46<00:49,  3.74it/s] 49%|████▉     | 176/361 [00:46<00:50,  3.69it/s] 49%|████▉     | 177/361 [00:46<00:48,  3.81it/s] 49%|████▉     | 178/361 [00:46<00:48,  3.80it/s] 50%|████▉     | 179/361 [00:47<00:49,  3.70it/s] 50%|████▉     | 180/361 [00:47<00:49,  3.69it/s] 50%|█████     | 181/361 [00:47<00:49,  3.63it/s] 50%|█████     | 182/361 [00:48<00:47,  3.77it/s] 51%|█████     | 183/361 [00:48<00:46,  3.84it/s] 51%|█████     | 184/361 [00:48<00:47,  3.76it/s] 51%|█████     | 185/361 [00:48<00:47,  3.71it/s] 52%|█████▏    | 186/361 [00:49<00:47,  3.69it/s] 52%|█████▏    | 187/361 [00:49<00:46,  3.74it/s] 52%|█████▏    | 188/361 [00:49<00:45,  3.84it/s] 52%|█████▏    | 189/361 [00:49<00:45,  3.75it/s] 53%|█████▎    | 190/361 [00:50<00:46,  3.69it/s] 53%|█████▎    | 191/361 [00:50<00:45,  3.75it/s] 53%|█████▎    | 192/361 [00:50<00:45,  3.70it/s] 53%|█████▎    | 193/361 [00:50<00:44,  3.82it/s] 54%|█████▎    | 194/361 [00:51<00:44,  3.79it/s] 54%|█████▍    | 195/361 [00:51<00:45,  3.67it/s] 54%|█████▍    | 196/361 [00:51<00:44,  3.69it/s] 55%|█████▍    | 197/361 [00:52<00:45,  3.62it/s] 55%|█████▍    | 198/361 [00:52<00:43,  3.77it/s] 55%|█████▌    | 199/361 [00:52<00:42,  3.83it/s] 55%|█████▌    | 200/361 [00:52<00:42,  3.75it/s] 56%|█████▌    | 201/361 [00:53<00:43,  3.69it/s] 56%|█████▌    | 202/361 [00:53<00:43,  3.63it/s] 56%|█████▌    | 203/361 [00:53<00:42,  3.73it/s] 57%|█████▋    | 204/361 [00:53<00:41,  3.80it/s] 57%|█████▋    | 205/361 [00:54<00:41,  3.73it/s] 57%|█████▋    | 206/361 [00:54<00:42,  3.68it/s] 57%|█████▋    | 207/361 [00:54<00:41,  3.70it/s] 58%|█████▊    | 208/361 [00:55<00:41,  3.69it/s] 58%|█████▊    | 209/361 [00:55<00:39,  3.80it/s] 58%|█████▊    | 210/361 [00:55<00:39,  3.79it/s] 58%|█████▊    | 211/361 [00:55<00:40,  3.67it/s] 59%|█████▊    | 212/361 [00:56<00:40,  3.68it/s] 59%|█████▉    | 213/361 [00:56<00:41,  3.61it/s] 59%|█████▉    | 214/361 [00:56<00:39,  3.75it/s] 60%|█████▉    | 215/361 [00:56<00:38,  3.82it/s] 60%|█████▉    | 216/361 [00:57<00:38,  3.73it/s] 60%|██████    | 217/361 [00:57<00:39,  3.68it/s] 60%|██████    | 218/361 [00:57<00:38,  3.68it/s] 61%|██████    | 219/361 [00:57<00:38,  3.70it/s] 61%|██████    | 220/361 [00:58<00:37,  3.81it/s] 61%|██████    | 221/361 [00:58<00:37,  3.72it/s] 61%|██████▏   | 222/361 [00:58<00:37,  3.68it/s] 62%|██████▏   | 223/361 [00:59<00:37,  3.68it/s] 62%|██████▏   | 224/361 [00:59<00:37,  3.62it/s] 62%|██████▏   | 225/361 [00:59<00:36,  3.75it/s] 63%|██████▎   | 226/361 [00:59<00:35,  3.80it/s] 63%|██████▎   | 227/361 [01:00<00:35,  3.74it/s] 63%|██████▎   | 228/361 [01:00<00:36,  3.69it/s] 63%|██████▎   | 229/361 [01:00<00:35,  3.68it/s] 64%|██████▎   | 230/361 [01:00<00:35,  3.70it/s] 64%|██████▍   | 231/361 [01:01<00:34,  3.81it/s] 64%|██████▍   | 232/361 [01:01<00:34,  3.79it/s] 65%|██████▍   | 233/361 [01:01<00:34,  3.67it/s] 65%|██████▍   | 234/361 [01:02<00:34,  3.69it/s] 65%|██████▌   | 235/361 [01:02<00:34,  3.62it/s] 65%|██████▌   | 236/361 [01:02<00:33,  3.76it/s] 66%|██████▌   | 237/361 [01:02<00:32,  3.85it/s] 66%|██████▌   | 238/361 [01:03<00:32,  3.76it/s] 66%|██████▌   | 239/361 [01:03<00:33,  3.68it/s] 66%|██████▋   | 240/361 [01:03<00:32,  3.72it/s] 67%|██████▋   | 241/361 [01:03<00:32,  3.67it/s] 67%|██████▋   | 242/361 [01:04<00:31,  3.79it/s] 67%|██████▋   | 243/361 [01:04<00:31,  3.72it/s] 68%|██████▊   | 244/361 [01:04<00:31,  3.67it/s] 68%|██████▊   | 245/361 [01:04<00:31,  3.65it/s] 68%|██████▊   | 246/361 [01:05<00:30,  3.78it/s] 68%|██████▊   | 247/361 [01:05<00:29,  3.86it/s] 69%|██████▊   | 248/361 [01:05<00:28,  3.91it/s] 69%|██████▉   | 249/361 [01:05<00:28,  3.97it/s] 69%|██████▉   | 250/361 [01:06<00:27,  4.01it/s] 70%|██████▉   | 251/361 [01:06<00:27,  4.04it/s] 70%|██████▉   | 252/361 [01:06<00:26,  4.05it/s] 70%|███████   | 253/361 [01:06<00:26,  4.06it/s] 70%|███████   | 254/361 [01:07<00:26,  4.07it/s] 71%|███████   | 255/361 [01:07<00:25,  4.09it/s] 71%|███████   | 256/361 [01:07<00:25,  4.09it/s] 71%|███████   | 257/361 [01:07<00:25,  4.08it/s] 71%|███████▏  | 258/361 [01:08<00:25,  4.09it/s] 72%|███████▏  | 259/361 [01:08<00:24,  4.10it/s] 72%|███████▏  | 260/361 [01:08<00:24,  4.10it/s] 72%|███████▏  | 261/361 [01:08<00:24,  4.09it/s] 73%|███████▎  | 262/361 [01:09<00:24,  4.08it/s] 73%|███████▎  | 263/361 [01:09<00:23,  4.09it/s] 73%|███████▎  | 264/361 [01:09<00:23,  4.10it/s] 73%|███████▎  | 265/361 [01:09<00:23,  4.10it/s] 74%|███████▎  | 266/361 [01:10<00:23,  4.09it/s] 74%|███████▍  | 267/361 [01:10<00:22,  4.09it/s] 74%|███████▍  | 268/361 [01:10<00:22,  4.10it/s] 75%|███████▍  | 269/361 [01:10<00:22,  4.10it/s] 75%|███████▍  | 270/361 [01:11<00:22,  4.00it/s] 75%|███████▌  | 271/361 [01:11<00:23,  3.88it/s] 75%|███████▌  | 272/361 [01:11<00:23,  3.80it/s] 76%|███████▌  | 273/361 [01:11<00:22,  3.89it/s] 76%|███████▌  | 274/361 [01:12<00:22,  3.91it/s] 76%|███████▌  | 275/361 [01:12<00:22,  3.78it/s] 76%|███████▋  | 276/361 [01:12<00:22,  3.76it/s] 77%|███████▋  | 277/361 [01:12<00:22,  3.71it/s] 77%|███████▋  | 278/361 [01:13<00:21,  3.81it/s] 77%|███████▋  | 279/361 [01:13<00:21,  3.80it/s] 78%|███████▊  | 280/361 [01:13<00:21,  3.71it/s] 78%|███████▊  | 281/361 [01:14<00:21,  3.67it/s] 78%|███████▊  | 282/361 [01:14<00:21,  3.73it/s] 78%|███████▊  | 283/361 [01:14<00:20,  3.76it/s] 79%|███████▊  | 284/361 [01:14<00:20,  3.70it/s] 79%|███████▉  | 285/361 [01:15<00:20,  3.73it/s] 79%|███████▉  | 286/361 [01:15<00:20,  3.67it/s] 80%|███████▉  | 287/361 [01:15<00:19,  3.76it/s] 80%|███████▉  | 288/361 [01:15<00:19,  3.68it/s] 80%|████████  | 289/361 [01:16<00:19,  3.70it/s] 80%|████████  | 290/361 [01:16<00:19,  3.65it/s] 81%|████████  | 291/361 [01:16<00:18,  3.77it/s] 81%|████████  | 292/361 [01:17<00:18,  3.77it/s] 81%|████████  | 293/361 [01:17<00:18,  3.69it/s] 81%|████████▏ | 294/361 [01:17<00:18,  3.63it/s] 82%|████████▏ | 295/361 [01:17<00:17,  3.72it/s] 82%|████████▏ | 296/361 [01:18<00:17,  3.76it/s] 82%|████████▏ | 297/361 [01:18<00:17,  3.68it/s] 83%|████████▎ | 298/361 [01:18<00:16,  3.71it/s] 83%|████████▎ | 299/361 [01:18<00:16,  3.66it/s] 83%|████████▎ | 300/361 [01:19<00:16,  3.76it/s] 83%|████████▎ | 301/361 [01:19<00:16,  3.69it/s] 84%|████████▎ | 302/361 [01:19<00:15,  3.69it/s] 84%|████████▍ | 303/361 [01:20<00:15,  3.66it/s] 84%|████████▍ | 304/361 [01:20<00:15,  3.78it/s] 84%|████████▍ | 305/361 [01:20<00:15,  3.73it/s] 85%|████████▍ | 306/361 [01:20<00:14,  3.71it/s] 85%|████████▌ | 307/361 [01:21<00:14,  3.61it/s] 85%|████████▌ | 308/361 [01:21<00:14,  3.74it/s] 86%|████████▌ | 309/361 [01:21<00:13,  3.79it/s] 86%|████████▌ | 310/361 [01:21<00:13,  3.70it/s] 86%|████████▌ | 311/361 [01:22<00:13,  3.73it/s] 86%|████████▋ | 312/361 [01:22<00:13,  3.67it/s] 87%|████████▋ | 313/361 [01:22<00:12,  3.77it/s] 87%|████████▋ | 314/361 [01:22<00:12,  3.71it/s] 87%|████████▋ | 315/361 [01:23<00:12,  3.68it/s] 88%|████████▊ | 316/361 [01:23<00:12,  3.65it/s] 88%|████████▊ | 317/361 [01:23<00:11,  3.77it/s] 88%|████████▊ | 318/361 [01:24<00:11,  3.71it/s] 88%|████████▊ | 319/361 [01:24<00:11,  3.69it/s] 89%|████████▊ | 320/361 [01:24<00:11,  3.61it/s] 89%|████████▉ | 321/361 [01:24<00:10,  3.75it/s] 89%|████████▉ | 322/361 [01:25<00:10,  3.80it/s] 89%|████████▉ | 323/361 [01:25<00:10,  3.72it/s] 90%|████████▉ | 324/361 [01:25<00:09,  3.72it/s] 90%|█████████ | 325/361 [01:25<00:09,  3.67it/s] 90%|█████████ | 326/361 [01:26<00:09,  3.77it/s] 91%|█████████ | 327/361 [01:26<00:09,  3.75it/s] 91%|█████████ | 328/361 [01:26<00:08,  3.69it/s] 91%|█████████ | 329/361 [01:27<00:08,  3.61it/s] 91%|█████████▏| 330/361 [01:27<00:08,  3.75it/s] 92%|█████████▏| 331/361 [01:27<00:07,  3.77it/s] 92%|█████████▏| 332/361 [01:27<00:07,  3.70it/s] 92%|█████████▏| 333/361 [01:28<00:07,  3.74it/s] 93%|█████████▎| 334/361 [01:28<00:07,  3.69it/s] 93%|█████████▎| 335/361 [01:28<00:06,  3.76it/s] 93%|█████████▎| 336/361 [01:28<00:06,  3.68it/s] 93%|█████████▎| 337/361 [01:29<00:06,  3.70it/s] 94%|█████████▎| 338/361 [01:29<00:06,  3.66it/s] 94%|█████████▍| 339/361 [01:29<00:05,  3.77it/s] 94%|█████████▍| 340/361 [01:29<00:05,  3.76it/s] 94%|█████████▍| 341/361 [01:30<00:05,  3.69it/s] 95%|█████████▍| 342/361 [01:30<00:05,  3.61it/s] 95%|█████████▌| 343/361 [01:30<00:04,  3.75it/s] 95%|█████████▌| 344/361 [01:31<00:04,  3.77it/s] 96%|█████████▌| 345/361 [01:31<00:04,  3.70it/s] 96%|█████████▌| 346/361 [01:31<00:04,  3.73it/s] 96%|█████████▌| 347/361 [01:31<00:03,  3.67it/s] 96%|█████████▋| 348/361 [01:32<00:03,  3.77it/s] 97%|█████████▋| 349/361 [01:32<00:03,  3.71it/s] 97%|█████████▋| 350/361 [01:32<00:02,  3.68it/s] 97%|█████████▋| 351/361 [01:32<00:02,  3.65it/s] 98%|█████████▊| 352/361 [01:33<00:02,  3.77it/s] 98%|█████████▊| 353/361 [01:33<00:02,  3.72it/s] 98%|█████████▊| 354/361 [01:33<00:01,  3.70it/s] 98%|█████████▊| 355/361 [01:34<00:01,  3.62it/s] 99%|█████████▊| 356/361 [01:34<00:01,  3.75it/s] 99%|█████████▉| 357/361 [01:34<00:01,  3.77it/s] 99%|█████████▉| 358/361 [01:34<00:00,  3.68it/s] 99%|█████████▉| 359/361 [01:35<00:00,  3.66it/s]100%|█████████▉| 360/361 [01:35<00:00,  3.70it/s]100%|██████████| 361/361 [01:35<00:00,  3.76it/s]accuracy:  0.5983379501385041
100%|██████████| 361/361 [01:40<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 188
Training dataset size: 96, validation dataset size: 188
Training dataset size: 96, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:20:30,903] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 03:20:30,977] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:20:31,134] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:42,  2.80s/it] 12%|█▎        | 2/16 [00:05<00:39,  2.81s/it] 19%|█▉        | 3/16 [00:08<00:36,  2.83s/it] 25%|██▌       | 4/16 [00:11<00:33,  2.76s/it] 31%|███▏      | 5/16 [00:14<00:31,  2.83s/it] 38%|███▊      | 6/16 [00:16<00:27,  2.77s/it] 44%|████▍     | 7/16 [00:19<00:25,  2.80s/it] 50%|█████     | 8/16 [00:22<00:21,  2.72s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:25<00:20,  2.89s/it] 62%|██████▎   | 10/16 [00:28<00:17,  2.97s/it]                                               {'loss': 0.628, 'grad_norm': 6.480288028717041, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:28<00:17,  2.97s/it] 69%|██████▉   | 11/16 [00:31<00:14,  2.97s/it] 75%|███████▌  | 12/16 [00:34<00:11,  2.95s/it] 81%|████████▏ | 13/16 [00:36<00:08,  2.73s/it] 88%|████████▊ | 14/16 [00:39<00:05,  2.80s/it] 94%|█████████▍| 15/16 [00:42<00:02,  2.88s/it]100%|██████████| 16/16 [00:45<00:00,  2.77s/it]                                               {'train_runtime': 46.0614, 'train_samples_per_second': 4.168, 'train_steps_per_second': 0.347, 'train_loss': 0.6012337058782578, 'epoch': 2.0}
100%|██████████| 16/16 [00:45<00:00,  2.77s/it]100%|██████████| 16/16 [00:45<00:00,  2.87s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:10,  3.01it/s]  1%|          | 2/212 [00:00<00:58,  3.57it/s]  1%|▏         | 3/212 [00:00<00:55,  3.80it/s]  2%|▏         | 4/212 [00:01<00:53,  3.92it/s]  2%|▏         | 5/212 [00:01<00:51,  4.00it/s]  3%|▎         | 6/212 [00:01<00:50,  4.05it/s]  3%|▎         | 7/212 [00:01<00:50,  4.09it/s]  4%|▍         | 8/212 [00:02<00:49,  4.10it/s]  4%|▍         | 9/212 [00:02<00:49,  4.10it/s]  5%|▍         | 10/212 [00:02<00:50,  4.01it/s]  5%|▌         | 11/212 [00:02<00:51,  3.89it/s]  6%|▌         | 12/212 [00:03<00:51,  3.88it/s]  6%|▌         | 13/212 [00:03<00:51,  3.87it/s]  7%|▋         | 14/212 [00:03<00:52,  3.80it/s]  7%|▋         | 15/212 [00:03<00:52,  3.74it/s]  8%|▊         | 16/212 [00:04<00:51,  3.79it/s]  8%|▊         | 17/212 [00:04<00:51,  3.77it/s]  8%|▊         | 18/212 [00:04<00:51,  3.74it/s]  9%|▉         | 19/212 [00:04<00:52,  3.69it/s]  9%|▉         | 20/212 [00:05<00:50,  3.77it/s] 10%|▉         | 21/212 [00:05<00:51,  3.73it/s] 10%|█         | 22/212 [00:05<00:51,  3.69it/s] 11%|█         | 23/212 [00:06<00:50,  3.75it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.73it/s] 12%|█▏        | 25/212 [00:06<00:50,  3.72it/s] 12%|█▏        | 26/212 [00:06<00:50,  3.68it/s] 13%|█▎        | 27/212 [00:07<00:49,  3.75it/s] 13%|█▎        | 28/212 [00:07<00:49,  3.71it/s] 14%|█▎        | 29/212 [00:07<00:49,  3.68it/s] 14%|█▍        | 30/212 [00:07<00:48,  3.75it/s] 15%|█▍        | 31/212 [00:08<00:48,  3.72it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.69it/s] 16%|█▌        | 33/212 [00:08<00:48,  3.67it/s] 16%|█▌        | 34/212 [00:08<00:47,  3.77it/s] 17%|█▋        | 35/212 [00:09<00:47,  3.71it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.69it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.73it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.73it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.70it/s] 19%|█▉        | 40/212 [00:10<00:47,  3.64it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.77it/s] 20%|█▉        | 42/212 [00:11<00:45,  3.73it/s] 20%|██        | 43/212 [00:11<00:45,  3.71it/s] 21%|██        | 44/212 [00:11<00:45,  3.67it/s] 21%|██        | 45/212 [00:11<00:44,  3.76it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.71it/s] 22%|██▏       | 47/212 [00:12<00:45,  3.65it/s] 23%|██▎       | 48/212 [00:12<00:44,  3.72it/s] 23%|██▎       | 49/212 [00:13<00:43,  3.72it/s] 24%|██▎       | 50/212 [00:13<00:43,  3.69it/s] 24%|██▍       | 51/212 [00:13<00:43,  3.66it/s] 25%|██▍       | 52/212 [00:13<00:42,  3.75it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.70it/s] 25%|██▌       | 54/212 [00:14<00:43,  3.66it/s] 26%|██▌       | 55/212 [00:14<00:41,  3.74it/s] 26%|██▋       | 56/212 [00:14<00:41,  3.73it/s] 27%|██▋       | 57/212 [00:15<00:41,  3.70it/s] 27%|██▋       | 58/212 [00:15<00:42,  3.66it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.75it/s] 28%|██▊       | 60/212 [00:15<00:41,  3.71it/s] 29%|██▉       | 61/212 [00:16<00:41,  3.66it/s] 29%|██▉       | 62/212 [00:16<00:40,  3.73it/s] 30%|██▉       | 63/212 [00:16<00:40,  3.72it/s] 30%|███       | 64/212 [00:17<00:40,  3.70it/s] 31%|███       | 65/212 [00:17<00:40,  3.66it/s] 31%|███       | 66/212 [00:17<00:38,  3.76it/s] 32%|███▏      | 67/212 [00:17<00:39,  3.71it/s] 32%|███▏      | 68/212 [00:18<00:39,  3.67it/s] 33%|███▎      | 69/212 [00:18<00:38,  3.73it/s] 33%|███▎      | 70/212 [00:18<00:38,  3.73it/s] 33%|███▎      | 71/212 [00:18<00:38,  3.71it/s] 34%|███▍      | 72/212 [00:19<00:38,  3.63it/s] 34%|███▍      | 73/212 [00:19<00:37,  3.75it/s] 35%|███▍      | 74/212 [00:19<00:37,  3.71it/s] 35%|███▌      | 75/212 [00:20<00:37,  3.69it/s] 36%|███▌      | 76/212 [00:20<00:37,  3.66it/s] 36%|███▋      | 77/212 [00:20<00:36,  3.73it/s] 37%|███▋      | 78/212 [00:20<00:36,  3.69it/s] 37%|███▋      | 79/212 [00:21<00:36,  3.67it/s] 38%|███▊      | 80/212 [00:21<00:35,  3.72it/s] 38%|███▊      | 81/212 [00:21<00:35,  3.73it/s] 39%|███▊      | 82/212 [00:21<00:35,  3.71it/s] 39%|███▉      | 83/212 [00:22<00:35,  3.64it/s] 40%|███▉      | 84/212 [00:22<00:34,  3.74it/s] 40%|████      | 85/212 [00:22<00:34,  3.72it/s] 41%|████      | 86/212 [00:23<00:34,  3.68it/s] 41%|████      | 87/212 [00:23<00:34,  3.65it/s] 42%|████▏     | 88/212 [00:23<00:33,  3.73it/s] 42%|████▏     | 89/212 [00:23<00:33,  3.69it/s] 42%|████▏     | 90/212 [00:24<00:33,  3.65it/s] 43%|████▎     | 91/212 [00:24<00:32,  3.72it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.71it/s] 44%|████▍     | 93/212 [00:24<00:32,  3.69it/s] 44%|████▍     | 94/212 [00:25<00:32,  3.65it/s] 45%|████▍     | 95/212 [00:25<00:31,  3.76it/s] 45%|████▌     | 96/212 [00:25<00:31,  3.72it/s] 46%|████▌     | 97/212 [00:25<00:31,  3.70it/s] 46%|████▌     | 98/212 [00:26<00:31,  3.67it/s] 47%|████▋     | 99/212 [00:26<00:30,  3.76it/s] 47%|████▋     | 100/212 [00:26<00:30,  3.70it/s] 48%|████▊     | 101/212 [00:27<00:29,  3.73it/s] 48%|████▊     | 102/212 [00:27<00:29,  3.70it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.72it/s] 49%|████▉     | 104/212 [00:27<00:29,  3.68it/s] 50%|████▉     | 105/212 [00:28<00:29,  3.68it/s] 50%|█████     | 106/212 [00:28<00:28,  3.69it/s] 50%|█████     | 107/212 [00:28<00:28,  3.70it/s] 51%|█████     | 108/212 [00:28<00:28,  3.68it/s] 51%|█████▏    | 109/212 [00:29<00:28,  3.64it/s] 52%|█████▏    | 110/212 [00:29<00:27,  3.76it/s] 52%|█████▏    | 111/212 [00:29<00:27,  3.72it/s] 53%|█████▎    | 112/212 [00:30<00:27,  3.70it/s] 53%|█████▎    | 113/212 [00:30<00:27,  3.65it/s] 54%|█████▍    | 114/212 [00:30<00:26,  3.76it/s] 54%|█████▍    | 115/212 [00:30<00:26,  3.72it/s] 55%|█████▍    | 116/212 [00:31<00:25,  3.70it/s] 55%|█████▌    | 117/212 [00:31<00:25,  3.66it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.75it/s] 56%|█████▌    | 119/212 [00:31<00:25,  3.70it/s] 57%|█████▋    | 120/212 [00:32<00:24,  3.76it/s] 57%|█████▋    | 121/212 [00:32<00:24,  3.69it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.74it/s] 58%|█████▊    | 123/212 [00:33<00:24,  3.70it/s] 58%|█████▊    | 124/212 [00:33<00:23,  3.73it/s] 59%|█████▉    | 125/212 [00:33<00:23,  3.70it/s] 59%|█████▉    | 126/212 [00:33<00:23,  3.71it/s] 60%|█████▉    | 127/212 [00:34<00:23,  3.70it/s] 60%|██████    | 128/212 [00:34<00:22,  3.66it/s] 61%|██████    | 129/212 [00:34<00:22,  3.72it/s] 61%|██████▏   | 130/212 [00:34<00:22,  3.71it/s] 62%|██████▏   | 131/212 [00:35<00:22,  3.68it/s] 62%|██████▏   | 132/212 [00:35<00:21,  3.64it/s] 63%|██████▎   | 133/212 [00:35<00:21,  3.74it/s] 63%|██████▎   | 134/212 [00:35<00:21,  3.70it/s] 64%|██████▎   | 135/212 [00:36<00:20,  3.68it/s] 64%|██████▍   | 136/212 [00:36<00:20,  3.64it/s] 65%|██████▍   | 137/212 [00:36<00:20,  3.73it/s] 65%|██████▌   | 138/212 [00:37<00:20,  3.69it/s] 66%|██████▌   | 139/212 [00:37<00:19,  3.75it/s] 66%|██████▌   | 140/212 [00:37<00:19,  3.69it/s] 67%|██████▋   | 141/212 [00:37<00:19,  3.70it/s] 67%|██████▋   | 142/212 [00:38<00:18,  3.69it/s] 67%|██████▋   | 143/212 [00:38<00:19,  3.61it/s] 68%|██████▊   | 144/212 [00:38<00:18,  3.73it/s] 68%|██████▊   | 145/212 [00:38<00:18,  3.69it/s] 69%|██████▉   | 146/212 [00:39<00:17,  3.67it/s] 69%|██████▉   | 147/212 [00:39<00:17,  3.63it/s] 70%|██████▉   | 148/212 [00:39<00:17,  3.73it/s] 70%|███████   | 149/212 [00:40<00:17,  3.68it/s] 71%|███████   | 150/212 [00:40<00:16,  3.70it/s] 71%|███████   | 151/212 [00:40<00:16,  3.69it/s] 72%|███████▏  | 152/212 [00:40<00:16,  3.70it/s] 72%|███████▏  | 153/212 [00:41<00:16,  3.67it/s] 73%|███████▎  | 154/212 [00:41<00:16,  3.62it/s] 73%|███████▎  | 155/212 [00:41<00:15,  3.69it/s] 74%|███████▎  | 156/212 [00:41<00:15,  3.69it/s] 74%|███████▍  | 157/212 [00:42<00:15,  3.65it/s] 75%|███████▍  | 158/212 [00:42<00:14,  3.61it/s] 75%|███████▌  | 159/212 [00:42<00:14,  3.71it/s] 75%|███████▌  | 160/212 [00:43<00:14,  3.67it/s] 76%|███████▌  | 161/212 [00:43<00:13,  3.73it/s] 76%|███████▋  | 162/212 [00:43<00:13,  3.67it/s] 77%|███████▋  | 163/212 [00:43<00:13,  3.70it/s] 77%|███████▋  | 164/212 [00:44<00:13,  3.66it/s] 78%|███████▊  | 165/212 [00:44<00:12,  3.67it/s] 78%|███████▊  | 166/212 [00:44<00:12,  3.78it/s] 79%|███████▉  | 167/212 [00:44<00:11,  3.85it/s] 79%|███████▉  | 168/212 [00:45<00:11,  3.92it/s] 80%|███████▉  | 169/212 [00:45<00:10,  3.96it/s] 80%|████████  | 170/212 [00:45<00:10,  3.99it/s] 81%|████████  | 171/212 [00:45<00:10,  4.01it/s] 81%|████████  | 172/212 [00:46<00:09,  4.03it/s] 82%|████████▏ | 173/212 [00:46<00:09,  4.04it/s] 82%|████████▏ | 174/212 [00:46<00:09,  4.04it/s] 83%|████████▎ | 175/212 [00:46<00:09,  4.06it/s] 83%|████████▎ | 176/212 [00:47<00:08,  4.06it/s] 83%|████████▎ | 177/212 [00:47<00:08,  4.06it/s] 84%|████████▍ | 178/212 [00:47<00:08,  4.06it/s] 84%|████████▍ | 179/212 [00:47<00:08,  4.07it/s] 85%|████████▍ | 180/212 [00:48<00:07,  4.07it/s] 85%|████████▌ | 181/212 [00:48<00:07,  4.07it/s] 86%|████████▌ | 182/212 [00:48<00:07,  4.08it/s] 86%|████████▋ | 183/212 [00:48<00:07,  4.08it/s] 87%|████████▋ | 184/212 [00:49<00:06,  4.07it/s] 87%|████████▋ | 185/212 [00:49<00:06,  4.07it/s] 88%|████████▊ | 186/212 [00:49<00:06,  4.07it/s] 88%|████████▊ | 187/212 [00:49<00:06,  4.06it/s] 89%|████████▊ | 188/212 [00:50<00:05,  4.06it/s] 89%|████████▉ | 189/212 [00:50<00:05,  4.06it/s] 90%|████████▉ | 190/212 [00:50<00:05,  3.97it/s] 90%|█████████ | 191/212 [00:50<00:05,  3.87it/s] 91%|█████████ | 192/212 [00:51<00:05,  3.83it/s] 91%|█████████ | 193/212 [00:51<00:04,  3.83it/s] 92%|█████████▏| 194/212 [00:51<00:04,  3.90it/s] 92%|█████████▏| 195/212 [00:51<00:04,  3.81it/s] 92%|█████████▏| 196/212 [00:52<00:04,  3.78it/s] 93%|█████████▎| 197/212 [00:52<00:03,  3.79it/s] 93%|█████████▎| 198/212 [00:52<00:03,  3.71it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.80it/s] 94%|█████████▍| 200/212 [00:53<00:03,  3.79it/s] 95%|█████████▍| 201/212 [00:53<00:02,  3.74it/s] 95%|█████████▌| 202/212 [00:53<00:02,  3.75it/s] 96%|█████████▌| 203/212 [00:54<00:02,  3.69it/s] 96%|█████████▌| 204/212 [00:54<00:02,  3.78it/s] 97%|█████████▋| 205/212 [00:54<00:01,  3.75it/s] 97%|█████████▋| 206/212 [00:54<00:01,  3.73it/s] 98%|█████████▊| 207/212 [00:55<00:01,  3.75it/s] 98%|█████████▊| 208/212 [00:55<00:01,  3.68it/s] 99%|█████████▊| 209/212 [00:55<00:00,  3.78it/s] 99%|█████████▉| 210/212 [00:55<00:00,  3.76it/s]100%|█████████▉| 211/212 [00:56<00:00,  3.72it/s]100%|██████████| 212/212 [00:56<00:00,  3.74it/s]accuracy:  0.8443396226415094
100%|██████████| 212/212 [00:59<00:00,  3.54it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Training dataset size: 96, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:22:43,454] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 03:22:46,883] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:22:47,021] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:43,  2.92s/it] 12%|█▎        | 2/16 [00:05<00:36,  2.63s/it] 19%|█▉        | 3/16 [00:08<00:38,  2.96s/it] 25%|██▌       | 4/16 [00:11<00:32,  2.70s/it] 31%|███▏      | 5/16 [00:13<00:28,  2.61s/it] 38%|███▊      | 6/16 [00:16<00:27,  2.71s/it] 44%|████▍     | 7/16 [00:18<00:23,  2.60s/it] 50%|█████     | 8/16 [00:21<00:22,  2.77s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:24<00:19,  2.78s/it] 62%|██████▎   | 10/16 [00:26<00:15,  2.60s/it]                                               {'loss': 0.86, 'grad_norm': 4.713520526885986, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:26<00:15,  2.60s/it] 69%|██████▉   | 11/16 [00:29<00:13,  2.65s/it] 75%|███████▌  | 12/16 [00:32<00:10,  2.72s/it] 81%|████████▏ | 13/16 [00:35<00:08,  2.80s/it] 88%|████████▊ | 14/16 [00:37<00:05,  2.64s/it] 94%|█████████▍| 15/16 [00:39<00:02,  2.51s/it]100%|██████████| 16/16 [00:42<00:00,  2.60s/it]                                               {'train_runtime': 43.3917, 'train_samples_per_second': 4.425, 'train_steps_per_second': 0.369, 'train_loss': 0.7991259694099426, 'epoch': 2.0}
100%|██████████| 16/16 [00:43<00:00,  2.60s/it]100%|██████████| 16/16 [00:43<00:00,  2.70s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<01:04,  2.53it/s]  1%|          | 2/163 [00:00<00:50,  3.17it/s]  2%|▏         | 3/163 [00:00<00:47,  3.39it/s]  2%|▏         | 4/163 [00:01<00:45,  3.51it/s]  3%|▎         | 5/163 [00:01<00:43,  3.61it/s]  4%|▎         | 6/163 [00:01<00:42,  3.73it/s]  4%|▍         | 7/163 [00:01<00:41,  3.72it/s]  5%|▍         | 8/163 [00:02<00:41,  3.73it/s]  6%|▌         | 9/163 [00:02<00:41,  3.69it/s]  6%|▌         | 10/163 [00:02<00:40,  3.81it/s]  7%|▋         | 11/163 [00:03<00:40,  3.78it/s]  7%|▋         | 12/163 [00:03<00:40,  3.74it/s]  8%|▊         | 13/163 [00:03<00:40,  3.70it/s]  9%|▊         | 14/163 [00:03<00:38,  3.82it/s]  9%|▉         | 15/163 [00:04<00:38,  3.80it/s] 10%|▉         | 16/163 [00:04<00:39,  3.75it/s] 10%|█         | 17/163 [00:04<00:39,  3.67it/s] 11%|█         | 18/163 [00:04<00:38,  3.79it/s] 12%|█▏        | 19/163 [00:05<00:37,  3.80it/s] 12%|█▏        | 20/163 [00:05<00:37,  3.77it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.78it/s] 13%|█▎        | 22/163 [00:05<00:37,  3.74it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.81it/s] 15%|█▍        | 24/163 [00:06<00:37,  3.75it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.74it/s] 16%|█▌        | 26/163 [00:07<00:37,  3.70it/s] 17%|█▋        | 27/163 [00:07<00:35,  3.82it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.79it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.74it/s] 18%|█▊        | 30/163 [00:08<00:36,  3.69it/s] 19%|█▉        | 31/163 [00:08<00:34,  3.80it/s] 20%|█▉        | 32/163 [00:08<00:34,  3.78it/s] 20%|██        | 33/163 [00:08<00:34,  3.74it/s] 21%|██        | 34/163 [00:09<00:35,  3.68it/s] 21%|██▏       | 35/163 [00:09<00:33,  3.80it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.79it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.77it/s] 23%|██▎       | 38/163 [00:10<00:33,  3.72it/s] 24%|██▍       | 39/163 [00:10<00:32,  3.76it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.79it/s] 25%|██▌       | 41/163 [00:11<00:32,  3.76it/s] 26%|██▌       | 42/163 [00:11<00:32,  3.71it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.76it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.81it/s] 28%|██▊       | 45/163 [00:12<00:31,  3.77it/s] 28%|██▊       | 46/163 [00:12<00:31,  3.75it/s] 29%|██▉       | 47/163 [00:12<00:31,  3.70it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.80it/s] 30%|███       | 49/163 [00:13<00:30,  3.75it/s] 31%|███       | 50/163 [00:13<00:30,  3.74it/s] 31%|███▏      | 51/163 [00:13<00:29,  3.75it/s] 32%|███▏      | 52/163 [00:13<00:28,  3.84it/s] 33%|███▎      | 53/163 [00:14<00:28,  3.91it/s] 33%|███▎      | 54/163 [00:14<00:27,  3.97it/s] 34%|███▎      | 55/163 [00:14<00:26,  4.02it/s] 34%|███▍      | 56/163 [00:14<00:26,  4.05it/s] 35%|███▍      | 57/163 [00:15<00:26,  4.07it/s] 36%|███▌      | 58/163 [00:15<00:25,  4.08it/s] 36%|███▌      | 59/163 [00:15<00:25,  4.07it/s] 37%|███▋      | 60/163 [00:15<00:25,  4.08it/s] 37%|███▋      | 61/163 [00:16<00:24,  4.10it/s] 38%|███▊      | 62/163 [00:16<00:24,  4.10it/s] 39%|███▊      | 63/163 [00:16<00:24,  4.09it/s] 39%|███▉      | 64/163 [00:16<00:24,  4.09it/s] 40%|███▉      | 65/163 [00:17<00:23,  4.10it/s] 40%|████      | 66/163 [00:17<00:23,  4.10it/s] 41%|████      | 67/163 [00:17<00:23,  4.10it/s] 42%|████▏     | 68/163 [00:17<00:23,  4.09it/s] 42%|████▏     | 69/163 [00:18<00:22,  4.09it/s] 43%|████▎     | 70/163 [00:18<00:22,  4.09it/s] 44%|████▎     | 71/163 [00:18<00:22,  4.10it/s] 44%|████▍     | 72/163 [00:18<00:22,  4.10it/s] 45%|████▍     | 73/163 [00:19<00:22,  4.09it/s] 45%|████▌     | 74/163 [00:19<00:21,  4.09it/s] 46%|████▌     | 75/163 [00:19<00:21,  4.10it/s] 47%|████▋     | 76/163 [00:19<00:21,  4.10it/s] 47%|████▋     | 77/163 [00:20<00:21,  4.09it/s] 48%|████▊     | 78/163 [00:20<00:21,  3.97it/s] 48%|████▊     | 79/163 [00:20<00:21,  3.88it/s] 49%|████▉     | 80/163 [00:20<00:21,  3.84it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.90it/s] 50%|█████     | 82/163 [00:21<00:21,  3.78it/s] 51%|█████     | 83/163 [00:21<00:21,  3.69it/s] 52%|█████▏    | 84/163 [00:21<00:21,  3.62it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.74it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.73it/s] 53%|█████▎    | 87/163 [00:22<00:21,  3.62it/s] 54%|█████▍    | 88/163 [00:23<00:20,  3.68it/s] 55%|█████▍    | 89/163 [00:23<00:20,  3.64it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.73it/s] 56%|█████▌    | 91/163 [00:23<00:19,  3.67it/s] 56%|█████▋    | 92/163 [00:24<00:19,  3.62it/s] 57%|█████▋    | 93/163 [00:24<00:19,  3.57it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.71it/s] 58%|█████▊    | 95/163 [00:24<00:18,  3.71it/s] 59%|█████▉    | 96/163 [00:25<00:18,  3.60it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.67it/s] 60%|██████    | 98/163 [00:25<00:17,  3.64it/s] 61%|██████    | 99/163 [00:26<00:17,  3.72it/s] 61%|██████▏   | 100/163 [00:26<00:17,  3.66it/s] 62%|██████▏   | 101/163 [00:26<00:17,  3.62it/s] 63%|██████▎   | 102/163 [00:26<00:17,  3.57it/s] 63%|██████▎   | 103/163 [00:27<00:16,  3.71it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.71it/s] 64%|██████▍   | 105/163 [00:27<00:16,  3.61it/s] 65%|██████▌   | 106/163 [00:27<00:15,  3.67it/s] 66%|██████▌   | 107/163 [00:28<00:15,  3.63it/s] 66%|██████▋   | 108/163 [00:28<00:14,  3.70it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.65it/s] 67%|██████▋   | 110/163 [00:29<00:14,  3.60it/s] 68%|██████▊   | 111/163 [00:29<00:14,  3.56it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.71it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.70it/s] 70%|██████▉   | 114/163 [00:30<00:13,  3.61it/s] 71%|███████   | 115/163 [00:30<00:13,  3.67it/s] 71%|███████   | 116/163 [00:30<00:12,  3.63it/s] 72%|███████▏  | 117/163 [00:30<00:12,  3.69it/s] 72%|███████▏  | 118/163 [00:31<00:12,  3.64it/s] 73%|███████▎  | 119/163 [00:31<00:12,  3.60it/s] 74%|███████▎  | 120/163 [00:31<00:12,  3.56it/s] 74%|███████▍  | 121/163 [00:32<00:11,  3.70it/s] 75%|███████▍  | 122/163 [00:32<00:11,  3.72it/s] 75%|███████▌  | 123/163 [00:32<00:11,  3.61it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.67it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.63it/s] 77%|███████▋  | 126/163 [00:33<00:10,  3.69it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.64it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.59it/s] 79%|███████▉  | 129/163 [00:34<00:09,  3.55it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.69it/s] 80%|████████  | 131/163 [00:34<00:08,  3.64it/s] 81%|████████  | 132/163 [00:35<00:08,  3.60it/s] 82%|████████▏ | 133/163 [00:35<00:08,  3.60it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.66it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.71it/s] 83%|████████▎ | 136/163 [00:36<00:07,  3.65it/s] 84%|████████▍ | 137/163 [00:36<00:07,  3.60it/s] 85%|████████▍ | 138/163 [00:36<00:07,  3.56it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.71it/s] 86%|████████▌ | 140/163 [00:37<00:06,  3.73it/s] 87%|████████▋ | 141/163 [00:37<00:06,  3.62it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.66it/s] 88%|████████▊ | 143/163 [00:38<00:05,  3.62it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.70it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.64it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.59it/s] 90%|█████████ | 147/163 [00:39<00:04,  3.55it/s] 91%|█████████ | 148/163 [00:39<00:04,  3.69it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.67it/s] 92%|█████████▏| 150/163 [00:40<00:03,  3.60it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.64it/s] 93%|█████████▎| 152/163 [00:40<00:03,  3.62it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.68it/s] 94%|█████████▍| 154/163 [00:41<00:02,  3.63it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.58it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.55it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.69it/s] 97%|█████████▋| 158/163 [00:42<00:01,  3.70it/s] 98%|█████████▊| 159/163 [00:42<00:01,  3.60it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.66it/s] 99%|█████████▉| 161/163 [00:43<00:00,  3.60it/s] 99%|█████████▉| 162/163 [00:43<00:00,  3.69it/s]100%|██████████| 163/163 [00:43<00:00,  3.63it/s]accuracy:  0.7361963190184049
100%|██████████| 163/163 [00:46<00:00,  3.53it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 171
Training dataset size: 96, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:24:42,568] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:24:42,623] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.33s/it][93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.31s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:24:43,916] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:41,  2.76s/it] 12%|█▎        | 2/16 [00:05<00:42,  3.02s/it] 19%|█▉        | 3/16 [00:08<00:36,  2.84s/it] 25%|██▌       | 4/16 [00:11<00:33,  2.83s/it] 31%|███▏      | 5/16 [00:13<00:29,  2.67s/it] 38%|███▊      | 6/16 [00:16<00:27,  2.77s/it] 44%|████▍     | 7/16 [00:19<00:25,  2.82s/it] 50%|█████     | 8/16 [00:22<00:22,  2.76s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:25<00:19,  2.83s/it] 62%|██████▎   | 10/16 [00:27<00:16,  2.72s/it]                                               {'loss': 0.5653, 'grad_norm': 15.585602760314941, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:27<00:16,  2.72s/it] 69%|██████▉   | 11/16 [00:30<00:13,  2.70s/it] 75%|███████▌  | 12/16 [00:32<00:10,  2.58s/it] 81%|████████▏ | 13/16 [00:35<00:07,  2.52s/it] 88%|████████▊ | 14/16 [00:37<00:04,  2.40s/it] 94%|█████████▍| 15/16 [00:40<00:02,  2.56s/it]100%|██████████| 16/16 [00:42<00:00,  2.57s/it]                                               {'train_runtime': 43.3877, 'train_samples_per_second': 4.425, 'train_steps_per_second': 0.369, 'train_loss': 0.4209436923265457, 'epoch': 2.0}
100%|██████████| 16/16 [00:43<00:00,  2.57s/it]100%|██████████| 16/16 [00:43<00:00,  2.70s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:14,  2.91it/s]  1%|          | 2/217 [00:00<01:03,  3.38it/s]  1%|▏         | 3/217 [00:00<01:00,  3.51it/s]  2%|▏         | 4/217 [00:01<00:58,  3.67it/s]  2%|▏         | 5/217 [00:01<00:58,  3.63it/s]  3%|▎         | 6/217 [00:01<00:56,  3.71it/s]  3%|▎         | 7/217 [00:01<00:56,  3.69it/s]  4%|▎         | 8/217 [00:02<00:55,  3.77it/s]  4%|▍         | 9/217 [00:02<00:56,  3.71it/s]  5%|▍         | 10/217 [00:02<00:55,  3.76it/s]  5%|▌         | 11/217 [00:03<00:55,  3.72it/s]  6%|▌         | 12/217 [00:03<00:54,  3.78it/s]  6%|▌         | 13/217 [00:03<00:54,  3.72it/s]  6%|▋         | 14/217 [00:03<00:53,  3.77it/s]  7%|▋         | 15/217 [00:04<00:54,  3.73it/s]  7%|▋         | 16/217 [00:04<00:53,  3.79it/s]  8%|▊         | 17/217 [00:04<00:53,  3.72it/s]  8%|▊         | 18/217 [00:04<00:53,  3.75it/s]  9%|▉         | 19/217 [00:05<00:53,  3.72it/s]  9%|▉         | 20/217 [00:05<00:51,  3.79it/s] 10%|▉         | 21/217 [00:05<00:52,  3.72it/s] 10%|█         | 22/217 [00:05<00:51,  3.77it/s] 11%|█         | 23/217 [00:06<00:52,  3.72it/s] 11%|█         | 24/217 [00:06<00:51,  3.78it/s] 12%|█▏        | 25/217 [00:06<00:51,  3.72it/s] 12%|█▏        | 26/217 [00:07<00:50,  3.77it/s] 12%|█▏        | 27/217 [00:07<00:50,  3.73it/s] 13%|█▎        | 28/217 [00:07<00:49,  3.79it/s] 13%|█▎        | 29/217 [00:07<00:50,  3.72it/s] 14%|█▍        | 30/217 [00:08<00:49,  3.75it/s] 14%|█▍        | 31/217 [00:08<00:50,  3.71it/s] 15%|█▍        | 32/217 [00:08<00:48,  3.78it/s] 15%|█▌        | 33/217 [00:08<00:49,  3.73it/s] 16%|█▌        | 34/217 [00:09<00:48,  3.77it/s] 16%|█▌        | 35/217 [00:09<00:48,  3.73it/s] 17%|█▋        | 36/217 [00:09<00:47,  3.78it/s] 17%|█▋        | 37/217 [00:09<00:48,  3.70it/s] 18%|█▊        | 38/217 [00:10<00:47,  3.74it/s] 18%|█▊        | 39/217 [00:10<00:48,  3.70it/s] 18%|█▊        | 40/217 [00:10<00:47,  3.76it/s] 19%|█▉        | 41/217 [00:11<00:47,  3.71it/s] 19%|█▉        | 42/217 [00:11<00:46,  3.74it/s] 20%|█▉        | 43/217 [00:11<00:46,  3.71it/s] 20%|██        | 44/217 [00:11<00:45,  3.78it/s] 21%|██        | 45/217 [00:12<00:46,  3.71it/s] 21%|██        | 46/217 [00:12<00:45,  3.75it/s] 22%|██▏       | 47/217 [00:12<00:45,  3.71it/s] 22%|██▏       | 48/217 [00:12<00:44,  3.77it/s] 23%|██▎       | 49/217 [00:13<00:45,  3.71it/s] 23%|██▎       | 50/217 [00:13<00:44,  3.74it/s] 24%|██▎       | 51/217 [00:13<00:44,  3.70it/s] 24%|██▍       | 52/217 [00:13<00:43,  3.77it/s] 24%|██▍       | 53/217 [00:14<00:44,  3.73it/s] 25%|██▍       | 54/217 [00:14<00:43,  3.76it/s] 25%|██▌       | 55/217 [00:14<00:43,  3.72it/s] 26%|██▌       | 56/217 [00:15<00:43,  3.73it/s] 26%|██▋       | 57/217 [00:15<00:42,  3.74it/s] 27%|██▋       | 58/217 [00:15<00:42,  3.73it/s] 27%|██▋       | 59/217 [00:15<00:42,  3.70it/s] 28%|██▊       | 60/217 [00:16<00:43,  3.64it/s] 28%|██▊       | 61/217 [00:16<00:41,  3.77it/s] 29%|██▊       | 62/217 [00:16<00:41,  3.73it/s] 29%|██▉       | 63/217 [00:16<00:41,  3.71it/s] 29%|██▉       | 64/217 [00:17<00:41,  3.67it/s] 30%|██▉       | 65/217 [00:17<00:40,  3.79it/s] 30%|███       | 66/217 [00:17<00:40,  3.74it/s] 31%|███       | 67/217 [00:17<00:40,  3.72it/s] 31%|███▏      | 68/217 [00:18<00:40,  3.68it/s] 32%|███▏      | 69/217 [00:18<00:39,  3.77it/s] 32%|███▏      | 70/217 [00:18<00:39,  3.71it/s] 33%|███▎      | 71/217 [00:19<00:39,  3.73it/s] 33%|███▎      | 72/217 [00:19<00:38,  3.73it/s] 34%|███▎      | 73/217 [00:19<00:38,  3.75it/s] 34%|███▍      | 74/217 [00:19<00:38,  3.71it/s] 35%|███▍      | 75/217 [00:20<00:38,  3.70it/s] 35%|███▌      | 76/217 [00:20<00:37,  3.73it/s] 35%|███▌      | 77/217 [00:20<00:37,  3.72it/s] 36%|███▌      | 78/217 [00:20<00:37,  3.71it/s] 36%|███▋      | 79/217 [00:21<00:37,  3.65it/s] 37%|███▋      | 80/217 [00:21<00:36,  3.78it/s] 37%|███▋      | 81/217 [00:21<00:36,  3.73it/s] 38%|███▊      | 82/217 [00:22<00:36,  3.71it/s] 38%|███▊      | 83/217 [00:22<00:36,  3.67it/s] 39%|███▊      | 84/217 [00:22<00:35,  3.76it/s] 39%|███▉      | 85/217 [00:22<00:35,  3.71it/s] 40%|███▉      | 86/217 [00:23<00:35,  3.71it/s] 40%|████      | 87/217 [00:23<00:34,  3.72it/s] 41%|████      | 88/217 [00:23<00:34,  3.73it/s] 41%|████      | 89/217 [00:23<00:34,  3.71it/s] 41%|████▏     | 90/217 [00:24<00:34,  3.70it/s] 42%|████▏     | 91/217 [00:24<00:33,  3.72it/s] 42%|████▏     | 92/217 [00:24<00:33,  3.72it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.70it/s] 43%|████▎     | 94/217 [00:25<00:33,  3.64it/s] 44%|████▍     | 95/217 [00:25<00:32,  3.77it/s] 44%|████▍     | 96/217 [00:25<00:32,  3.72it/s] 45%|████▍     | 97/217 [00:26<00:32,  3.71it/s] 45%|████▌     | 98/217 [00:26<00:32,  3.66it/s] 46%|████▌     | 99/217 [00:26<00:31,  3.76it/s] 46%|████▌     | 100/217 [00:26<00:31,  3.71it/s] 47%|████▋     | 101/217 [00:27<00:31,  3.67it/s] 47%|████▋     | 102/217 [00:27<00:30,  3.73it/s] 47%|████▋     | 103/217 [00:27<00:30,  3.72it/s] 48%|████▊     | 104/217 [00:27<00:30,  3.69it/s] 48%|████▊     | 105/217 [00:28<00:30,  3.66it/s] 49%|████▉     | 106/217 [00:28<00:29,  3.74it/s] 49%|████▉     | 107/217 [00:28<00:29,  3.70it/s] 50%|████▉     | 108/217 [00:29<00:29,  3.65it/s] 50%|█████     | 109/217 [00:29<00:29,  3.72it/s] 51%|█████     | 110/217 [00:29<00:28,  3.71it/s] 51%|█████     | 111/217 [00:29<00:28,  3.69it/s] 52%|█████▏    | 112/217 [00:30<00:28,  3.65it/s] 52%|█████▏    | 113/217 [00:30<00:27,  3.75it/s] 53%|█████▎    | 114/217 [00:30<00:27,  3.70it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.65it/s] 53%|█████▎    | 116/217 [00:31<00:27,  3.71it/s] 54%|█████▍    | 117/217 [00:31<00:27,  3.70it/s] 54%|█████▍    | 118/217 [00:31<00:26,  3.69it/s] 55%|█████▍    | 119/217 [00:32<00:26,  3.65it/s] 55%|█████▌    | 120/217 [00:32<00:25,  3.74it/s] 56%|█████▌    | 121/217 [00:32<00:26,  3.69it/s] 56%|█████▌    | 122/217 [00:32<00:26,  3.65it/s] 57%|█████▋    | 123/217 [00:33<00:25,  3.71it/s] 57%|█████▋    | 124/217 [00:33<00:25,  3.71it/s] 58%|█████▊    | 125/217 [00:33<00:24,  3.69it/s] 58%|█████▊    | 126/217 [00:33<00:25,  3.62it/s] 59%|█████▊    | 127/217 [00:34<00:24,  3.71it/s] 59%|█████▉    | 128/217 [00:34<00:24,  3.69it/s] 59%|█████▉    | 129/217 [00:34<00:24,  3.67it/s] 60%|█████▉    | 130/217 [00:35<00:23,  3.64it/s] 60%|██████    | 131/217 [00:35<00:23,  3.71it/s] 61%|██████    | 132/217 [00:35<00:23,  3.68it/s] 61%|██████▏   | 133/217 [00:35<00:22,  3.67it/s] 62%|██████▏   | 134/217 [00:36<00:22,  3.70it/s] 62%|██████▏   | 135/217 [00:36<00:22,  3.71it/s] 63%|██████▎   | 136/217 [00:36<00:21,  3.70it/s] 63%|██████▎   | 137/217 [00:36<00:21,  3.67it/s] 64%|██████▎   | 138/217 [00:37<00:21,  3.73it/s] 64%|██████▍   | 139/217 [00:37<00:21,  3.71it/s] 65%|██████▍   | 140/217 [00:37<00:20,  3.70it/s] 65%|██████▍   | 141/217 [00:38<00:21,  3.61it/s] 65%|██████▌   | 142/217 [00:38<00:20,  3.74it/s] 66%|██████▌   | 143/217 [00:38<00:19,  3.70it/s] 66%|██████▋   | 144/217 [00:38<00:19,  3.68it/s] 67%|██████▋   | 145/217 [00:39<00:19,  3.64it/s] 67%|██████▋   | 146/217 [00:39<00:19,  3.71it/s] 68%|██████▊   | 147/217 [00:39<00:19,  3.66it/s] 68%|██████▊   | 148/217 [00:39<00:19,  3.63it/s] 69%|██████▊   | 149/217 [00:40<00:18,  3.69it/s] 69%|██████▉   | 150/217 [00:40<00:18,  3.68it/s] 70%|██████▉   | 151/217 [00:40<00:18,  3.66it/s] 70%|███████   | 152/217 [00:40<00:17,  3.64it/s] 71%|███████   | 153/217 [00:41<00:17,  3.73it/s] 71%|███████   | 154/217 [00:41<00:17,  3.68it/s] 71%|███████▏  | 155/217 [00:41<00:17,  3.63it/s] 72%|███████▏  | 156/217 [00:42<00:16,  3.70it/s] 72%|███████▏  | 157/217 [00:42<00:16,  3.70it/s] 73%|███████▎  | 158/217 [00:42<00:16,  3.67it/s] 73%|███████▎  | 159/217 [00:42<00:15,  3.64it/s] 74%|███████▎  | 160/217 [00:43<00:15,  3.71it/s] 74%|███████▍  | 161/217 [00:43<00:15,  3.67it/s] 75%|███████▍  | 162/217 [00:43<00:14,  3.68it/s] 75%|███████▌  | 163/217 [00:43<00:14,  3.69it/s] 76%|███████▌  | 164/217 [00:44<00:14,  3.72it/s] 76%|███████▌  | 165/217 [00:44<00:14,  3.69it/s] 76%|███████▋  | 166/217 [00:44<00:13,  3.69it/s] 77%|███████▋  | 167/217 [00:45<00:13,  3.71it/s] 77%|███████▋  | 168/217 [00:45<00:13,  3.70it/s] 78%|███████▊  | 169/217 [00:45<00:13,  3.67it/s] 78%|███████▊  | 170/217 [00:45<00:12,  3.62it/s] 79%|███████▉  | 171/217 [00:46<00:12,  3.72it/s] 79%|███████▉  | 172/217 [00:46<00:12,  3.68it/s] 80%|███████▉  | 173/217 [00:46<00:11,  3.73it/s] 80%|████████  | 174/217 [00:46<00:11,  3.67it/s] 81%|████████  | 175/217 [00:47<00:11,  3.70it/s] 81%|████████  | 176/217 [00:47<00:11,  3.66it/s] 82%|████████▏ | 177/217 [00:47<00:10,  3.65it/s] 82%|████████▏ | 178/217 [00:48<00:10,  3.69it/s] 82%|████████▏ | 179/217 [00:48<00:10,  3.69it/s] 83%|████████▎ | 180/217 [00:48<00:10,  3.67it/s] 83%|████████▎ | 181/217 [00:48<00:09,  3.62it/s] 84%|████████▍ | 182/217 [00:49<00:09,  3.73it/s] 84%|████████▍ | 183/217 [00:49<00:09,  3.69it/s] 85%|████████▍ | 184/217 [00:49<00:09,  3.67it/s] 85%|████████▌ | 185/217 [00:49<00:08,  3.63it/s] 86%|████████▌ | 186/217 [00:50<00:08,  3.69it/s] 86%|████████▌ | 187/217 [00:50<00:08,  3.65it/s] 87%|████████▋ | 188/217 [00:50<00:08,  3.60it/s] 87%|████████▋ | 189/217 [00:51<00:07,  3.68it/s] 88%|████████▊ | 190/217 [00:51<00:07,  3.68it/s] 88%|████████▊ | 191/217 [00:51<00:07,  3.64it/s] 88%|████████▊ | 192/217 [00:51<00:06,  3.62it/s] 89%|████████▉ | 193/217 [00:52<00:06,  3.70it/s] 89%|████████▉ | 194/217 [00:52<00:06,  3.65it/s] 90%|████████▉ | 195/217 [00:52<00:06,  3.62it/s] 90%|█████████ | 196/217 [00:52<00:05,  3.69it/s] 91%|█████████ | 197/217 [00:53<00:05,  3.68it/s] 91%|█████████ | 198/217 [00:53<00:05,  3.66it/s] 92%|█████████▏| 199/217 [00:53<00:04,  3.63it/s] 92%|█████████▏| 200/217 [00:54<00:04,  3.72it/s] 93%|█████████▎| 201/217 [00:54<00:04,  3.67it/s] 93%|█████████▎| 202/217 [00:54<00:04,  3.66it/s] 94%|█████████▎| 203/217 [00:54<00:03,  3.68it/s] 94%|█████████▍| 204/217 [00:55<00:03,  3.70it/s] 94%|█████████▍| 205/217 [00:55<00:03,  3.67it/s] 95%|█████████▍| 206/217 [00:55<00:02,  3.70it/s] 95%|█████████▌| 207/217 [00:55<00:02,  3.79it/s] 96%|█████████▌| 208/217 [00:56<00:02,  3.87it/s] 96%|█████████▋| 209/217 [00:56<00:02,  3.93it/s] 97%|█████████▋| 210/217 [00:56<00:01,  3.96it/s] 97%|█████████▋| 211/217 [00:56<00:01,  4.00it/s] 98%|█████████▊| 212/217 [00:57<00:01,  4.02it/s] 98%|█████████▊| 213/217 [00:57<00:00,  4.03it/s] 99%|█████████▊| 214/217 [00:57<00:00,  4.05it/s] 99%|█████████▉| 215/217 [00:57<00:00,  4.07it/s]100%|█████████▉| 216/217 [00:58<00:00,  4.07it/s]100%|██████████| 217/217 [00:58<00:00,  4.07it/s]accuracy:  0.8202764976958525
100%|██████████| 217/217 [01:01<00:00,  3.52it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:26:56,262] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:26:56,929] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 03:26:57,400] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:41,  2.74s/it] 12%|█▎        | 2/16 [00:04<00:34,  2.44s/it] 19%|█▉        | 3/16 [00:08<00:35,  2.72s/it] 25%|██▌       | 4/16 [00:10<00:29,  2.50s/it] 31%|███▏      | 5/16 [00:12<00:26,  2.41s/it] 38%|███▊      | 6/16 [00:14<00:23,  2.37s/it] 44%|████▍     | 7/16 [00:17<00:21,  2.40s/it] 50%|█████     | 8/16 [00:19<00:18,  2.29s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:21<00:16,  2.32s/it] 62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it]                                               {'loss': 0.9468, 'grad_norm': 10.790075302124023, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:24<00:14,  2.47s/it] 69%|██████▉   | 11/16 [00:26<00:12,  2.47s/it] 75%|███████▌  | 12/16 [00:29<00:09,  2.40s/it] 81%|████████▏ | 13/16 [00:31<00:07,  2.48s/it] 88%|████████▊ | 14/16 [00:33<00:04,  2.37s/it] 94%|█████████▍| 15/16 [00:36<00:02,  2.36s/it]100%|██████████| 16/16 [00:38<00:00,  2.35s/it]                                               {'train_runtime': 39.2525, 'train_samples_per_second': 4.891, 'train_steps_per_second': 0.408, 'train_loss': 1.0226505398750305, 'epoch': 2.0}
100%|██████████| 16/16 [00:39<00:00,  2.35s/it]100%|██████████| 16/16 [00:39<00:00,  2.44s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:54,  2.72it/s]  1%|▏         | 2/150 [00:00<00:46,  3.20it/s]  2%|▏         | 3/150 [00:00<00:43,  3.41it/s]  3%|▎         | 4/150 [00:01<00:40,  3.60it/s]  3%|▎         | 5/150 [00:01<00:38,  3.73it/s]  4%|▍         | 6/150 [00:01<00:39,  3.69it/s]  5%|▍         | 7/150 [00:01<00:38,  3.73it/s]  5%|▌         | 8/150 [00:02<00:38,  3.68it/s]  6%|▌         | 9/150 [00:02<00:36,  3.82it/s]  7%|▋         | 10/150 [00:02<00:36,  3.87it/s]  7%|▋         | 11/150 [00:03<00:36,  3.78it/s]  8%|▊         | 12/150 [00:03<00:36,  3.80it/s]  9%|▊         | 13/150 [00:03<00:36,  3.74it/s]  9%|▉         | 14/150 [00:03<00:35,  3.87it/s] 10%|█         | 15/150 [00:04<00:34,  3.88it/s] 11%|█         | 16/150 [00:04<00:35,  3.79it/s] 11%|█▏        | 17/150 [00:04<00:34,  3.80it/s] 12%|█▏        | 18/150 [00:04<00:35,  3.74it/s] 13%|█▎        | 19/150 [00:05<00:33,  3.85it/s] 13%|█▎        | 20/150 [00:05<00:34,  3.79it/s] 14%|█▍        | 21/150 [00:05<00:34,  3.77it/s] 15%|█▍        | 22/150 [00:05<00:33,  3.82it/s] 15%|█▌        | 23/150 [00:06<00:33,  3.74it/s] 16%|█▌        | 24/150 [00:06<00:32,  3.85it/s] 17%|█▋        | 25/150 [00:06<00:33,  3.78it/s] 17%|█▋        | 26/150 [00:06<00:32,  3.76it/s] 18%|█▊        | 27/150 [00:07<00:32,  3.77it/s] 19%|█▊        | 28/150 [00:07<00:32,  3.75it/s] 19%|█▉        | 29/150 [00:07<00:31,  3.84it/s] 20%|██        | 30/150 [00:08<00:31,  3.78it/s] 21%|██        | 31/150 [00:08<00:31,  3.75it/s] 21%|██▏       | 32/150 [00:08<00:32,  3.68it/s] 22%|██▏       | 33/150 [00:08<00:30,  3.81it/s] 23%|██▎       | 34/150 [00:09<00:30,  3.83it/s] 23%|██▎       | 35/150 [00:09<00:30,  3.76it/s] 24%|██▍       | 36/150 [00:09<00:30,  3.77it/s] 25%|██▍       | 37/150 [00:09<00:30,  3.72it/s] 25%|██▌       | 38/150 [00:10<00:29,  3.83it/s] 26%|██▌       | 39/150 [00:10<00:28,  3.85it/s] 27%|██▋       | 40/150 [00:10<00:29,  3.78it/s] 27%|██▋       | 41/150 [00:10<00:28,  3.78it/s] 28%|██▊       | 42/150 [00:11<00:29,  3.72it/s] 29%|██▊       | 43/150 [00:11<00:27,  3.83it/s] 29%|██▉       | 44/150 [00:11<00:27,  3.83it/s] 30%|███       | 45/150 [00:11<00:28,  3.74it/s] 31%|███       | 46/150 [00:12<00:27,  3.74it/s] 31%|███▏      | 47/150 [00:12<00:26,  3.85it/s] 32%|███▏      | 48/150 [00:12<00:25,  3.94it/s] 33%|███▎      | 49/150 [00:12<00:25,  4.01it/s] 33%|███▎      | 50/150 [00:13<00:24,  4.05it/s] 34%|███▍      | 51/150 [00:13<00:24,  4.09it/s] 35%|███▍      | 52/150 [00:13<00:23,  4.11it/s] 35%|███▌      | 53/150 [00:13<00:23,  4.13it/s] 36%|███▌      | 54/150 [00:14<00:23,  4.14it/s] 37%|███▋      | 55/150 [00:14<00:22,  4.15it/s] 37%|███▋      | 56/150 [00:14<00:22,  4.15it/s] 38%|███▊      | 57/150 [00:14<00:22,  4.16it/s] 39%|███▊      | 58/150 [00:15<00:22,  4.16it/s] 39%|███▉      | 59/150 [00:15<00:21,  4.16it/s] 40%|████      | 60/150 [00:15<00:21,  4.16it/s] 41%|████      | 61/150 [00:15<00:21,  4.16it/s] 41%|████▏     | 62/150 [00:16<00:21,  4.16it/s] 42%|████▏     | 63/150 [00:16<00:20,  4.16it/s] 43%|████▎     | 64/150 [00:16<00:20,  4.17it/s] 43%|████▎     | 65/150 [00:16<00:20,  4.17it/s] 44%|████▍     | 66/150 [00:17<00:20,  4.16it/s] 45%|████▍     | 67/150 [00:17<00:19,  4.16it/s] 45%|████▌     | 68/150 [00:17<00:19,  4.16it/s] 46%|████▌     | 69/150 [00:17<00:19,  4.16it/s] 47%|████▋     | 70/150 [00:18<00:19,  4.16it/s] 47%|████▋     | 71/150 [00:18<00:19,  4.15it/s] 48%|████▊     | 72/150 [00:18<00:18,  4.13it/s] 49%|████▊     | 73/150 [00:18<00:19,  3.98it/s] 49%|████▉     | 74/150 [00:19<00:19,  3.87it/s] 50%|█████     | 75/150 [00:19<00:18,  3.95it/s] 51%|█████     | 76/150 [00:19<00:19,  3.87it/s] 51%|█████▏    | 77/150 [00:19<00:19,  3.81it/s] 52%|█████▏    | 78/150 [00:20<00:19,  3.75it/s] 53%|█████▎    | 79/150 [00:20<00:18,  3.86it/s] 53%|█████▎    | 80/150 [00:20<00:18,  3.80it/s] 54%|█████▍    | 81/150 [00:20<00:18,  3.75it/s] 55%|█████▍    | 82/150 [00:21<00:18,  3.70it/s] 55%|█████▌    | 83/150 [00:21<00:17,  3.81it/s] 56%|█████▌    | 84/150 [00:21<00:17,  3.77it/s] 57%|█████▋    | 85/150 [00:21<00:17,  3.74it/s] 57%|█████▋    | 86/150 [00:22<00:17,  3.69it/s] 58%|█████▊    | 87/150 [00:22<00:16,  3.81it/s] 59%|█████▊    | 88/150 [00:22<00:16,  3.77it/s] 59%|█████▉    | 89/150 [00:23<00:16,  3.74it/s] 60%|██████    | 90/150 [00:23<00:16,  3.70it/s] 61%|██████    | 91/150 [00:23<00:15,  3.82it/s] 61%|██████▏   | 92/150 [00:23<00:15,  3.78it/s] 62%|██████▏   | 93/150 [00:24<00:15,  3.73it/s] 63%|██████▎   | 94/150 [00:24<00:15,  3.69it/s] 63%|██████▎   | 95/150 [00:24<00:14,  3.80it/s] 64%|██████▍   | 96/150 [00:24<00:14,  3.76it/s] 65%|██████▍   | 97/150 [00:25<00:14,  3.73it/s] 65%|██████▌   | 98/150 [00:25<00:14,  3.68it/s] 66%|██████▌   | 99/150 [00:25<00:13,  3.80it/s] 67%|██████▋   | 100/150 [00:25<00:13,  3.76it/s] 67%|██████▋   | 101/150 [00:26<00:13,  3.72it/s] 68%|██████▊   | 102/150 [00:26<00:13,  3.68it/s] 69%|██████▊   | 103/150 [00:26<00:12,  3.79it/s] 69%|██████▉   | 104/150 [00:27<00:12,  3.76it/s] 70%|███████   | 105/150 [00:27<00:12,  3.73it/s] 71%|███████   | 106/150 [00:27<00:11,  3.69it/s] 71%|███████▏  | 107/150 [00:27<00:11,  3.80it/s] 72%|███████▏  | 108/150 [00:28<00:11,  3.76it/s] 73%|███████▎  | 109/150 [00:28<00:11,  3.72it/s] 73%|███████▎  | 110/150 [00:28<00:10,  3.68it/s] 74%|███████▍  | 111/150 [00:28<00:10,  3.79it/s] 75%|███████▍  | 112/150 [00:29<00:10,  3.75it/s] 75%|███████▌  | 113/150 [00:29<00:09,  3.72it/s] 76%|███████▌  | 114/150 [00:29<00:09,  3.68it/s] 77%|███████▋  | 115/150 [00:29<00:09,  3.80it/s] 77%|███████▋  | 116/150 [00:30<00:09,  3.76it/s] 78%|███████▊  | 117/150 [00:30<00:08,  3.72it/s] 79%|███████▊  | 118/150 [00:30<00:08,  3.67it/s] 79%|███████▉  | 119/150 [00:31<00:08,  3.78it/s] 80%|████████  | 120/150 [00:31<00:08,  3.73it/s] 81%|████████  | 121/150 [00:31<00:07,  3.70it/s] 81%|████████▏ | 122/150 [00:31<00:07,  3.66it/s] 82%|████████▏ | 123/150 [00:32<00:07,  3.77it/s] 83%|████████▎ | 124/150 [00:32<00:06,  3.74it/s] 83%|████████▎ | 125/150 [00:32<00:06,  3.71it/s] 84%|████████▍ | 126/150 [00:32<00:06,  3.67it/s] 85%|████████▍ | 127/150 [00:33<00:06,  3.80it/s] 85%|████████▌ | 128/150 [00:33<00:05,  3.76it/s] 86%|████████▌ | 129/150 [00:33<00:05,  3.72it/s] 87%|████████▋ | 130/150 [00:34<00:05,  3.67it/s] 87%|████████▋ | 131/150 [00:34<00:05,  3.79it/s] 88%|████████▊ | 132/150 [00:34<00:04,  3.74it/s] 89%|████████▊ | 133/150 [00:34<00:04,  3.71it/s] 89%|████████▉ | 134/150 [00:35<00:04,  3.67it/s] 90%|█████████ | 135/150 [00:35<00:03,  3.79it/s] 91%|█████████ | 136/150 [00:35<00:03,  3.74it/s] 91%|█████████▏| 137/150 [00:35<00:03,  3.71it/s] 92%|█████████▏| 138/150 [00:36<00:03,  3.67it/s] 93%|█████████▎| 139/150 [00:36<00:02,  3.80it/s] 93%|█████████▎| 140/150 [00:36<00:02,  3.76it/s] 94%|█████████▍| 141/150 [00:36<00:02,  3.72it/s] 95%|█████████▍| 142/150 [00:37<00:02,  3.67it/s] 95%|█████████▌| 143/150 [00:37<00:01,  3.78it/s] 96%|█████████▌| 144/150 [00:37<00:01,  3.74it/s] 97%|█████████▋| 145/150 [00:38<00:01,  3.70it/s] 97%|█████████▋| 146/150 [00:38<00:01,  3.66it/s] 98%|█████████▊| 147/150 [00:38<00:00,  3.77it/s] 99%|█████████▊| 148/150 [00:38<00:00,  3.72it/s] 99%|█████████▉| 149/150 [00:39<00:00,  3.70it/s]100%|██████████| 150/150 [00:39<00:00,  3.66it/s]accuracy:  0.62
100%|██████████| 150/150 [00:41<00:00,  3.59it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.54s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.22s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.93s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:28:45,205] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:28:46,289] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 03:28:46,305] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:40,  2.67s/it] 12%|█▎        | 2/16 [00:04<00:31,  2.23s/it] 19%|█▉        | 3/16 [00:06<00:29,  2.25s/it] 25%|██▌       | 4/16 [00:09<00:28,  2.41s/it] 31%|███▏      | 5/16 [00:11<00:26,  2.38s/it] 38%|███▊      | 6/16 [00:14<00:23,  2.36s/it] 44%|████▍     | 7/16 [00:16<00:21,  2.36s/it] 50%|█████     | 8/16 [00:18<00:18,  2.29s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:21<00:16,  2.36s/it] 62%|██████▎   | 10/16 [00:23<00:14,  2.48s/it]                                               {'loss': 1.7519, 'grad_norm': 10.769593238830566, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:23<00:14,  2.48s/it] 69%|██████▉   | 11/16 [00:26<00:12,  2.57s/it] 75%|███████▌  | 12/16 [00:29<00:10,  2.59s/it] 81%|████████▏ | 13/16 [00:31<00:07,  2.51s/it] 88%|████████▊ | 14/16 [00:33<00:04,  2.32s/it] 94%|█████████▍| 15/16 [00:35<00:02,  2.28s/it]100%|██████████| 16/16 [00:38<00:00,  2.41s/it]                                               {'train_runtime': 39.1378, 'train_samples_per_second': 4.906, 'train_steps_per_second': 0.409, 'train_loss': 1.637273371219635, 'epoch': 2.0}
100%|██████████| 16/16 [00:38<00:00,  2.41s/it]100%|██████████| 16/16 [00:38<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:07,  2.94it/s]  1%|          | 2/200 [00:00<00:58,  3.36it/s]  2%|▏         | 3/200 [00:00<00:55,  3.57it/s]  2%|▏         | 4/200 [00:01<00:52,  3.72it/s]  2%|▎         | 5/200 [00:01<00:53,  3.65it/s]  3%|▎         | 6/200 [00:01<00:53,  3.61it/s]  4%|▎         | 7/200 [00:01<00:53,  3.59it/s]  4%|▍         | 8/200 [00:02<00:51,  3.74it/s]  4%|▍         | 9/200 [00:02<00:50,  3.77it/s]  5%|▌         | 10/200 [00:02<00:51,  3.66it/s]  6%|▌         | 11/200 [00:03<00:50,  3.72it/s]  6%|▌         | 12/200 [00:03<00:51,  3.67it/s]  6%|▋         | 13/200 [00:03<00:50,  3.74it/s]  7%|▋         | 14/200 [00:03<00:50,  3.67it/s]  8%|▊         | 15/200 [00:04<00:50,  3.63it/s]  8%|▊         | 16/200 [00:04<00:51,  3.60it/s]  8%|▊         | 17/200 [00:04<00:48,  3.74it/s]  9%|▉         | 18/200 [00:04<00:49,  3.68it/s] 10%|▉         | 19/200 [00:05<00:49,  3.65it/s] 10%|█         | 20/200 [00:05<00:49,  3.67it/s] 10%|█         | 21/200 [00:05<00:48,  3.69it/s] 11%|█         | 22/200 [00:06<00:47,  3.74it/s] 12%|█▏        | 23/200 [00:06<00:48,  3.68it/s] 12%|█▏        | 24/200 [00:06<00:48,  3.65it/s] 12%|█▎        | 25/200 [00:06<00:48,  3.61it/s] 13%|█▎        | 26/200 [00:07<00:46,  3.75it/s] 14%|█▎        | 27/200 [00:07<00:46,  3.69it/s] 14%|█▍        | 28/200 [00:07<00:47,  3.66it/s] 14%|█▍        | 29/200 [00:07<00:46,  3.65it/s] 15%|█▌        | 30/200 [00:08<00:45,  3.71it/s] 16%|█▌        | 31/200 [00:08<00:44,  3.76it/s] 16%|█▌        | 32/200 [00:08<00:46,  3.64it/s] 16%|█▋        | 33/200 [00:09<00:45,  3.70it/s] 17%|█▋        | 34/200 [00:09<00:45,  3.66it/s] 18%|█▊        | 35/200 [00:09<00:44,  3.71it/s] 18%|█▊        | 36/200 [00:09<00:44,  3.66it/s] 18%|█▊        | 37/200 [00:10<00:44,  3.63it/s] 19%|█▉        | 38/200 [00:10<00:45,  3.60it/s] 20%|█▉        | 39/200 [00:10<00:43,  3.74it/s] 20%|██        | 40/200 [00:10<00:43,  3.71it/s] 20%|██        | 41/200 [00:11<00:43,  3.64it/s] 21%|██        | 42/200 [00:11<00:43,  3.66it/s] 22%|██▏       | 43/200 [00:11<00:42,  3.70it/s] 22%|██▏       | 44/200 [00:11<00:41,  3.74it/s] 22%|██▎       | 45/200 [00:12<00:42,  3.64it/s] 23%|██▎       | 46/200 [00:12<00:41,  3.67it/s] 24%|██▎       | 47/200 [00:12<00:41,  3.66it/s] 24%|██▍       | 48/200 [00:13<00:40,  3.72it/s] 24%|██▍       | 49/200 [00:13<00:41,  3.66it/s] 25%|██▌       | 50/200 [00:13<00:41,  3.62it/s] 26%|██▌       | 51/200 [00:13<00:41,  3.59it/s] 26%|██▌       | 52/200 [00:14<00:39,  3.74it/s] 26%|██▋       | 53/200 [00:14<00:39,  3.68it/s] 27%|██▋       | 54/200 [00:14<00:40,  3.63it/s] 28%|██▊       | 55/200 [00:15<00:40,  3.58it/s] 28%|██▊       | 56/200 [00:15<00:38,  3.73it/s] 28%|██▊       | 57/200 [00:15<00:38,  3.74it/s] 29%|██▉       | 58/200 [00:15<00:39,  3.63it/s] 30%|██▉       | 59/200 [00:16<00:38,  3.65it/s] 30%|███       | 60/200 [00:16<00:37,  3.68it/s] 30%|███       | 61/200 [00:16<00:37,  3.72it/s] 31%|███       | 62/200 [00:16<00:38,  3.62it/s] 32%|███▏      | 63/200 [00:17<00:37,  3.63it/s] 32%|███▏      | 64/200 [00:17<00:37,  3.66it/s] 32%|███▎      | 65/200 [00:17<00:36,  3.71it/s] 33%|███▎      | 66/200 [00:18<00:36,  3.66it/s] 34%|███▎      | 67/200 [00:18<00:36,  3.62it/s] 34%|███▍      | 68/200 [00:18<00:36,  3.59it/s] 34%|███▍      | 69/200 [00:18<00:35,  3.73it/s] 35%|███▌      | 70/200 [00:19<00:35,  3.66it/s] 36%|███▌      | 71/200 [00:19<00:35,  3.62it/s] 36%|███▌      | 72/200 [00:19<00:35,  3.58it/s] 36%|███▋      | 73/200 [00:19<00:34,  3.73it/s] 37%|███▋      | 74/200 [00:20<00:34,  3.66it/s] 38%|███▊      | 75/200 [00:20<00:34,  3.62it/s] 38%|███▊      | 76/200 [00:20<00:34,  3.58it/s] 38%|███▊      | 77/200 [00:21<00:32,  3.73it/s] 39%|███▉      | 78/200 [00:21<00:32,  3.71it/s] 40%|███▉      | 79/200 [00:21<00:33,  3.63it/s] 40%|████      | 80/200 [00:21<00:32,  3.65it/s] 40%|████      | 81/200 [00:22<00:32,  3.68it/s] 41%|████      | 82/200 [00:22<00:31,  3.74it/s] 42%|████▏     | 83/200 [00:22<00:31,  3.67it/s] 42%|████▏     | 84/200 [00:22<00:32,  3.61it/s] 42%|████▎     | 85/200 [00:23<00:32,  3.57it/s] 43%|████▎     | 86/200 [00:23<00:30,  3.72it/s] 44%|████▎     | 87/200 [00:23<00:30,  3.66it/s] 44%|████▍     | 88/200 [00:24<00:30,  3.63it/s] 44%|████▍     | 89/200 [00:24<00:30,  3.65it/s] 45%|████▌     | 90/200 [00:24<00:29,  3.67it/s] 46%|████▌     | 91/200 [00:24<00:29,  3.73it/s] 46%|████▌     | 92/200 [00:25<00:29,  3.67it/s] 46%|████▋     | 93/200 [00:25<00:29,  3.63it/s] 47%|████▋     | 94/200 [00:25<00:29,  3.59it/s] 48%|████▊     | 95/200 [00:25<00:28,  3.73it/s] 48%|████▊     | 96/200 [00:26<00:28,  3.67it/s] 48%|████▊     | 97/200 [00:26<00:28,  3.63it/s] 49%|████▉     | 98/200 [00:26<00:27,  3.70it/s] 50%|████▉     | 99/200 [00:27<00:27,  3.66it/s] 50%|█████     | 100/200 [00:27<00:26,  3.72it/s] 50%|█████     | 101/200 [00:27<00:27,  3.66it/s] 51%|█████     | 102/200 [00:27<00:27,  3.62it/s] 52%|█████▏    | 103/200 [00:28<00:27,  3.58it/s] 52%|█████▏    | 104/200 [00:28<00:25,  3.72it/s] 52%|█████▎    | 105/200 [00:28<00:25,  3.66it/s] 53%|█████▎    | 106/200 [00:28<00:25,  3.62it/s] 54%|█████▎    | 107/200 [00:29<00:25,  3.61it/s] 54%|█████▍    | 108/200 [00:29<00:24,  3.68it/s] 55%|█████▍    | 109/200 [00:29<00:24,  3.71it/s] 55%|█████▌    | 110/200 [00:30<00:24,  3.61it/s] 56%|█████▌    | 111/200 [00:30<00:24,  3.61it/s] 56%|█████▌    | 112/200 [00:30<00:23,  3.74it/s] 56%|█████▋    | 113/200 [00:30<00:22,  3.83it/s] 57%|█████▋    | 114/200 [00:31<00:22,  3.90it/s] 57%|█████▊    | 115/200 [00:31<00:21,  3.96it/s] 58%|█████▊    | 116/200 [00:31<00:21,  4.00it/s] 58%|█████▊    | 117/200 [00:31<00:20,  4.02it/s] 59%|█████▉    | 118/200 [00:32<00:20,  4.03it/s] 60%|█████▉    | 119/200 [00:32<00:19,  4.05it/s] 60%|██████    | 120/200 [00:32<00:19,  4.07it/s] 60%|██████    | 121/200 [00:32<00:19,  4.07it/s] 61%|██████    | 122/200 [00:33<00:19,  4.07it/s] 62%|██████▏   | 123/200 [00:33<00:18,  4.08it/s] 62%|██████▏   | 124/200 [00:33<00:18,  4.08it/s] 62%|██████▎   | 125/200 [00:33<00:18,  4.08it/s] 63%|██████▎   | 126/200 [00:34<00:18,  4.07it/s] 64%|██████▎   | 127/200 [00:34<00:17,  4.08it/s] 64%|██████▍   | 128/200 [00:34<00:17,  4.09it/s] 64%|██████▍   | 129/200 [00:34<00:17,  4.08it/s] 65%|██████▌   | 130/200 [00:34<00:17,  4.08it/s] 66%|██████▌   | 131/200 [00:35<00:16,  4.08it/s] 66%|██████▌   | 132/200 [00:35<00:16,  4.09it/s] 66%|██████▋   | 133/200 [00:35<00:16,  4.08it/s] 67%|██████▋   | 134/200 [00:35<00:16,  4.08it/s] 68%|██████▊   | 135/200 [00:36<00:15,  4.09it/s] 68%|██████▊   | 136/200 [00:36<00:15,  4.09it/s] 68%|██████▊   | 137/200 [00:36<00:15,  4.09it/s] 69%|██████▉   | 138/200 [00:36<00:15,  4.08it/s] 70%|██████▉   | 139/200 [00:37<00:14,  4.08it/s] 70%|███████   | 140/200 [00:37<00:14,  4.08it/s] 70%|███████   | 141/200 [00:37<00:14,  4.08it/s] 71%|███████   | 142/200 [00:37<00:14,  4.08it/s] 72%|███████▏  | 143/200 [00:38<00:13,  4.08it/s] 72%|███████▏  | 144/200 [00:38<00:13,  4.08it/s] 72%|███████▎  | 145/200 [00:38<00:13,  4.02it/s] 73%|███████▎  | 146/200 [00:38<00:13,  3.92it/s] 74%|███████▎  | 147/200 [00:39<00:13,  3.84it/s] 74%|███████▍  | 148/200 [00:39<00:13,  3.82it/s] 74%|███████▍  | 149/200 [00:39<00:13,  3.89it/s] 75%|███████▌  | 150/200 [00:39<00:12,  3.86it/s] 76%|███████▌  | 151/200 [00:40<00:12,  3.80it/s] 76%|███████▌  | 152/200 [00:40<00:12,  3.79it/s] 76%|███████▋  | 153/200 [00:40<00:12,  3.72it/s] 77%|███████▋  | 154/200 [00:41<00:12,  3.82it/s] 78%|███████▊  | 155/200 [00:41<00:11,  3.81it/s] 78%|███████▊  | 156/200 [00:41<00:11,  3.77it/s] 78%|███████▊  | 157/200 [00:41<00:11,  3.78it/s] 79%|███████▉  | 158/200 [00:42<00:11,  3.71it/s] 80%|███████▉  | 159/200 [00:42<00:10,  3.80it/s] 80%|████████  | 160/200 [00:42<00:10,  3.79it/s] 80%|████████  | 161/200 [00:42<00:10,  3.75it/s] 81%|████████  | 162/200 [00:43<00:10,  3.75it/s] 82%|████████▏ | 163/200 [00:43<00:10,  3.69it/s] 82%|████████▏ | 164/200 [00:43<00:09,  3.79it/s] 82%|████████▎ | 165/200 [00:43<00:09,  3.76it/s] 83%|████████▎ | 166/200 [00:44<00:09,  3.73it/s] 84%|████████▎ | 167/200 [00:44<00:08,  3.74it/s] 84%|████████▍ | 168/200 [00:44<00:08,  3.67it/s] 84%|████████▍ | 169/200 [00:45<00:08,  3.79it/s] 85%|████████▌ | 170/200 [00:45<00:07,  3.77it/s] 86%|████████▌ | 171/200 [00:45<00:07,  3.72it/s] 86%|████████▌ | 172/200 [00:45<00:07,  3.74it/s] 86%|████████▋ | 173/200 [00:46<00:07,  3.75it/s] 87%|████████▋ | 174/200 [00:46<00:06,  3.85it/s] 88%|████████▊ | 175/200 [00:46<00:06,  3.91it/s] 88%|████████▊ | 176/200 [00:46<00:06,  3.95it/s] 88%|████████▊ | 177/200 [00:47<00:05,  3.99it/s] 89%|████████▉ | 178/200 [00:47<00:05,  4.01it/s] 90%|████████▉ | 179/200 [00:47<00:05,  4.02it/s] 90%|█████████ | 180/200 [00:47<00:04,  4.04it/s] 90%|█████████ | 181/200 [00:48<00:04,  4.05it/s] 91%|█████████ | 182/200 [00:48<00:04,  4.02it/s] 92%|█████████▏| 183/200 [00:48<00:04,  3.91it/s] 92%|█████████▏| 184/200 [00:48<00:04,  3.85it/s] 92%|█████████▎| 185/200 [00:49<00:03,  3.78it/s] 93%|█████████▎| 186/200 [00:49<00:03,  3.87it/s] 94%|█████████▎| 187/200 [00:49<00:03,  3.88it/s] 94%|█████████▍| 188/200 [00:49<00:03,  3.81it/s] 94%|█████████▍| 189/200 [00:50<00:02,  3.75it/s] 95%|█████████▌| 190/200 [00:50<00:02,  3.66it/s] 96%|█████████▌| 191/200 [00:50<00:02,  3.78it/s] 96%|█████████▌| 192/200 [00:50<00:02,  3.82it/s] 96%|█████████▋| 193/200 [00:51<00:01,  3.77it/s] 97%|█████████▋| 194/200 [00:51<00:01,  3.73it/s] 98%|█████████▊| 195/200 [00:51<00:01,  3.64it/s] 98%|█████████▊| 196/200 [00:52<00:01,  3.77it/s] 98%|█████████▊| 197/200 [00:52<00:00,  3.82it/s] 99%|█████████▉| 198/200 [00:52<00:00,  3.77it/s]100%|█████████▉| 199/200 [00:52<00:00,  3.72it/s]100%|██████████| 200/200 [00:53<00:00,  3.64it/s]accuracy:  0.55
100%|██████████| 200/200 [00:56<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 191
Training dataset size: 96, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.75s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:30:49,049] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.73s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.03s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:30:50,647] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:30:51,826] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:38,  2.55s/it] 12%|█▎        | 2/16 [00:05<00:35,  2.57s/it] 19%|█▉        | 3/16 [00:08<00:35,  2.74s/it] 25%|██▌       | 4/16 [00:10<00:31,  2.66s/it] 31%|███▏      | 5/16 [00:13<00:31,  2.82s/it] 38%|███▊      | 6/16 [00:17<00:29,  2.99s/it] 44%|████▍     | 7/16 [00:19<00:24,  2.70s/it] 50%|█████     | 8/16 [00:21<00:20,  2.54s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:24<00:18,  2.68s/it] 62%|██████▎   | 10/16 [00:27<00:16,  2.76s/it]                                               {'loss': 1.466, 'grad_norm': 16.244831085205078, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:27<00:16,  2.76s/it] 69%|██████▉   | 11/16 [00:29<00:13,  2.68s/it] 75%|███████▌  | 12/16 [00:31<00:09,  2.48s/it] 81%|████████▏ | 13/16 [00:34<00:07,  2.47s/it] 88%|████████▊ | 14/16 [00:36<00:04,  2.42s/it] 94%|█████████▍| 15/16 [00:38<00:02,  2.43s/it]100%|██████████| 16/16 [00:41<00:00,  2.56s/it]                                               {'train_runtime': 42.498, 'train_samples_per_second': 4.518, 'train_steps_per_second': 0.376, 'train_loss': 1.361541986465454, 'epoch': 2.0}
100%|██████████| 16/16 [00:42<00:00,  2.56s/it]100%|██████████| 16/16 [00:42<00:00,  2.64s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:28,  3.15it/s]  1%|          | 2/279 [00:00<01:15,  3.68it/s]  1%|          | 3/279 [00:00<01:11,  3.88it/s]  1%|▏         | 4/279 [00:01<01:08,  3.99it/s]  2%|▏         | 5/279 [00:01<01:07,  4.04it/s]  2%|▏         | 6/279 [00:01<01:06,  4.08it/s]  3%|▎         | 7/279 [00:01<01:06,  4.10it/s]  3%|▎         | 8/279 [00:02<01:06,  4.09it/s]  3%|▎         | 9/279 [00:02<01:05,  4.11it/s]  4%|▎         | 10/279 [00:02<01:05,  4.13it/s]  4%|▍         | 11/279 [00:02<01:04,  4.14it/s]  4%|▍         | 12/279 [00:02<01:04,  4.15it/s]  5%|▍         | 13/279 [00:03<01:04,  4.15it/s]  5%|▌         | 14/279 [00:03<01:03,  4.15it/s]  5%|▌         | 15/279 [00:03<01:03,  4.15it/s]  6%|▌         | 16/279 [00:03<01:03,  4.15it/s]  6%|▌         | 17/279 [00:04<01:03,  4.16it/s]  6%|▋         | 18/279 [00:04<01:02,  4.16it/s]  7%|▋         | 19/279 [00:04<01:02,  4.16it/s]  7%|▋         | 20/279 [00:04<01:02,  4.16it/s]  8%|▊         | 21/279 [00:05<01:01,  4.17it/s]  8%|▊         | 22/279 [00:05<01:01,  4.17it/s]  8%|▊         | 23/279 [00:05<01:01,  4.17it/s]  9%|▊         | 24/279 [00:05<01:01,  4.17it/s]  9%|▉         | 25/279 [00:06<01:00,  4.17it/s]  9%|▉         | 26/279 [00:06<01:00,  4.17it/s] 10%|▉         | 27/279 [00:06<01:00,  4.17it/s] 10%|█         | 28/279 [00:06<01:00,  4.17it/s] 10%|█         | 29/279 [00:07<00:59,  4.17it/s] 11%|█         | 30/279 [00:07<00:59,  4.16it/s] 11%|█         | 31/279 [00:07<00:59,  4.17it/s] 11%|█▏        | 32/279 [00:07<00:59,  4.17it/s] 12%|█▏        | 33/279 [00:08<00:59,  4.17it/s] 12%|█▏        | 34/279 [00:08<00:58,  4.17it/s] 13%|█▎        | 35/279 [00:08<00:58,  4.17it/s] 13%|█▎        | 36/279 [00:08<00:58,  4.17it/s] 13%|█▎        | 37/279 [00:08<00:58,  4.16it/s] 14%|█▎        | 38/279 [00:09<00:57,  4.16it/s] 14%|█▍        | 39/279 [00:09<00:57,  4.15it/s] 14%|█▍        | 40/279 [00:09<00:57,  4.15it/s] 15%|█▍        | 41/279 [00:09<00:57,  4.14it/s] 15%|█▌        | 42/279 [00:10<00:57,  4.14it/s] 15%|█▌        | 43/279 [00:10<00:57,  4.13it/s] 16%|█▌        | 44/279 [00:10<00:56,  4.13it/s] 16%|█▌        | 45/279 [00:10<00:56,  4.14it/s] 16%|█▋        | 46/279 [00:11<00:56,  4.14it/s] 17%|█▋        | 47/279 [00:11<00:55,  4.15it/s] 17%|█▋        | 48/279 [00:11<00:55,  4.14it/s] 18%|█▊        | 49/279 [00:11<00:55,  4.15it/s] 18%|█▊        | 50/279 [00:12<00:55,  4.14it/s] 18%|█▊        | 51/279 [00:12<00:55,  4.14it/s] 19%|█▊        | 52/279 [00:12<00:54,  4.13it/s] 19%|█▉        | 53/279 [00:12<00:54,  4.12it/s] 19%|█▉        | 54/279 [00:13<00:54,  4.12it/s] 20%|█▉        | 55/279 [00:13<00:54,  4.12it/s] 20%|██        | 56/279 [00:13<00:54,  4.12it/s] 20%|██        | 57/279 [00:13<00:53,  4.13it/s] 21%|██        | 58/279 [00:14<00:53,  4.13it/s] 21%|██        | 59/279 [00:14<00:53,  4.13it/s] 22%|██▏       | 60/279 [00:14<00:53,  4.12it/s] 22%|██▏       | 61/279 [00:14<00:52,  4.12it/s] 22%|██▏       | 62/279 [00:15<00:52,  4.11it/s] 23%|██▎       | 63/279 [00:15<00:52,  4.12it/s] 23%|██▎       | 64/279 [00:15<00:52,  4.13it/s] 23%|██▎       | 65/279 [00:15<00:51,  4.14it/s] 24%|██▎       | 66/279 [00:15<00:51,  4.14it/s] 24%|██▍       | 67/279 [00:16<00:51,  4.15it/s] 24%|██▍       | 68/279 [00:16<00:50,  4.15it/s] 25%|██▍       | 69/279 [00:16<00:50,  4.14it/s] 25%|██▌       | 70/279 [00:16<00:50,  4.13it/s] 25%|██▌       | 71/279 [00:17<00:50,  4.12it/s] 26%|██▌       | 72/279 [00:17<00:50,  4.11it/s] 26%|██▌       | 73/279 [00:17<00:50,  4.11it/s] 27%|██▋       | 74/279 [00:17<00:49,  4.12it/s] 27%|██▋       | 75/279 [00:18<00:49,  4.12it/s] 27%|██▋       | 76/279 [00:18<00:49,  4.12it/s] 28%|██▊       | 77/279 [00:18<00:49,  4.11it/s] 28%|██▊       | 78/279 [00:18<00:48,  4.11it/s] 28%|██▊       | 79/279 [00:19<00:48,  4.11it/s] 29%|██▊       | 80/279 [00:19<00:48,  4.11it/s] 29%|██▉       | 81/279 [00:19<00:48,  4.12it/s] 29%|██▉       | 82/279 [00:19<00:47,  4.12it/s] 30%|██▉       | 83/279 [00:20<00:47,  4.11it/s] 30%|███       | 84/279 [00:20<00:47,  4.10it/s] 30%|███       | 85/279 [00:20<00:47,  4.10it/s] 31%|███       | 86/279 [00:20<00:46,  4.11it/s] 31%|███       | 87/279 [00:21<00:46,  4.11it/s] 32%|███▏      | 88/279 [00:21<00:46,  4.11it/s] 32%|███▏      | 89/279 [00:21<00:46,  4.10it/s] 32%|███▏      | 90/279 [00:21<00:46,  4.10it/s] 33%|███▎      | 91/279 [00:22<00:45,  4.11it/s] 33%|███▎      | 92/279 [00:22<00:45,  4.07it/s] 33%|███▎      | 93/279 [00:22<00:47,  3.96it/s] 34%|███▎      | 94/279 [00:22<00:47,  3.87it/s] 34%|███▍      | 95/279 [00:23<00:47,  3.86it/s] 34%|███▍      | 96/279 [00:23<00:47,  3.85it/s] 35%|███▍      | 97/279 [00:23<00:48,  3.74it/s] 35%|███▌      | 98/279 [00:23<00:49,  3.66it/s] 35%|███▌      | 99/279 [00:24<00:47,  3.78it/s] 36%|███▌      | 100/279 [00:24<00:48,  3.73it/s] 36%|███▌      | 101/279 [00:24<00:48,  3.68it/s] 37%|███▋      | 102/279 [00:25<00:48,  3.65it/s] 37%|███▋      | 103/279 [00:25<00:46,  3.75it/s] 37%|███▋      | 104/279 [00:25<00:46,  3.75it/s] 38%|███▊      | 105/279 [00:25<00:47,  3.67it/s] 38%|███▊      | 106/279 [00:26<00:47,  3.62it/s] 38%|███▊      | 107/279 [00:26<00:45,  3.76it/s] 39%|███▊      | 108/279 [00:26<00:46,  3.71it/s] 39%|███▉      | 109/279 [00:26<00:46,  3.67it/s] 39%|███▉      | 110/279 [00:27<00:46,  3.62it/s] 40%|███▉      | 111/279 [00:27<00:44,  3.76it/s] 40%|████      | 112/279 [00:27<00:44,  3.71it/s] 41%|████      | 113/279 [00:28<00:45,  3.68it/s] 41%|████      | 114/279 [00:28<00:45,  3.62it/s] 41%|████      | 115/279 [00:28<00:43,  3.75it/s] 42%|████▏     | 116/279 [00:28<00:44,  3.70it/s] 42%|████▏     | 117/279 [00:29<00:44,  3.66it/s] 42%|████▏     | 118/279 [00:29<00:44,  3.63it/s] 43%|████▎     | 119/279 [00:29<00:42,  3.72it/s] 43%|████▎     | 120/279 [00:29<00:43,  3.65it/s] 43%|████▎     | 121/279 [00:30<00:42,  3.71it/s] 44%|████▎     | 122/279 [00:30<00:42,  3.67it/s] 44%|████▍     | 123/279 [00:30<00:42,  3.71it/s] 44%|████▍     | 124/279 [00:31<00:42,  3.63it/s] 45%|████▍     | 125/279 [00:31<00:42,  3.59it/s] 45%|████▌     | 126/279 [00:31<00:41,  3.73it/s] 46%|████▌     | 127/279 [00:31<00:41,  3.69it/s] 46%|████▌     | 128/279 [00:32<00:41,  3.66it/s] 46%|████▌     | 129/279 [00:32<00:41,  3.64it/s] 47%|████▋     | 130/279 [00:32<00:40,  3.72it/s] 47%|████▋     | 131/279 [00:32<00:40,  3.65it/s] 47%|████▋     | 132/279 [00:33<00:39,  3.71it/s] 48%|████▊     | 133/279 [00:33<00:39,  3.68it/s] 48%|████▊     | 134/279 [00:33<00:38,  3.73it/s] 48%|████▊     | 135/279 [00:33<00:39,  3.65it/s] 49%|████▊     | 136/279 [00:34<00:39,  3.59it/s] 49%|████▉     | 137/279 [00:34<00:38,  3.74it/s] 49%|████▉     | 138/279 [00:34<00:38,  3.69it/s] 50%|████▉     | 139/279 [00:35<00:38,  3.66it/s] 50%|█████     | 140/279 [00:35<00:38,  3.60it/s] 51%|█████     | 141/279 [00:35<00:36,  3.74it/s] 51%|█████     | 142/279 [00:35<00:37,  3.69it/s] 51%|█████▏    | 143/279 [00:36<00:37,  3.65it/s] 52%|█████▏    | 144/279 [00:36<00:37,  3.63it/s] 52%|█████▏    | 145/279 [00:36<00:36,  3.72it/s] 52%|█████▏    | 146/279 [00:36<00:36,  3.65it/s] 53%|█████▎    | 147/279 [00:37<00:36,  3.66it/s] 53%|█████▎    | 148/279 [00:37<00:35,  3.73it/s] 53%|█████▎    | 149/279 [00:37<00:33,  3.83it/s] 54%|█████▍    | 150/279 [00:38<00:32,  3.91it/s] 54%|█████▍    | 151/279 [00:38<00:32,  3.96it/s] 54%|█████▍    | 152/279 [00:38<00:31,  4.00it/s] 55%|█████▍    | 153/279 [00:38<00:31,  4.02it/s] 55%|█████▌    | 154/279 [00:38<00:30,  4.04it/s] 56%|█████▌    | 155/279 [00:39<00:30,  4.05it/s] 56%|█████▌    | 156/279 [00:39<00:30,  4.06it/s] 56%|█████▋    | 157/279 [00:39<00:30,  4.06it/s] 57%|█████▋    | 158/279 [00:39<00:29,  4.08it/s] 57%|█████▋    | 159/279 [00:40<00:29,  4.08it/s] 57%|█████▋    | 160/279 [00:40<00:29,  4.08it/s] 58%|█████▊    | 161/279 [00:40<00:28,  4.07it/s] 58%|█████▊    | 162/279 [00:40<00:28,  4.08it/s] 58%|█████▊    | 163/279 [00:41<00:28,  4.09it/s] 59%|█████▉    | 164/279 [00:41<00:28,  4.08it/s] 59%|█████▉    | 165/279 [00:41<00:27,  4.08it/s] 59%|█████▉    | 166/279 [00:41<00:27,  4.08it/s] 60%|█████▉    | 167/279 [00:42<00:27,  4.08it/s] 60%|██████    | 168/279 [00:42<00:27,  4.07it/s] 61%|██████    | 169/279 [00:42<00:27,  4.07it/s] 61%|██████    | 170/279 [00:42<00:26,  4.08it/s] 61%|██████▏   | 171/279 [00:43<00:26,  4.09it/s] 62%|██████▏   | 172/279 [00:43<00:26,  4.08it/s] 62%|██████▏   | 173/279 [00:43<00:26,  4.07it/s] 62%|██████▏   | 174/279 [00:43<00:25,  4.08it/s] 63%|██████▎   | 175/279 [00:44<00:25,  4.08it/s] 63%|██████▎   | 176/279 [00:44<00:25,  4.08it/s] 63%|██████▎   | 177/279 [00:44<00:24,  4.08it/s] 64%|██████▍   | 178/279 [00:44<00:24,  4.09it/s] 64%|██████▍   | 179/279 [00:45<00:24,  4.01it/s] 65%|██████▍   | 180/279 [00:45<00:25,  3.90it/s] 65%|██████▍   | 181/279 [00:45<00:25,  3.87it/s] 65%|██████▌   | 182/279 [00:45<00:25,  3.86it/s] 66%|██████▌   | 183/279 [00:46<00:25,  3.78it/s] 66%|██████▌   | 184/279 [00:46<00:25,  3.69it/s] 66%|██████▋   | 185/279 [00:46<00:24,  3.76it/s] 67%|██████▋   | 186/279 [00:47<00:24,  3.73it/s] 67%|██████▋   | 187/279 [00:47<00:24,  3.69it/s] 67%|██████▋   | 188/279 [00:47<00:24,  3.65it/s] 68%|██████▊   | 189/279 [00:47<00:24,  3.73it/s] 68%|██████▊   | 190/279 [00:48<00:24,  3.68it/s] 68%|██████▊   | 191/279 [00:48<00:23,  3.67it/s] 69%|██████▉   | 192/279 [00:48<00:23,  3.71it/s] 69%|██████▉   | 193/279 [00:48<00:23,  3.73it/s] 70%|██████▉   | 194/279 [00:49<00:22,  3.70it/s] 70%|██████▉   | 195/279 [00:49<00:22,  3.66it/s] 70%|███████   | 196/279 [00:49<00:22,  3.73it/s] 71%|███████   | 197/279 [00:49<00:22,  3.70it/s] 71%|███████   | 198/279 [00:50<00:22,  3.67it/s] 71%|███████▏  | 199/279 [00:50<00:21,  3.64it/s] 72%|███████▏  | 200/279 [00:50<00:21,  3.73it/s] 72%|███████▏  | 201/279 [00:51<00:21,  3.68it/s] 72%|███████▏  | 202/279 [00:51<00:20,  3.72it/s] 73%|███████▎  | 203/279 [00:51<00:20,  3.65it/s] 73%|███████▎  | 204/279 [00:51<00:20,  3.69it/s] 73%|███████▎  | 205/279 [00:52<00:20,  3.65it/s] 74%|███████▍  | 206/279 [00:52<00:20,  3.62it/s] 74%|███████▍  | 207/279 [00:52<00:19,  3.70it/s] 75%|███████▍  | 208/279 [00:52<00:19,  3.69it/s] 75%|███████▍  | 209/279 [00:53<00:19,  3.67it/s] 75%|███████▌  | 210/279 [00:53<00:18,  3.64it/s] 76%|███████▌  | 211/279 [00:53<00:18,  3.73it/s] 76%|███████▌  | 212/279 [00:54<00:18,  3.68it/s] 76%|███████▋  | 213/279 [00:54<00:18,  3.65it/s] 77%|███████▋  | 214/279 [00:54<00:17,  3.70it/s] 77%|███████▋  | 215/279 [00:54<00:17,  3.70it/s] 77%|███████▋  | 216/279 [00:55<00:17,  3.67it/s] 78%|███████▊  | 217/279 [00:55<00:17,  3.63it/s] 78%|███████▊  | 218/279 [00:55<00:16,  3.72it/s] 78%|███████▊  | 219/279 [00:55<00:16,  3.68it/s] 79%|███████▉  | 220/279 [00:56<00:15,  3.73it/s] 79%|███████▉  | 221/279 [00:56<00:15,  3.68it/s] 80%|███████▉  | 222/279 [00:56<00:15,  3.68it/s] 80%|███████▉  | 223/279 [00:57<00:15,  3.65it/s] 80%|████████  | 224/279 [00:57<00:15,  3.60it/s] 81%|████████  | 225/279 [00:57<00:14,  3.72it/s] 81%|████████  | 226/279 [00:57<00:14,  3.68it/s] 81%|████████▏ | 227/279 [00:58<00:14,  3.67it/s] 82%|████████▏ | 228/279 [00:58<00:13,  3.65it/s] 82%|████████▏ | 229/279 [00:58<00:13,  3.76it/s] 82%|████████▏ | 230/279 [00:58<00:13,  3.71it/s] 83%|████████▎ | 231/279 [00:59<00:13,  3.69it/s] 83%|████████▎ | 232/279 [00:59<00:12,  3.65it/s] 84%|████████▎ | 233/279 [00:59<00:12,  3.73it/s] 84%|████████▍ | 234/279 [01:00<00:12,  3.68it/s] 84%|████████▍ | 235/279 [01:00<00:11,  3.67it/s] 85%|████████▍ | 236/279 [01:00<00:11,  3.67it/s] 85%|████████▍ | 237/279 [01:00<00:11,  3.69it/s] 85%|████████▌ | 238/279 [01:01<00:11,  3.67it/s] 86%|████████▌ | 239/279 [01:01<00:11,  3.61it/s] 86%|████████▌ | 240/279 [01:01<00:10,  3.71it/s] 86%|████████▋ | 241/279 [01:01<00:10,  3.71it/s] 87%|████████▋ | 242/279 [01:02<00:10,  3.68it/s] 87%|████████▋ | 243/279 [01:02<00:09,  3.64it/s] 87%|████████▋ | 244/279 [01:02<00:09,  3.72it/s] 88%|████████▊ | 245/279 [01:03<00:09,  3.67it/s] 88%|████████▊ | 246/279 [01:03<00:09,  3.62it/s] 89%|████████▊ | 247/279 [01:03<00:08,  3.69it/s] 89%|████████▉ | 248/279 [01:03<00:08,  3.68it/s] 89%|████████▉ | 249/279 [01:04<00:08,  3.66it/s] 90%|████████▉ | 250/279 [01:04<00:08,  3.60it/s] 90%|████████▉ | 251/279 [01:04<00:07,  3.72it/s] 90%|█████████ | 252/279 [01:04<00:07,  3.68it/s] 91%|█████████ | 253/279 [01:05<00:07,  3.66it/s] 91%|█████████ | 254/279 [01:05<00:06,  3.62it/s] 91%|█████████▏| 255/279 [01:05<00:06,  3.71it/s] 92%|█████████▏| 256/279 [01:06<00:06,  3.67it/s] 92%|█████████▏| 257/279 [01:06<00:06,  3.66it/s] 92%|█████████▏| 258/279 [01:06<00:05,  3.67it/s] 93%|█████████▎| 259/279 [01:06<00:05,  3.67it/s] 93%|█████████▎| 260/279 [01:07<00:05,  3.64it/s] 94%|█████████▎| 261/279 [01:07<00:05,  3.59it/s] 94%|█████████▍| 262/279 [01:07<00:04,  3.71it/s] 94%|█████████▍| 263/279 [01:07<00:04,  3.68it/s] 95%|█████████▍| 264/279 [01:08<00:04,  3.67it/s] 95%|█████████▍| 265/279 [01:08<00:03,  3.62it/s] 95%|█████████▌| 266/279 [01:08<00:03,  3.72it/s] 96%|█████████▌| 267/279 [01:09<00:03,  3.67it/s] 96%|█████████▌| 268/279 [01:09<00:02,  3.70it/s] 96%|█████████▋| 269/279 [01:09<00:02,  3.65it/s] 97%|█████████▋| 270/279 [01:09<00:02,  3.70it/s] 97%|█████████▋| 271/279 [01:10<00:02,  3.66it/s] 97%|█████████▋| 272/279 [01:10<00:01,  3.65it/s] 98%|█████████▊| 273/279 [01:10<00:01,  3.77it/s] 98%|█████████▊| 274/279 [01:10<00:01,  3.84it/s] 99%|█████████▊| 275/279 [01:11<00:01,  3.90it/s] 99%|█████████▉| 276/279 [01:11<00:00,  3.95it/s] 99%|█████████▉| 277/279 [01:11<00:00,  3.98it/s]100%|█████████▉| 278/279 [01:11<00:00,  4.00it/s]100%|██████████| 279/279 [01:12<00:00,  4.03it/s]accuracy:  0.5304659498207885
100%|██████████| 279/279 [01:16<00:00,  3.65it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 96, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.80s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 03:33:18,945] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 03:33:19,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:33:19,350] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:03<00:45,  3.06s/it] 12%|█▎        | 2/16 [00:05<00:38,  2.75s/it] 19%|█▉        | 3/16 [00:07<00:33,  2.57s/it] 25%|██▌       | 4/16 [00:09<00:28,  2.36s/it] 31%|███▏      | 5/16 [00:11<00:23,  2.17s/it] 38%|███▊      | 6/16 [00:14<00:22,  2.24s/it] 44%|████▍     | 7/16 [00:16<00:19,  2.17s/it] 50%|█████     | 8/16 [00:17<00:16,  2.04s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:20<00:15,  2.22s/it] 62%|██████▎   | 10/16 [00:23<00:15,  2.51s/it]                                               {'loss': 0.3629, 'grad_norm': 8.463582038879395, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:23<00:15,  2.51s/it] 69%|██████▉   | 11/16 [00:26<00:12,  2.44s/it] 75%|███████▌  | 12/16 [00:28<00:09,  2.30s/it] 81%|████████▏ | 13/16 [00:30<00:07,  2.49s/it] 88%|████████▊ | 14/16 [00:33<00:04,  2.37s/it] 94%|█████████▍| 15/16 [00:35<00:02,  2.32s/it]100%|██████████| 16/16 [00:37<00:00,  2.18s/it]                                               {'train_runtime': 37.7787, 'train_samples_per_second': 5.082, 'train_steps_per_second': 0.424, 'train_loss': 0.3038322776556015, 'epoch': 2.0}
100%|██████████| 16/16 [00:37<00:00,  2.18s/it]100%|██████████| 16/16 [00:37<00:00,  2.35s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:48,  2.50it/s]  1%|          | 2/272 [00:00<01:22,  3.28it/s]  1%|          | 3/272 [00:00<01:18,  3.42it/s]  1%|▏         | 4/272 [00:01<01:16,  3.50it/s]  2%|▏         | 5/272 [00:01<01:13,  3.63it/s]  2%|▏         | 6/272 [00:01<01:13,  3.63it/s]  3%|▎         | 7/272 [00:01<01:10,  3.76it/s]  3%|▎         | 8/272 [00:02<01:11,  3.71it/s]  3%|▎         | 9/272 [00:02<01:11,  3.66it/s]  4%|▎         | 10/272 [00:02<01:12,  3.63it/s]  4%|▍         | 11/272 [00:03<01:09,  3.77it/s]  4%|▍         | 12/272 [00:03<01:08,  3.80it/s]  5%|▍         | 13/272 [00:03<01:09,  3.73it/s]  5%|▌         | 14/272 [00:03<01:10,  3.68it/s]  6%|▌         | 15/272 [00:04<01:09,  3.68it/s]  6%|▌         | 16/272 [00:04<01:08,  3.76it/s]  6%|▋         | 17/272 [00:04<01:07,  3.80it/s]  7%|▋         | 18/272 [00:04<01:09,  3.67it/s]  7%|▋         | 19/272 [00:05<01:08,  3.71it/s]  7%|▋         | 20/272 [00:05<01:08,  3.68it/s]  8%|▊         | 21/272 [00:05<01:05,  3.81it/s]  8%|▊         | 22/272 [00:06<01:06,  3.74it/s]  8%|▊         | 23/272 [00:06<01:07,  3.68it/s]  9%|▉         | 24/272 [00:06<01:08,  3.63it/s]  9%|▉         | 25/272 [00:06<01:05,  3.78it/s] 10%|▉         | 26/272 [00:07<01:04,  3.82it/s] 10%|▉         | 27/272 [00:07<01:04,  3.79it/s] 10%|█         | 28/272 [00:07<01:05,  3.71it/s] 11%|█         | 29/272 [00:07<01:06,  3.67it/s] 11%|█         | 30/272 [00:08<01:03,  3.80it/s] 11%|█▏        | 31/272 [00:08<01:02,  3.83it/s] 12%|█▏        | 32/272 [00:08<01:04,  3.70it/s] 12%|█▏        | 33/272 [00:08<01:03,  3.74it/s] 12%|█▎        | 34/272 [00:09<01:04,  3.70it/s] 13%|█▎        | 35/272 [00:09<01:02,  3.79it/s] 13%|█▎        | 36/272 [00:09<01:03,  3.73it/s] 14%|█▎        | 37/272 [00:10<01:03,  3.67it/s] 14%|█▍        | 38/272 [00:10<01:04,  3.63it/s] 14%|█▍        | 39/272 [00:10<01:01,  3.78it/s] 15%|█▍        | 40/272 [00:10<01:00,  3.81it/s] 15%|█▌        | 41/272 [00:11<01:02,  3.69it/s] 15%|█▌        | 42/272 [00:11<01:01,  3.72it/s] 16%|█▌        | 43/272 [00:11<01:02,  3.68it/s] 16%|█▌        | 44/272 [00:11<01:00,  3.79it/s] 17%|█▋        | 45/272 [00:12<01:01,  3.71it/s] 17%|█▋        | 46/272 [00:12<01:01,  3.67it/s] 17%|█▋        | 47/272 [00:12<01:02,  3.61it/s] 18%|█▊        | 48/272 [00:12<00:59,  3.77it/s] 18%|█▊        | 49/272 [00:13<00:58,  3.81it/s] 18%|█▊        | 50/272 [00:13<01:00,  3.69it/s] 19%|█▉        | 51/272 [00:13<00:59,  3.71it/s] 19%|█▉        | 52/272 [00:14<00:59,  3.68it/s] 19%|█▉        | 53/272 [00:14<00:57,  3.78it/s] 20%|█▉        | 54/272 [00:14<00:58,  3.71it/s] 20%|██        | 55/272 [00:14<00:59,  3.66it/s] 21%|██        | 56/272 [00:15<00:59,  3.61it/s] 21%|██        | 57/272 [00:15<00:57,  3.75it/s] 21%|██▏       | 58/272 [00:15<00:56,  3.78it/s] 22%|██▏       | 59/272 [00:15<00:58,  3.66it/s] 22%|██▏       | 60/272 [00:16<00:57,  3.71it/s] 22%|██▏       | 61/272 [00:16<00:57,  3.68it/s] 23%|██▎       | 62/272 [00:16<00:55,  3.79it/s] 23%|██▎       | 63/272 [00:17<00:56,  3.71it/s] 24%|██▎       | 64/272 [00:17<00:56,  3.66it/s] 24%|██▍       | 65/272 [00:17<00:57,  3.60it/s] 24%|██▍       | 66/272 [00:17<00:54,  3.75it/s] 25%|██▍       | 67/272 [00:18<00:54,  3.80it/s] 25%|██▌       | 68/272 [00:18<00:54,  3.72it/s] 25%|██▌       | 69/272 [00:18<00:55,  3.67it/s] 26%|██▌       | 70/272 [00:18<00:55,  3.62it/s] 26%|██▌       | 71/272 [00:19<00:53,  3.76it/s] 26%|██▋       | 72/272 [00:19<00:52,  3.78it/s] 27%|██▋       | 73/272 [00:19<00:54,  3.66it/s] 27%|██▋       | 74/272 [00:19<00:53,  3.71it/s] 28%|██▊       | 75/272 [00:20<00:53,  3.67it/s] 28%|██▊       | 76/272 [00:20<00:52,  3.76it/s] 28%|██▊       | 77/272 [00:20<00:52,  3.70it/s] 29%|██▊       | 78/272 [00:21<00:53,  3.65it/s] 29%|██▉       | 79/272 [00:21<00:53,  3.60it/s] 29%|██▉       | 80/272 [00:21<00:51,  3.75it/s] 30%|██▉       | 81/272 [00:21<00:50,  3.75it/s] 30%|███       | 82/272 [00:22<00:52,  3.63it/s] 31%|███       | 83/272 [00:22<00:51,  3.70it/s] 31%|███       | 84/272 [00:22<00:51,  3.65it/s] 31%|███▏      | 85/272 [00:22<00:50,  3.74it/s] 32%|███▏      | 86/272 [00:23<00:50,  3.69it/s] 32%|███▏      | 87/272 [00:23<00:50,  3.64it/s] 32%|███▏      | 88/272 [00:23<00:51,  3.60it/s] 33%|███▎      | 89/272 [00:24<00:48,  3.75it/s] 33%|███▎      | 90/272 [00:24<00:48,  3.77it/s] 33%|███▎      | 91/272 [00:24<00:49,  3.66it/s] 34%|███▍      | 92/272 [00:24<00:48,  3.71it/s] 34%|███▍      | 93/272 [00:25<00:48,  3.66it/s] 35%|███▍      | 94/272 [00:25<00:47,  3.75it/s] 35%|███▍      | 95/272 [00:25<00:47,  3.69it/s] 35%|███▌      | 96/272 [00:25<00:48,  3.65it/s] 36%|███▌      | 97/272 [00:26<00:48,  3.60it/s] 36%|███▌      | 98/272 [00:26<00:46,  3.74it/s] 36%|███▋      | 99/272 [00:26<00:45,  3.77it/s] 37%|███▋      | 100/272 [00:27<00:47,  3.65it/s] 37%|███▋      | 101/272 [00:27<00:46,  3.70it/s] 38%|███▊      | 102/272 [00:27<00:46,  3.65it/s] 38%|███▊      | 103/272 [00:27<00:45,  3.74it/s] 38%|███▊      | 104/272 [00:28<00:45,  3.69it/s] 39%|███▊      | 105/272 [00:28<00:45,  3.64it/s] 39%|███▉      | 106/272 [00:28<00:46,  3.59it/s] 39%|███▉      | 107/272 [00:28<00:44,  3.73it/s] 40%|███▉      | 108/272 [00:29<00:43,  3.75it/s] 40%|████      | 109/272 [00:29<00:44,  3.63it/s] 40%|████      | 110/272 [00:29<00:44,  3.64it/s] 41%|████      | 111/272 [00:30<00:42,  3.77it/s] 41%|████      | 112/272 [00:30<00:41,  3.85it/s] 42%|████▏     | 113/272 [00:30<00:40,  3.93it/s] 42%|████▏     | 114/272 [00:30<00:39,  3.98it/s] 42%|████▏     | 115/272 [00:30<00:39,  4.02it/s] 43%|████▎     | 116/272 [00:31<00:38,  4.04it/s] 43%|████▎     | 117/272 [00:31<00:38,  4.06it/s] 43%|████▎     | 118/272 [00:31<00:37,  4.07it/s] 44%|████▍     | 119/272 [00:31<00:37,  4.09it/s] 44%|████▍     | 120/272 [00:32<00:37,  4.10it/s] 44%|████▍     | 121/272 [00:32<00:37,  3.98it/s] 45%|████▍     | 122/272 [00:32<00:38,  3.90it/s] 45%|████▌     | 123/272 [00:33<00:38,  3.83it/s] 46%|████▌     | 124/272 [00:33<00:37,  3.91it/s] 46%|████▌     | 125/272 [00:33<00:37,  3.90it/s] 46%|████▋     | 126/272 [00:33<00:38,  3.83it/s] 47%|████▋     | 127/272 [00:34<00:38,  3.78it/s] 47%|████▋     | 128/272 [00:34<00:38,  3.72it/s] 47%|████▋     | 129/272 [00:34<00:37,  3.82it/s] 48%|████▊     | 130/272 [00:34<00:36,  3.85it/s] 48%|████▊     | 131/272 [00:35<00:37,  3.81it/s] 49%|████▊     | 132/272 [00:35<00:37,  3.77it/s] 49%|████▉     | 133/272 [00:35<00:37,  3.72it/s] 49%|████▉     | 134/272 [00:35<00:36,  3.80it/s] 50%|████▉     | 135/272 [00:36<00:35,  3.85it/s] 50%|█████     | 136/272 [00:36<00:35,  3.80it/s] 50%|█████     | 137/272 [00:36<00:36,  3.75it/s] 51%|█████     | 138/272 [00:36<00:36,  3.68it/s] 51%|█████     | 139/272 [00:37<00:35,  3.79it/s] 51%|█████▏    | 140/272 [00:37<00:34,  3.82it/s] 52%|█████▏    | 141/272 [00:37<00:34,  3.78it/s] 52%|█████▏    | 142/272 [00:38<00:34,  3.75it/s] 53%|█████▎    | 143/272 [00:38<00:35,  3.67it/s] 53%|█████▎    | 144/272 [00:38<00:33,  3.80it/s] 53%|█████▎    | 145/272 [00:38<00:32,  3.86it/s] 54%|█████▎    | 146/272 [00:39<00:33,  3.80it/s] 54%|█████▍    | 147/272 [00:39<00:33,  3.76it/s] 54%|█████▍    | 148/272 [00:39<00:33,  3.68it/s] 55%|█████▍    | 149/272 [00:39<00:32,  3.81it/s] 55%|█████▌    | 150/272 [00:40<00:31,  3.85it/s] 56%|█████▌    | 151/272 [00:40<00:31,  3.79it/s] 56%|█████▌    | 152/272 [00:40<00:31,  3.79it/s] 56%|█████▋    | 153/272 [00:40<00:31,  3.77it/s] 57%|█████▋    | 154/272 [00:41<00:30,  3.86it/s] 57%|█████▋    | 155/272 [00:41<00:29,  3.93it/s] 57%|█████▋    | 156/272 [00:41<00:29,  3.97it/s] 58%|█████▊    | 157/272 [00:41<00:28,  4.01it/s] 58%|█████▊    | 158/272 [00:42<00:28,  4.04it/s] 58%|█████▊    | 159/272 [00:42<00:27,  4.07it/s] 59%|█████▉    | 160/272 [00:42<00:27,  4.08it/s] 59%|█████▉    | 161/272 [00:42<00:27,  4.08it/s] 60%|█████▉    | 162/272 [00:43<00:26,  4.09it/s] 60%|█████▉    | 163/272 [00:43<00:26,  4.10it/s] 60%|██████    | 164/272 [00:43<00:26,  4.10it/s] 61%|██████    | 165/272 [00:43<00:26,  4.08it/s] 61%|██████    | 166/272 [00:44<00:26,  3.97it/s] 61%|██████▏   | 167/272 [00:44<00:26,  3.90it/s] 62%|██████▏   | 168/272 [00:44<00:26,  3.87it/s] 62%|██████▏   | 169/272 [00:44<00:26,  3.93it/s] 62%|██████▎   | 170/272 [00:45<00:26,  3.91it/s] 63%|██████▎   | 171/272 [00:45<00:26,  3.80it/s] 63%|██████▎   | 172/272 [00:45<00:26,  3.72it/s] 64%|██████▎   | 173/272 [00:46<00:26,  3.69it/s] 64%|██████▍   | 174/272 [00:46<00:26,  3.75it/s] 64%|██████▍   | 175/272 [00:46<00:25,  3.83it/s] 65%|██████▍   | 176/272 [00:46<00:25,  3.75it/s] 65%|██████▌   | 177/272 [00:47<00:25,  3.69it/s] 65%|██████▌   | 178/272 [00:47<00:25,  3.73it/s] 66%|██████▌   | 179/272 [00:47<00:25,  3.67it/s] 66%|██████▌   | 180/272 [00:47<00:24,  3.79it/s] 67%|██████▋   | 181/272 [00:48<00:23,  3.80it/s] 67%|██████▋   | 182/272 [00:48<00:24,  3.73it/s] 67%|██████▋   | 183/272 [00:48<00:24,  3.68it/s] 68%|██████▊   | 184/272 [00:48<00:24,  3.62it/s] 68%|██████▊   | 185/272 [00:49<00:23,  3.75it/s] 68%|██████▊   | 186/272 [00:49<00:22,  3.83it/s] 69%|██████▉   | 187/272 [00:49<00:22,  3.77it/s] 69%|██████▉   | 188/272 [00:50<00:22,  3.70it/s] 69%|██████▉   | 189/272 [00:50<00:22,  3.69it/s] 70%|██████▉   | 190/272 [00:50<00:21,  3.74it/s] 70%|███████   | 191/272 [00:50<00:21,  3.84it/s] 71%|███████   | 192/272 [00:51<00:21,  3.76it/s] 71%|███████   | 193/272 [00:51<00:21,  3.70it/s] 71%|███████▏  | 194/272 [00:51<00:21,  3.70it/s] 72%|███████▏  | 195/272 [00:51<00:21,  3.63it/s] 72%|███████▏  | 196/272 [00:52<00:20,  3.77it/s] 72%|███████▏  | 197/272 [00:52<00:19,  3.85it/s] 73%|███████▎  | 198/272 [00:52<00:19,  3.77it/s] 73%|███████▎  | 199/272 [00:52<00:19,  3.70it/s] 74%|███████▎  | 200/272 [00:53<00:19,  3.73it/s] 74%|███████▍  | 201/272 [00:53<00:19,  3.70it/s] 74%|███████▍  | 202/272 [00:53<00:18,  3.81it/s] 75%|███████▍  | 203/272 [00:54<00:18,  3.75it/s] 75%|███████▌  | 204/272 [00:54<00:18,  3.67it/s] 75%|███████▌  | 205/272 [00:54<00:18,  3.65it/s] 76%|███████▌  | 206/272 [00:54<00:17,  3.77it/s] 76%|███████▌  | 207/272 [00:55<00:16,  3.87it/s] 76%|███████▋  | 208/272 [00:55<00:16,  3.94it/s] 77%|███████▋  | 209/272 [00:55<00:15,  3.99it/s] 77%|███████▋  | 210/272 [00:55<00:15,  4.03it/s] 78%|███████▊  | 211/272 [00:56<00:15,  4.04it/s] 78%|███████▊  | 212/272 [00:56<00:14,  4.06it/s] 78%|███████▊  | 213/272 [00:56<00:14,  4.07it/s] 79%|███████▊  | 214/272 [00:56<00:14,  4.08it/s] 79%|███████▉  | 215/272 [00:57<00:13,  4.09it/s] 79%|███████▉  | 216/272 [00:57<00:13,  4.09it/s] 80%|███████▉  | 217/272 [00:57<00:13,  4.09it/s] 80%|████████  | 218/272 [00:57<00:13,  4.09it/s] 81%|████████  | 219/272 [00:58<00:12,  4.10it/s] 81%|████████  | 220/272 [00:58<00:12,  4.10it/s] 81%|████████▏ | 221/272 [00:58<00:12,  4.10it/s] 82%|████████▏ | 222/272 [00:58<00:12,  3.98it/s] 82%|████████▏ | 223/272 [00:59<00:12,  3.87it/s] 82%|████████▏ | 224/272 [00:59<00:12,  3.86it/s] 83%|████████▎ | 225/272 [00:59<00:12,  3.88it/s] 83%|████████▎ | 226/272 [00:59<00:12,  3.81it/s] 83%|████████▎ | 227/272 [01:00<00:12,  3.69it/s] 84%|████████▍ | 228/272 [01:00<00:11,  3.80it/s] 84%|████████▍ | 229/272 [01:00<00:11,  3.78it/s] 85%|████████▍ | 230/272 [01:00<00:11,  3.76it/s] 85%|████████▍ | 231/272 [01:01<00:11,  3.65it/s] 85%|████████▌ | 232/272 [01:01<00:10,  3.77it/s] 86%|████████▌ | 233/272 [01:01<00:10,  3.75it/s] 86%|████████▌ | 234/272 [01:01<00:10,  3.73it/s] 86%|████████▋ | 235/272 [01:02<00:10,  3.64it/s] 87%|████████▋ | 236/272 [01:02<00:09,  3.76it/s] 87%|████████▋ | 237/272 [01:02<00:09,  3.77it/s] 88%|████████▊ | 238/272 [01:03<00:09,  3.73it/s] 88%|████████▊ | 239/272 [01:03<00:09,  3.65it/s] 88%|████████▊ | 240/272 [01:03<00:08,  3.74it/s] 89%|████████▊ | 241/272 [01:03<00:08,  3.77it/s] 89%|████████▉ | 242/272 [01:04<00:08,  3.74it/s] 89%|████████▉ | 243/272 [01:04<00:07,  3.66it/s] 90%|████████▉ | 244/272 [01:04<00:07,  3.76it/s] 90%|█████████ | 245/272 [01:04<00:07,  3.79it/s] 90%|█████████ | 246/272 [01:05<00:06,  3.75it/s] 91%|█████████ | 247/272 [01:05<00:06,  3.65it/s] 91%|█████████ | 248/272 [01:05<00:06,  3.76it/s] 92%|█████████▏| 249/272 [01:05<00:06,  3.77it/s] 92%|█████████▏| 250/272 [01:06<00:05,  3.74it/s] 92%|█████████▏| 251/272 [01:06<00:05,  3.64it/s] 93%|█████████▎| 252/272 [01:06<00:05,  3.76it/s] 93%|█████████▎| 253/272 [01:07<00:05,  3.76it/s] 93%|█████████▎| 254/272 [01:07<00:04,  3.74it/s] 94%|█████████▍| 255/272 [01:07<00:04,  3.64it/s] 94%|█████████▍| 256/272 [01:07<00:04,  3.76it/s] 94%|█████████▍| 257/272 [01:08<00:03,  3.77it/s] 95%|█████████▍| 258/272 [01:08<00:03,  3.75it/s] 95%|█████████▌| 259/272 [01:08<00:03,  3.63it/s] 96%|█████████▌| 260/272 [01:08<00:03,  3.76it/s] 96%|█████████▌| 261/272 [01:09<00:02,  3.77it/s] 96%|█████████▋| 262/272 [01:09<00:02,  3.73it/s] 97%|█████████▋| 263/272 [01:09<00:02,  3.63it/s] 97%|█████████▋| 264/272 [01:10<00:02,  3.76it/s] 97%|█████████▋| 265/272 [01:10<00:01,  3.77it/s] 98%|█████████▊| 266/272 [01:10<00:01,  3.73it/s] 98%|█████████▊| 267/272 [01:10<00:01,  3.63it/s] 99%|█████████▊| 268/272 [01:11<00:01,  3.76it/s] 99%|█████████▉| 269/272 [01:11<00:00,  3.77it/s] 99%|█████████▉| 270/272 [01:11<00:00,  3.75it/s]100%|█████████▉| 271/272 [01:11<00:00,  3.64it/s]100%|██████████| 272/272 [01:12<00:00,  3.77it/s]accuracy:  0.8639705882352942
100%|██████████| 272/272 [01:16<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 149
Training dataset size: 96, validation dataset size: 149
Training dataset size: 96, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.37s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 03:35:40,709] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
training start
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 03:35:40,825] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 03:35:40,961] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:42,  2.82s/it] 12%|█▎        | 2/16 [00:05<00:35,  2.55s/it] 19%|█▉        | 3/16 [00:07<00:32,  2.48s/it] 25%|██▌       | 4/16 [00:09<00:28,  2.41s/it] 31%|███▏      | 5/16 [00:12<00:26,  2.40s/it] 38%|███▊      | 6/16 [00:15<00:26,  2.68s/it] 44%|████▍     | 7/16 [00:17<00:23,  2.60s/it] 50%|█████     | 8/16 [00:20<00:21,  2.68s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:23<00:18,  2.67s/it] 62%|██████▎   | 10/16 [00:26<00:16,  2.78s/it]                                               {'loss': 0.681, 'grad_norm': 11.084925651550293, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:26<00:16,  2.78s/it] 69%|██████▉   | 11/16 [00:28<00:13,  2.67s/it] 75%|███████▌  | 12/16 [00:31<00:10,  2.71s/it] 81%|████████▏ | 13/16 [00:34<00:08,  2.78s/it] 88%|████████▊ | 14/16 [00:37<00:05,  2.72s/it] 94%|█████████▍| 15/16 [00:39<00:02,  2.61s/it]100%|██████████| 16/16 [00:41<00:00,  2.57s/it]                                               {'train_runtime': 42.6367, 'train_samples_per_second': 4.503, 'train_steps_per_second': 0.375, 'train_loss': 0.7460997104644775, 'epoch': 2.0}
100%|██████████| 16/16 [00:42<00:00,  2.57s/it]100%|██████████| 16/16 [00:42<00:00,  2.65s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<01:02,  2.79it/s]  1%|          | 2/175 [00:00<00:53,  3.23it/s]  2%|▏         | 3/175 [00:00<00:51,  3.36it/s]  2%|▏         | 4/175 [00:01<00:46,  3.64it/s]  3%|▎         | 5/175 [00:01<00:45,  3.71it/s]  3%|▎         | 6/175 [00:01<00:46,  3.67it/s]  4%|▍         | 7/175 [00:01<00:44,  3.75it/s]  5%|▍         | 8/175 [00:02<00:45,  3.70it/s]  5%|▌         | 9/175 [00:02<00:43,  3.78it/s]  6%|▌         | 10/175 [00:02<00:44,  3.73it/s]  6%|▋         | 11/175 [00:03<00:43,  3.74it/s]  7%|▋         | 12/175 [00:03<00:44,  3.68it/s]  7%|▋         | 13/175 [00:03<00:42,  3.82it/s]  8%|▊         | 14/175 [00:03<00:42,  3.83it/s]  9%|▊         | 15/175 [00:04<00:42,  3.76it/s]  9%|▉         | 16/175 [00:04<00:41,  3.80it/s] 10%|▉         | 17/175 [00:04<00:42,  3.73it/s] 10%|█         | 18/175 [00:04<00:41,  3.79it/s] 11%|█         | 19/175 [00:05<00:41,  3.74it/s] 11%|█▏        | 20/175 [00:05<00:41,  3.77it/s] 12%|█▏        | 21/175 [00:05<00:41,  3.72it/s] 13%|█▎        | 22/175 [00:05<00:40,  3.81it/s] 13%|█▎        | 23/175 [00:06<00:40,  3.77it/s] 14%|█▎        | 24/175 [00:06<00:40,  3.74it/s] 14%|█▍        | 25/175 [00:06<00:40,  3.68it/s] 15%|█▍        | 26/175 [00:07<00:39,  3.80it/s] 15%|█▌        | 27/175 [00:07<00:39,  3.75it/s] 16%|█▌        | 28/175 [00:07<00:39,  3.71it/s] 17%|█▋        | 29/175 [00:07<00:40,  3.65it/s] 17%|█▋        | 30/175 [00:08<00:38,  3.78it/s] 18%|█▊        | 31/175 [00:08<00:37,  3.79it/s] 18%|█▊        | 32/175 [00:08<00:38,  3.72it/s] 19%|█▉        | 33/175 [00:08<00:37,  3.78it/s] 19%|█▉        | 34/175 [00:09<00:38,  3.71it/s] 20%|██        | 35/175 [00:09<00:37,  3.76it/s] 21%|██        | 36/175 [00:09<00:37,  3.71it/s] 21%|██        | 37/175 [00:09<00:36,  3.74it/s] 22%|██▏       | 38/175 [00:10<00:37,  3.68it/s] 22%|██▏       | 39/175 [00:10<00:36,  3.76it/s] 23%|██▎       | 40/175 [00:10<00:36,  3.72it/s] 23%|██▎       | 41/175 [00:11<00:35,  3.74it/s] 24%|██▍       | 42/175 [00:11<00:35,  3.70it/s] 25%|██▍       | 43/175 [00:11<00:34,  3.81it/s] 25%|██▌       | 44/175 [00:11<00:34,  3.76it/s] 26%|██▌       | 45/175 [00:12<00:34,  3.73it/s] 26%|██▋       | 46/175 [00:12<00:35,  3.67it/s] 27%|██▋       | 47/175 [00:12<00:33,  3.80it/s] 27%|██▋       | 48/175 [00:12<00:33,  3.78it/s] 28%|██▊       | 49/175 [00:13<00:33,  3.74it/s] 29%|██▊       | 50/175 [00:13<00:33,  3.78it/s] 29%|██▉       | 51/175 [00:13<00:33,  3.72it/s] 30%|██▉       | 52/175 [00:13<00:32,  3.78it/s] 30%|███       | 53/175 [00:14<00:32,  3.71it/s] 31%|███       | 54/175 [00:14<00:32,  3.74it/s] 31%|███▏      | 55/175 [00:14<00:32,  3.69it/s] 32%|███▏      | 56/175 [00:15<00:31,  3.79it/s] 33%|███▎      | 57/175 [00:15<00:31,  3.74it/s] 33%|███▎      | 58/175 [00:15<00:31,  3.71it/s] 34%|███▎      | 59/175 [00:15<00:31,  3.65it/s] 34%|███▍      | 60/175 [00:16<00:30,  3.78it/s] 35%|███▍      | 61/175 [00:16<00:30,  3.76it/s] 35%|███▌      | 62/175 [00:16<00:30,  3.72it/s] 36%|███▌      | 63/175 [00:16<00:29,  3.77it/s] 37%|███▋      | 64/175 [00:17<00:29,  3.71it/s] 37%|███▋      | 65/175 [00:17<00:29,  3.76it/s] 38%|███▊      | 66/175 [00:17<00:29,  3.70it/s] 38%|███▊      | 67/175 [00:17<00:28,  3.73it/s] 39%|███▉      | 68/175 [00:18<00:29,  3.69it/s] 39%|███▉      | 69/175 [00:18<00:28,  3.76it/s] 40%|████      | 70/175 [00:18<00:28,  3.69it/s] 41%|████      | 71/175 [00:19<00:27,  3.72it/s] 41%|████      | 72/175 [00:19<00:27,  3.75it/s] 42%|████▏     | 73/175 [00:19<00:26,  3.85it/s] 42%|████▏     | 74/175 [00:19<00:25,  3.93it/s] 43%|████▎     | 75/175 [00:20<00:25,  3.98it/s] 43%|████▎     | 76/175 [00:20<00:24,  4.01it/s] 44%|████▍     | 77/175 [00:20<00:24,  4.04it/s] 45%|████▍     | 78/175 [00:20<00:23,  4.06it/s] 45%|████▌     | 79/175 [00:21<00:23,  4.08it/s] 46%|████▌     | 80/175 [00:21<00:23,  4.09it/s] 46%|████▋     | 81/175 [00:21<00:22,  4.09it/s] 47%|████▋     | 82/175 [00:21<00:22,  4.08it/s] 47%|████▋     | 83/175 [00:21<00:22,  4.09it/s] 48%|████▊     | 84/175 [00:22<00:22,  4.11it/s] 49%|████▊     | 85/175 [00:22<00:21,  4.11it/s] 49%|████▉     | 86/175 [00:22<00:21,  4.10it/s] 50%|████▉     | 87/175 [00:22<00:21,  4.10it/s] 50%|█████     | 88/175 [00:23<00:21,  4.10it/s] 51%|█████     | 89/175 [00:23<00:20,  4.11it/s] 51%|█████▏    | 90/175 [00:23<00:20,  4.11it/s] 52%|█████▏    | 91/175 [00:23<00:20,  4.11it/s] 53%|█████▎    | 92/175 [00:24<00:20,  4.03it/s] 53%|█████▎    | 93/175 [00:24<00:20,  3.91it/s] 54%|█████▎    | 94/175 [00:24<00:20,  3.86it/s] 54%|█████▍    | 95/175 [00:24<00:20,  3.92it/s] 55%|█████▍    | 96/175 [00:25<00:20,  3.83it/s] 55%|█████▌    | 97/175 [00:25<00:20,  3.76it/s] 56%|█████▌    | 98/175 [00:25<00:20,  3.70it/s] 57%|█████▋    | 99/175 [00:26<00:19,  3.81it/s] 57%|█████▋    | 100/175 [00:26<00:20,  3.75it/s] 58%|█████▊    | 101/175 [00:26<00:19,  3.72it/s] 58%|█████▊    | 102/175 [00:26<00:19,  3.67it/s] 59%|█████▉    | 103/175 [00:27<00:19,  3.79it/s] 59%|█████▉    | 104/175 [00:27<00:18,  3.77it/s] 60%|██████    | 105/175 [00:27<00:18,  3.72it/s] 61%|██████    | 106/175 [00:27<00:18,  3.65it/s] 61%|██████    | 107/175 [00:28<00:17,  3.78it/s] 62%|██████▏   | 108/175 [00:28<00:17,  3.77it/s] 62%|██████▏   | 109/175 [00:28<00:17,  3.74it/s] 63%|██████▎   | 110/175 [00:29<00:17,  3.77it/s] 63%|██████▎   | 111/175 [00:29<00:17,  3.72it/s] 64%|██████▍   | 112/175 [00:29<00:16,  3.77it/s] 65%|██████▍   | 113/175 [00:29<00:16,  3.73it/s] 65%|██████▌   | 114/175 [00:30<00:16,  3.76it/s] 66%|██████▌   | 115/175 [00:30<00:16,  3.68it/s] 66%|██████▋   | 116/175 [00:30<00:15,  3.76it/s] 67%|██████▋   | 117/175 [00:30<00:15,  3.72it/s] 67%|██████▋   | 118/175 [00:31<00:15,  3.76it/s] 68%|██████▊   | 119/175 [00:31<00:15,  3.70it/s] 69%|██████▊   | 120/175 [00:31<00:14,  3.75it/s] 69%|██████▉   | 121/175 [00:31<00:14,  3.72it/s] 70%|██████▉   | 122/175 [00:32<00:14,  3.75it/s] 70%|███████   | 123/175 [00:32<00:14,  3.69it/s] 71%|███████   | 124/175 [00:32<00:13,  3.77it/s] 71%|███████▏  | 125/175 [00:33<00:13,  3.72it/s] 72%|███████▏  | 126/175 [00:33<00:13,  3.74it/s] 73%|███████▎  | 127/175 [00:33<00:13,  3.68it/s] 73%|███████▎  | 128/175 [00:33<00:12,  3.76it/s] 74%|███████▎  | 129/175 [00:34<00:12,  3.72it/s] 74%|███████▍  | 130/175 [00:34<00:11,  3.76it/s] 75%|███████▍  | 131/175 [00:34<00:11,  3.70it/s] 75%|███████▌  | 132/175 [00:34<00:11,  3.80it/s] 76%|███████▌  | 133/175 [00:35<00:11,  3.75it/s] 77%|███████▋  | 134/175 [00:35<00:11,  3.72it/s] 77%|███████▋  | 135/175 [00:35<00:10,  3.67it/s] 78%|███████▊  | 136/175 [00:35<00:10,  3.79it/s] 78%|███████▊  | 137/175 [00:36<00:10,  3.76it/s] 79%|███████▉  | 138/175 [00:36<00:09,  3.71it/s] 79%|███████▉  | 139/175 [00:36<00:09,  3.66it/s] 80%|████████  | 140/175 [00:37<00:09,  3.77it/s] 81%|████████  | 141/175 [00:37<00:09,  3.75it/s] 81%|████████  | 142/175 [00:37<00:08,  3.73it/s] 82%|████████▏ | 143/175 [00:37<00:08,  3.74it/s] 82%|████████▏ | 144/175 [00:38<00:08,  3.72it/s] 83%|████████▎ | 145/175 [00:38<00:07,  3.76it/s] 83%|████████▎ | 146/175 [00:38<00:07,  3.73it/s] 84%|████████▍ | 147/175 [00:38<00:07,  3.75it/s] 85%|████████▍ | 148/175 [00:39<00:07,  3.69it/s] 85%|████████▌ | 149/175 [00:39<00:06,  3.74it/s] 86%|████████▌ | 150/175 [00:39<00:06,  3.70it/s] 86%|████████▋ | 151/175 [00:40<00:06,  3.72it/s] 87%|████████▋ | 152/175 [00:40<00:06,  3.68it/s] 87%|████████▋ | 153/175 [00:40<00:05,  3.74it/s] 88%|████████▊ | 154/175 [00:40<00:05,  3.71it/s] 89%|████████▊ | 155/175 [00:41<00:05,  3.72it/s] 89%|████████▉ | 156/175 [00:41<00:05,  3.68it/s] 90%|████████▉ | 157/175 [00:41<00:04,  3.74it/s] 90%|█████████ | 158/175 [00:41<00:04,  3.70it/s] 91%|█████████ | 159/175 [00:42<00:04,  3.73it/s] 91%|█████████▏| 160/175 [00:42<00:04,  3.68it/s] 92%|█████████▏| 161/175 [00:42<00:03,  3.72it/s] 93%|█████████▎| 162/175 [00:42<00:03,  3.69it/s] 93%|█████████▎| 163/175 [00:43<00:03,  3.69it/s] 94%|█████████▎| 164/175 [00:43<00:02,  3.67it/s] 94%|█████████▍| 165/175 [00:43<00:02,  3.73it/s] 95%|█████████▍| 166/175 [00:44<00:02,  3.69it/s] 95%|█████████▌| 167/175 [00:44<00:02,  3.70it/s] 96%|█████████▌| 168/175 [00:44<00:01,  3.68it/s] 97%|█████████▋| 169/175 [00:44<00:01,  3.73it/s] 97%|█████████▋| 170/175 [00:45<00:01,  3.70it/s] 98%|█████████▊| 171/175 [00:45<00:01,  3.69it/s] 98%|█████████▊| 172/175 [00:45<00:00,  3.69it/s] 99%|█████████▉| 173/175 [00:45<00:00,  3.74it/s] 99%|█████████▉| 174/175 [00:46<00:00,  3.71it/s]100%|██████████| 175/175 [00:46<00:00,  3.72it/s]accuracy:  0.7885714285714286
100%|██████████| 175/175 [00:49<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 96, validation dataset size: 119
Training dataset size: 96, validation dataset size: 119
Training dataset size: 96, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 03:37:41,324] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 03:37:41,692] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:37:41,984] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/16 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  6%|▋         | 1/16 [00:02<00:40,  2.72s/it] 12%|█▎        | 2/16 [00:04<00:32,  2.29s/it] 19%|█▉        | 3/16 [00:07<00:32,  2.50s/it] 25%|██▌       | 4/16 [00:09<00:29,  2.49s/it] 31%|███▏      | 5/16 [00:12<00:27,  2.46s/it] 38%|███▊      | 6/16 [00:15<00:26,  2.60s/it] 44%|████▍     | 7/16 [00:17<00:22,  2.52s/it] 50%|█████     | 8/16 [00:19<00:18,  2.35s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 56%|█████▋    | 9/16 [00:22<00:17,  2.48s/it] 62%|██████▎   | 10/16 [00:25<00:16,  2.71s/it]                                               {'loss': 0.9729, 'grad_norm': 9.028732299804688, 'learning_rate': 3.4549150281252635e-06, 'epoch': 1.25}
 62%|██████▎   | 10/16 [00:25<00:16,  2.71s/it] 69%|██████▉   | 11/16 [00:27<00:12,  2.55s/it] 75%|███████▌  | 12/16 [00:30<00:10,  2.56s/it] 81%|████████▏ | 13/16 [00:32<00:07,  2.47s/it] 88%|████████▊ | 14/16 [00:35<00:05,  2.51s/it] 94%|█████████▍| 15/16 [00:37<00:02,  2.45s/it]100%|██████████| 16/16 [00:39<00:00,  2.43s/it]                                               {'train_runtime': 40.4924, 'train_samples_per_second': 4.742, 'train_steps_per_second': 0.395, 'train_loss': 0.9612656235694885, 'epoch': 2.0}
100%|██████████| 16/16 [00:40<00:00,  2.43s/it]100%|██████████| 16/16 [00:40<00:00,  2.52s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:59,  2.55it/s]  1%|▏         | 2/153 [00:00<00:46,  3.23it/s]  2%|▏         | 3/153 [00:00<00:43,  3.42it/s]  3%|▎         | 4/153 [00:01<00:41,  3.60it/s]  3%|▎         | 5/153 [00:01<00:41,  3.59it/s]  4%|▍         | 6/153 [00:01<00:39,  3.68it/s]  5%|▍         | 7/153 [00:01<00:39,  3.68it/s]  5%|▌         | 8/153 [00:02<00:39,  3.64it/s]  6%|▌         | 9/153 [00:02<00:37,  3.79it/s]  7%|▋         | 10/153 [00:02<00:37,  3.77it/s]  7%|▋         | 11/153 [00:03<00:37,  3.75it/s]  8%|▊         | 12/153 [00:03<00:37,  3.71it/s]  8%|▊         | 13/153 [00:03<00:36,  3.80it/s]  9%|▉         | 14/153 [00:03<00:36,  3.76it/s] 10%|▉         | 15/153 [00:04<00:36,  3.81it/s] 10%|█         | 16/153 [00:04<00:36,  3.73it/s] 11%|█         | 17/153 [00:04<00:36,  3.77it/s] 12%|█▏        | 18/153 [00:04<00:36,  3.74it/s] 12%|█▏        | 19/153 [00:05<00:35,  3.80it/s] 13%|█▎        | 20/153 [00:05<00:35,  3.73it/s] 14%|█▎        | 21/153 [00:05<00:35,  3.77it/s] 14%|█▍        | 22/153 [00:05<00:35,  3.72it/s] 15%|█▌        | 23/153 [00:06<00:35,  3.66it/s] 16%|█▌        | 24/153 [00:06<00:34,  3.78it/s] 16%|█▋        | 25/153 [00:06<00:34,  3.74it/s] 17%|█▋        | 26/153 [00:07<00:33,  3.74it/s] 18%|█▊        | 27/153 [00:07<00:34,  3.69it/s] 18%|█▊        | 28/153 [00:07<00:33,  3.78it/s] 19%|█▉        | 29/153 [00:07<00:33,  3.74it/s] 20%|█▉        | 30/153 [00:08<00:32,  3.74it/s] 20%|██        | 31/153 [00:08<00:32,  3.75it/s] 21%|██        | 32/153 [00:08<00:32,  3.75it/s] 22%|██▏       | 33/153 [00:08<00:32,  3.74it/s] 22%|██▏       | 34/153 [00:09<00:32,  3.67it/s] 23%|██▎       | 35/153 [00:09<00:31,  3.80it/s] 24%|██▎       | 36/153 [00:09<00:30,  3.78it/s] 24%|██▍       | 37/153 [00:09<00:30,  3.77it/s] 25%|██▍       | 38/153 [00:10<00:30,  3.72it/s] 25%|██▌       | 39/153 [00:10<00:30,  3.77it/s] 26%|██▌       | 40/153 [00:10<00:30,  3.74it/s] 27%|██▋       | 41/153 [00:11<00:30,  3.70it/s] 27%|██▋       | 42/153 [00:11<00:29,  3.75it/s] 28%|██▊       | 43/153 [00:11<00:29,  3.77it/s] 29%|██▉       | 44/153 [00:11<00:29,  3.73it/s] 29%|██▉       | 45/153 [00:12<00:29,  3.67it/s] 30%|███       | 46/153 [00:12<00:28,  3.79it/s] 31%|███       | 47/153 [00:12<00:28,  3.75it/s] 31%|███▏      | 48/153 [00:12<00:28,  3.74it/s] 32%|███▏      | 49/153 [00:13<00:28,  3.69it/s] 33%|███▎      | 50/153 [00:13<00:27,  3.77it/s] 33%|███▎      | 51/153 [00:13<00:27,  3.71it/s] 34%|███▍      | 52/153 [00:14<00:27,  3.65it/s] 35%|███▍      | 53/153 [00:14<00:26,  3.77it/s] 35%|███▌      | 54/153 [00:14<00:26,  3.73it/s] 36%|███▌      | 55/153 [00:14<00:26,  3.75it/s] 37%|███▋      | 56/153 [00:15<00:26,  3.70it/s] 37%|███▋      | 57/153 [00:15<00:25,  3.77it/s] 38%|███▊      | 58/153 [00:15<00:25,  3.73it/s] 39%|███▊      | 59/153 [00:15<00:25,  3.68it/s] 39%|███▉      | 60/153 [00:16<00:24,  3.76it/s] 40%|███▉      | 61/153 [00:16<00:24,  3.74it/s] 41%|████      | 62/153 [00:16<00:24,  3.72it/s] 41%|████      | 63/153 [00:16<00:24,  3.68it/s] 42%|████▏     | 64/153 [00:17<00:23,  3.75it/s] 42%|████▏     | 65/153 [00:17<00:23,  3.72it/s] 43%|████▎     | 66/153 [00:17<00:23,  3.71it/s] 44%|████▍     | 67/153 [00:18<00:22,  3.82it/s] 44%|████▍     | 68/153 [00:18<00:21,  3.89it/s] 45%|████▌     | 69/153 [00:18<00:21,  3.94it/s] 46%|████▌     | 70/153 [00:18<00:20,  3.99it/s] 46%|████▋     | 71/153 [00:18<00:20,  4.03it/s] 47%|████▋     | 72/153 [00:19<00:19,  4.05it/s] 48%|████▊     | 73/153 [00:19<00:19,  4.06it/s] 48%|████▊     | 74/153 [00:19<00:19,  4.07it/s] 49%|████▉     | 75/153 [00:19<00:19,  4.08it/s] 50%|████▉     | 76/153 [00:20<00:18,  4.09it/s] 50%|█████     | 77/153 [00:20<00:18,  4.09it/s] 51%|█████     | 78/153 [00:20<00:18,  4.08it/s] 52%|█████▏    | 79/153 [00:20<00:18,  4.09it/s] 52%|█████▏    | 80/153 [00:21<00:17,  4.10it/s] 53%|█████▎    | 81/153 [00:21<00:17,  4.10it/s] 54%|█████▎    | 82/153 [00:21<00:17,  4.10it/s] 54%|█████▍    | 83/153 [00:21<00:17,  4.09it/s] 55%|█████▍    | 84/153 [00:22<00:16,  4.09it/s] 56%|█████▌    | 85/153 [00:22<00:16,  4.10it/s] 56%|█████▌    | 86/153 [00:22<00:16,  4.10it/s] 57%|█████▋    | 87/153 [00:22<00:16,  4.10it/s] 58%|█████▊    | 88/153 [00:23<00:15,  4.09it/s] 58%|█████▊    | 89/153 [00:23<00:15,  4.10it/s] 59%|█████▉    | 90/153 [00:23<00:15,  4.10it/s] 59%|█████▉    | 91/153 [00:23<00:15,  4.09it/s] 60%|██████    | 92/153 [00:24<00:14,  4.09it/s] 61%|██████    | 93/153 [00:24<00:14,  4.09it/s] 61%|██████▏   | 94/153 [00:24<00:14,  4.09it/s] 62%|██████▏   | 95/153 [00:24<00:14,  4.09it/s] 63%|██████▎   | 96/153 [00:25<00:13,  4.08it/s] 63%|██████▎   | 97/153 [00:25<00:13,  4.08it/s] 64%|██████▍   | 98/153 [00:25<00:13,  4.09it/s] 65%|██████▍   | 99/153 [00:25<00:13,  4.09it/s] 65%|██████▌   | 100/153 [00:26<00:12,  4.09it/s] 66%|██████▌   | 101/153 [00:26<00:12,  4.09it/s] 67%|██████▋   | 102/153 [00:26<00:12,  4.09it/s] 67%|██████▋   | 103/153 [00:26<00:12,  4.10it/s] 68%|██████▊   | 104/153 [00:27<00:11,  4.09it/s] 69%|██████▊   | 105/153 [00:27<00:11,  4.08it/s] 69%|██████▉   | 106/153 [00:27<00:11,  4.09it/s] 70%|██████▉   | 107/153 [00:27<00:11,  4.10it/s] 71%|███████   | 108/153 [00:28<00:10,  4.10it/s] 71%|███████   | 109/153 [00:28<00:10,  4.09it/s] 72%|███████▏  | 110/153 [00:28<00:10,  4.09it/s] 73%|███████▎  | 111/153 [00:28<00:10,  4.07it/s] 73%|███████▎  | 112/153 [00:29<00:10,  3.95it/s] 74%|███████▍  | 113/153 [00:29<00:10,  3.87it/s] 75%|███████▍  | 114/153 [00:29<00:10,  3.84it/s] 75%|███████▌  | 115/153 [00:29<00:09,  3.84it/s] 76%|███████▌  | 116/153 [00:30<00:09,  3.72it/s] 76%|███████▋  | 117/153 [00:30<00:09,  3.71it/s] 77%|███████▋  | 118/153 [00:30<00:09,  3.72it/s] 78%|███████▊  | 119/153 [00:30<00:09,  3.75it/s] 78%|███████▊  | 120/153 [00:31<00:09,  3.67it/s] 79%|███████▉  | 121/153 [00:31<00:08,  3.61it/s] 80%|███████▉  | 122/153 [00:31<00:08,  3.74it/s] 80%|████████  | 123/153 [00:32<00:08,  3.65it/s] 81%|████████  | 124/153 [00:32<00:07,  3.65it/s] 82%|████████▏ | 125/153 [00:32<00:07,  3.73it/s] 82%|████████▏ | 126/153 [00:32<00:07,  3.84it/s] 83%|████████▎ | 127/153 [00:33<00:06,  3.91it/s] 84%|████████▎ | 128/153 [00:33<00:06,  3.95it/s] 84%|████████▍ | 129/153 [00:33<00:06,  3.99it/s] 85%|████████▍ | 130/153 [00:33<00:05,  4.02it/s] 86%|████████▌ | 131/153 [00:34<00:05,  4.03it/s] 86%|████████▋ | 132/153 [00:34<00:05,  4.04it/s] 87%|████████▋ | 133/153 [00:34<00:04,  4.05it/s] 88%|████████▊ | 134/153 [00:34<00:04,  4.07it/s] 88%|████████▊ | 135/153 [00:35<00:04,  4.06it/s] 89%|████████▉ | 136/153 [00:35<00:04,  4.07it/s] 90%|████████▉ | 137/153 [00:35<00:03,  4.07it/s] 90%|█████████ | 138/153 [00:35<00:03,  4.08it/s] 91%|█████████ | 139/153 [00:35<00:03,  4.07it/s] 92%|█████████▏| 140/153 [00:36<00:03,  4.07it/s] 92%|█████████▏| 141/153 [00:36<00:02,  4.08it/s] 93%|█████████▎| 142/153 [00:36<00:02,  4.07it/s] 93%|█████████▎| 143/153 [00:36<00:02,  4.06it/s] 94%|█████████▍| 144/153 [00:37<00:02,  4.07it/s] 95%|█████████▍| 145/153 [00:37<00:01,  4.08it/s] 95%|█████████▌| 146/153 [00:37<00:01,  4.07it/s] 96%|█████████▌| 147/153 [00:37<00:01,  4.07it/s] 97%|█████████▋| 148/153 [00:38<00:01,  4.08it/s] 97%|█████████▋| 149/153 [00:38<00:00,  4.08it/s] 98%|█████████▊| 150/153 [00:38<00:00,  4.07it/s] 99%|█████████▊| 151/153 [00:38<00:00,  4.07it/s] 99%|█████████▉| 152/153 [00:39<00:00,  4.08it/s]100%|██████████| 153/153 [00:39<00:00,  4.08it/s]accuracy:  0.6470588235294118
100%|██████████| 153/153 [00:41<00:00,  3.65it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 152
Training dataset size: 144, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:39:32,147] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:39:32,742] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:39:35,433] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:57,  2.50s/it]  8%|▊         | 2/24 [00:04<00:47,  2.14s/it] 12%|█▎        | 3/24 [00:06<00:48,  2.30s/it] 17%|█▋        | 4/24 [00:08<00:41,  2.06s/it] 21%|██        | 5/24 [00:11<00:43,  2.30s/it] 25%|██▌       | 6/24 [00:13<00:43,  2.40s/it] 29%|██▉       | 7/24 [00:15<00:38,  2.24s/it] 33%|███▎      | 8/24 [00:17<00:35,  2.21s/it] 38%|███▊      | 9/24 [00:20<00:33,  2.24s/it] 42%|████▏     | 10/24 [00:22<00:31,  2.27s/it]                                               {'loss': 1.012, 'grad_norm': 17.365617752075195, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:22<00:31,  2.27s/it] 46%|████▌     | 11/24 [00:24<00:29,  2.23s/it] 50%|█████     | 12/24 [00:27<00:28,  2.35s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:29<00:26,  2.43s/it] 58%|█████▊    | 14/24 [00:32<00:25,  2.52s/it] 62%|██████▎   | 15/24 [00:34<00:21,  2.44s/it] 67%|██████▋   | 16/24 [00:37<00:19,  2.43s/it] 71%|███████   | 17/24 [00:39<00:16,  2.30s/it] 75%|███████▌  | 18/24 [00:41<00:13,  2.23s/it] 79%|███████▉  | 19/24 [00:43<00:10,  2.15s/it] 83%|████████▎ | 20/24 [00:44<00:07,  1.98s/it]                                               {'loss': 0.8947, 'grad_norm': 5.050742149353027, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:44<00:07,  1.98s/it] 88%|████████▊ | 21/24 [00:46<00:05,  1.88s/it] 92%|█████████▏| 22/24 [00:49<00:04,  2.11s/it] 96%|█████████▌| 23/24 [00:51<00:02,  2.20s/it]100%|██████████| 24/24 [00:54<00:00,  2.30s/it]                                               {'train_runtime': 54.8548, 'train_samples_per_second': 5.25, 'train_steps_per_second': 0.438, 'train_loss': 0.9367755651473999, 'epoch': 2.0}
100%|██████████| 24/24 [00:54<00:00,  2.30s/it]100%|██████████| 24/24 [00:54<00:00,  2.28s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:10,  2.99it/s]  1%|          | 2/212 [00:00<01:02,  3.36it/s]  1%|▏         | 3/212 [00:00<01:00,  3.47it/s]  2%|▏         | 4/212 [00:01<00:55,  3.72it/s]  2%|▏         | 5/212 [00:01<00:55,  3.74it/s]  3%|▎         | 6/212 [00:01<00:55,  3.72it/s]  3%|▎         | 7/212 [00:01<00:55,  3.67it/s]  4%|▍         | 8/212 [00:02<00:53,  3.81it/s]  4%|▍         | 9/212 [00:02<00:53,  3.82it/s]  5%|▍         | 10/212 [00:02<00:53,  3.77it/s]  5%|▌         | 11/212 [00:02<00:52,  3.82it/s]  6%|▌         | 12/212 [00:03<00:53,  3.75it/s]  6%|▌         | 13/212 [00:03<00:52,  3.82it/s]  7%|▋         | 14/212 [00:03<00:52,  3.77it/s]  7%|▋         | 15/212 [00:04<00:51,  3.81it/s]  8%|▊         | 16/212 [00:04<00:52,  3.75it/s]  8%|▊         | 17/212 [00:04<00:51,  3.82it/s]  8%|▊         | 18/212 [00:04<00:51,  3.77it/s]  9%|▉         | 19/212 [00:05<00:50,  3.81it/s]  9%|▉         | 20/212 [00:05<00:51,  3.74it/s] 10%|▉         | 21/212 [00:05<00:50,  3.82it/s] 10%|█         | 22/212 [00:05<00:50,  3.78it/s] 11%|█         | 23/212 [00:06<00:49,  3.81it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.74it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.81it/s] 12%|█▏        | 26/212 [00:06<00:49,  3.77it/s] 13%|█▎        | 27/212 [00:07<00:48,  3.80it/s] 13%|█▎        | 28/212 [00:07<00:49,  3.73it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.79it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.75it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.80it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.73it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.79it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.76it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.81it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.74it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.80it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.77it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.81it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.73it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.77it/s] 20%|█▉        | 42/212 [00:11<00:45,  3.74it/s] 20%|██        | 43/212 [00:11<00:44,  3.76it/s] 21%|██        | 44/212 [00:11<00:44,  3.74it/s] 21%|██        | 45/212 [00:11<00:44,  3.77it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.74it/s] 22%|██▏       | 47/212 [00:12<00:44,  3.74it/s] 23%|██▎       | 48/212 [00:12<00:44,  3.73it/s] 23%|██▎       | 49/212 [00:13<00:43,  3.78it/s] 24%|██▎       | 50/212 [00:13<00:43,  3.75it/s] 24%|██▍       | 51/212 [00:13<00:42,  3.78it/s] 25%|██▍       | 52/212 [00:13<00:42,  3.73it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.79it/s] 25%|██▌       | 54/212 [00:14<00:42,  3.74it/s] 26%|██▌       | 55/212 [00:14<00:41,  3.78it/s] 26%|██▋       | 56/212 [00:14<00:41,  3.74it/s] 27%|██▋       | 57/212 [00:15<00:40,  3.79it/s] 27%|██▋       | 58/212 [00:15<00:41,  3.75it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.76it/s] 28%|██▊       | 60/212 [00:15<00:40,  3.75it/s] 29%|██▉       | 61/212 [00:16<00:39,  3.80it/s] 29%|██▉       | 62/212 [00:16<00:39,  3.75it/s] 30%|██▉       | 63/212 [00:16<00:39,  3.76it/s] 30%|███       | 64/212 [00:17<00:39,  3.73it/s] 31%|███       | 65/212 [00:17<00:38,  3.77it/s] 31%|███       | 66/212 [00:17<00:39,  3.74it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.73it/s] 32%|███▏      | 68/212 [00:18<00:38,  3.73it/s] 33%|███▎      | 69/212 [00:18<00:37,  3.77it/s] 33%|███▎      | 70/212 [00:18<00:38,  3.74it/s] 33%|███▎      | 71/212 [00:18<00:37,  3.74it/s] 34%|███▍      | 72/212 [00:19<00:37,  3.73it/s] 34%|███▍      | 73/212 [00:19<00:36,  3.77it/s] 35%|███▍      | 74/212 [00:19<00:36,  3.75it/s] 35%|███▌      | 75/212 [00:19<00:36,  3.78it/s] 36%|███▌      | 76/212 [00:20<00:36,  3.70it/s] 36%|███▋      | 77/212 [00:20<00:35,  3.78it/s] 37%|███▋      | 78/212 [00:20<00:35,  3.74it/s] 37%|███▋      | 79/212 [00:21<00:35,  3.75it/s] 38%|███▊      | 80/212 [00:21<00:35,  3.73it/s] 38%|███▊      | 81/212 [00:21<00:34,  3.79it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.74it/s] 39%|███▉      | 83/212 [00:22<00:34,  3.75it/s] 40%|███▉      | 84/212 [00:22<00:34,  3.72it/s] 40%|████      | 85/212 [00:22<00:33,  3.76it/s] 41%|████      | 86/212 [00:22<00:33,  3.73it/s] 41%|████      | 87/212 [00:23<00:33,  3.73it/s] 42%|████▏     | 88/212 [00:23<00:33,  3.73it/s] 42%|████▏     | 89/212 [00:23<00:32,  3.77it/s] 42%|████▏     | 90/212 [00:23<00:32,  3.73it/s] 43%|████▎     | 91/212 [00:24<00:32,  3.71it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.72it/s] 44%|████▍     | 93/212 [00:24<00:31,  3.76it/s] 44%|████▍     | 94/212 [00:25<00:31,  3.72it/s] 45%|████▍     | 95/212 [00:25<00:31,  3.77it/s] 45%|████▌     | 96/212 [00:25<00:31,  3.72it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.77it/s] 46%|████▌     | 98/212 [00:26<00:30,  3.73it/s] 47%|████▋     | 99/212 [00:26<00:30,  3.73it/s] 47%|████▋     | 100/212 [00:26<00:30,  3.71it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.76it/s] 48%|████▊     | 102/212 [00:27<00:29,  3.72it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.72it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.73it/s] 50%|████▉     | 105/212 [00:27<00:28,  3.77it/s] 50%|█████     | 106/212 [00:28<00:28,  3.74it/s] 50%|█████     | 107/212 [00:28<00:28,  3.72it/s] 51%|█████     | 108/212 [00:28<00:27,  3.73it/s] 51%|█████▏    | 109/212 [00:29<00:27,  3.76it/s] 52%|█████▏    | 110/212 [00:29<00:27,  3.71it/s] 52%|█████▏    | 111/212 [00:29<00:27,  3.68it/s] 53%|█████▎    | 112/212 [00:29<00:26,  3.74it/s] 53%|█████▎    | 113/212 [00:30<00:26,  3.77it/s] 54%|█████▍    | 114/212 [00:30<00:26,  3.73it/s] 54%|█████▍    | 115/212 [00:30<00:26,  3.69it/s] 55%|█████▍    | 116/212 [00:30<00:25,  3.74it/s] 55%|█████▌    | 117/212 [00:31<00:25,  3.77it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.73it/s] 56%|█████▌    | 119/212 [00:31<00:25,  3.68it/s] 57%|█████▋    | 120/212 [00:32<00:24,  3.72it/s] 57%|█████▋    | 121/212 [00:32<00:24,  3.76it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.71it/s] 58%|█████▊    | 123/212 [00:32<00:24,  3.67it/s] 58%|█████▊    | 124/212 [00:33<00:23,  3.72it/s] 59%|█████▉    | 125/212 [00:33<00:23,  3.75it/s] 59%|█████▉    | 126/212 [00:33<00:23,  3.71it/s] 60%|█████▉    | 127/212 [00:33<00:23,  3.68it/s] 60%|██████    | 128/212 [00:34<00:22,  3.72it/s] 61%|██████    | 129/212 [00:34<00:22,  3.75it/s] 61%|██████▏   | 130/212 [00:34<00:22,  3.71it/s] 62%|██████▏   | 131/212 [00:35<00:22,  3.67it/s] 62%|██████▏   | 132/212 [00:35<00:21,  3.73it/s] 63%|██████▎   | 133/212 [00:35<00:21,  3.76it/s] 63%|██████▎   | 134/212 [00:35<00:20,  3.72it/s] 64%|██████▎   | 135/212 [00:36<00:20,  3.68it/s] 64%|██████▍   | 136/212 [00:36<00:20,  3.74it/s] 65%|██████▍   | 137/212 [00:36<00:19,  3.76it/s] 65%|██████▌   | 138/212 [00:36<00:19,  3.72it/s] 66%|██████▌   | 139/212 [00:37<00:19,  3.67it/s] 66%|██████▌   | 140/212 [00:37<00:19,  3.73it/s] 67%|██████▋   | 141/212 [00:37<00:18,  3.76it/s] 67%|██████▋   | 142/212 [00:37<00:18,  3.72it/s] 67%|██████▋   | 143/212 [00:38<00:18,  3.66it/s] 68%|██████▊   | 144/212 [00:38<00:18,  3.72it/s] 68%|██████▊   | 145/212 [00:38<00:17,  3.76it/s] 69%|██████▉   | 146/212 [00:39<00:17,  3.72it/s] 69%|██████▉   | 147/212 [00:39<00:17,  3.66it/s] 70%|██████▉   | 148/212 [00:39<00:17,  3.73it/s] 70%|███████   | 149/212 [00:39<00:16,  3.74it/s] 71%|███████   | 150/212 [00:40<00:16,  3.71it/s] 71%|███████   | 151/212 [00:40<00:16,  3.65it/s] 72%|███████▏  | 152/212 [00:40<00:16,  3.71it/s] 72%|███████▏  | 153/212 [00:40<00:15,  3.74it/s] 73%|███████▎  | 154/212 [00:41<00:15,  3.70it/s] 73%|███████▎  | 155/212 [00:41<00:15,  3.64it/s] 74%|███████▎  | 156/212 [00:41<00:15,  3.72it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.74it/s] 75%|███████▍  | 158/212 [00:42<00:14,  3.71it/s] 75%|███████▌  | 159/212 [00:42<00:14,  3.65it/s] 75%|███████▌  | 160/212 [00:42<00:13,  3.72it/s] 76%|███████▌  | 161/212 [00:43<00:13,  3.75it/s] 76%|███████▋  | 162/212 [00:43<00:13,  3.71it/s] 77%|███████▋  | 163/212 [00:43<00:13,  3.65it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.72it/s] 78%|███████▊  | 165/212 [00:44<00:12,  3.75it/s] 78%|███████▊  | 166/212 [00:44<00:12,  3.71it/s] 79%|███████▉  | 167/212 [00:44<00:12,  3.65it/s] 79%|███████▉  | 168/212 [00:44<00:11,  3.72it/s] 80%|███████▉  | 169/212 [00:45<00:11,  3.74it/s] 80%|████████  | 170/212 [00:45<00:11,  3.70it/s] 81%|████████  | 171/212 [00:45<00:11,  3.65it/s] 81%|████████  | 172/212 [00:46<00:10,  3.71it/s] 82%|████████▏ | 173/212 [00:46<00:10,  3.75it/s] 82%|████████▏ | 174/212 [00:46<00:10,  3.71it/s] 83%|████████▎ | 175/212 [00:46<00:10,  3.65it/s] 83%|████████▎ | 176/212 [00:47<00:09,  3.72it/s] 83%|████████▎ | 177/212 [00:47<00:09,  3.74it/s] 84%|████████▍ | 178/212 [00:47<00:09,  3.70it/s] 84%|████████▍ | 179/212 [00:47<00:08,  3.71it/s] 85%|████████▍ | 180/212 [00:48<00:08,  3.80it/s] 85%|████████▌ | 181/212 [00:48<00:07,  3.88it/s] 86%|████████▌ | 182/212 [00:48<00:07,  3.94it/s] 86%|████████▋ | 183/212 [00:48<00:07,  3.98it/s] 87%|████████▋ | 184/212 [00:49<00:06,  4.00it/s] 87%|████████▋ | 185/212 [00:49<00:06,  4.03it/s] 88%|████████▊ | 186/212 [00:49<00:06,  4.05it/s] 88%|████████▊ | 187/212 [00:49<00:06,  4.06it/s] 89%|████████▊ | 188/212 [00:50<00:05,  4.06it/s] 89%|████████▉ | 189/212 [00:50<00:05,  4.06it/s] 90%|████████▉ | 190/212 [00:50<00:05,  4.07it/s] 90%|█████████ | 191/212 [00:50<00:05,  4.07it/s] 91%|█████████ | 192/212 [00:51<00:04,  4.07it/s] 91%|█████████ | 193/212 [00:51<00:04,  4.07it/s] 92%|█████████▏| 194/212 [00:51<00:04,  4.06it/s] 92%|█████████▏| 195/212 [00:51<00:04,  4.06it/s] 92%|█████████▏| 196/212 [00:52<00:03,  4.07it/s] 93%|█████████▎| 197/212 [00:52<00:03,  4.07it/s] 93%|█████████▎| 198/212 [00:52<00:03,  4.07it/s] 94%|█████████▍| 199/212 [00:52<00:03,  4.06it/s] 94%|█████████▍| 200/212 [00:53<00:02,  4.07it/s] 95%|█████████▍| 201/212 [00:53<00:02,  4.06it/s] 95%|█████████▌| 202/212 [00:53<00:02,  4.06it/s] 96%|█████████▌| 203/212 [00:53<00:02,  4.07it/s] 96%|█████████▌| 204/212 [00:54<00:01,  4.08it/s] 97%|█████████▋| 205/212 [00:54<00:01,  4.06it/s] 97%|█████████▋| 206/212 [00:54<00:01,  4.06it/s] 98%|█████████▊| 207/212 [00:54<00:01,  4.07it/s] 98%|█████████▊| 208/212 [00:55<00:00,  4.06it/s] 99%|█████████▊| 209/212 [00:55<00:00,  4.06it/s] 99%|█████████▉| 210/212 [00:55<00:00,  4.07it/s]100%|█████████▉| 211/212 [00:55<00:00,  4.07it/s]100%|██████████| 212/212 [00:56<00:00,  4.07it/s]accuracy:  0.6509433962264151
100%|██████████| 212/212 [00:59<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 135
Training dataset size: 144, validation dataset size: 135
Training dataset size: 144, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.34s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:41:56,967] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 03:41:57,132] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:41:57,531] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:59,  2.58s/it]  8%|▊         | 2/24 [00:05<00:56,  2.55s/it] 12%|█▎        | 3/24 [00:07<00:51,  2.48s/it] 17%|█▋        | 4/24 [00:09<00:46,  2.34s/it] 21%|██        | 5/24 [00:12<00:45,  2.38s/it] 25%|██▌       | 6/24 [00:14<00:42,  2.35s/it] 29%|██▉       | 7/24 [00:16<00:38,  2.28s/it] 33%|███▎      | 8/24 [00:18<00:37,  2.32s/it] 38%|███▊      | 9/24 [00:21<00:34,  2.33s/it] 42%|████▏     | 10/24 [00:23<00:33,  2.38s/it]                                               {'loss': 0.6471, 'grad_norm': 6.616103649139404, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:23<00:33,  2.38s/it] 46%|████▌     | 11/24 [00:26<00:30,  2.34s/it] 50%|█████     | 12/24 [00:28<00:27,  2.31s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:30<00:26,  2.38s/it] 58%|█████▊    | 14/24 [00:33<00:24,  2.50s/it] 62%|██████▎   | 15/24 [00:36<00:22,  2.50s/it] 67%|██████▋   | 16/24 [00:38<00:19,  2.44s/it] 71%|███████   | 17/24 [00:41<00:17,  2.50s/it] 75%|███████▌  | 18/24 [00:43<00:15,  2.52s/it] 79%|███████▉  | 19/24 [00:45<00:12,  2.46s/it] 83%|████████▎ | 20/24 [00:48<00:09,  2.42s/it]                                               {'loss': 0.5862, 'grad_norm': 0.9393756985664368, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:48<00:09,  2.42s/it] 88%|████████▊ | 21/24 [00:50<00:07,  2.35s/it] 92%|█████████▏| 22/24 [00:53<00:05,  2.52s/it] 96%|█████████▌| 23/24 [00:55<00:02,  2.40s/it]100%|██████████| 24/24 [00:58<00:00,  2.46s/it]                                               {'train_runtime': 58.7034, 'train_samples_per_second': 4.906, 'train_steps_per_second': 0.409, 'train_loss': 0.5791934430599213, 'epoch': 2.0}
100%|██████████| 24/24 [00:58<00:00,  2.46s/it]100%|██████████| 24/24 [00:58<00:00,  2.44s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:53,  3.04it/s]  1%|          | 2/163 [00:00<00:46,  3.43it/s]  2%|▏         | 3/163 [00:00<00:45,  3.53it/s]  2%|▏         | 4/163 [00:01<00:44,  3.56it/s]  3%|▎         | 5/163 [00:01<00:42,  3.76it/s]  4%|▎         | 6/163 [00:01<00:42,  3.73it/s]  4%|▍         | 7/163 [00:01<00:42,  3.71it/s]  5%|▍         | 8/163 [00:02<00:42,  3.68it/s]  6%|▌         | 9/163 [00:02<00:40,  3.80it/s]  6%|▌         | 10/163 [00:02<00:40,  3.75it/s]  7%|▋         | 11/163 [00:02<00:40,  3.73it/s]  7%|▋         | 12/163 [00:03<00:40,  3.69it/s]  8%|▊         | 13/163 [00:03<00:39,  3.81it/s]  9%|▊         | 14/163 [00:03<00:39,  3.77it/s]  9%|▉         | 15/163 [00:04<00:39,  3.74it/s] 10%|▉         | 16/163 [00:04<00:39,  3.70it/s] 10%|█         | 17/163 [00:04<00:38,  3.83it/s] 11%|█         | 18/163 [00:04<00:38,  3.79it/s] 12%|█▏        | 19/163 [00:05<00:38,  3.75it/s] 12%|█▏        | 20/163 [00:05<00:38,  3.71it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.83it/s] 13%|█▎        | 22/163 [00:05<00:37,  3.78it/s] 14%|█▍        | 23/163 [00:06<00:37,  3.74it/s] 15%|█▍        | 24/163 [00:06<00:37,  3.70it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.81it/s] 16%|█▌        | 26/163 [00:06<00:36,  3.76it/s] 17%|█▋        | 27/163 [00:07<00:36,  3.74it/s] 17%|█▋        | 28/163 [00:07<00:36,  3.69it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.81it/s] 18%|█▊        | 30/163 [00:08<00:35,  3.76it/s] 19%|█▉        | 31/163 [00:08<00:35,  3.73it/s] 20%|█▉        | 32/163 [00:08<00:35,  3.69it/s] 20%|██        | 33/163 [00:08<00:34,  3.82it/s] 21%|██        | 34/163 [00:09<00:34,  3.79it/s] 21%|██▏       | 35/163 [00:09<00:34,  3.74it/s] 22%|██▏       | 36/163 [00:09<00:34,  3.70it/s] 23%|██▎       | 37/163 [00:09<00:32,  3.82it/s] 23%|██▎       | 38/163 [00:10<00:33,  3.78it/s] 24%|██▍       | 39/163 [00:10<00:33,  3.73it/s] 25%|██▍       | 40/163 [00:10<00:33,  3.69it/s] 25%|██▌       | 41/163 [00:10<00:32,  3.80it/s] 26%|██▌       | 42/163 [00:11<00:32,  3.75it/s] 26%|██▋       | 43/163 [00:11<00:32,  3.72it/s] 27%|██▋       | 44/163 [00:11<00:32,  3.68it/s] 28%|██▊       | 45/163 [00:12<00:30,  3.81it/s] 28%|██▊       | 46/163 [00:12<00:31,  3.77it/s] 29%|██▉       | 47/163 [00:12<00:31,  3.73it/s] 29%|██▉       | 48/163 [00:12<00:31,  3.68it/s] 30%|███       | 49/163 [00:13<00:30,  3.80it/s] 31%|███       | 50/163 [00:13<00:30,  3.74it/s] 31%|███▏      | 51/163 [00:13<00:30,  3.72it/s] 32%|███▏      | 52/163 [00:13<00:30,  3.69it/s] 33%|███▎      | 53/163 [00:14<00:28,  3.81it/s] 33%|███▎      | 54/163 [00:14<00:29,  3.75it/s] 34%|███▎      | 55/163 [00:14<00:29,  3.72it/s] 34%|███▍      | 56/163 [00:15<00:29,  3.68it/s] 35%|███▍      | 57/163 [00:15<00:27,  3.79it/s] 36%|███▌      | 58/163 [00:15<00:28,  3.75it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.72it/s] 37%|███▋      | 60/163 [00:16<00:28,  3.68it/s] 37%|███▋      | 61/163 [00:16<00:26,  3.80it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.77it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.73it/s] 39%|███▉      | 64/163 [00:17<00:26,  3.68it/s] 40%|███▉      | 65/163 [00:17<00:25,  3.79it/s] 40%|████      | 66/163 [00:17<00:25,  3.74it/s] 41%|████      | 67/163 [00:17<00:25,  3.70it/s] 42%|████▏     | 68/163 [00:18<00:25,  3.67it/s] 42%|████▏     | 69/163 [00:18<00:24,  3.79it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.74it/s] 44%|████▎     | 71/163 [00:19<00:24,  3.71it/s] 44%|████▍     | 72/163 [00:19<00:24,  3.68it/s] 45%|████▍     | 73/163 [00:19<00:23,  3.80it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.74it/s] 46%|████▌     | 75/163 [00:20<00:23,  3.71it/s] 47%|████▋     | 76/163 [00:20<00:23,  3.66it/s] 47%|████▋     | 77/163 [00:20<00:22,  3.78it/s] 48%|████▊     | 78/163 [00:20<00:22,  3.75it/s] 48%|████▊     | 79/163 [00:21<00:22,  3.71it/s] 49%|████▉     | 80/163 [00:21<00:22,  3.67it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.79it/s] 50%|█████     | 82/163 [00:21<00:21,  3.74it/s] 51%|█████     | 83/163 [00:22<00:21,  3.71it/s] 52%|█████▏    | 84/163 [00:22<00:21,  3.68it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.80it/s] 53%|█████▎    | 86/163 [00:23<00:20,  3.75it/s] 53%|█████▎    | 87/163 [00:23<00:20,  3.71it/s] 54%|█████▍    | 88/163 [00:23<00:20,  3.66it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.78it/s] 55%|█████▌    | 90/163 [00:24<00:19,  3.74it/s] 56%|█████▌    | 91/163 [00:24<00:19,  3.70it/s] 56%|█████▋    | 92/163 [00:24<00:19,  3.66it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.78it/s] 58%|█████▊    | 94/163 [00:25<00:18,  3.73it/s] 58%|█████▊    | 95/163 [00:25<00:18,  3.70it/s] 59%|█████▉    | 96/163 [00:25<00:18,  3.66it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.78it/s] 60%|██████    | 98/163 [00:26<00:17,  3.74it/s] 61%|██████    | 99/163 [00:26<00:17,  3.71it/s] 61%|██████▏   | 100/163 [00:26<00:17,  3.67it/s] 62%|██████▏   | 101/163 [00:27<00:16,  3.79it/s] 63%|██████▎   | 102/163 [00:27<00:16,  3.74it/s] 63%|██████▎   | 103/163 [00:27<00:16,  3.71it/s] 64%|██████▍   | 104/163 [00:27<00:16,  3.66it/s] 64%|██████▍   | 105/163 [00:28<00:15,  3.77it/s] 65%|██████▌   | 106/163 [00:28<00:15,  3.72it/s] 66%|██████▌   | 107/163 [00:28<00:15,  3.70it/s] 66%|██████▋   | 108/163 [00:28<00:15,  3.66it/s] 67%|██████▋   | 109/163 [00:29<00:14,  3.76it/s] 67%|██████▋   | 110/163 [00:29<00:14,  3.72it/s] 68%|██████▊   | 111/163 [00:29<00:14,  3.69it/s] 69%|██████▊   | 112/163 [00:30<00:13,  3.65it/s] 69%|██████▉   | 113/163 [00:30<00:13,  3.77it/s] 70%|██████▉   | 114/163 [00:30<00:13,  3.72it/s] 71%|███████   | 115/163 [00:30<00:13,  3.69it/s] 71%|███████   | 116/163 [00:31<00:12,  3.65it/s] 72%|███████▏  | 117/163 [00:31<00:12,  3.77it/s] 72%|███████▏  | 118/163 [00:31<00:12,  3.73it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.69it/s] 74%|███████▎  | 120/163 [00:32<00:11,  3.66it/s] 74%|███████▍  | 121/163 [00:32<00:11,  3.76it/s] 75%|███████▍  | 122/163 [00:32<00:11,  3.72it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.69it/s] 76%|███████▌  | 124/163 [00:33<00:10,  3.66it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.76it/s] 77%|███████▋  | 126/163 [00:33<00:09,  3.70it/s] 78%|███████▊  | 127/163 [00:34<00:09,  3.65it/s] 79%|███████▊  | 128/163 [00:34<00:09,  3.78it/s] 79%|███████▉  | 129/163 [00:34<00:08,  3.87it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.93it/s] 80%|████████  | 131/163 [00:35<00:08,  3.98it/s] 81%|████████  | 132/163 [00:35<00:07,  4.03it/s] 82%|████████▏ | 133/163 [00:35<00:07,  4.06it/s] 82%|████████▏ | 134/163 [00:35<00:07,  4.09it/s] 83%|████████▎ | 135/163 [00:36<00:06,  4.10it/s] 83%|████████▎ | 136/163 [00:36<00:06,  4.11it/s] 84%|████████▍ | 137/163 [00:36<00:06,  4.11it/s] 85%|████████▍ | 138/163 [00:36<00:06,  4.11it/s] 85%|████████▌ | 139/163 [00:37<00:06,  3.98it/s] 86%|████████▌ | 140/163 [00:37<00:05,  3.90it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.84it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.93it/s] 88%|████████▊ | 143/163 [00:38<00:05,  3.94it/s] 88%|████████▊ | 144/163 [00:38<00:04,  3.87it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.80it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.71it/s] 90%|█████████ | 147/163 [00:39<00:04,  3.83it/s] 91%|█████████ | 148/163 [00:39<00:03,  3.87it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.81it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.76it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.68it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.80it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.83it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.78it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.74it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.66it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.79it/s] 97%|█████████▋| 158/163 [00:42<00:01,  3.84it/s] 98%|█████████▊| 159/163 [00:42<00:01,  3.79it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.75it/s] 99%|█████████▉| 161/163 [00:42<00:00,  3.67it/s] 99%|█████████▉| 162/163 [00:43<00:00,  3.80it/s]100%|██████████| 163/163 [00:43<00:00,  3.89it/s]accuracy:  0.8834355828220859
100%|██████████| 163/163 [00:45<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 198
Training dataset size: 144, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.28s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:44:10,094] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:44:10,316] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:44:11,797] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<01:05,  2.83s/it]  8%|▊         | 2/24 [00:04<00:51,  2.36s/it] 12%|█▎        | 3/24 [00:07<00:53,  2.57s/it] 17%|█▋        | 4/24 [00:10<00:49,  2.49s/it] 21%|██        | 5/24 [00:12<00:49,  2.60s/it] 25%|██▌       | 6/24 [00:15<00:49,  2.74s/it] 29%|██▉       | 7/24 [00:18<00:44,  2.61s/it] 33%|███▎      | 8/24 [00:20<00:40,  2.52s/it] 38%|███▊      | 9/24 [00:23<00:38,  2.53s/it] 42%|████▏     | 10/24 [00:25<00:35,  2.54s/it]                                               {'loss': 0.5605, 'grad_norm': 7.1172966957092285, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:25<00:35,  2.54s/it] 46%|████▌     | 11/24 [00:28<00:33,  2.57s/it] 50%|█████     | 12/24 [00:30<00:29,  2.46s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:33<00:27,  2.53s/it] 58%|█████▊    | 14/24 [00:35<00:25,  2.57s/it] 62%|██████▎   | 15/24 [00:37<00:21,  2.41s/it] 67%|██████▋   | 16/24 [00:40<00:20,  2.54s/it] 71%|███████   | 17/24 [00:43<00:17,  2.49s/it] 75%|███████▌  | 18/24 [00:45<00:14,  2.38s/it] 79%|███████▉  | 19/24 [00:47<00:12,  2.43s/it] 83%|████████▎ | 20/24 [00:50<00:09,  2.44s/it]                                               {'loss': 0.4493, 'grad_norm': 4.090165615081787, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:50<00:09,  2.44s/it] 88%|████████▊ | 21/24 [00:52<00:07,  2.47s/it] 92%|█████████▏| 22/24 [00:55<00:05,  2.59s/it] 96%|█████████▌| 23/24 [00:58<00:02,  2.67s/it]100%|██████████| 24/24 [01:00<00:00,  2.50s/it]                                               {'train_runtime': 61.3139, 'train_samples_per_second': 4.697, 'train_steps_per_second': 0.391, 'train_loss': 0.6084033449490865, 'epoch': 2.0}
100%|██████████| 24/24 [01:01<00:00,  2.50s/it]100%|██████████| 24/24 [01:01<00:00,  2.55s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:18,  2.96it/s]  1%|          | 2/232 [00:00<01:07,  3.41it/s]  1%|▏         | 3/232 [00:00<01:05,  3.49it/s]  2%|▏         | 4/232 [00:01<01:02,  3.65it/s]  2%|▏         | 5/232 [00:01<01:02,  3.62it/s]  3%|▎         | 6/232 [00:01<01:01,  3.70it/s]  3%|▎         | 7/232 [00:01<01:01,  3.65it/s]  3%|▎         | 8/232 [00:02<01:00,  3.71it/s]  4%|▍         | 9/232 [00:02<01:00,  3.67it/s]  4%|▍         | 10/232 [00:02<00:59,  3.73it/s]  5%|▍         | 11/232 [00:03<01:00,  3.67it/s]  5%|▌         | 12/232 [00:03<00:59,  3.73it/s]  6%|▌         | 13/232 [00:03<00:59,  3.67it/s]  6%|▌         | 14/232 [00:03<00:58,  3.74it/s]  6%|▋         | 15/232 [00:04<00:58,  3.69it/s]  7%|▋         | 16/232 [00:04<00:57,  3.74it/s]  7%|▋         | 17/232 [00:04<00:58,  3.68it/s]  8%|▊         | 18/232 [00:04<00:57,  3.74it/s]  8%|▊         | 19/232 [00:05<00:57,  3.70it/s]  9%|▊         | 20/232 [00:05<00:56,  3.75it/s]  9%|▉         | 21/232 [00:05<00:57,  3.70it/s]  9%|▉         | 22/232 [00:05<00:55,  3.77it/s] 10%|▉         | 23/232 [00:06<00:56,  3.71it/s] 10%|█         | 24/232 [00:06<00:55,  3.76it/s] 11%|█         | 25/232 [00:06<00:55,  3.70it/s] 11%|█         | 26/232 [00:07<00:54,  3.75it/s] 12%|█▏        | 27/232 [00:07<00:55,  3.70it/s] 12%|█▏        | 28/232 [00:07<00:54,  3.74it/s] 12%|█▎        | 29/232 [00:07<00:55,  3.68it/s] 13%|█▎        | 30/232 [00:08<00:53,  3.74it/s] 13%|█▎        | 31/232 [00:08<00:54,  3.69it/s] 14%|█▍        | 32/232 [00:08<00:53,  3.73it/s] 14%|█▍        | 33/232 [00:08<00:54,  3.68it/s] 15%|█▍        | 34/232 [00:09<00:52,  3.74it/s] 15%|█▌        | 35/232 [00:09<00:53,  3.70it/s] 16%|█▌        | 36/232 [00:09<00:52,  3.74it/s] 16%|█▌        | 37/232 [00:10<00:52,  3.68it/s] 16%|█▋        | 38/232 [00:10<00:52,  3.72it/s] 17%|█▋        | 39/232 [00:10<00:52,  3.67it/s] 17%|█▋        | 40/232 [00:10<00:51,  3.73it/s] 18%|█▊        | 41/232 [00:11<00:52,  3.67it/s] 18%|█▊        | 42/232 [00:11<00:50,  3.73it/s] 19%|█▊        | 43/232 [00:11<00:51,  3.69it/s] 19%|█▉        | 44/232 [00:11<00:50,  3.73it/s] 19%|█▉        | 45/232 [00:12<00:50,  3.68it/s] 20%|█▉        | 46/232 [00:12<00:49,  3.74it/s] 20%|██        | 47/232 [00:12<00:50,  3.69it/s] 21%|██        | 48/232 [00:12<00:49,  3.74it/s] 21%|██        | 49/232 [00:13<00:49,  3.68it/s] 22%|██▏       | 50/232 [00:13<00:48,  3.73it/s] 22%|██▏       | 51/232 [00:13<00:49,  3.67it/s] 22%|██▏       | 52/232 [00:14<00:48,  3.72it/s] 23%|██▎       | 53/232 [00:14<00:48,  3.67it/s] 23%|██▎       | 54/232 [00:14<00:47,  3.73it/s] 24%|██▎       | 55/232 [00:14<00:48,  3.68it/s] 24%|██▍       | 56/232 [00:15<00:47,  3.73it/s] 25%|██▍       | 57/232 [00:15<00:47,  3.67it/s] 25%|██▌       | 58/232 [00:15<00:46,  3.74it/s] 25%|██▌       | 59/232 [00:15<00:46,  3.69it/s] 26%|██▌       | 60/232 [00:16<00:46,  3.74it/s] 26%|██▋       | 61/232 [00:16<00:46,  3.67it/s] 27%|██▋       | 62/232 [00:16<00:45,  3.73it/s] 27%|██▋       | 63/232 [00:17<00:45,  3.68it/s] 28%|██▊       | 64/232 [00:17<00:45,  3.73it/s] 28%|██▊       | 65/232 [00:17<00:45,  3.67it/s] 28%|██▊       | 66/232 [00:17<00:44,  3.73it/s] 29%|██▉       | 67/232 [00:18<00:44,  3.68it/s] 29%|██▉       | 68/232 [00:18<00:44,  3.73it/s] 30%|██▉       | 69/232 [00:18<00:44,  3.67it/s] 30%|███       | 70/232 [00:18<00:43,  3.73it/s] 31%|███       | 71/232 [00:19<00:43,  3.67it/s] 31%|███       | 72/232 [00:19<00:42,  3.72it/s] 31%|███▏      | 73/232 [00:19<00:43,  3.67it/s] 32%|███▏      | 74/232 [00:20<00:42,  3.73it/s] 32%|███▏      | 75/232 [00:20<00:42,  3.68it/s] 33%|███▎      | 76/232 [00:20<00:41,  3.73it/s] 33%|███▎      | 77/232 [00:20<00:42,  3.68it/s] 34%|███▎      | 78/232 [00:21<00:41,  3.73it/s] 34%|███▍      | 79/232 [00:21<00:41,  3.68it/s] 34%|███▍      | 80/232 [00:21<00:40,  3.72it/s] 35%|███▍      | 81/232 [00:21<00:41,  3.68it/s] 35%|███▌      | 82/232 [00:22<00:40,  3.73it/s] 36%|███▌      | 83/232 [00:22<00:40,  3.69it/s] 36%|███▌      | 84/232 [00:22<00:39,  3.73it/s] 37%|███▋      | 85/232 [00:22<00:39,  3.68it/s] 37%|███▋      | 86/232 [00:23<00:38,  3.74it/s] 38%|███▊      | 87/232 [00:23<00:39,  3.69it/s] 38%|███▊      | 88/232 [00:23<00:38,  3.73it/s] 38%|███▊      | 89/232 [00:24<00:38,  3.68it/s] 39%|███▉      | 90/232 [00:24<00:37,  3.75it/s] 39%|███▉      | 91/232 [00:24<00:38,  3.69it/s] 40%|███▉      | 92/232 [00:24<00:37,  3.73it/s] 40%|████      | 93/232 [00:25<00:37,  3.67it/s] 41%|████      | 94/232 [00:25<00:37,  3.73it/s] 41%|████      | 95/232 [00:25<00:37,  3.68it/s] 41%|████▏     | 96/232 [00:25<00:36,  3.72it/s] 42%|████▏     | 97/232 [00:26<00:36,  3.67it/s] 42%|████▏     | 98/232 [00:26<00:35,  3.73it/s] 43%|████▎     | 99/232 [00:26<00:36,  3.67it/s] 43%|████▎     | 100/232 [00:27<00:35,  3.72it/s] 44%|████▎     | 101/232 [00:27<00:35,  3.67it/s] 44%|████▍     | 102/232 [00:27<00:34,  3.73it/s] 44%|████▍     | 103/232 [00:27<00:35,  3.67it/s] 45%|████▍     | 104/232 [00:28<00:34,  3.73it/s] 45%|████▌     | 105/232 [00:28<00:34,  3.68it/s] 46%|████▌     | 106/232 [00:28<00:33,  3.74it/s] 46%|████▌     | 107/232 [00:28<00:33,  3.69it/s] 47%|████▋     | 108/232 [00:29<00:33,  3.72it/s] 47%|████▋     | 109/232 [00:29<00:33,  3.67it/s] 47%|████▋     | 110/232 [00:29<00:32,  3.72it/s] 48%|████▊     | 111/232 [00:30<00:32,  3.67it/s] 48%|████▊     | 112/232 [00:30<00:32,  3.72it/s] 49%|████▊     | 113/232 [00:30<00:32,  3.66it/s] 49%|████▉     | 114/232 [00:30<00:31,  3.71it/s] 50%|████▉     | 115/232 [00:31<00:32,  3.65it/s] 50%|█████     | 116/232 [00:31<00:31,  3.70it/s] 50%|█████     | 117/232 [00:31<00:31,  3.65it/s] 51%|█████     | 118/232 [00:31<00:30,  3.71it/s] 51%|█████▏    | 119/232 [00:32<00:30,  3.67it/s] 52%|█████▏    | 120/232 [00:32<00:30,  3.71it/s] 52%|█████▏    | 121/232 [00:32<00:30,  3.66it/s] 53%|█████▎    | 122/232 [00:32<00:29,  3.72it/s] 53%|█████▎    | 123/232 [00:33<00:29,  3.68it/s] 53%|█████▎    | 124/232 [00:33<00:29,  3.72it/s] 54%|█████▍    | 125/232 [00:33<00:29,  3.67it/s] 54%|█████▍    | 126/232 [00:34<00:28,  3.73it/s] 55%|█████▍    | 127/232 [00:34<00:28,  3.68it/s] 55%|█████▌    | 128/232 [00:34<00:27,  3.72it/s] 56%|█████▌    | 129/232 [00:34<00:28,  3.67it/s] 56%|█████▌    | 130/232 [00:35<00:27,  3.73it/s] 56%|█████▋    | 131/232 [00:35<00:27,  3.68it/s] 57%|█████▋    | 132/232 [00:35<00:26,  3.72it/s] 57%|█████▋    | 133/232 [00:35<00:26,  3.67it/s] 58%|█████▊    | 134/232 [00:36<00:26,  3.72it/s] 58%|█████▊    | 135/232 [00:36<00:26,  3.67it/s] 59%|█████▊    | 136/232 [00:36<00:25,  3.72it/s] 59%|█████▉    | 137/232 [00:37<00:25,  3.66it/s] 59%|█████▉    | 138/232 [00:37<00:25,  3.71it/s] 60%|█████▉    | 139/232 [00:37<00:25,  3.66it/s] 60%|██████    | 140/232 [00:37<00:24,  3.71it/s] 61%|██████    | 141/232 [00:38<00:24,  3.65it/s] 61%|██████    | 142/232 [00:38<00:24,  3.71it/s] 62%|██████▏   | 143/232 [00:38<00:24,  3.65it/s] 62%|██████▏   | 144/232 [00:38<00:23,  3.71it/s] 62%|██████▎   | 145/232 [00:39<00:23,  3.66it/s] 63%|██████▎   | 146/232 [00:39<00:23,  3.71it/s] 63%|██████▎   | 147/232 [00:39<00:23,  3.67it/s] 64%|██████▍   | 148/232 [00:40<00:22,  3.71it/s] 64%|██████▍   | 149/232 [00:40<00:22,  3.66it/s] 65%|██████▍   | 150/232 [00:40<00:22,  3.70it/s] 65%|██████▌   | 151/232 [00:40<00:22,  3.66it/s] 66%|██████▌   | 152/232 [00:41<00:21,  3.71it/s] 66%|██████▌   | 153/232 [00:41<00:21,  3.65it/s] 66%|██████▋   | 154/232 [00:41<00:21,  3.70it/s] 67%|██████▋   | 155/232 [00:41<00:21,  3.66it/s] 67%|██████▋   | 156/232 [00:42<00:20,  3.70it/s] 68%|██████▊   | 157/232 [00:42<00:20,  3.66it/s] 68%|██████▊   | 158/232 [00:42<00:19,  3.71it/s] 69%|██████▊   | 159/232 [00:43<00:19,  3.66it/s] 69%|██████▉   | 160/232 [00:43<00:19,  3.71it/s] 69%|██████▉   | 161/232 [00:43<00:19,  3.66it/s] 70%|██████▉   | 162/232 [00:43<00:18,  3.70it/s] 70%|███████   | 163/232 [00:44<00:18,  3.64it/s] 71%|███████   | 164/232 [00:44<00:18,  3.69it/s] 71%|███████   | 165/232 [00:44<00:18,  3.64it/s] 72%|███████▏  | 166/232 [00:44<00:17,  3.70it/s] 72%|███████▏  | 167/232 [00:45<00:17,  3.66it/s] 72%|███████▏  | 168/232 [00:45<00:17,  3.71it/s] 73%|███████▎  | 169/232 [00:45<00:17,  3.65it/s] 73%|███████▎  | 170/232 [00:46<00:16,  3.69it/s] 74%|███████▎  | 171/232 [00:46<00:16,  3.63it/s] 74%|███████▍  | 172/232 [00:46<00:16,  3.68it/s] 75%|███████▍  | 173/232 [00:46<00:16,  3.64it/s] 75%|███████▌  | 174/232 [00:47<00:15,  3.70it/s] 75%|███████▌  | 175/232 [00:47<00:15,  3.65it/s] 76%|███████▌  | 176/232 [00:47<00:15,  3.70it/s] 76%|███████▋  | 177/232 [00:47<00:15,  3.65it/s] 77%|███████▋  | 178/232 [00:48<00:14,  3.69it/s] 77%|███████▋  | 179/232 [00:48<00:14,  3.64it/s] 78%|███████▊  | 180/232 [00:48<00:14,  3.69it/s] 78%|███████▊  | 181/232 [00:49<00:14,  3.64it/s] 78%|███████▊  | 182/232 [00:49<00:13,  3.70it/s] 79%|███████▉  | 183/232 [00:49<00:13,  3.63it/s] 79%|███████▉  | 184/232 [00:49<00:13,  3.65it/s] 80%|███████▉  | 185/232 [00:50<00:12,  3.65it/s] 80%|████████  | 186/232 [00:50<00:12,  3.68it/s] 81%|████████  | 187/232 [00:50<00:12,  3.64it/s] 81%|████████  | 188/232 [00:50<00:12,  3.65it/s] 81%|████████▏ | 189/232 [00:51<00:11,  3.66it/s] 82%|████████▏ | 190/232 [00:51<00:11,  3.70it/s] 82%|████████▏ | 191/232 [00:51<00:11,  3.64it/s] 83%|████████▎ | 192/232 [00:52<00:10,  3.66it/s] 83%|████████▎ | 193/232 [00:52<00:10,  3.65it/s] 84%|████████▎ | 194/232 [00:52<00:10,  3.69it/s] 84%|████████▍ | 195/232 [00:52<00:10,  3.63it/s] 84%|████████▍ | 196/232 [00:53<00:09,  3.63it/s] 85%|████████▍ | 197/232 [00:53<00:09,  3.67it/s] 85%|████████▌ | 198/232 [00:53<00:09,  3.70it/s] 86%|████████▌ | 199/232 [00:53<00:09,  3.65it/s] 86%|████████▌ | 200/232 [00:54<00:08,  3.66it/s] 87%|████████▋ | 201/232 [00:54<00:08,  3.67it/s] 87%|████████▋ | 202/232 [00:54<00:08,  3.70it/s] 88%|████████▊ | 203/232 [00:55<00:07,  3.63it/s] 88%|████████▊ | 204/232 [00:55<00:07,  3.63it/s] 88%|████████▊ | 205/232 [00:55<00:07,  3.67it/s] 89%|████████▉ | 206/232 [00:55<00:07,  3.68it/s] 89%|████████▉ | 207/232 [00:56<00:06,  3.63it/s] 90%|████████▉ | 208/232 [00:56<00:06,  3.64it/s] 90%|█████████ | 209/232 [00:56<00:06,  3.67it/s] 91%|█████████ | 210/232 [00:56<00:05,  3.69it/s] 91%|█████████ | 211/232 [00:57<00:05,  3.65it/s] 91%|█████████▏| 212/232 [00:57<00:05,  3.66it/s] 92%|█████████▏| 213/232 [00:57<00:05,  3.67it/s] 92%|█████████▏| 214/232 [00:58<00:04,  3.71it/s] 93%|█████████▎| 215/232 [00:58<00:04,  3.64it/s] 93%|█████████▎| 216/232 [00:58<00:04,  3.64it/s] 94%|█████████▎| 217/232 [00:58<00:04,  3.74it/s] 94%|█████████▍| 218/232 [00:59<00:03,  3.84it/s] 94%|█████████▍| 219/232 [00:59<00:03,  3.91it/s] 95%|█████████▍| 220/232 [00:59<00:03,  3.94it/s] 95%|█████████▌| 221/232 [00:59<00:02,  3.98it/s] 96%|█████████▌| 222/232 [01:00<00:02,  4.01it/s] 96%|█████████▌| 223/232 [01:00<00:02,  4.02it/s] 97%|█████████▋| 224/232 [01:00<00:01,  4.03it/s] 97%|█████████▋| 225/232 [01:00<00:01,  4.05it/s] 97%|█████████▋| 226/232 [01:01<00:01,  4.01it/s] 98%|█████████▊| 227/232 [01:01<00:01,  3.90it/s] 98%|█████████▊| 228/232 [01:01<00:01,  3.83it/s] 99%|█████████▊| 229/232 [01:01<00:00,  3.80it/s] 99%|█████████▉| 230/232 [01:02<00:00,  3.87it/s]100%|█████████▉| 231/232 [01:02<00:00,  3.88it/s]100%|██████████| 232/232 [01:02<00:00,  3.81it/s]accuracy:  0.8793103448275862
100%|██████████| 232/232 [01:06<00:00,  3.51it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:46:46,658] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.48s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:46:48,809] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:46:49,657] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:03<01:09,  3.02s/it]  8%|▊         | 2/24 [00:04<00:51,  2.34s/it] 12%|█▎        | 3/24 [00:07<00:55,  2.64s/it] 17%|█▋        | 4/24 [00:10<00:52,  2.64s/it] 21%|██        | 5/24 [00:12<00:47,  2.50s/it] 25%|██▌       | 6/24 [00:15<00:43,  2.44s/it] 29%|██▉       | 7/24 [00:17<00:41,  2.42s/it] 33%|███▎      | 8/24 [00:19<00:38,  2.39s/it] 38%|███▊      | 9/24 [00:21<00:32,  2.17s/it] 42%|████▏     | 10/24 [00:24<00:31,  2.28s/it]                                               {'loss': 1.2542, 'grad_norm': 8.955236434936523, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:24<00:31,  2.28s/it] 46%|████▌     | 11/24 [00:26<00:29,  2.29s/it] 50%|█████     | 12/24 [00:28<00:26,  2.19s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:31<00:26,  2.38s/it] 58%|█████▊    | 14/24 [00:33<00:23,  2.37s/it] 62%|██████▎   | 15/24 [00:35<00:21,  2.37s/it] 67%|██████▋   | 16/24 [00:38<00:19,  2.42s/it] 71%|███████   | 17/24 [00:40<00:16,  2.41s/it] 75%|███████▌  | 18/24 [00:43<00:14,  2.36s/it] 79%|███████▉  | 19/24 [00:45<00:11,  2.32s/it] 83%|████████▎ | 20/24 [00:47<00:09,  2.35s/it]                                               {'loss': 0.7647, 'grad_norm': 5.510226249694824, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:47<00:09,  2.35s/it] 88%|████████▊ | 21/24 [00:49<00:06,  2.32s/it] 92%|█████████▏| 22/24 [00:51<00:04,  2.22s/it] 96%|█████████▌| 23/24 [00:54<00:02,  2.37s/it]100%|██████████| 24/24 [00:56<00:00,  2.29s/it]                                               {'train_runtime': 57.383, 'train_samples_per_second': 5.019, 'train_steps_per_second': 0.418, 'train_loss': 1.0013610323270161, 'epoch': 2.0}
100%|██████████| 24/24 [00:57<00:00,  2.29s/it]100%|██████████| 24/24 [00:57<00:00,  2.38s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:19,  3.12it/s]  1%|          | 2/248 [00:00<01:07,  3.64it/s]  1%|          | 3/248 [00:00<01:03,  3.84it/s]  2%|▏         | 4/248 [00:01<01:01,  3.95it/s]  2%|▏         | 5/248 [00:01<01:00,  4.02it/s]  2%|▏         | 6/248 [00:01<00:59,  4.06it/s]  3%|▎         | 7/248 [00:01<00:58,  4.09it/s]  3%|▎         | 8/248 [00:02<00:58,  4.12it/s]  4%|▎         | 9/248 [00:02<00:57,  4.13it/s]  4%|▍         | 10/248 [00:02<00:58,  4.07it/s]  4%|▍         | 11/248 [00:02<00:59,  3.97it/s]  5%|▍         | 12/248 [00:03<01:00,  3.89it/s]  5%|▌         | 13/248 [00:03<01:00,  3.90it/s]  6%|▌         | 14/248 [00:03<00:59,  3.91it/s]  6%|▌         | 15/248 [00:03<01:01,  3.80it/s]  6%|▋         | 16/248 [00:04<01:02,  3.72it/s]  7%|▋         | 17/248 [00:04<01:02,  3.72it/s]  7%|▋         | 18/248 [00:04<01:00,  3.78it/s]  8%|▊         | 19/248 [00:04<00:59,  3.82it/s]  8%|▊         | 20/248 [00:05<01:00,  3.74it/s]  8%|▊         | 21/248 [00:05<01:01,  3.69it/s]  9%|▉         | 22/248 [00:05<01:01,  3.69it/s]  9%|▉         | 23/248 [00:05<00:59,  3.76it/s] 10%|▉         | 24/248 [00:06<00:58,  3.81it/s] 10%|█         | 25/248 [00:06<00:59,  3.74it/s] 10%|█         | 26/248 [00:06<01:00,  3.68it/s] 11%|█         | 27/248 [00:07<01:00,  3.68it/s] 11%|█▏        | 28/248 [00:07<00:58,  3.76it/s] 12%|█▏        | 29/248 [00:07<00:57,  3.79it/s] 12%|█▏        | 30/248 [00:07<00:58,  3.72it/s] 12%|█▎        | 31/248 [00:08<00:59,  3.67it/s] 13%|█▎        | 32/248 [00:08<00:59,  3.61it/s] 13%|█▎        | 33/248 [00:08<00:57,  3.76it/s] 14%|█▎        | 34/248 [00:08<00:56,  3.78it/s] 14%|█▍        | 35/248 [00:09<00:58,  3.66it/s] 15%|█▍        | 36/248 [00:09<00:57,  3.71it/s] 15%|█▍        | 37/248 [00:09<00:57,  3.66it/s] 15%|█▌        | 38/248 [00:10<00:56,  3.74it/s] 16%|█▌        | 39/248 [00:10<00:56,  3.69it/s] 16%|█▌        | 40/248 [00:10<00:56,  3.65it/s] 17%|█▋        | 41/248 [00:10<00:57,  3.60it/s] 17%|█▋        | 42/248 [00:11<00:55,  3.74it/s] 17%|█▋        | 43/248 [00:11<00:54,  3.76it/s] 18%|█▊        | 44/248 [00:11<00:55,  3.65it/s] 18%|█▊        | 45/248 [00:11<00:54,  3.70it/s] 19%|█▊        | 46/248 [00:12<00:55,  3.65it/s] 19%|█▉        | 47/248 [00:12<00:53,  3.74it/s] 19%|█▉        | 48/248 [00:12<00:54,  3.69it/s] 20%|█▉        | 49/248 [00:13<00:54,  3.64it/s] 20%|██        | 50/248 [00:13<00:55,  3.59it/s] 21%|██        | 51/248 [00:13<00:52,  3.74it/s] 21%|██        | 52/248 [00:13<00:52,  3.77it/s] 21%|██▏       | 53/248 [00:14<00:53,  3.66it/s] 22%|██▏       | 54/248 [00:14<00:52,  3.70it/s] 22%|██▏       | 55/248 [00:14<00:52,  3.66it/s] 23%|██▎       | 56/248 [00:14<00:51,  3.75it/s] 23%|██▎       | 57/248 [00:15<00:51,  3.69it/s] 23%|██▎       | 58/248 [00:15<00:52,  3.64it/s] 24%|██▍       | 59/248 [00:15<00:52,  3.59it/s] 24%|██▍       | 60/248 [00:15<00:50,  3.74it/s] 25%|██▍       | 61/248 [00:16<00:49,  3.78it/s] 25%|██▌       | 62/248 [00:16<00:50,  3.71it/s] 25%|██▌       | 63/248 [00:16<00:50,  3.66it/s] 26%|██▌       | 64/248 [00:17<00:51,  3.60it/s] 26%|██▌       | 65/248 [00:17<00:48,  3.75it/s] 27%|██▋       | 66/248 [00:17<00:48,  3.77it/s] 27%|██▋       | 67/248 [00:17<00:49,  3.65it/s] 27%|██▋       | 68/248 [00:18<00:48,  3.69it/s] 28%|██▊       | 69/248 [00:18<00:49,  3.64it/s] 28%|██▊       | 70/248 [00:18<00:47,  3.72it/s] 29%|██▊       | 71/248 [00:18<00:48,  3.67it/s] 29%|██▉       | 72/248 [00:19<00:48,  3.63it/s] 29%|██▉       | 73/248 [00:19<00:48,  3.59it/s] 30%|██▉       | 74/248 [00:19<00:46,  3.74it/s] 30%|███       | 75/248 [00:20<00:46,  3.75it/s] 31%|███       | 76/248 [00:20<00:47,  3.64it/s] 31%|███       | 77/248 [00:20<00:46,  3.70it/s] 31%|███▏      | 78/248 [00:20<00:46,  3.66it/s] 32%|███▏      | 79/248 [00:21<00:45,  3.76it/s] 32%|███▏      | 80/248 [00:21<00:45,  3.70it/s] 33%|███▎      | 81/248 [00:21<00:45,  3.64it/s] 33%|███▎      | 82/248 [00:21<00:46,  3.60it/s] 33%|███▎      | 83/248 [00:22<00:44,  3.74it/s] 34%|███▍      | 84/248 [00:22<00:43,  3.77it/s] 34%|███▍      | 85/248 [00:22<00:44,  3.64it/s] 35%|███▍      | 86/248 [00:23<00:43,  3.70it/s] 35%|███▌      | 87/248 [00:23<00:44,  3.66it/s] 35%|███▌      | 88/248 [00:23<00:42,  3.74it/s] 36%|███▌      | 89/248 [00:23<00:43,  3.68it/s] 36%|███▋      | 90/248 [00:24<00:43,  3.63it/s] 37%|███▋      | 91/248 [00:24<00:43,  3.59it/s] 37%|███▋      | 92/248 [00:24<00:41,  3.72it/s] 38%|███▊      | 93/248 [00:24<00:41,  3.75it/s] 38%|███▊      | 94/248 [00:25<00:42,  3.63it/s] 38%|███▊      | 95/248 [00:25<00:41,  3.68it/s] 39%|███▊      | 96/248 [00:25<00:41,  3.63it/s] 39%|███▉      | 97/248 [00:26<00:40,  3.73it/s] 40%|███▉      | 98/248 [00:26<00:40,  3.67it/s] 40%|███▉      | 99/248 [00:26<00:41,  3.62it/s] 40%|████      | 100/248 [00:26<00:41,  3.58it/s] 41%|████      | 101/248 [00:27<00:39,  3.72it/s] 41%|████      | 102/248 [00:27<00:38,  3.75it/s] 42%|████▏     | 103/248 [00:27<00:39,  3.63it/s] 42%|████▏     | 104/248 [00:27<00:39,  3.68it/s] 42%|████▏     | 105/248 [00:28<00:39,  3.64it/s] 43%|████▎     | 106/248 [00:28<00:38,  3.71it/s] 43%|████▎     | 107/248 [00:28<00:38,  3.67it/s] 44%|████▎     | 108/248 [00:29<00:38,  3.62it/s] 44%|████▍     | 109/248 [00:29<00:38,  3.58it/s] 44%|████▍     | 110/248 [00:29<00:37,  3.73it/s] 45%|████▍     | 111/248 [00:29<00:36,  3.75it/s] 45%|████▌     | 112/248 [00:30<00:37,  3.62it/s] 46%|████▌     | 113/248 [00:30<00:36,  3.68it/s] 46%|████▌     | 114/248 [00:30<00:36,  3.64it/s] 46%|████▋     | 115/248 [00:30<00:35,  3.72it/s] 47%|████▋     | 116/248 [00:31<00:36,  3.67it/s] 47%|████▋     | 117/248 [00:31<00:36,  3.61it/s] 48%|████▊     | 118/248 [00:31<00:36,  3.57it/s] 48%|████▊     | 119/248 [00:32<00:34,  3.72it/s] 48%|████▊     | 120/248 [00:32<00:34,  3.74it/s] 49%|████▉     | 121/248 [00:32<00:35,  3.62it/s] 49%|████▉     | 122/248 [00:32<00:34,  3.68it/s] 50%|████▉     | 123/248 [00:33<00:34,  3.64it/s] 50%|█████     | 124/248 [00:33<00:33,  3.71it/s] 50%|█████     | 125/248 [00:33<00:33,  3.65it/s] 51%|█████     | 126/248 [00:33<00:33,  3.61it/s] 51%|█████     | 127/248 [00:34<00:33,  3.56it/s] 52%|█████▏    | 128/248 [00:34<00:32,  3.72it/s] 52%|█████▏    | 129/248 [00:34<00:31,  3.73it/s] 52%|█████▏    | 130/248 [00:35<00:32,  3.62it/s] 53%|█████▎    | 131/248 [00:35<00:31,  3.68it/s] 53%|█████▎    | 132/248 [00:35<00:31,  3.64it/s] 54%|█████▎    | 133/248 [00:35<00:30,  3.72it/s] 54%|█████▍    | 134/248 [00:36<00:31,  3.67it/s] 54%|█████▍    | 135/248 [00:36<00:31,  3.62it/s] 55%|█████▍    | 136/248 [00:36<00:31,  3.58it/s] 55%|█████▌    | 137/248 [00:36<00:29,  3.72it/s] 56%|█████▌    | 138/248 [00:37<00:29,  3.75it/s] 56%|█████▌    | 139/248 [00:37<00:29,  3.63it/s] 56%|█████▋    | 140/248 [00:37<00:29,  3.68it/s] 57%|█████▋    | 141/248 [00:38<00:29,  3.64it/s] 57%|█████▋    | 142/248 [00:38<00:28,  3.73it/s] 58%|█████▊    | 143/248 [00:38<00:28,  3.67it/s] 58%|█████▊    | 144/248 [00:38<00:28,  3.62it/s] 58%|█████▊    | 145/248 [00:39<00:28,  3.58it/s] 59%|█████▉    | 146/248 [00:39<00:27,  3.71it/s] 59%|█████▉    | 147/248 [00:39<00:27,  3.72it/s] 60%|█████▉    | 148/248 [00:39<00:27,  3.61it/s] 60%|██████    | 149/248 [00:40<00:27,  3.66it/s] 60%|██████    | 150/248 [00:40<00:26,  3.63it/s] 61%|██████    | 151/248 [00:40<00:26,  3.72it/s] 61%|██████▏   | 152/248 [00:41<00:26,  3.67it/s] 62%|██████▏   | 153/248 [00:41<00:26,  3.62it/s] 62%|██████▏   | 154/248 [00:41<00:26,  3.57it/s] 62%|██████▎   | 155/248 [00:41<00:25,  3.70it/s] 63%|██████▎   | 156/248 [00:42<00:24,  3.73it/s] 63%|██████▎   | 157/248 [00:42<00:25,  3.61it/s] 64%|██████▎   | 158/248 [00:42<00:24,  3.66it/s] 64%|██████▍   | 159/248 [00:42<00:24,  3.62it/s] 65%|██████▍   | 160/248 [00:43<00:23,  3.71it/s] 65%|██████▍   | 161/248 [00:43<00:23,  3.66it/s] 65%|██████▌   | 162/248 [00:43<00:23,  3.61it/s] 66%|██████▌   | 163/248 [00:44<00:23,  3.57it/s] 66%|██████▌   | 164/248 [00:44<00:22,  3.72it/s] 67%|██████▋   | 165/248 [00:44<00:22,  3.74it/s] 67%|██████▋   | 166/248 [00:44<00:22,  3.62it/s] 67%|██████▋   | 167/248 [00:45<00:22,  3.68it/s] 68%|██████▊   | 168/248 [00:45<00:21,  3.64it/s] 68%|██████▊   | 169/248 [00:45<00:21,  3.73it/s] 69%|██████▊   | 170/248 [00:45<00:21,  3.68it/s] 69%|██████▉   | 171/248 [00:46<00:21,  3.62it/s] 69%|██████▉   | 172/248 [00:46<00:21,  3.57it/s] 70%|██████▉   | 173/248 [00:46<00:20,  3.71it/s] 70%|███████   | 174/248 [00:47<00:19,  3.73it/s] 71%|███████   | 175/248 [00:47<00:20,  3.62it/s] 71%|███████   | 176/248 [00:47<00:19,  3.67it/s] 71%|███████▏  | 177/248 [00:47<00:19,  3.63it/s] 72%|███████▏  | 178/248 [00:48<00:18,  3.71it/s] 72%|███████▏  | 179/248 [00:48<00:18,  3.66it/s] 73%|███████▎  | 180/248 [00:48<00:18,  3.61it/s] 73%|███████▎  | 181/248 [00:49<00:18,  3.57it/s] 73%|███████▎  | 182/248 [00:49<00:17,  3.71it/s] 74%|███████▍  | 183/248 [00:49<00:17,  3.74it/s] 74%|███████▍  | 184/248 [00:49<00:17,  3.62it/s] 75%|███████▍  | 185/248 [00:50<00:17,  3.68it/s] 75%|███████▌  | 186/248 [00:50<00:17,  3.64it/s] 75%|███████▌  | 187/248 [00:50<00:16,  3.75it/s] 76%|███████▌  | 188/248 [00:50<00:15,  3.84it/s] 76%|███████▌  | 189/248 [00:51<00:15,  3.90it/s] 77%|███████▋  | 190/248 [00:51<00:15,  3.77it/s] 77%|███████▋  | 191/248 [00:51<00:15,  3.68it/s] 77%|███████▋  | 192/248 [00:51<00:15,  3.62it/s] 78%|███████▊  | 193/248 [00:52<00:14,  3.74it/s] 78%|███████▊  | 194/248 [00:52<00:14,  3.74it/s] 79%|███████▊  | 195/248 [00:52<00:14,  3.62it/s] 79%|███████▉  | 196/248 [00:53<00:14,  3.67it/s] 79%|███████▉  | 197/248 [00:53<00:14,  3.63it/s] 80%|███████▉  | 198/248 [00:53<00:13,  3.72it/s] 80%|████████  | 199/248 [00:53<00:13,  3.65it/s] 81%|████████  | 200/248 [00:54<00:13,  3.60it/s] 81%|████████  | 201/248 [00:54<00:13,  3.56it/s] 81%|████████▏ | 202/248 [00:54<00:12,  3.70it/s] 82%|████████▏ | 203/248 [00:54<00:12,  3.73it/s] 82%|████████▏ | 204/248 [00:55<00:12,  3.61it/s] 83%|████████▎ | 205/248 [00:55<00:11,  3.66it/s] 83%|████████▎ | 206/248 [00:55<00:11,  3.61it/s] 83%|████████▎ | 207/248 [00:56<00:11,  3.70it/s] 84%|████████▍ | 208/248 [00:56<00:10,  3.65it/s] 84%|████████▍ | 209/248 [00:56<00:10,  3.60it/s] 85%|████████▍ | 210/248 [00:56<00:10,  3.56it/s] 85%|████████▌ | 211/248 [00:57<00:10,  3.70it/s] 85%|████████▌ | 212/248 [00:57<00:09,  3.71it/s] 86%|████████▌ | 213/248 [00:57<00:09,  3.60it/s] 86%|████████▋ | 214/248 [00:57<00:09,  3.65it/s] 87%|████████▋ | 215/248 [00:58<00:09,  3.62it/s] 87%|████████▋ | 216/248 [00:58<00:08,  3.69it/s] 88%|████████▊ | 217/248 [00:58<00:08,  3.64it/s] 88%|████████▊ | 218/248 [00:59<00:08,  3.59it/s] 88%|████████▊ | 219/248 [00:59<00:08,  3.55it/s] 89%|████████▊ | 220/248 [00:59<00:07,  3.70it/s] 89%|████████▉ | 221/248 [00:59<00:07,  3.72it/s] 90%|████████▉ | 222/248 [01:00<00:07,  3.60it/s] 90%|████████▉ | 223/248 [01:00<00:06,  3.65it/s] 90%|█████████ | 224/248 [01:00<00:06,  3.61it/s] 91%|█████████ | 225/248 [01:00<00:06,  3.71it/s] 91%|█████████ | 226/248 [01:01<00:06,  3.65it/s] 92%|█████████▏| 227/248 [01:01<00:05,  3.60it/s] 92%|█████████▏| 228/248 [01:01<00:05,  3.55it/s] 92%|█████████▏| 229/248 [01:02<00:05,  3.69it/s] 93%|█████████▎| 230/248 [01:02<00:04,  3.71it/s] 93%|█████████▎| 231/248 [01:02<00:04,  3.60it/s] 94%|█████████▎| 232/248 [01:02<00:04,  3.66it/s] 94%|█████████▍| 233/248 [01:03<00:04,  3.61it/s] 94%|█████████▍| 234/248 [01:03<00:03,  3.68it/s] 95%|█████████▍| 235/248 [01:03<00:03,  3.63it/s] 95%|█████████▌| 236/248 [01:04<00:03,  3.59it/s] 96%|█████████▌| 237/248 [01:04<00:03,  3.54it/s] 96%|█████████▌| 238/248 [01:04<00:02,  3.68it/s] 96%|█████████▋| 239/248 [01:04<00:02,  3.71it/s] 97%|█████████▋| 240/248 [01:05<00:02,  3.59it/s] 97%|█████████▋| 241/248 [01:05<00:01,  3.65it/s] 98%|█████████▊| 242/248 [01:05<00:01,  3.61it/s] 98%|█████████▊| 243/248 [01:05<00:01,  3.70it/s] 98%|█████████▊| 244/248 [01:06<00:01,  3.65it/s] 99%|█████████▉| 245/248 [01:06<00:00,  3.59it/s] 99%|█████████▉| 246/248 [01:06<00:00,  3.55it/s]100%|█████████▉| 247/248 [01:07<00:00,  3.68it/s]100%|██████████| 248/248 [01:07<00:00,  3.71it/s]accuracy:  0.6088709677419355
100%|██████████| 248/248 [01:11<00:00,  3.49it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 236
Training dataset size: 144, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 03:49:25,777] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 03:49:25,909] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 03:49:25,913] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<01:01,  2.68s/it]  8%|▊         | 2/24 [00:05<01:00,  2.73s/it] 12%|█▎        | 3/24 [00:07<00:50,  2.40s/it] 17%|█▋        | 4/24 [00:10<00:51,  2.59s/it] 21%|██        | 5/24 [00:12<00:45,  2.41s/it] 25%|██▌       | 6/24 [00:14<00:42,  2.37s/it] 29%|██▉       | 7/24 [00:17<00:40,  2.40s/it] 33%|███▎      | 8/24 [00:19<00:38,  2.43s/it] 38%|███▊      | 9/24 [00:21<00:35,  2.35s/it] 42%|████▏     | 10/24 [00:24<00:34,  2.50s/it]                                               {'loss': 0.924, 'grad_norm': 10.743551254272461, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:24<00:34,  2.50s/it] 46%|████▌     | 11/24 [00:27<00:32,  2.46s/it] 50%|█████     | 12/24 [00:29<00:29,  2.44s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:33<00:33,  3.02s/it] 58%|█████▊    | 14/24 [00:36<00:29,  2.95s/it] 62%|██████▎   | 15/24 [00:38<00:24,  2.76s/it] 67%|██████▋   | 16/24 [00:40<00:20,  2.55s/it] 71%|███████   | 17/24 [00:43<00:17,  2.50s/it] 75%|███████▌  | 18/24 [00:45<00:14,  2.35s/it] 79%|███████▉  | 19/24 [00:48<00:12,  2.46s/it] 83%|████████▎ | 20/24 [00:50<00:09,  2.43s/it]                                               {'loss': 0.9462, 'grad_norm': 9.455314636230469, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:50<00:09,  2.43s/it] 88%|████████▊ | 21/24 [00:52<00:07,  2.40s/it] 92%|█████████▏| 22/24 [00:55<00:04,  2.48s/it] 96%|█████████▌| 23/24 [00:57<00:02,  2.39s/it]100%|██████████| 24/24 [00:59<00:00,  2.29s/it]                                               {'train_runtime': 60.3257, 'train_samples_per_second': 4.774, 'train_steps_per_second': 0.398, 'train_loss': 1.0042373339335124, 'epoch': 2.0}
100%|██████████| 24/24 [01:00<00:00,  2.29s/it]100%|██████████| 24/24 [01:00<00:00,  2.51s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<02:05,  2.88it/s]  1%|          | 2/361 [00:00<01:49,  3.27it/s]  1%|          | 3/361 [00:00<01:40,  3.57it/s]  1%|          | 4/361 [00:01<01:39,  3.58it/s]  1%|▏         | 5/361 [00:01<01:39,  3.57it/s]  2%|▏         | 6/361 [00:01<01:39,  3.55it/s]  2%|▏         | 7/361 [00:01<01:34,  3.73it/s]  2%|▏         | 8/361 [00:02<01:33,  3.76it/s]  2%|▏         | 9/361 [00:02<01:36,  3.65it/s]  3%|▎         | 10/361 [00:02<01:34,  3.70it/s]  3%|▎         | 11/361 [00:03<01:35,  3.66it/s]  3%|▎         | 12/361 [00:03<01:32,  3.78it/s]  4%|▎         | 13/361 [00:03<01:34,  3.70it/s]  4%|▍         | 14/361 [00:03<01:34,  3.66it/s]  4%|▍         | 15/361 [00:04<01:35,  3.61it/s]  4%|▍         | 16/361 [00:04<01:31,  3.76it/s]  5%|▍         | 17/361 [00:04<01:31,  3.77it/s]  5%|▍         | 18/361 [00:04<01:33,  3.65it/s]  5%|▌         | 19/361 [00:05<01:31,  3.72it/s]  6%|▌         | 20/361 [00:05<01:32,  3.69it/s]  6%|▌         | 21/361 [00:05<01:30,  3.76it/s]  6%|▌         | 22/361 [00:06<01:31,  3.70it/s]  6%|▋         | 23/361 [00:06<01:32,  3.65it/s]  7%|▋         | 24/361 [00:06<01:33,  3.61it/s]  7%|▋         | 25/361 [00:06<01:29,  3.76it/s]  7%|▋         | 26/361 [00:07<01:28,  3.77it/s]  7%|▋         | 27/361 [00:07<01:31,  3.66it/s]  8%|▊         | 28/361 [00:07<01:29,  3.71it/s]  8%|▊         | 29/361 [00:07<01:30,  3.67it/s]  8%|▊         | 30/361 [00:08<01:28,  3.75it/s]  9%|▊         | 31/361 [00:08<01:30,  3.65it/s]  9%|▉         | 32/361 [00:08<01:29,  3.67it/s]  9%|▉         | 33/361 [00:08<01:27,  3.74it/s]  9%|▉         | 34/361 [00:09<01:25,  3.85it/s] 10%|▉         | 35/361 [00:09<01:23,  3.92it/s] 10%|▉         | 36/361 [00:09<01:21,  3.98it/s] 10%|█         | 37/361 [00:09<01:20,  4.02it/s] 11%|█         | 38/361 [00:10<01:19,  4.06it/s] 11%|█         | 39/361 [00:10<01:18,  4.09it/s] 11%|█         | 40/361 [00:10<01:18,  4.11it/s] 11%|█▏        | 41/361 [00:10<01:17,  4.12it/s] 12%|█▏        | 42/361 [00:11<01:17,  4.13it/s] 12%|█▏        | 43/361 [00:11<01:16,  4.13it/s] 12%|█▏        | 44/361 [00:11<01:16,  4.14it/s] 12%|█▏        | 45/361 [00:11<01:16,  4.15it/s] 13%|█▎        | 46/361 [00:12<01:15,  4.15it/s] 13%|█▎        | 47/361 [00:12<01:15,  4.14it/s] 13%|█▎        | 48/361 [00:12<01:15,  4.13it/s] 14%|█▎        | 49/361 [00:12<01:15,  4.12it/s] 14%|█▍        | 50/361 [00:13<01:17,  4.02it/s] 14%|█▍        | 51/361 [00:13<01:19,  3.91it/s] 14%|█▍        | 52/361 [00:13<01:20,  3.82it/s] 15%|█▍        | 53/361 [00:13<01:18,  3.91it/s] 15%|█▍        | 54/361 [00:14<01:18,  3.93it/s] 15%|█▌        | 55/361 [00:14<01:19,  3.86it/s] 16%|█▌        | 56/361 [00:14<01:20,  3.81it/s] 16%|█▌        | 57/361 [00:14<01:21,  3.72it/s] 16%|█▌        | 58/361 [00:15<01:19,  3.84it/s] 16%|█▋        | 59/361 [00:15<01:17,  3.88it/s] 17%|█▋        | 60/361 [00:15<01:18,  3.83it/s] 17%|█▋        | 61/361 [00:16<01:19,  3.79it/s] 17%|█▋        | 62/361 [00:16<01:20,  3.70it/s] 17%|█▋        | 63/361 [00:16<01:17,  3.82it/s] 18%|█▊        | 64/361 [00:16<01:16,  3.87it/s] 18%|█▊        | 65/361 [00:17<01:17,  3.81it/s] 18%|█▊        | 66/361 [00:17<01:18,  3.77it/s] 19%|█▊        | 67/361 [00:17<01:19,  3.68it/s] 19%|█▉        | 68/361 [00:17<01:16,  3.81it/s] 19%|█▉        | 69/361 [00:18<01:15,  3.86it/s] 19%|█▉        | 70/361 [00:18<01:16,  3.82it/s] 20%|█▉        | 71/361 [00:18<01:16,  3.77it/s] 20%|█▉        | 72/361 [00:18<01:18,  3.68it/s] 20%|██        | 73/361 [00:19<01:15,  3.81it/s] 20%|██        | 74/361 [00:19<01:14,  3.84it/s] 21%|██        | 75/361 [00:19<01:15,  3.80it/s] 21%|██        | 76/361 [00:19<01:15,  3.76it/s] 21%|██▏       | 77/361 [00:20<01:17,  3.68it/s] 22%|██▏       | 78/361 [00:20<01:14,  3.79it/s] 22%|██▏       | 79/361 [00:20<01:13,  3.83it/s] 22%|██▏       | 80/361 [00:21<01:13,  3.80it/s] 22%|██▏       | 81/361 [00:21<01:14,  3.76it/s] 23%|██▎       | 82/361 [00:21<01:15,  3.67it/s] 23%|██▎       | 83/361 [00:21<01:13,  3.79it/s] 23%|██▎       | 84/361 [00:22<01:11,  3.87it/s] 24%|██▎       | 85/361 [00:22<01:12,  3.81it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.76it/s] 24%|██▍       | 87/361 [00:22<01:14,  3.68it/s] 24%|██▍       | 88/361 [00:23<01:11,  3.80it/s] 25%|██▍       | 89/361 [00:23<01:10,  3.86it/s] 25%|██▍       | 90/361 [00:23<01:11,  3.81it/s] 25%|██▌       | 91/361 [00:23<01:11,  3.77it/s] 25%|██▌       | 92/361 [00:24<01:13,  3.68it/s] 26%|██▌       | 93/361 [00:24<01:10,  3.81it/s] 26%|██▌       | 94/361 [00:24<01:09,  3.87it/s] 26%|██▋       | 95/361 [00:24<01:09,  3.82it/s] 27%|██▋       | 96/361 [00:25<01:10,  3.77it/s] 27%|██▋       | 97/361 [00:25<01:11,  3.68it/s] 27%|██▋       | 98/361 [00:25<01:09,  3.80it/s] 27%|██▋       | 99/361 [00:26<01:07,  3.86it/s] 28%|██▊       | 100/361 [00:26<01:08,  3.81it/s] 28%|██▊       | 101/361 [00:26<01:09,  3.77it/s] 28%|██▊       | 102/361 [00:26<01:10,  3.68it/s] 29%|██▊       | 103/361 [00:27<01:07,  3.81it/s] 29%|██▉       | 104/361 [00:27<01:06,  3.88it/s] 29%|██▉       | 105/361 [00:27<01:07,  3.82it/s] 29%|██▉       | 106/361 [00:27<01:07,  3.77it/s] 30%|██▉       | 107/361 [00:28<01:08,  3.69it/s] 30%|██▉       | 108/361 [00:28<01:06,  3.81it/s] 30%|███       | 109/361 [00:28<01:04,  3.89it/s] 30%|███       | 110/361 [00:28<01:05,  3.83it/s] 31%|███       | 111/361 [00:29<01:06,  3.76it/s] 31%|███       | 112/361 [00:29<01:07,  3.68it/s] 31%|███▏      | 113/361 [00:29<01:05,  3.80it/s] 32%|███▏      | 114/361 [00:30<01:04,  3.85it/s] 32%|███▏      | 115/361 [00:30<01:04,  3.80it/s] 32%|███▏      | 116/361 [00:30<01:05,  3.76it/s] 32%|███▏      | 117/361 [00:30<01:06,  3.68it/s] 33%|███▎      | 118/361 [00:31<01:03,  3.80it/s] 33%|███▎      | 119/361 [00:31<01:03,  3.83it/s] 33%|███▎      | 120/361 [00:31<01:03,  3.81it/s] 34%|███▎      | 121/361 [00:31<01:03,  3.76it/s] 34%|███▍      | 122/361 [00:32<01:04,  3.68it/s] 34%|███▍      | 123/361 [00:32<01:02,  3.79it/s] 34%|███▍      | 124/361 [00:32<01:01,  3.83it/s] 35%|███▍      | 125/361 [00:32<01:02,  3.78it/s] 35%|███▍      | 126/361 [00:33<01:02,  3.75it/s] 35%|███▌      | 127/361 [00:33<01:03,  3.67it/s] 35%|███▌      | 128/361 [00:33<01:01,  3.80it/s] 36%|███▌      | 129/361 [00:33<01:00,  3.84it/s] 36%|███▌      | 130/361 [00:34<01:00,  3.80it/s] 36%|███▋      | 131/361 [00:34<01:01,  3.76it/s] 37%|███▋      | 132/361 [00:34<01:02,  3.67it/s] 37%|███▋      | 133/361 [00:35<01:00,  3.80it/s] 37%|███▋      | 134/361 [00:35<00:59,  3.82it/s] 37%|███▋      | 135/361 [00:35<00:59,  3.78it/s] 38%|███▊      | 136/361 [00:35<01:00,  3.74it/s] 38%|███▊      | 137/361 [00:36<01:01,  3.66it/s] 38%|███▊      | 138/361 [00:36<00:58,  3.79it/s] 39%|███▊      | 139/361 [00:36<00:57,  3.86it/s] 39%|███▉      | 140/361 [00:36<00:58,  3.81it/s] 39%|███▉      | 141/361 [00:37<00:58,  3.76it/s] 39%|███▉      | 142/361 [00:37<00:59,  3.68it/s] 40%|███▉      | 143/361 [00:37<00:57,  3.80it/s] 40%|███▉      | 144/361 [00:37<00:56,  3.84it/s] 40%|████      | 145/361 [00:38<00:57,  3.77it/s] 40%|████      | 146/361 [00:38<00:56,  3.79it/s] 41%|████      | 147/361 [00:38<00:56,  3.77it/s] 41%|████      | 148/361 [00:39<00:55,  3.87it/s] 41%|████▏     | 149/361 [00:39<00:53,  3.93it/s] 42%|████▏     | 150/361 [00:39<00:53,  3.97it/s] 42%|████▏     | 151/361 [00:39<00:52,  4.01it/s] 42%|████▏     | 152/361 [00:39<00:51,  4.04it/s] 42%|████▏     | 153/361 [00:40<00:51,  4.06it/s] 43%|████▎     | 154/361 [00:40<00:50,  4.07it/s] 43%|████▎     | 155/361 [00:40<00:50,  4.07it/s] 43%|████▎     | 156/361 [00:40<00:50,  4.07it/s] 43%|████▎     | 157/361 [00:41<00:49,  4.09it/s] 44%|████▍     | 158/361 [00:41<00:49,  4.10it/s] 44%|████▍     | 159/361 [00:41<00:49,  4.09it/s] 44%|████▍     | 160/361 [00:41<00:49,  4.08it/s] 45%|████▍     | 161/361 [00:42<00:48,  4.09it/s] 45%|████▍     | 162/361 [00:42<00:48,  4.10it/s] 45%|████▌     | 163/361 [00:42<00:48,  4.09it/s] 45%|████▌     | 164/361 [00:42<00:48,  4.08it/s] 46%|████▌     | 165/361 [00:43<00:48,  4.08it/s] 46%|████▌     | 166/361 [00:43<00:47,  4.09it/s] 46%|████▋     | 167/361 [00:43<00:47,  4.09it/s] 47%|████▋     | 168/361 [00:43<00:47,  4.08it/s] 47%|████▋     | 169/361 [00:44<00:47,  4.08it/s] 47%|████▋     | 170/361 [00:44<00:46,  4.08it/s] 47%|████▋     | 171/361 [00:44<00:46,  4.08it/s] 48%|████▊     | 172/361 [00:44<00:46,  4.08it/s] 48%|████▊     | 173/361 [00:45<00:46,  4.08it/s] 48%|████▊     | 174/361 [00:45<00:45,  4.09it/s] 48%|████▊     | 175/361 [00:45<00:45,  4.08it/s] 49%|████▉     | 176/361 [00:45<00:45,  4.04it/s] 49%|████▉     | 177/361 [00:46<00:47,  3.91it/s] 49%|████▉     | 178/361 [00:46<00:47,  3.83it/s] 50%|████▉     | 179/361 [00:46<00:47,  3.87it/s] 50%|████▉     | 180/361 [00:46<00:47,  3.79it/s] 50%|█████     | 181/361 [00:47<00:48,  3.72it/s] 50%|█████     | 182/361 [00:47<00:47,  3.77it/s] 51%|█████     | 183/361 [00:47<00:47,  3.75it/s] 51%|█████     | 184/361 [00:48<00:47,  3.72it/s] 51%|█████     | 185/361 [00:48<00:48,  3.66it/s] 52%|█████▏    | 186/361 [00:48<00:46,  3.73it/s] 52%|█████▏    | 187/361 [00:48<00:46,  3.70it/s] 52%|█████▏    | 188/361 [00:49<00:47,  3.65it/s] 52%|█████▏    | 189/361 [00:49<00:46,  3.72it/s] 53%|█████▎    | 190/361 [00:49<00:45,  3.73it/s] 53%|█████▎    | 191/361 [00:49<00:45,  3.71it/s] 53%|█████▎    | 192/361 [00:50<00:45,  3.67it/s] 53%|█████▎    | 193/361 [00:50<00:44,  3.76it/s] 54%|█████▎    | 194/361 [00:50<00:44,  3.72it/s] 54%|█████▍    | 195/361 [00:51<00:45,  3.67it/s] 54%|█████▍    | 196/361 [00:51<00:44,  3.75it/s] 55%|█████▍    | 197/361 [00:51<00:43,  3.76it/s] 55%|█████▍    | 198/361 [00:51<00:43,  3.73it/s] 55%|█████▌    | 199/361 [00:52<00:44,  3.65it/s] 55%|█████▌    | 200/361 [00:52<00:42,  3.78it/s] 56%|█████▌    | 201/361 [00:52<00:42,  3.73it/s] 56%|█████▌    | 202/361 [00:52<00:42,  3.73it/s] 56%|█████▌    | 203/361 [00:53<00:42,  3.68it/s] 57%|█████▋    | 204/361 [00:53<00:42,  3.74it/s] 57%|█████▋    | 205/361 [00:53<00:42,  3.70it/s] 57%|█████▋    | 206/361 [00:53<00:42,  3.65it/s] 57%|█████▋    | 207/361 [00:54<00:41,  3.72it/s] 58%|█████▊    | 208/361 [00:54<00:41,  3.73it/s] 58%|█████▊    | 209/361 [00:54<00:41,  3.70it/s] 58%|█████▊    | 210/361 [00:55<00:41,  3.64it/s] 58%|█████▊    | 211/361 [00:55<00:40,  3.73it/s] 59%|█████▊    | 212/361 [00:55<00:40,  3.70it/s] 59%|█████▉    | 213/361 [00:55<00:40,  3.66it/s] 59%|█████▉    | 214/361 [00:56<00:39,  3.73it/s] 60%|█████▉    | 215/361 [00:56<00:39,  3.73it/s] 60%|█████▉    | 216/361 [00:56<00:39,  3.71it/s] 60%|██████    | 217/361 [00:56<00:39,  3.66it/s] 60%|██████    | 218/361 [00:57<00:38,  3.74it/s] 61%|██████    | 219/361 [00:57<00:38,  3.70it/s] 61%|██████    | 220/361 [00:57<00:38,  3.64it/s] 61%|██████    | 221/361 [00:58<00:37,  3.71it/s] 61%|██████▏   | 222/361 [00:58<00:37,  3.71it/s] 62%|██████▏   | 223/361 [00:58<00:37,  3.69it/s] 62%|██████▏   | 224/361 [00:58<00:37,  3.65it/s] 62%|██████▏   | 225/361 [00:59<00:36,  3.74it/s] 63%|██████▎   | 226/361 [00:59<00:36,  3.70it/s] 63%|██████▎   | 227/361 [00:59<00:36,  3.65it/s] 63%|██████▎   | 228/361 [00:59<00:35,  3.72it/s] 63%|██████▎   | 229/361 [01:00<00:35,  3.72it/s] 64%|██████▎   | 230/361 [01:00<00:35,  3.70it/s] 64%|██████▍   | 231/361 [01:00<00:35,  3.65it/s] 64%|██████▍   | 232/361 [01:00<00:34,  3.73it/s] 65%|██████▍   | 233/361 [01:01<00:34,  3.69it/s] 65%|██████▍   | 234/361 [01:01<00:34,  3.65it/s] 65%|██████▌   | 235/361 [01:01<00:33,  3.73it/s] 65%|██████▌   | 236/361 [01:02<00:33,  3.72it/s] 66%|██████▌   | 237/361 [01:02<00:33,  3.70it/s] 66%|██████▌   | 238/361 [01:02<00:33,  3.66it/s] 66%|██████▌   | 239/361 [01:02<00:32,  3.72it/s] 66%|██████▋   | 240/361 [01:03<00:32,  3.70it/s] 67%|██████▋   | 241/361 [01:03<00:32,  3.65it/s] 67%|██████▋   | 242/361 [01:03<00:31,  3.72it/s] 67%|██████▋   | 243/361 [01:03<00:31,  3.73it/s] 68%|██████▊   | 244/361 [01:04<00:31,  3.70it/s] 68%|██████▊   | 245/361 [01:04<00:31,  3.64it/s] 68%|██████▊   | 246/361 [01:04<00:30,  3.75it/s] 68%|██████▊   | 247/361 [01:05<00:30,  3.71it/s] 69%|██████▊   | 248/361 [01:05<00:30,  3.70it/s] 69%|██████▉   | 249/361 [01:05<00:30,  3.66it/s] 69%|██████▉   | 250/361 [01:05<00:29,  3.73it/s] 70%|██████▉   | 251/361 [01:06<00:29,  3.70it/s] 70%|██████▉   | 252/361 [01:06<00:29,  3.65it/s] 70%|███████   | 253/361 [01:06<00:29,  3.72it/s] 70%|███████   | 254/361 [01:06<00:28,  3.71it/s] 71%|███████   | 255/361 [01:07<00:28,  3.70it/s] 71%|███████   | 256/361 [01:07<00:28,  3.66it/s] 71%|███████   | 257/361 [01:07<00:27,  3.75it/s] 71%|███████▏  | 258/361 [01:08<00:27,  3.71it/s] 72%|███████▏  | 259/361 [01:08<00:27,  3.68it/s] 72%|███████▏  | 260/361 [01:08<00:27,  3.73it/s] 72%|███████▏  | 261/361 [01:08<00:26,  3.73it/s] 73%|███████▎  | 262/361 [01:09<00:26,  3.70it/s] 73%|███████▎  | 263/361 [01:09<00:26,  3.65it/s] 73%|███████▎  | 264/361 [01:09<00:25,  3.74it/s] 73%|███████▎  | 265/361 [01:09<00:25,  3.71it/s] 74%|███████▎  | 266/361 [01:10<00:25,  3.66it/s] 74%|███████▍  | 267/361 [01:10<00:25,  3.73it/s] 74%|███████▍  | 268/361 [01:10<00:25,  3.72it/s] 75%|███████▍  | 269/361 [01:11<00:24,  3.69it/s] 75%|███████▍  | 270/361 [01:11<00:24,  3.66it/s] 75%|███████▌  | 271/361 [01:11<00:24,  3.75it/s] 75%|███████▌  | 272/361 [01:11<00:24,  3.70it/s] 76%|███████▌  | 273/361 [01:12<00:24,  3.65it/s] 76%|███████▌  | 274/361 [01:12<00:23,  3.72it/s] 76%|███████▌  | 275/361 [01:12<00:23,  3.71it/s] 76%|███████▋  | 276/361 [01:12<00:22,  3.70it/s] 77%|███████▋  | 277/361 [01:13<00:22,  3.65it/s] 77%|███████▋  | 278/361 [01:13<00:22,  3.72it/s] 77%|███████▋  | 279/361 [01:13<00:22,  3.69it/s] 78%|███████▊  | 280/361 [01:13<00:22,  3.64it/s] 78%|███████▊  | 281/361 [01:14<00:21,  3.71it/s] 78%|███████▊  | 282/361 [01:14<00:21,  3.71it/s] 78%|███████▊  | 283/361 [01:14<00:21,  3.68it/s] 79%|███████▊  | 284/361 [01:15<00:21,  3.63it/s] 79%|███████▉  | 285/361 [01:15<00:20,  3.75it/s] 79%|███████▉  | 286/361 [01:15<00:20,  3.71it/s] 80%|███████▉  | 287/361 [01:15<00:19,  3.71it/s] 80%|███████▉  | 288/361 [01:16<00:19,  3.66it/s] 80%|████████  | 289/361 [01:16<00:19,  3.72it/s] 80%|████████  | 290/361 [01:16<00:19,  3.69it/s] 81%|████████  | 291/361 [01:16<00:19,  3.64it/s] 81%|████████  | 292/361 [01:17<00:18,  3.71it/s] 81%|████████  | 293/361 [01:17<00:18,  3.71it/s] 81%|████████▏ | 294/361 [01:17<00:18,  3.69it/s] 82%|████████▏ | 295/361 [01:18<00:18,  3.65it/s] 82%|████████▏ | 296/361 [01:18<00:17,  3.73it/s] 82%|████████▏ | 297/361 [01:18<00:17,  3.69it/s] 83%|████████▎ | 298/361 [01:18<00:17,  3.64it/s] 83%|████████▎ | 299/361 [01:19<00:16,  3.71it/s] 83%|████████▎ | 300/361 [01:19<00:16,  3.72it/s] 83%|████████▎ | 301/361 [01:19<00:16,  3.70it/s] 84%|████████▎ | 302/361 [01:19<00:16,  3.66it/s] 84%|████████▍ | 303/361 [01:20<00:15,  3.73it/s] 84%|████████▍ | 304/361 [01:20<00:15,  3.70it/s] 84%|████████▍ | 305/361 [01:20<00:15,  3.66it/s] 85%|████████▍ | 306/361 [01:21<00:14,  3.73it/s] 85%|████████▌ | 307/361 [01:21<00:14,  3.73it/s] 85%|████████▌ | 308/361 [01:21<00:14,  3.70it/s] 86%|████████▌ | 309/361 [01:21<00:14,  3.63it/s] 86%|████████▌ | 310/361 [01:22<00:13,  3.75it/s] 86%|████████▌ | 311/361 [01:22<00:13,  3.71it/s] 86%|████████▋ | 312/361 [01:22<00:13,  3.70it/s] 87%|████████▋ | 313/361 [01:22<00:13,  3.65it/s] 87%|████████▋ | 314/361 [01:23<00:12,  3.72it/s] 87%|████████▋ | 315/361 [01:23<00:12,  3.69it/s] 88%|████████▊ | 316/361 [01:23<00:12,  3.66it/s] 88%|████████▊ | 317/361 [01:23<00:11,  3.72it/s] 88%|████████▊ | 318/361 [01:24<00:11,  3.72it/s] 88%|████████▊ | 319/361 [01:24<00:11,  3.70it/s] 89%|████████▊ | 320/361 [01:24<00:11,  3.66it/s] 89%|████████▉ | 321/361 [01:25<00:10,  3.76it/s] 89%|████████▉ | 322/361 [01:25<00:10,  3.71it/s] 89%|████████▉ | 323/361 [01:25<00:10,  3.66it/s] 90%|████████▉ | 324/361 [01:25<00:09,  3.72it/s] 90%|█████████ | 325/361 [01:26<00:09,  3.71it/s] 90%|█████████ | 326/361 [01:26<00:09,  3.70it/s] 91%|█████████ | 327/361 [01:26<00:09,  3.65it/s] 91%|█████████ | 328/361 [01:26<00:08,  3.73it/s] 91%|█████████ | 329/361 [01:27<00:08,  3.70it/s] 91%|█████████▏| 330/361 [01:27<00:08,  3.65it/s] 92%|█████████▏| 331/361 [01:27<00:08,  3.72it/s] 92%|█████████▏| 332/361 [01:28<00:07,  3.72it/s] 92%|█████████▏| 333/361 [01:28<00:07,  3.70it/s] 93%|█████████▎| 334/361 [01:28<00:07,  3.65it/s] 93%|█████████▎| 335/361 [01:28<00:07,  3.71it/s] 93%|█████████▎| 336/361 [01:29<00:06,  3.68it/s] 93%|█████████▎| 337/361 [01:29<00:06,  3.65it/s] 94%|█████████▎| 338/361 [01:29<00:06,  3.71it/s] 94%|█████████▍| 339/361 [01:29<00:05,  3.72it/s] 94%|█████████▍| 340/361 [01:30<00:05,  3.70it/s] 94%|█████████▍| 341/361 [01:30<00:05,  3.63it/s] 95%|█████████▍| 342/361 [01:30<00:05,  3.75it/s] 95%|█████████▌| 343/361 [01:31<00:04,  3.71it/s] 95%|█████████▌| 344/361 [01:31<00:04,  3.70it/s] 96%|█████████▌| 345/361 [01:31<00:04,  3.66it/s] 96%|█████████▌| 346/361 [01:31<00:04,  3.71it/s] 96%|█████████▌| 347/361 [01:32<00:03,  3.69it/s] 96%|█████████▋| 348/361 [01:32<00:03,  3.65it/s] 97%|█████████▋| 349/361 [01:32<00:03,  3.72it/s] 97%|█████████▋| 350/361 [01:32<00:02,  3.71it/s] 97%|█████████▋| 351/361 [01:33<00:02,  3.71it/s] 98%|█████████▊| 352/361 [01:33<00:02,  3.72it/s] 98%|█████████▊| 353/361 [01:33<00:02,  3.81it/s] 98%|█████████▊| 354/361 [01:33<00:01,  3.89it/s] 98%|█████████▊| 355/361 [01:34<00:01,  3.95it/s] 99%|█████████▊| 356/361 [01:34<00:01,  3.99it/s] 99%|█████████▉| 357/361 [01:34<00:00,  4.01it/s] 99%|█████████▉| 358/361 [01:34<00:00,  4.04it/s] 99%|█████████▉| 359/361 [01:35<00:00,  4.06it/s]100%|█████████▉| 360/361 [01:35<00:00,  4.07it/s]100%|██████████| 361/361 [01:35<00:00,  4.08it/s]accuracy:  0.590027700831025
100%|██████████| 361/361 [01:40<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 188
Training dataset size: 144, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:52:33,451] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 03:52:33,478] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:52:33,808] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<01:02,  2.72s/it]  8%|▊         | 2/24 [00:05<00:59,  2.72s/it] 12%|█▎        | 3/24 [00:08<00:58,  2.80s/it] 17%|█▋        | 4/24 [00:10<00:53,  2.69s/it] 21%|██        | 5/24 [00:13<00:51,  2.72s/it] 25%|██▌       | 6/24 [00:16<00:48,  2.71s/it] 29%|██▉       | 7/24 [00:18<00:45,  2.65s/it] 33%|███▎      | 8/24 [00:21<00:42,  2.64s/it] 38%|███▊      | 9/24 [00:24<00:41,  2.78s/it] 42%|████▏     | 10/24 [00:27<00:38,  2.75s/it]                                               {'loss': 0.6755, 'grad_norm': 6.333878517150879, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:27<00:38,  2.75s/it] 46%|████▌     | 11/24 [00:29<00:33,  2.61s/it] 50%|█████     | 12/24 [00:31<00:30,  2.53s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:34<00:29,  2.68s/it] 58%|█████▊    | 14/24 [00:37<00:26,  2.63s/it] 62%|██████▎   | 15/24 [00:40<00:25,  2.85s/it] 67%|██████▋   | 16/24 [00:43<00:22,  2.77s/it] 71%|███████   | 17/24 [00:46<00:19,  2.75s/it] 75%|███████▌  | 18/24 [00:48<00:16,  2.77s/it] 79%|███████▉  | 19/24 [00:51<00:13,  2.75s/it] 83%|████████▎ | 20/24 [00:54<00:11,  2.77s/it]                                               {'loss': 0.5884, 'grad_norm': 5.451286792755127, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:54<00:11,  2.77s/it] 88%|████████▊ | 21/24 [00:57<00:08,  2.72s/it] 92%|█████████▏| 22/24 [01:00<00:05,  2.92s/it] 96%|█████████▌| 23/24 [01:03<00:02,  2.99s/it]100%|██████████| 24/24 [01:05<00:00,  2.74s/it]                                               {'train_runtime': 66.3742, 'train_samples_per_second': 4.339, 'train_steps_per_second': 0.362, 'train_loss': 0.5829070756832758, 'epoch': 2.0}
100%|██████████| 24/24 [01:06<00:00,  2.74s/it]100%|██████████| 24/24 [01:06<00:00,  2.76s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:09,  3.03it/s]  1%|          | 2/212 [00:00<01:02,  3.39it/s]  1%|▏         | 3/212 [00:00<00:58,  3.57it/s]  2%|▏         | 4/212 [00:01<00:57,  3.60it/s]  2%|▏         | 5/212 [00:01<00:55,  3.74it/s]  3%|▎         | 6/212 [00:01<00:55,  3.73it/s]  3%|▎         | 7/212 [00:01<00:55,  3.71it/s]  4%|▍         | 8/212 [00:02<00:55,  3.66it/s]  4%|▍         | 9/212 [00:02<00:53,  3.81it/s]  5%|▍         | 10/212 [00:02<00:52,  3.85it/s]  5%|▌         | 11/212 [00:02<00:53,  3.78it/s]  6%|▌         | 12/212 [00:03<00:53,  3.76it/s]  6%|▌         | 13/212 [00:03<00:53,  3.70it/s]  7%|▋         | 14/212 [00:03<00:51,  3.82it/s]  7%|▋         | 15/212 [00:04<00:51,  3.83it/s]  8%|▊         | 16/212 [00:04<00:51,  3.78it/s]  8%|▊         | 17/212 [00:04<00:51,  3.79it/s]  8%|▊         | 18/212 [00:04<00:52,  3.73it/s]  9%|▉         | 19/212 [00:05<00:50,  3.83it/s]  9%|▉         | 20/212 [00:05<00:50,  3.78it/s] 10%|▉         | 21/212 [00:05<00:51,  3.74it/s] 10%|█         | 22/212 [00:05<00:51,  3.68it/s] 11%|█         | 23/212 [00:06<00:49,  3.80it/s] 11%|█▏        | 24/212 [00:06<00:49,  3.82it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.74it/s] 12%|█▏        | 26/212 [00:06<00:49,  3.75it/s] 13%|█▎        | 27/212 [00:07<00:50,  3.69it/s] 13%|█▎        | 28/212 [00:07<00:48,  3.81it/s] 14%|█▎        | 29/212 [00:07<00:47,  3.83it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.77it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.78it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.71it/s] 16%|█▌        | 33/212 [00:08<00:46,  3.81it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.77it/s] 17%|█▋        | 35/212 [00:09<00:47,  3.74it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.67it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.79it/s] 18%|█▊        | 38/212 [00:10<00:45,  3.81it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.75it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.74it/s] 19%|█▉        | 41/212 [00:10<00:46,  3.70it/s] 20%|█▉        | 42/212 [00:11<00:44,  3.82it/s] 20%|██        | 43/212 [00:11<00:44,  3.83it/s] 21%|██        | 44/212 [00:11<00:44,  3.77it/s] 21%|██        | 45/212 [00:11<00:44,  3.79it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.72it/s] 22%|██▏       | 47/212 [00:12<00:43,  3.83it/s] 23%|██▎       | 48/212 [00:12<00:43,  3.77it/s] 23%|██▎       | 49/212 [00:13<00:43,  3.73it/s] 24%|██▎       | 50/212 [00:13<00:44,  3.65it/s] 24%|██▍       | 51/212 [00:13<00:42,  3.79it/s] 25%|██▍       | 52/212 [00:13<00:41,  3.81it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.74it/s] 25%|██▌       | 54/212 [00:14<00:42,  3.74it/s] 26%|██▌       | 55/212 [00:14<00:42,  3.67it/s] 26%|██▋       | 56/212 [00:14<00:41,  3.79it/s] 27%|██▋       | 57/212 [00:15<00:40,  3.80it/s] 27%|██▋       | 58/212 [00:15<00:41,  3.73it/s] 28%|██▊       | 59/212 [00:15<00:41,  3.69it/s] 28%|██▊       | 60/212 [00:15<00:39,  3.80it/s] 29%|██▉       | 61/212 [00:16<00:38,  3.88it/s] 29%|██▉       | 62/212 [00:16<00:37,  3.95it/s] 30%|██▉       | 63/212 [00:16<00:37,  4.00it/s] 30%|███       | 64/212 [00:16<00:36,  4.03it/s] 31%|███       | 65/212 [00:17<00:36,  4.05it/s] 31%|███       | 66/212 [00:17<00:35,  4.06it/s] 32%|███▏      | 67/212 [00:17<00:35,  4.08it/s] 32%|███▏      | 68/212 [00:17<00:35,  4.09it/s] 33%|███▎      | 69/212 [00:18<00:34,  4.10it/s] 33%|███▎      | 70/212 [00:18<00:34,  4.10it/s] 33%|███▎      | 71/212 [00:18<00:34,  4.09it/s] 34%|███▍      | 72/212 [00:18<00:34,  4.10it/s] 34%|███▍      | 73/212 [00:19<00:33,  4.10it/s] 35%|███▍      | 74/212 [00:19<00:33,  4.11it/s] 35%|███▌      | 75/212 [00:19<00:33,  4.11it/s] 36%|███▌      | 76/212 [00:19<00:33,  4.10it/s] 36%|███▋      | 77/212 [00:20<00:32,  4.10it/s] 37%|███▋      | 78/212 [00:20<00:32,  4.10it/s] 37%|███▋      | 79/212 [00:20<00:32,  4.11it/s] 38%|███▊      | 80/212 [00:20<00:32,  4.12it/s] 38%|███▊      | 81/212 [00:21<00:31,  4.11it/s] 39%|███▊      | 82/212 [00:21<00:31,  4.11it/s] 39%|███▉      | 83/212 [00:21<00:31,  4.10it/s] 40%|███▉      | 84/212 [00:21<00:31,  4.10it/s] 40%|████      | 85/212 [00:22<00:30,  4.11it/s] 41%|████      | 86/212 [00:22<00:30,  4.11it/s] 41%|████      | 87/212 [00:22<00:30,  4.11it/s] 42%|████▏     | 88/212 [00:22<00:30,  4.10it/s] 42%|████▏     | 89/212 [00:23<00:29,  4.10it/s] 42%|████▏     | 90/212 [00:23<00:29,  4.11it/s] 43%|████▎     | 91/212 [00:23<00:29,  4.12it/s] 43%|████▎     | 92/212 [00:23<00:29,  4.12it/s] 44%|████▍     | 93/212 [00:24<00:28,  4.11it/s] 44%|████▍     | 94/212 [00:24<00:28,  4.10it/s] 45%|████▍     | 95/212 [00:24<00:28,  4.10it/s] 45%|████▌     | 96/212 [00:24<00:28,  4.11it/s] 46%|████▌     | 97/212 [00:25<00:27,  4.11it/s] 46%|████▌     | 98/212 [00:25<00:27,  4.10it/s] 47%|████▋     | 99/212 [00:25<00:27,  4.09it/s] 47%|████▋     | 100/212 [00:25<00:27,  4.09it/s] 48%|████▊     | 101/212 [00:26<00:27,  4.01it/s] 48%|████▊     | 102/212 [00:26<00:28,  3.91it/s] 49%|████▊     | 103/212 [00:26<00:28,  3.79it/s] 49%|████▉     | 104/212 [00:26<00:27,  3.88it/s] 50%|████▉     | 105/212 [00:27<00:27,  3.84it/s] 50%|█████     | 106/212 [00:27<00:28,  3.74it/s] 50%|█████     | 107/212 [00:27<00:28,  3.74it/s] 51%|█████     | 108/212 [00:27<00:27,  3.84it/s] 51%|█████▏    | 109/212 [00:28<00:26,  3.92it/s] 52%|█████▏    | 110/212 [00:28<00:25,  3.96it/s] 52%|█████▏    | 111/212 [00:28<00:25,  3.99it/s] 53%|█████▎    | 112/212 [00:28<00:24,  4.02it/s] 53%|█████▎    | 113/212 [00:29<00:24,  4.05it/s] 54%|█████▍    | 114/212 [00:29<00:24,  4.07it/s] 54%|█████▍    | 115/212 [00:29<00:23,  4.07it/s] 55%|█████▍    | 116/212 [00:29<00:23,  4.07it/s] 55%|█████▌    | 117/212 [00:30<00:23,  4.08it/s] 56%|█████▌    | 118/212 [00:30<00:22,  4.09it/s] 56%|█████▌    | 119/212 [00:30<00:22,  4.08it/s] 57%|█████▋    | 120/212 [00:30<00:22,  4.08it/s] 57%|█████▋    | 121/212 [00:31<00:22,  4.08it/s] 58%|█████▊    | 122/212 [00:31<00:21,  4.09it/s] 58%|█████▊    | 123/212 [00:31<00:21,  4.09it/s] 58%|█████▊    | 124/212 [00:31<00:21,  4.08it/s] 59%|█████▉    | 125/212 [00:32<00:21,  4.08it/s] 59%|█████▉    | 126/212 [00:32<00:21,  4.09it/s] 60%|█████▉    | 127/212 [00:32<00:20,  4.09it/s] 60%|██████    | 128/212 [00:32<00:20,  4.08it/s] 61%|██████    | 129/212 [00:32<00:20,  4.08it/s] 61%|██████▏   | 130/212 [00:33<00:20,  4.08it/s] 62%|██████▏   | 131/212 [00:33<00:19,  4.08it/s] 62%|██████▏   | 132/212 [00:33<00:19,  4.08it/s] 63%|██████▎   | 133/212 [00:33<00:19,  4.08it/s] 63%|██████▎   | 134/212 [00:34<00:19,  4.09it/s] 64%|██████▎   | 135/212 [00:34<00:18,  4.09it/s] 64%|██████▍   | 136/212 [00:34<00:18,  4.08it/s] 65%|██████▍   | 137/212 [00:34<00:18,  4.08it/s] 65%|██████▌   | 138/212 [00:35<00:18,  4.10it/s] 66%|██████▌   | 139/212 [00:35<00:17,  4.10it/s] 66%|██████▌   | 140/212 [00:35<00:17,  4.10it/s] 67%|██████▋   | 141/212 [00:35<00:17,  4.09it/s] 67%|██████▋   | 142/212 [00:36<00:17,  4.09it/s] 67%|██████▋   | 143/212 [00:36<00:16,  4.09it/s] 68%|██████▊   | 144/212 [00:36<00:16,  4.09it/s] 68%|██████▊   | 145/212 [00:36<00:16,  4.08it/s] 69%|██████▉   | 146/212 [00:37<00:16,  4.08it/s] 69%|██████▉   | 147/212 [00:37<00:15,  4.09it/s] 70%|██████▉   | 148/212 [00:37<00:15,  4.09it/s] 70%|███████   | 149/212 [00:37<00:15,  4.08it/s] 71%|███████   | 150/212 [00:38<00:15,  4.08it/s] 71%|███████   | 151/212 [00:38<00:15,  4.06it/s] 72%|███████▏  | 152/212 [00:38<00:15,  3.94it/s] 72%|███████▏  | 153/212 [00:38<00:15,  3.87it/s] 73%|███████▎  | 154/212 [00:39<00:15,  3.84it/s] 73%|███████▎  | 155/212 [00:39<00:14,  3.87it/s] 74%|███████▎  | 156/212 [00:39<00:14,  3.76it/s] 74%|███████▍  | 157/212 [00:40<00:14,  3.68it/s] 75%|███████▍  | 158/212 [00:40<00:14,  3.61it/s] 75%|███████▌  | 159/212 [00:40<00:14,  3.74it/s] 75%|███████▌  | 160/212 [00:40<00:13,  3.75it/s] 76%|███████▌  | 161/212 [00:41<00:14,  3.63it/s] 76%|███████▋  | 162/212 [00:41<00:13,  3.66it/s] 77%|███████▋  | 163/212 [00:41<00:13,  3.63it/s] 77%|███████▋  | 164/212 [00:41<00:12,  3.75it/s] 78%|███████▊  | 165/212 [00:42<00:12,  3.66it/s] 78%|███████▊  | 166/212 [00:42<00:12,  3.61it/s] 79%|███████▉  | 167/212 [00:42<00:12,  3.56it/s] 79%|███████▉  | 168/212 [00:43<00:11,  3.70it/s] 80%|███████▉  | 169/212 [00:43<00:11,  3.75it/s] 80%|████████  | 170/212 [00:43<00:11,  3.67it/s] 81%|████████  | 171/212 [00:43<00:11,  3.62it/s] 81%|████████  | 172/212 [00:44<00:11,  3.57it/s] 82%|████████▏ | 173/212 [00:44<00:10,  3.72it/s] 82%|████████▏ | 174/212 [00:44<00:10,  3.75it/s] 83%|████████▎ | 175/212 [00:44<00:10,  3.63it/s] 83%|████████▎ | 176/212 [00:45<00:09,  3.65it/s] 83%|████████▎ | 177/212 [00:45<00:09,  3.62it/s] 84%|████████▍ | 178/212 [00:45<00:09,  3.74it/s] 84%|████████▍ | 179/212 [00:46<00:08,  3.67it/s] 85%|████████▍ | 180/212 [00:46<00:08,  3.62it/s] 85%|████████▌ | 181/212 [00:46<00:08,  3.69it/s] 86%|████████▌ | 182/212 [00:46<00:08,  3.66it/s] 86%|████████▋ | 183/212 [00:47<00:07,  3.72it/s] 87%|████████▋ | 184/212 [00:47<00:07,  3.66it/s] 87%|████████▋ | 185/212 [00:47<00:07,  3.60it/s] 88%|████████▊ | 186/212 [00:47<00:07,  3.56it/s] 88%|████████▊ | 187/212 [00:48<00:06,  3.71it/s] 89%|████████▊ | 188/212 [00:48<00:06,  3.73it/s] 89%|████████▉ | 189/212 [00:48<00:06,  3.62it/s] 90%|████████▉ | 190/212 [00:49<00:06,  3.66it/s] 90%|█████████ | 191/212 [00:49<00:05,  3.61it/s] 91%|█████████ | 192/212 [00:49<00:05,  3.73it/s] 91%|█████████ | 193/212 [00:49<00:05,  3.67it/s] 92%|█████████▏| 194/212 [00:50<00:04,  3.61it/s] 92%|█████████▏| 195/212 [00:50<00:04,  3.57it/s] 92%|█████████▏| 196/212 [00:50<00:04,  3.71it/s] 93%|█████████▎| 197/212 [00:50<00:04,  3.74it/s] 93%|█████████▎| 198/212 [00:51<00:03,  3.61it/s] 94%|█████████▍| 199/212 [00:51<00:03,  3.63it/s] 94%|█████████▍| 200/212 [00:51<00:03,  3.59it/s] 95%|█████████▍| 201/212 [00:52<00:02,  3.71it/s] 95%|█████████▌| 202/212 [00:52<00:02,  3.73it/s] 96%|█████████▌| 203/212 [00:52<00:02,  3.61it/s] 96%|█████████▌| 204/212 [00:52<00:02,  3.66it/s] 97%|█████████▋| 205/212 [00:53<00:01,  3.62it/s] 97%|█████████▋| 206/212 [00:53<00:01,  3.71it/s] 98%|█████████▊| 207/212 [00:53<00:01,  3.66it/s] 98%|█████████▊| 208/212 [00:53<00:01,  3.60it/s] 99%|█████████▊| 209/212 [00:54<00:00,  3.56it/s] 99%|█████████▉| 210/212 [00:54<00:00,  3.71it/s]100%|█████████▉| 211/212 [00:54<00:00,  3.73it/s]100%|██████████| 212/212 [00:55<00:00,  3.61it/s]accuracy:  0.8632075471698113
100%|██████████| 212/212 [00:58<00:00,  3.64it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 134
Training dataset size: 144, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.13s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:55:04,424] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[2025-03-12 03:55:04,575] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:55:05,382] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:59,  2.60s/it]  8%|▊         | 2/24 [00:05<00:58,  2.64s/it] 12%|█▎        | 3/24 [00:08<01:02,  2.96s/it] 17%|█▋        | 4/24 [00:11<00:56,  2.81s/it] 21%|██        | 5/24 [00:13<00:51,  2.71s/it] 25%|██▌       | 6/24 [00:16<00:48,  2.69s/it] 29%|██▉       | 7/24 [00:18<00:45,  2.65s/it] 33%|███▎      | 8/24 [00:21<00:42,  2.63s/it] 38%|███▊      | 9/24 [00:24<00:39,  2.63s/it] 42%|████▏     | 10/24 [00:26<00:37,  2.65s/it]                                               {'loss': 0.8131, 'grad_norm': 14.212677955627441, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:26<00:37,  2.65s/it] 46%|████▌     | 11/24 [00:29<00:33,  2.57s/it] 50%|█████     | 12/24 [00:31<00:29,  2.48s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:34<00:29,  2.64s/it] 58%|█████▊    | 14/24 [00:37<00:27,  2.74s/it] 62%|██████▎   | 15/24 [00:40<00:25,  2.80s/it] 67%|██████▋   | 16/24 [00:43<00:22,  2.76s/it] 71%|███████   | 17/24 [00:45<00:18,  2.71s/it] 75%|███████▌  | 18/24 [00:47<00:15,  2.55s/it] 79%|███████▉  | 19/24 [00:50<00:13,  2.63s/it] 83%|████████▎ | 20/24 [00:53<00:10,  2.59s/it]                                               {'loss': 0.7256, 'grad_norm': 3.063796043395996, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:53<00:10,  2.59s/it] 88%|████████▊ | 21/24 [00:55<00:07,  2.50s/it] 92%|█████████▏| 22/24 [00:58<00:05,  2.67s/it] 96%|█████████▌| 23/24 [01:01<00:02,  2.68s/it]100%|██████████| 24/24 [01:04<00:00,  2.73s/it]                                               {'train_runtime': 64.7065, 'train_samples_per_second': 4.451, 'train_steps_per_second': 0.371, 'train_loss': 0.7479523619016012, 'epoch': 2.0}
100%|██████████| 24/24 [01:04<00:00,  2.73s/it]100%|██████████| 24/24 [01:04<00:00,  2.69s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:50,  3.22it/s]  1%|          | 2/163 [00:00<00:43,  3.69it/s]  2%|▏         | 3/163 [00:00<00:41,  3.88it/s]  2%|▏         | 4/163 [00:01<00:40,  3.93it/s]  3%|▎         | 5/163 [00:01<00:39,  4.00it/s]  4%|▎         | 6/163 [00:01<00:38,  4.04it/s]  4%|▍         | 7/163 [00:01<00:38,  4.06it/s]  5%|▍         | 8/163 [00:02<00:38,  4.08it/s]  6%|▌         | 9/163 [00:02<00:37,  4.10it/s]  6%|▌         | 10/163 [00:02<00:37,  4.12it/s]  7%|▋         | 11/163 [00:02<00:36,  4.13it/s]  7%|▋         | 12/163 [00:02<00:36,  4.14it/s]  8%|▊         | 13/163 [00:03<00:36,  4.15it/s]  9%|▊         | 14/163 [00:03<00:35,  4.15it/s]  9%|▉         | 15/163 [00:03<00:35,  4.15it/s] 10%|▉         | 16/163 [00:03<00:35,  4.15it/s] 10%|█         | 17/163 [00:04<00:35,  4.15it/s] 11%|█         | 18/163 [00:04<00:35,  4.14it/s] 12%|█▏        | 19/163 [00:04<00:34,  4.13it/s] 12%|█▏        | 20/163 [00:04<00:34,  4.12it/s] 13%|█▎        | 21/163 [00:05<00:34,  4.12it/s] 13%|█▎        | 22/163 [00:05<00:34,  4.13it/s] 14%|█▍        | 23/163 [00:05<00:33,  4.14it/s] 15%|█▍        | 24/163 [00:05<00:33,  4.14it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.15it/s] 16%|█▌        | 26/163 [00:06<00:33,  4.15it/s] 17%|█▋        | 27/163 [00:06<00:32,  4.15it/s] 17%|█▋        | 28/163 [00:06<00:32,  4.15it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.14it/s] 18%|█▊        | 30/163 [00:07<00:32,  4.13it/s] 19%|█▉        | 31/163 [00:07<00:32,  4.12it/s] 20%|█▉        | 32/163 [00:07<00:31,  4.12it/s] 20%|██        | 33/163 [00:08<00:31,  4.12it/s] 21%|██        | 34/163 [00:08<00:31,  4.13it/s] 21%|██▏       | 35/163 [00:08<00:30,  4.14it/s] 22%|██▏       | 36/163 [00:08<00:30,  4.15it/s] 23%|██▎       | 37/163 [00:09<00:30,  4.15it/s] 23%|██▎       | 38/163 [00:09<00:30,  4.15it/s] 24%|██▍       | 39/163 [00:09<00:29,  4.15it/s] 25%|██▍       | 40/163 [00:09<00:29,  4.15it/s] 25%|██▌       | 41/163 [00:09<00:29,  4.15it/s] 26%|██▌       | 42/163 [00:10<00:29,  4.14it/s] 26%|██▋       | 43/163 [00:10<00:29,  4.13it/s] 27%|██▋       | 44/163 [00:10<00:29,  3.99it/s] 28%|██▊       | 45/163 [00:11<00:30,  3.91it/s] 28%|██▊       | 46/163 [00:11<00:30,  3.88it/s] 29%|██▉       | 47/163 [00:11<00:29,  3.88it/s] 29%|██▉       | 48/163 [00:11<00:30,  3.79it/s] 30%|███       | 49/163 [00:12<00:30,  3.77it/s] 31%|███       | 50/163 [00:12<00:29,  3.78it/s] 31%|███▏      | 51/163 [00:12<00:29,  3.80it/s] 32%|███▏      | 52/163 [00:12<00:29,  3.75it/s] 33%|███▎      | 53/163 [00:13<00:29,  3.71it/s] 33%|███▎      | 54/163 [00:13<00:29,  3.76it/s] 34%|███▎      | 55/163 [00:13<00:28,  3.78it/s] 34%|███▍      | 56/163 [00:13<00:28,  3.74it/s] 35%|███▍      | 57/163 [00:14<00:28,  3.72it/s] 36%|███▌      | 58/163 [00:14<00:27,  3.76it/s] 36%|███▌      | 59/163 [00:14<00:27,  3.77it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.72it/s] 37%|███▋      | 61/163 [00:15<00:27,  3.70it/s] 38%|███▊      | 62/163 [00:15<00:26,  3.75it/s] 39%|███▊      | 63/163 [00:15<00:26,  3.75it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.71it/s] 40%|███▉      | 65/163 [00:16<00:26,  3.67it/s] 40%|████      | 66/163 [00:16<00:25,  3.74it/s] 41%|████      | 67/163 [00:16<00:25,  3.72it/s] 42%|████▏     | 68/163 [00:17<00:25,  3.70it/s] 42%|████▏     | 69/163 [00:17<00:25,  3.66it/s] 43%|████▎     | 70/163 [00:17<00:24,  3.75it/s] 44%|████▎     | 71/163 [00:17<00:24,  3.71it/s] 44%|████▍     | 72/163 [00:18<00:24,  3.74it/s] 45%|████▍     | 73/163 [00:18<00:24,  3.68it/s] 45%|████▌     | 74/163 [00:18<00:23,  3.73it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.69it/s] 47%|████▋     | 76/163 [00:19<00:23,  3.68it/s] 47%|████▋     | 77/163 [00:19<00:23,  3.72it/s] 48%|████▊     | 78/163 [00:19<00:22,  3.74it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.70it/s] 49%|████▉     | 80/163 [00:20<00:22,  3.69it/s] 50%|████▉     | 81/163 [00:20<00:22,  3.72it/s] 50%|█████     | 82/163 [00:20<00:21,  3.73it/s] 51%|█████     | 83/163 [00:21<00:21,  3.70it/s] 52%|█████▏    | 84/163 [00:21<00:21,  3.67it/s] 52%|█████▏    | 85/163 [00:21<00:20,  3.72it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.74it/s] 53%|█████▎    | 87/163 [00:22<00:20,  3.70it/s] 54%|█████▍    | 88/163 [00:22<00:20,  3.70it/s] 55%|█████▍    | 89/163 [00:22<00:19,  3.71it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.73it/s] 56%|█████▌    | 91/163 [00:23<00:19,  3.69it/s] 56%|█████▋    | 92/163 [00:23<00:19,  3.66it/s] 57%|█████▋    | 93/163 [00:23<00:18,  3.72it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.74it/s] 58%|█████▊    | 95/163 [00:24<00:18,  3.70it/s] 59%|█████▉    | 96/163 [00:24<00:18,  3.67it/s] 60%|█████▉    | 97/163 [00:24<00:17,  3.72it/s] 60%|██████    | 98/163 [00:25<00:17,  3.74it/s] 61%|██████    | 99/163 [00:25<00:17,  3.70it/s] 61%|██████▏   | 100/163 [00:25<00:16,  3.75it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.69it/s] 63%|██████▎   | 102/163 [00:26<00:16,  3.73it/s] 63%|██████▎   | 103/163 [00:26<00:16,  3.68it/s] 64%|██████▍   | 104/163 [00:26<00:16,  3.66it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.71it/s] 65%|██████▌   | 106/163 [00:27<00:15,  3.70it/s] 66%|██████▌   | 107/163 [00:27<00:15,  3.68it/s] 66%|██████▋   | 108/163 [00:27<00:15,  3.63it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.75it/s] 67%|██████▋   | 110/163 [00:28<00:14,  3.70it/s] 68%|██████▊   | 111/163 [00:28<00:14,  3.67it/s] 69%|██████▊   | 112/163 [00:29<00:14,  3.64it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.72it/s] 70%|██████▉   | 114/163 [00:29<00:13,  3.69it/s] 71%|███████   | 115/163 [00:29<00:12,  3.73it/s] 71%|███████   | 116/163 [00:30<00:12,  3.68it/s] 72%|███████▏  | 117/163 [00:30<00:12,  3.73it/s] 72%|███████▏  | 118/163 [00:30<00:12,  3.69it/s] 73%|███████▎  | 119/163 [00:30<00:11,  3.68it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.71it/s] 74%|███████▍  | 121/163 [00:31<00:11,  3.71it/s] 75%|███████▍  | 122/163 [00:31<00:11,  3.69it/s] 75%|███████▌  | 123/163 [00:32<00:11,  3.62it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.75it/s] 77%|███████▋  | 125/163 [00:32<00:10,  3.72it/s] 77%|███████▋  | 126/163 [00:32<00:10,  3.69it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.63it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.73it/s] 79%|███████▉  | 129/163 [00:33<00:09,  3.69it/s] 80%|███████▉  | 130/163 [00:33<00:08,  3.68it/s] 80%|████████  | 131/163 [00:34<00:08,  3.65it/s] 81%|████████  | 132/163 [00:34<00:08,  3.74it/s] 82%|████████▏ | 133/163 [00:34<00:08,  3.69it/s] 82%|████████▏ | 134/163 [00:34<00:07,  3.74it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.73it/s] 83%|████████▎ | 136/163 [00:35<00:07,  3.83it/s] 84%|████████▍ | 137/163 [00:35<00:06,  3.91it/s] 85%|████████▍ | 138/163 [00:35<00:06,  3.97it/s] 85%|████████▌ | 139/163 [00:36<00:06,  4.00it/s] 86%|████████▌ | 140/163 [00:36<00:05,  4.01it/s] 87%|████████▋ | 141/163 [00:36<00:05,  4.04it/s] 87%|████████▋ | 142/163 [00:36<00:05,  4.06it/s] 88%|████████▊ | 143/163 [00:37<00:04,  4.07it/s] 88%|████████▊ | 144/163 [00:37<00:04,  4.07it/s] 89%|████████▉ | 145/163 [00:37<00:04,  4.07it/s] 90%|████████▉ | 146/163 [00:37<00:04,  4.08it/s] 90%|█████████ | 147/163 [00:38<00:03,  4.08it/s] 91%|█████████ | 148/163 [00:38<00:03,  4.07it/s] 91%|█████████▏| 149/163 [00:38<00:03,  4.07it/s] 92%|█████████▏| 150/163 [00:38<00:03,  4.08it/s] 93%|█████████▎| 151/163 [00:39<00:02,  4.08it/s] 93%|█████████▎| 152/163 [00:39<00:02,  4.07it/s] 94%|█████████▍| 153/163 [00:39<00:02,  4.08it/s] 94%|█████████▍| 154/163 [00:39<00:02,  4.08it/s] 95%|█████████▌| 155/163 [00:40<00:01,  4.00it/s] 96%|█████████▌| 156/163 [00:40<00:01,  3.89it/s] 96%|█████████▋| 157/163 [00:40<00:01,  3.81it/s] 97%|█████████▋| 158/163 [00:40<00:01,  3.89it/s] 98%|█████████▊| 159/163 [00:41<00:01,  3.83it/s] 98%|█████████▊| 160/163 [00:41<00:00,  3.76it/s] 99%|█████████▉| 161/163 [00:41<00:00,  3.70it/s] 99%|█████████▉| 162/163 [00:42<00:00,  3.80it/s]100%|██████████| 163/163 [00:42<00:00,  3.78it/s]accuracy:  0.7668711656441718
100%|██████████| 163/163 [00:44<00:00,  3.63it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 171
Training dataset size: 144, validation dataset size: 171
Training dataset size: 144, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.50s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.13s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:57:20,350] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 03:57:20,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:57:20,866] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:03<01:09,  3.02s/it]  8%|▊         | 2/24 [00:05<01:03,  2.90s/it] 12%|█▎        | 3/24 [00:07<00:51,  2.46s/it] 17%|█▋        | 4/24 [00:10<00:53,  2.68s/it] 21%|██        | 5/24 [00:12<00:47,  2.48s/it] 25%|██▌       | 6/24 [00:15<00:43,  2.43s/it] 29%|██▉       | 7/24 [00:17<00:41,  2.44s/it] 33%|███▎      | 8/24 [00:20<00:40,  2.56s/it] 38%|███▊      | 9/24 [00:22<00:37,  2.49s/it] 42%|████▏     | 10/24 [00:24<00:32,  2.32s/it]                                               {'loss': 0.5858, 'grad_norm': 1.6627033948898315, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:24<00:32,  2.32s/it] 46%|████▌     | 11/24 [00:27<00:31,  2.45s/it] 50%|█████     | 12/24 [00:30<00:29,  2.48s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:33<00:30,  2.77s/it] 58%|█████▊    | 14/24 [00:36<00:28,  2.82s/it] 62%|██████▎   | 15/24 [00:39<00:26,  2.97s/it] 67%|██████▋   | 16/24 [00:42<00:23,  2.88s/it] 71%|███████   | 17/24 [00:45<00:20,  2.88s/it] 75%|███████▌  | 18/24 [00:47<00:16,  2.73s/it] 79%|███████▉  | 19/24 [00:50<00:13,  2.65s/it] 83%|████████▎ | 20/24 [00:52<00:10,  2.68s/it]                                               {'loss': 0.5922, 'grad_norm': 9.076606750488281, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:52<00:10,  2.68s/it] 88%|████████▊ | 21/24 [00:54<00:07,  2.47s/it] 92%|█████████▏| 22/24 [00:57<00:05,  2.55s/it] 96%|█████████▌| 23/24 [01:00<00:02,  2.62s/it]100%|██████████| 24/24 [01:02<00:00,  2.53s/it]                                               {'train_runtime': 63.3466, 'train_samples_per_second': 4.546, 'train_steps_per_second': 0.379, 'train_loss': 0.5894292891025543, 'epoch': 2.0}
100%|██████████| 24/24 [01:03<00:00,  2.53s/it]100%|██████████| 24/24 [01:03<00:00,  2.63s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:08,  3.16it/s]  1%|          | 2/217 [00:00<01:03,  3.41it/s]  1%|▏         | 3/217 [00:00<00:59,  3.62it/s]  2%|▏         | 4/217 [00:01<00:58,  3.61it/s]  2%|▏         | 5/217 [00:01<00:57,  3.70it/s]  3%|▎         | 6/217 [00:01<00:57,  3.69it/s]  3%|▎         | 7/217 [00:01<00:55,  3.76it/s]  4%|▎         | 8/217 [00:02<00:56,  3.72it/s]  4%|▍         | 9/217 [00:02<00:55,  3.76it/s]  5%|▍         | 10/217 [00:02<00:55,  3.73it/s]  5%|▌         | 11/217 [00:02<00:54,  3.77it/s]  6%|▌         | 12/217 [00:03<00:54,  3.73it/s]  6%|▌         | 13/217 [00:03<00:54,  3.76it/s]  6%|▋         | 14/217 [00:03<00:54,  3.72it/s]  7%|▋         | 15/217 [00:04<00:53,  3.79it/s]  7%|▋         | 16/217 [00:04<00:53,  3.73it/s]  8%|▊         | 17/217 [00:04<00:52,  3.78it/s]  8%|▊         | 18/217 [00:04<00:53,  3.73it/s]  9%|▉         | 19/217 [00:05<00:52,  3.79it/s]  9%|▉         | 20/217 [00:05<00:52,  3.73it/s] 10%|▉         | 21/217 [00:05<00:51,  3.78it/s] 10%|█         | 22/217 [00:05<00:52,  3.74it/s] 11%|█         | 23/217 [00:06<00:51,  3.79it/s] 11%|█         | 24/217 [00:06<00:51,  3.72it/s] 12%|█▏        | 25/217 [00:06<00:51,  3.75it/s] 12%|█▏        | 26/217 [00:06<00:51,  3.72it/s] 12%|█▏        | 27/217 [00:07<00:50,  3.79it/s] 13%|█▎        | 28/217 [00:07<00:50,  3.73it/s] 13%|█▎        | 29/217 [00:07<00:49,  3.77it/s] 14%|█▍        | 30/217 [00:08<00:50,  3.72it/s] 14%|█▍        | 31/217 [00:08<00:49,  3.77it/s] 15%|█▍        | 32/217 [00:08<00:49,  3.73it/s] 15%|█▌        | 33/217 [00:08<00:48,  3.77it/s] 16%|█▌        | 34/217 [00:09<00:48,  3.74it/s] 16%|█▌        | 35/217 [00:09<00:48,  3.74it/s] 17%|█▋        | 36/217 [00:09<00:46,  3.85it/s] 17%|█▋        | 37/217 [00:09<00:45,  3.93it/s] 18%|█▊        | 38/217 [00:10<00:44,  3.99it/s] 18%|█▊        | 39/217 [00:10<00:44,  4.01it/s] 18%|█▊        | 40/217 [00:10<00:43,  4.04it/s] 19%|█▉        | 41/217 [00:10<00:43,  4.07it/s] 19%|█▉        | 42/217 [00:11<00:42,  4.09it/s] 20%|█▉        | 43/217 [00:11<00:42,  4.10it/s] 20%|██        | 44/217 [00:11<00:42,  4.11it/s] 21%|██        | 45/217 [00:11<00:41,  4.12it/s] 21%|██        | 46/217 [00:12<00:41,  4.12it/s] 22%|██▏       | 47/217 [00:12<00:41,  4.11it/s] 22%|██▏       | 48/217 [00:12<00:41,  4.11it/s] 23%|██▎       | 49/217 [00:12<00:40,  4.12it/s] 23%|██▎       | 50/217 [00:13<00:40,  4.13it/s] 24%|██▎       | 51/217 [00:13<00:40,  4.13it/s] 24%|██▍       | 52/217 [00:13<00:39,  4.14it/s] 24%|██▍       | 53/217 [00:13<00:39,  4.14it/s] 25%|██▍       | 54/217 [00:13<00:39,  4.14it/s] 25%|██▌       | 55/217 [00:14<00:39,  4.14it/s] 26%|██▌       | 56/217 [00:14<00:39,  4.13it/s] 26%|██▋       | 57/217 [00:14<00:38,  4.12it/s] 27%|██▋       | 58/217 [00:14<00:38,  4.13it/s] 27%|██▋       | 59/217 [00:15<00:38,  4.07it/s] 28%|██▊       | 60/217 [00:15<00:39,  3.94it/s] 28%|██▊       | 61/217 [00:15<00:40,  3.84it/s] 29%|██▊       | 62/217 [00:15<00:39,  3.93it/s] 29%|██▉       | 63/217 [00:16<00:39,  3.91it/s] 29%|██▉       | 64/217 [00:16<00:40,  3.79it/s] 30%|██▉       | 65/217 [00:16<00:40,  3.74it/s] 30%|███       | 66/217 [00:17<00:39,  3.80it/s] 31%|███       | 67/217 [00:17<00:39,  3.82it/s] 31%|███▏      | 68/217 [00:17<00:39,  3.73it/s] 32%|███▏      | 69/217 [00:17<00:39,  3.77it/s] 32%|███▏      | 70/217 [00:18<00:39,  3.71it/s] 33%|███▎      | 71/217 [00:18<00:38,  3.77it/s] 33%|███▎      | 72/217 [00:18<00:39,  3.68it/s] 34%|███▎      | 73/217 [00:18<00:38,  3.74it/s] 34%|███▍      | 74/217 [00:19<00:38,  3.68it/s] 35%|███▍      | 75/217 [00:19<00:37,  3.76it/s] 35%|███▌      | 76/217 [00:19<00:38,  3.68it/s] 35%|███▌      | 77/217 [00:20<00:37,  3.73it/s] 36%|███▌      | 78/217 [00:20<00:37,  3.67it/s] 36%|███▋      | 79/217 [00:20<00:36,  3.77it/s] 37%|███▋      | 80/217 [00:20<00:37,  3.68it/s] 37%|███▋      | 81/217 [00:21<00:36,  3.71it/s] 38%|███▊      | 82/217 [00:21<00:36,  3.66it/s] 38%|███▊      | 83/217 [00:21<00:35,  3.78it/s] 39%|███▊      | 84/217 [00:21<00:35,  3.73it/s] 39%|███▉      | 85/217 [00:22<00:35,  3.69it/s] 40%|███▉      | 86/217 [00:22<00:36,  3.63it/s] 40%|████      | 87/217 [00:22<00:34,  3.77it/s] 41%|████      | 88/217 [00:22<00:34,  3.79it/s] 41%|████      | 89/217 [00:23<00:34,  3.71it/s] 41%|████▏     | 90/217 [00:23<00:35,  3.63it/s] 42%|████▏     | 91/217 [00:23<00:33,  3.76it/s] 42%|████▏     | 92/217 [00:24<00:33,  3.78it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.70it/s] 43%|████▎     | 94/217 [00:24<00:33,  3.63it/s] 44%|████▍     | 95/217 [00:24<00:32,  3.75it/s] 44%|████▍     | 96/217 [00:25<00:32,  3.77it/s] 45%|████▍     | 97/217 [00:25<00:32,  3.70it/s] 45%|████▌     | 98/217 [00:25<00:32,  3.66it/s] 46%|████▌     | 99/217 [00:25<00:31,  3.74it/s] 46%|████▌     | 100/217 [00:26<00:31,  3.76it/s] 47%|████▋     | 101/217 [00:26<00:31,  3.68it/s] 47%|████▋     | 102/217 [00:26<00:31,  3.61it/s] 47%|████▋     | 103/217 [00:27<00:30,  3.75it/s] 48%|████▊     | 104/217 [00:27<00:30,  3.76it/s] 48%|████▊     | 105/217 [00:27<00:30,  3.69it/s] 49%|████▉     | 106/217 [00:27<00:30,  3.62it/s] 49%|████▉     | 107/217 [00:28<00:29,  3.74it/s] 50%|████▉     | 108/217 [00:28<00:28,  3.77it/s] 50%|█████     | 109/217 [00:28<00:29,  3.69it/s] 51%|█████     | 110/217 [00:28<00:29,  3.63it/s] 51%|█████     | 111/217 [00:29<00:28,  3.75it/s] 52%|█████▏    | 112/217 [00:29<00:27,  3.78it/s] 52%|█████▏    | 113/217 [00:29<00:28,  3.70it/s] 53%|█████▎    | 114/217 [00:29<00:27,  3.75it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.70it/s] 53%|█████▎    | 116/217 [00:30<00:26,  3.77it/s] 54%|█████▍    | 117/217 [00:30<00:27,  3.68it/s] 54%|█████▍    | 118/217 [00:31<00:26,  3.74it/s] 55%|█████▍    | 119/217 [00:31<00:26,  3.68it/s] 55%|█████▌    | 120/217 [00:31<00:25,  3.76it/s] 56%|█████▌    | 121/217 [00:31<00:26,  3.68it/s] 56%|█████▌    | 122/217 [00:32<00:25,  3.67it/s] 57%|█████▋    | 123/217 [00:32<00:24,  3.79it/s] 57%|█████▋    | 124/217 [00:32<00:24,  3.87it/s] 58%|█████▊    | 125/217 [00:32<00:23,  3.95it/s] 58%|█████▊    | 126/217 [00:33<00:22,  4.00it/s] 59%|█████▊    | 127/217 [00:33<00:22,  4.04it/s] 59%|█████▉    | 128/217 [00:33<00:21,  4.07it/s] 59%|█████▉    | 129/217 [00:33<00:21,  4.08it/s] 60%|█████▉    | 130/217 [00:34<00:21,  4.09it/s] 60%|██████    | 131/217 [00:34<00:21,  4.09it/s] 61%|██████    | 132/217 [00:34<00:20,  4.10it/s] 61%|██████▏   | 133/217 [00:34<00:20,  4.11it/s] 62%|██████▏   | 134/217 [00:35<00:20,  4.12it/s] 62%|██████▏   | 135/217 [00:35<00:19,  4.13it/s] 63%|██████▎   | 136/217 [00:35<00:19,  4.13it/s] 63%|██████▎   | 137/217 [00:35<00:19,  4.13it/s] 64%|██████▎   | 138/217 [00:36<00:19,  4.12it/s] 64%|██████▍   | 139/217 [00:36<00:18,  4.12it/s] 65%|██████▍   | 140/217 [00:36<00:18,  4.11it/s] 65%|██████▍   | 141/217 [00:36<00:18,  4.12it/s] 65%|██████▌   | 142/217 [00:37<00:18,  4.13it/s] 66%|██████▌   | 143/217 [00:37<00:17,  4.13it/s] 66%|██████▋   | 144/217 [00:37<00:17,  4.12it/s] 67%|██████▋   | 145/217 [00:37<00:17,  4.11it/s] 67%|██████▋   | 146/217 [00:37<00:17,  4.11it/s] 68%|██████▊   | 147/217 [00:38<00:17,  4.11it/s] 68%|██████▊   | 148/217 [00:38<00:16,  4.12it/s] 69%|██████▊   | 149/217 [00:38<00:16,  4.03it/s] 69%|██████▉   | 150/217 [00:39<00:17,  3.92it/s] 70%|██████▉   | 151/217 [00:39<00:17,  3.85it/s] 70%|███████   | 152/217 [00:39<00:16,  3.93it/s] 71%|███████   | 153/217 [00:39<00:16,  3.81it/s] 71%|███████   | 154/217 [00:40<00:16,  3.71it/s] 71%|███████▏  | 155/217 [00:40<00:16,  3.76it/s] 72%|███████▏  | 156/217 [00:40<00:16,  3.69it/s] 72%|███████▏  | 157/217 [00:40<00:15,  3.76it/s] 73%|███████▎  | 158/217 [00:41<00:16,  3.68it/s] 73%|███████▎  | 159/217 [00:41<00:15,  3.63it/s] 74%|███████▎  | 160/217 [00:41<00:15,  3.59it/s] 74%|███████▍  | 161/217 [00:41<00:14,  3.74it/s] 75%|███████▍  | 162/217 [00:42<00:14,  3.75it/s] 75%|███████▌  | 163/217 [00:42<00:14,  3.64it/s] 76%|███████▌  | 164/217 [00:42<00:14,  3.69it/s] 76%|███████▌  | 165/217 [00:43<00:14,  3.64it/s] 76%|███████▋  | 166/217 [00:43<00:13,  3.73it/s] 77%|███████▋  | 167/217 [00:43<00:13,  3.66it/s] 77%|███████▋  | 168/217 [00:43<00:13,  3.62it/s] 78%|███████▊  | 169/217 [00:44<00:13,  3.58it/s] 78%|███████▊  | 170/217 [00:44<00:12,  3.73it/s] 79%|███████▉  | 171/217 [00:44<00:12,  3.75it/s] 79%|███████▉  | 172/217 [00:44<00:12,  3.64it/s] 80%|███████▉  | 173/217 [00:45<00:11,  3.70it/s] 80%|████████  | 174/217 [00:45<00:11,  3.66it/s] 81%|████████  | 175/217 [00:45<00:11,  3.75it/s] 81%|████████  | 176/217 [00:46<00:11,  3.67it/s] 82%|████████▏ | 177/217 [00:46<00:11,  3.63it/s] 82%|████████▏ | 178/217 [00:46<00:10,  3.58it/s] 82%|████████▏ | 179/217 [00:46<00:10,  3.73it/s] 83%|████████▎ | 180/217 [00:47<00:10,  3.67it/s] 83%|████████▎ | 181/217 [00:47<00:09,  3.63it/s] 84%|████████▍ | 182/217 [00:47<00:09,  3.69it/s] 84%|████████▍ | 183/217 [00:47<00:09,  3.65it/s] 85%|████████▍ | 184/217 [00:48<00:08,  3.73it/s] 85%|████████▌ | 185/217 [00:48<00:08,  3.66it/s] 86%|████████▌ | 186/217 [00:48<00:08,  3.62it/s] 86%|████████▌ | 187/217 [00:49<00:08,  3.58it/s] 87%|████████▋ | 188/217 [00:49<00:07,  3.72it/s] 87%|████████▋ | 189/217 [00:49<00:07,  3.74it/s] 88%|████████▊ | 190/217 [00:49<00:07,  3.63it/s] 88%|████████▊ | 191/217 [00:50<00:07,  3.68it/s] 88%|████████▊ | 192/217 [00:50<00:06,  3.63it/s] 89%|████████▉ | 193/217 [00:50<00:06,  3.73it/s] 89%|████████▉ | 194/217 [00:50<00:06,  3.65it/s] 90%|████████▉ | 195/217 [00:51<00:06,  3.61it/s] 90%|█████████ | 196/217 [00:51<00:05,  3.57it/s] 91%|█████████ | 197/217 [00:51<00:05,  3.72it/s] 91%|█████████ | 198/217 [00:52<00:05,  3.74it/s] 92%|█████████▏| 199/217 [00:52<00:04,  3.62it/s] 92%|█████████▏| 200/217 [00:52<00:04,  3.67it/s] 93%|█████████▎| 201/217 [00:52<00:04,  3.63it/s] 93%|█████████▎| 202/217 [00:53<00:04,  3.73it/s] 94%|█████████▎| 203/217 [00:53<00:03,  3.66it/s] 94%|█████████▍| 204/217 [00:53<00:03,  3.61it/s] 94%|█████████▍| 205/217 [00:54<00:03,  3.57it/s] 95%|█████████▍| 206/217 [00:54<00:02,  3.72it/s] 95%|█████████▌| 207/217 [00:54<00:02,  3.74it/s] 96%|█████████▌| 208/217 [00:54<00:02,  3.62it/s] 96%|█████████▋| 209/217 [00:55<00:02,  3.67it/s] 97%|█████████▋| 210/217 [00:55<00:01,  3.63it/s] 97%|█████████▋| 211/217 [00:55<00:01,  3.73it/s] 98%|█████████▊| 212/217 [00:55<00:01,  3.67it/s] 98%|█████████▊| 213/217 [00:56<00:01,  3.61it/s] 99%|█████████▊| 214/217 [00:56<00:00,  3.58it/s] 99%|█████████▉| 215/217 [00:56<00:00,  3.73it/s]100%|█████████▉| 216/217 [00:56<00:00,  3.76it/s]100%|██████████| 217/217 [00:57<00:00,  3.63it/s]accuracy:  0.8248847926267281
100%|██████████| 217/217 [01:00<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 103
Training dataset size: 144, validation dataset size: 103
Training dataset size: 144, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:59:52,633] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.05s/it]Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.49s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 03:59:53,902] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 03:59:54,392] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:50,  2.19s/it]  8%|▊         | 2/24 [00:04<00:54,  2.45s/it] 12%|█▎        | 3/24 [00:07<00:56,  2.70s/it] 17%|█▋        | 4/24 [00:10<00:51,  2.60s/it] 21%|██        | 5/24 [00:12<00:48,  2.57s/it] 25%|██▌       | 6/24 [00:14<00:41,  2.32s/it] 29%|██▉       | 7/24 [00:16<00:38,  2.28s/it] 33%|███▎      | 8/24 [00:19<00:36,  2.29s/it] 38%|███▊      | 9/24 [00:21<00:34,  2.31s/it] 42%|████▏     | 10/24 [00:23<00:31,  2.28s/it]                                               {'loss': 1.0811, 'grad_norm': 1.4204928874969482, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:23<00:31,  2.28s/it] 46%|████▌     | 11/24 [00:25<00:29,  2.26s/it] 50%|█████     | 12/24 [00:28<00:27,  2.28s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:30<00:25,  2.35s/it] 58%|█████▊    | 14/24 [00:32<00:22,  2.28s/it] 62%|██████▎   | 15/24 [00:34<00:20,  2.24s/it] 67%|██████▋   | 16/24 [00:37<00:17,  2.20s/it] 71%|███████   | 17/24 [00:39<00:14,  2.13s/it] 75%|███████▌  | 18/24 [00:40<00:11,  1.98s/it] 79%|███████▉  | 19/24 [00:43<00:11,  2.26s/it] 83%|████████▎ | 20/24 [00:45<00:08,  2.25s/it]                                               {'loss': 0.8439, 'grad_norm': 8.697903633117676, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:45<00:08,  2.25s/it] 88%|████████▊ | 21/24 [00:48<00:07,  2.36s/it] 92%|█████████▏| 22/24 [00:51<00:04,  2.46s/it] 96%|█████████▌| 23/24 [00:53<00:02,  2.38s/it]100%|██████████| 24/24 [00:55<00:00,  2.24s/it]                                               {'train_runtime': 55.8932, 'train_samples_per_second': 5.153, 'train_steps_per_second': 0.429, 'train_loss': 0.8616343239943186, 'epoch': 2.0}
100%|██████████| 24/24 [00:55<00:00,  2.24s/it]100%|██████████| 24/24 [00:55<00:00,  2.32s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:56,  2.65it/s]  1%|▏         | 2/150 [00:00<00:44,  3.29it/s]  2%|▏         | 3/150 [00:00<00:42,  3.46it/s]  3%|▎         | 4/150 [00:01<00:41,  3.55it/s]  3%|▎         | 5/150 [00:01<00:40,  3.55it/s]  4%|▍         | 6/150 [00:01<00:38,  3.74it/s]  5%|▍         | 7/150 [00:01<00:37,  3.78it/s]  5%|▌         | 8/150 [00:02<00:38,  3.73it/s]  6%|▌         | 9/150 [00:02<00:37,  3.77it/s]  7%|▋         | 10/150 [00:02<00:37,  3.72it/s]  7%|▋         | 11/150 [00:03<00:36,  3.82it/s]  8%|▊         | 12/150 [00:03<00:36,  3.77it/s]  9%|▊         | 13/150 [00:03<00:36,  3.73it/s]  9%|▉         | 14/150 [00:03<00:37,  3.67it/s] 10%|█         | 15/150 [00:04<00:35,  3.79it/s] 11%|█         | 16/150 [00:04<00:35,  3.82it/s] 11%|█▏        | 17/150 [00:04<00:35,  3.76it/s] 12%|█▏        | 18/150 [00:04<00:34,  3.78it/s] 13%|█▎        | 19/150 [00:05<00:35,  3.72it/s] 13%|█▎        | 20/150 [00:05<00:34,  3.82it/s] 14%|█▍        | 21/150 [00:05<00:33,  3.86it/s] 15%|█▍        | 22/150 [00:05<00:33,  3.78it/s] 15%|█▌        | 23/150 [00:06<00:33,  3.77it/s] 16%|█▌        | 24/150 [00:06<00:33,  3.72it/s] 17%|█▋        | 25/150 [00:06<00:32,  3.83it/s] 17%|█▋        | 26/150 [00:06<00:32,  3.83it/s] 18%|█▊        | 27/150 [00:07<00:32,  3.75it/s] 19%|█▊        | 28/150 [00:07<00:32,  3.80it/s] 19%|█▉        | 29/150 [00:07<00:32,  3.72it/s] 20%|██        | 30/150 [00:08<00:31,  3.81it/s] 21%|██        | 31/150 [00:08<00:31,  3.76it/s] 21%|██▏       | 32/150 [00:08<00:31,  3.73it/s] 22%|██▏       | 33/150 [00:08<00:31,  3.67it/s] 23%|██▎       | 34/150 [00:09<00:30,  3.79it/s] 23%|██▎       | 35/150 [00:09<00:30,  3.81it/s] 24%|██▍       | 36/150 [00:09<00:30,  3.74it/s] 25%|██▍       | 37/150 [00:09<00:29,  3.79it/s] 25%|██▌       | 38/150 [00:10<00:30,  3.72it/s] 26%|██▌       | 39/150 [00:10<00:29,  3.81it/s] 27%|██▋       | 40/150 [00:10<00:29,  3.77it/s] 27%|██▋       | 41/150 [00:10<00:29,  3.74it/s] 28%|██▊       | 42/150 [00:11<00:29,  3.68it/s] 29%|██▊       | 43/150 [00:11<00:28,  3.81it/s] 29%|██▉       | 44/150 [00:11<00:27,  3.83it/s] 30%|███       | 45/150 [00:12<00:27,  3.76it/s] 31%|███       | 46/150 [00:12<00:27,  3.79it/s] 31%|███▏      | 47/150 [00:12<00:27,  3.72it/s] 32%|███▏      | 48/150 [00:12<00:26,  3.81it/s] 33%|███▎      | 49/150 [00:13<00:26,  3.77it/s] 33%|███▎      | 50/150 [00:13<00:26,  3.73it/s] 34%|███▍      | 51/150 [00:13<00:26,  3.67it/s] 35%|███▍      | 52/150 [00:13<00:25,  3.79it/s] 35%|███▌      | 53/150 [00:14<00:25,  3.79it/s] 36%|███▌      | 54/150 [00:14<00:25,  3.74it/s] 37%|███▋      | 55/150 [00:14<00:25,  3.75it/s] 37%|███▋      | 56/150 [00:14<00:25,  3.72it/s] 38%|███▊      | 57/150 [00:15<00:24,  3.79it/s] 39%|███▊      | 58/150 [00:15<00:24,  3.72it/s] 39%|███▉      | 59/150 [00:15<00:24,  3.74it/s] 40%|████      | 60/150 [00:16<00:24,  3.67it/s] 41%|████      | 61/150 [00:16<00:23,  3.79it/s] 41%|████▏     | 62/150 [00:16<00:23,  3.79it/s] 42%|████▏     | 63/150 [00:16<00:23,  3.74it/s] 43%|████▎     | 64/150 [00:17<00:22,  3.75it/s] 43%|████▎     | 65/150 [00:17<00:22,  3.73it/s] 44%|████▍     | 66/150 [00:17<00:22,  3.80it/s] 45%|████▍     | 67/150 [00:17<00:22,  3.76it/s] 45%|████▌     | 68/150 [00:18<00:21,  3.73it/s] 46%|████▌     | 69/150 [00:18<00:22,  3.67it/s] 47%|████▋     | 70/150 [00:18<00:21,  3.79it/s] 47%|████▋     | 71/150 [00:18<00:20,  3.79it/s] 48%|████▊     | 72/150 [00:19<00:20,  3.74it/s] 49%|████▊     | 73/150 [00:19<00:20,  3.78it/s] 49%|████▉     | 74/150 [00:19<00:20,  3.71it/s] 50%|█████     | 75/150 [00:20<00:19,  3.79it/s] 51%|█████     | 76/150 [00:20<00:19,  3.72it/s] 51%|█████▏    | 77/150 [00:20<00:19,  3.73it/s] 52%|█████▏    | 78/150 [00:20<00:19,  3.68it/s] 53%|█████▎    | 79/150 [00:21<00:18,  3.80it/s] 53%|█████▎    | 80/150 [00:21<00:18,  3.80it/s] 54%|█████▍    | 81/150 [00:21<00:18,  3.74it/s] 55%|█████▍    | 82/150 [00:21<00:18,  3.76it/s] 55%|█████▌    | 83/150 [00:22<00:17,  3.72it/s] 56%|█████▌    | 84/150 [00:22<00:17,  3.81it/s] 57%|█████▋    | 85/150 [00:22<00:17,  3.76it/s] 57%|█████▋    | 86/150 [00:22<00:17,  3.73it/s] 58%|█████▊    | 87/150 [00:23<00:17,  3.67it/s] 59%|█████▊    | 88/150 [00:23<00:16,  3.79it/s] 59%|█████▉    | 89/150 [00:23<00:16,  3.81it/s] 60%|██████    | 90/150 [00:24<00:16,  3.75it/s] 61%|██████    | 91/150 [00:24<00:15,  3.77it/s] 61%|██████▏   | 92/150 [00:24<00:15,  3.71it/s] 62%|██████▏   | 93/150 [00:24<00:15,  3.80it/s] 63%|██████▎   | 94/150 [00:25<00:14,  3.76it/s] 63%|██████▎   | 95/150 [00:25<00:14,  3.73it/s] 64%|██████▍   | 96/150 [00:25<00:14,  3.66it/s] 65%|██████▍   | 97/150 [00:25<00:14,  3.78it/s] 65%|██████▌   | 98/150 [00:26<00:13,  3.80it/s] 66%|██████▌   | 99/150 [00:26<00:13,  3.73it/s] 67%|██████▋   | 100/150 [00:26<00:13,  3.75it/s] 67%|██████▋   | 101/150 [00:26<00:13,  3.72it/s] 68%|██████▊   | 102/150 [00:27<00:12,  3.79it/s] 69%|██████▊   | 103/150 [00:27<00:12,  3.74it/s] 69%|██████▉   | 104/150 [00:27<00:12,  3.71it/s] 70%|███████   | 105/150 [00:28<00:12,  3.65it/s] 71%|███████   | 106/150 [00:28<00:11,  3.76it/s] 71%|███████▏  | 107/150 [00:28<00:11,  3.77it/s] 72%|███████▏  | 108/150 [00:28<00:11,  3.71it/s] 73%|███████▎  | 109/150 [00:29<00:11,  3.73it/s] 73%|███████▎  | 110/150 [00:29<00:10,  3.70it/s] 74%|███████▍  | 111/150 [00:29<00:10,  3.78it/s] 75%|███████▍  | 112/150 [00:29<00:10,  3.74it/s] 75%|███████▌  | 113/150 [00:30<00:09,  3.72it/s] 76%|███████▌  | 114/150 [00:30<00:09,  3.65it/s] 77%|███████▋  | 115/150 [00:30<00:09,  3.78it/s] 77%|███████▋  | 116/150 [00:30<00:08,  3.79it/s] 78%|███████▊  | 117/150 [00:31<00:08,  3.72it/s] 79%|███████▊  | 118/150 [00:31<00:08,  3.74it/s] 79%|███████▉  | 119/150 [00:31<00:08,  3.71it/s] 80%|████████  | 120/150 [00:32<00:07,  3.77it/s] 81%|████████  | 121/150 [00:32<00:07,  3.71it/s] 81%|████████▏ | 122/150 [00:32<00:07,  3.71it/s] 82%|████████▏ | 123/150 [00:32<00:07,  3.64it/s] 83%|████████▎ | 124/150 [00:33<00:06,  3.77it/s] 83%|████████▎ | 125/150 [00:33<00:06,  3.77it/s] 84%|████████▍ | 126/150 [00:33<00:06,  3.71it/s] 85%|████████▍ | 127/150 [00:33<00:06,  3.76it/s] 85%|████████▌ | 128/150 [00:34<00:05,  3.70it/s] 86%|████████▌ | 129/150 [00:34<00:05,  3.78it/s] 87%|████████▋ | 130/150 [00:34<00:05,  3.74it/s] 87%|████████▋ | 131/150 [00:35<00:05,  3.71it/s] 88%|████████▊ | 132/150 [00:35<00:04,  3.64it/s] 89%|████████▊ | 133/150 [00:35<00:04,  3.76it/s] 89%|████████▉ | 134/150 [00:35<00:04,  3.77it/s] 90%|█████████ | 135/150 [00:36<00:04,  3.71it/s] 91%|█████████ | 136/150 [00:36<00:03,  3.75it/s] 91%|█████████▏| 137/150 [00:36<00:03,  3.70it/s] 92%|█████████▏| 138/150 [00:36<00:03,  3.76it/s] 93%|█████████▎| 139/150 [00:37<00:02,  3.72it/s] 93%|█████████▎| 140/150 [00:37<00:02,  3.70it/s] 94%|█████████▍| 141/150 [00:37<00:02,  3.64it/s] 95%|█████████▍| 142/150 [00:37<00:02,  3.75it/s] 95%|█████████▌| 143/150 [00:38<00:01,  3.77it/s] 96%|█████████▌| 144/150 [00:38<00:01,  3.70it/s] 97%|█████████▋| 145/150 [00:38<00:01,  3.74it/s] 97%|█████████▋| 146/150 [00:39<00:01,  3.69it/s] 98%|█████████▊| 147/150 [00:39<00:00,  3.78it/s] 99%|█████████▊| 148/150 [00:39<00:00,  3.74it/s] 99%|█████████▉| 149/150 [00:39<00:00,  3.71it/s]100%|██████████| 150/150 [00:40<00:00,  3.66it/s]accuracy:  0.6266666666666667
100%|██████████| 150/150 [00:42<00:00,  3.52it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 145
Training dataset size: 144, validation dataset size: 145
Training dataset size: 144, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.63s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.95s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:01:58,851] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:01:59,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
training start
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:01:59,790] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:56,  2.44s/it]  8%|▊         | 2/24 [00:04<00:50,  2.29s/it] 12%|█▎        | 3/24 [00:07<00:51,  2.46s/it] 17%|█▋        | 4/24 [00:09<00:50,  2.51s/it] 21%|██        | 5/24 [00:12<00:45,  2.39s/it] 25%|██▌       | 6/24 [00:14<00:41,  2.32s/it] 29%|██▉       | 7/24 [00:16<00:39,  2.30s/it] 33%|███▎      | 8/24 [00:18<00:35,  2.25s/it] 38%|███▊      | 9/24 [00:21<00:35,  2.34s/it] 42%|████▏     | 10/24 [00:23<00:32,  2.32s/it]                                               {'loss': 1.3568, 'grad_norm': 13.672323226928711, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:23<00:32,  2.32s/it] 46%|████▌     | 11/24 [00:25<00:30,  2.37s/it] 50%|█████     | 12/24 [00:28<00:28,  2.36s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:30<00:26,  2.38s/it] 58%|█████▊    | 14/24 [00:33<00:24,  2.43s/it] 62%|██████▎   | 15/24 [00:35<00:21,  2.41s/it] 67%|██████▋   | 16/24 [00:37<00:18,  2.28s/it] 71%|███████   | 17/24 [00:39<00:16,  2.31s/it] 75%|███████▌  | 18/24 [00:42<00:14,  2.36s/it] 79%|███████▉  | 19/24 [00:45<00:12,  2.55s/it] 83%|████████▎ | 20/24 [00:47<00:10,  2.50s/it]                                               {'loss': 1.3857, 'grad_norm': 12.348458290100098, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:47<00:10,  2.50s/it] 88%|████████▊ | 21/24 [00:49<00:07,  2.36s/it] 92%|█████████▏| 22/24 [00:52<00:04,  2.40s/it] 96%|█████████▌| 23/24 [00:54<00:02,  2.35s/it]100%|██████████| 24/24 [00:57<00:00,  2.51s/it]                                               {'train_runtime': 58.3278, 'train_samples_per_second': 4.938, 'train_steps_per_second': 0.411, 'train_loss': 1.2996658086776733, 'epoch': 2.0}
100%|██████████| 24/24 [00:58<00:00,  2.51s/it]100%|██████████| 24/24 [00:58<00:00,  2.42s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:09,  2.87it/s]  1%|          | 2/200 [00:00<01:01,  3.24it/s]  2%|▏         | 3/200 [00:00<00:54,  3.60it/s]  2%|▏         | 4/200 [00:01<00:53,  3.64it/s]  2%|▎         | 5/200 [00:01<00:53,  3.67it/s]  3%|▎         | 6/200 [00:01<00:53,  3.66it/s]  4%|▎         | 7/200 [00:01<00:51,  3.77it/s]  4%|▍         | 8/200 [00:02<00:51,  3.74it/s]  4%|▍         | 9/200 [00:02<00:51,  3.70it/s]  5%|▌         | 10/200 [00:02<00:50,  3.77it/s]  6%|▌         | 11/200 [00:02<00:49,  3.79it/s]  6%|▌         | 12/200 [00:03<00:50,  3.75it/s]  6%|▋         | 13/200 [00:03<00:50,  3.67it/s]  7%|▋         | 14/200 [00:03<00:49,  3.79it/s]  8%|▊         | 15/200 [00:04<00:49,  3.74it/s]  8%|▊         | 16/200 [00:04<00:49,  3.74it/s]  8%|▊         | 17/200 [00:04<00:49,  3.70it/s]  9%|▉         | 18/200 [00:04<00:48,  3.77it/s] 10%|▉         | 19/200 [00:05<00:48,  3.74it/s] 10%|█         | 20/200 [00:05<00:48,  3.71it/s] 10%|█         | 21/200 [00:05<00:47,  3.76it/s] 11%|█         | 22/200 [00:05<00:47,  3.77it/s] 12%|█▏        | 23/200 [00:06<00:47,  3.74it/s] 12%|█▏        | 24/200 [00:06<00:47,  3.67it/s] 12%|█▎        | 25/200 [00:06<00:46,  3.80it/s] 13%|█▎        | 26/200 [00:07<00:46,  3.76it/s] 14%|█▎        | 27/200 [00:07<00:46,  3.76it/s] 14%|█▍        | 28/200 [00:07<00:46,  3.72it/s] 14%|█▍        | 29/200 [00:07<00:45,  3.79it/s] 15%|█▌        | 30/200 [00:08<00:45,  3.75it/s] 16%|█▌        | 31/200 [00:08<00:45,  3.70it/s] 16%|█▌        | 32/200 [00:08<00:44,  3.77it/s] 16%|█▋        | 33/200 [00:08<00:44,  3.77it/s] 17%|█▋        | 34/200 [00:09<00:44,  3.74it/s] 18%|█▊        | 35/200 [00:09<00:44,  3.67it/s] 18%|█▊        | 36/200 [00:09<00:43,  3.79it/s] 18%|█▊        | 37/200 [00:09<00:43,  3.75it/s] 19%|█▉        | 38/200 [00:10<00:43,  3.75it/s] 20%|█▉        | 39/200 [00:10<00:43,  3.70it/s] 20%|██        | 40/200 [00:10<00:42,  3.77it/s] 20%|██        | 41/200 [00:11<00:42,  3.74it/s] 21%|██        | 42/200 [00:11<00:42,  3.69it/s] 22%|██▏       | 43/200 [00:11<00:41,  3.76it/s] 22%|██▏       | 44/200 [00:11<00:41,  3.76it/s] 22%|██▎       | 45/200 [00:12<00:41,  3.74it/s] 23%|██▎       | 46/200 [00:12<00:41,  3.69it/s] 24%|██▎       | 47/200 [00:12<00:40,  3.77it/s] 24%|██▍       | 48/200 [00:12<00:40,  3.73it/s] 24%|██▍       | 49/200 [00:13<00:41,  3.67it/s] 25%|██▌       | 50/200 [00:13<00:39,  3.75it/s] 26%|██▌       | 51/200 [00:13<00:39,  3.75it/s] 26%|██▌       | 52/200 [00:13<00:39,  3.74it/s] 26%|██▋       | 53/200 [00:14<00:39,  3.69it/s] 27%|██▋       | 54/200 [00:14<00:38,  3.78it/s] 28%|██▊       | 55/200 [00:14<00:38,  3.73it/s] 28%|██▊       | 56/200 [00:15<00:39,  3.68it/s] 28%|██▊       | 57/200 [00:15<00:38,  3.75it/s] 29%|██▉       | 58/200 [00:15<00:37,  3.75it/s] 30%|██▉       | 59/200 [00:15<00:37,  3.72it/s] 30%|███       | 60/200 [00:16<00:38,  3.67it/s] 30%|███       | 61/200 [00:16<00:36,  3.78it/s] 31%|███       | 62/200 [00:16<00:36,  3.74it/s] 32%|███▏      | 63/200 [00:16<00:36,  3.74it/s] 32%|███▏      | 64/200 [00:17<00:36,  3.70it/s] 32%|███▎      | 65/200 [00:17<00:35,  3.77it/s] 33%|███▎      | 66/200 [00:17<00:35,  3.73it/s] 34%|███▎      | 67/200 [00:17<00:36,  3.69it/s] 34%|███▍      | 68/200 [00:18<00:35,  3.76it/s] 34%|███▍      | 69/200 [00:18<00:34,  3.76it/s] 35%|███▌      | 70/200 [00:18<00:34,  3.74it/s] 36%|███▌      | 71/200 [00:19<00:35,  3.67it/s] 36%|███▌      | 72/200 [00:19<00:33,  3.79it/s] 36%|███▋      | 73/200 [00:19<00:33,  3.74it/s] 37%|███▋      | 74/200 [00:19<00:33,  3.73it/s] 38%|███▊      | 75/200 [00:20<00:33,  3.68it/s] 38%|███▊      | 76/200 [00:20<00:32,  3.76it/s] 38%|███▊      | 77/200 [00:20<00:33,  3.72it/s] 39%|███▉      | 78/200 [00:20<00:32,  3.71it/s] 40%|███▉      | 79/200 [00:21<00:32,  3.73it/s] 40%|████      | 80/200 [00:21<00:31,  3.75it/s] 40%|████      | 81/200 [00:21<00:32,  3.72it/s] 41%|████      | 82/200 [00:22<00:32,  3.65it/s] 42%|████▏     | 83/200 [00:22<00:31,  3.77it/s] 42%|████▏     | 84/200 [00:22<00:31,  3.73it/s] 42%|████▎     | 85/200 [00:22<00:30,  3.72it/s] 43%|████▎     | 86/200 [00:23<00:31,  3.67it/s] 44%|████▎     | 87/200 [00:23<00:30,  3.76it/s] 44%|████▍     | 88/200 [00:23<00:30,  3.72it/s] 44%|████▍     | 89/200 [00:23<00:30,  3.67it/s] 45%|████▌     | 90/200 [00:24<00:29,  3.73it/s] 46%|████▌     | 91/200 [00:24<00:29,  3.74it/s] 46%|████▌     | 92/200 [00:24<00:29,  3.71it/s] 46%|████▋     | 93/200 [00:24<00:29,  3.65it/s] 47%|████▋     | 94/200 [00:25<00:28,  3.77it/s] 48%|████▊     | 95/200 [00:25<00:28,  3.74it/s] 48%|████▊     | 96/200 [00:25<00:27,  3.72it/s] 48%|████▊     | 97/200 [00:26<00:27,  3.68it/s] 49%|████▉     | 98/200 [00:26<00:27,  3.76it/s] 50%|████▉     | 99/200 [00:26<00:27,  3.71it/s] 50%|█████     | 100/200 [00:26<00:27,  3.66it/s] 50%|█████     | 101/200 [00:27<00:26,  3.72it/s] 51%|█████     | 102/200 [00:27<00:26,  3.74it/s] 52%|█████▏    | 103/200 [00:27<00:26,  3.71it/s] 52%|█████▏    | 104/200 [00:27<00:26,  3.64it/s] 52%|█████▎    | 105/200 [00:28<00:25,  3.77it/s] 53%|█████▎    | 106/200 [00:28<00:25,  3.72it/s] 54%|█████▎    | 107/200 [00:28<00:24,  3.73it/s] 54%|█████▍    | 108/200 [00:29<00:25,  3.67it/s] 55%|█████▍    | 109/200 [00:29<00:24,  3.73it/s] 55%|█████▌    | 110/200 [00:29<00:24,  3.71it/s] 56%|█████▌    | 111/200 [00:29<00:24,  3.66it/s] 56%|█████▌    | 112/200 [00:30<00:23,  3.73it/s] 56%|█████▋    | 113/200 [00:30<00:23,  3.73it/s] 57%|█████▋    | 114/200 [00:30<00:23,  3.70it/s] 57%|█████▊    | 115/200 [00:30<00:23,  3.66it/s] 58%|█████▊    | 116/200 [00:31<00:22,  3.76it/s] 58%|█████▊    | 117/200 [00:31<00:22,  3.71it/s] 59%|█████▉    | 118/200 [00:31<00:22,  3.66it/s] 60%|█████▉    | 119/200 [00:31<00:21,  3.73it/s] 60%|██████    | 120/200 [00:32<00:21,  3.72it/s] 60%|██████    | 121/200 [00:32<00:21,  3.70it/s] 61%|██████    | 122/200 [00:32<00:21,  3.66it/s] 62%|██████▏   | 123/200 [00:33<00:20,  3.73it/s] 62%|██████▏   | 124/200 [00:33<00:20,  3.70it/s] 62%|██████▎   | 125/200 [00:33<00:20,  3.65it/s] 63%|██████▎   | 126/200 [00:33<00:19,  3.73it/s] 64%|██████▎   | 127/200 [00:34<00:19,  3.73it/s] 64%|██████▍   | 128/200 [00:34<00:19,  3.72it/s] 64%|██████▍   | 129/200 [00:34<00:19,  3.65it/s] 65%|██████▌   | 130/200 [00:34<00:18,  3.77it/s] 66%|██████▌   | 131/200 [00:35<00:18,  3.73it/s] 66%|██████▌   | 132/200 [00:35<00:18,  3.73it/s] 66%|██████▋   | 133/200 [00:35<00:18,  3.68it/s] 67%|██████▋   | 134/200 [00:36<00:17,  3.72it/s] 68%|██████▊   | 135/200 [00:36<00:17,  3.68it/s] 68%|██████▊   | 136/200 [00:36<00:17,  3.63it/s] 68%|██████▊   | 137/200 [00:36<00:16,  3.75it/s] 69%|██████▉   | 138/200 [00:37<00:16,  3.71it/s] 70%|██████▉   | 139/200 [00:37<00:16,  3.71it/s] 70%|███████   | 140/200 [00:37<00:16,  3.66it/s] 70%|███████   | 141/200 [00:37<00:15,  3.74it/s] 71%|███████   | 142/200 [00:38<00:15,  3.70it/s] 72%|███████▏  | 143/200 [00:38<00:15,  3.67it/s] 72%|███████▏  | 144/200 [00:38<00:15,  3.73it/s] 72%|███████▎  | 145/200 [00:38<00:14,  3.75it/s] 73%|███████▎  | 146/200 [00:39<00:14,  3.73it/s] 74%|███████▎  | 147/200 [00:39<00:14,  3.73it/s] 74%|███████▍  | 148/200 [00:39<00:13,  3.83it/s] 74%|███████▍  | 149/200 [00:40<00:13,  3.89it/s] 75%|███████▌  | 150/200 [00:40<00:12,  3.94it/s] 76%|███████▌  | 151/200 [00:40<00:12,  3.99it/s] 76%|███████▌  | 152/200 [00:40<00:11,  4.02it/s] 76%|███████▋  | 153/200 [00:41<00:11,  4.03it/s] 77%|███████▋  | 154/200 [00:41<00:11,  4.04it/s] 78%|███████▊  | 155/200 [00:41<00:11,  4.06it/s] 78%|███████▊  | 156/200 [00:41<00:10,  4.07it/s] 78%|███████▊  | 157/200 [00:41<00:10,  4.07it/s] 79%|███████▉  | 158/200 [00:42<00:10,  4.08it/s] 80%|███████▉  | 159/200 [00:42<00:10,  4.08it/s] 80%|████████  | 160/200 [00:42<00:09,  4.09it/s] 80%|████████  | 161/200 [00:42<00:09,  4.08it/s] 81%|████████  | 162/200 [00:43<00:09,  4.08it/s] 82%|████████▏ | 163/200 [00:43<00:09,  4.08it/s] 82%|████████▏ | 164/200 [00:43<00:08,  4.08it/s] 82%|████████▎ | 165/200 [00:43<00:08,  4.08it/s] 83%|████████▎ | 166/200 [00:44<00:08,  4.07it/s] 84%|████████▎ | 167/200 [00:44<00:08,  4.08it/s] 84%|████████▍ | 168/200 [00:44<00:07,  4.08it/s] 84%|████████▍ | 169/200 [00:44<00:07,  4.07it/s] 85%|████████▌ | 170/200 [00:45<00:07,  4.07it/s] 86%|████████▌ | 171/200 [00:45<00:07,  4.07it/s] 86%|████████▌ | 172/200 [00:45<00:06,  4.07it/s] 86%|████████▋ | 173/200 [00:45<00:06,  4.07it/s] 87%|████████▋ | 174/200 [00:46<00:06,  4.08it/s] 88%|████████▊ | 175/200 [00:46<00:06,  4.08it/s] 88%|████████▊ | 176/200 [00:46<00:05,  4.07it/s] 88%|████████▊ | 177/200 [00:46<00:05,  4.07it/s] 89%|████████▉ | 178/200 [00:47<00:05,  4.08it/s] 90%|████████▉ | 179/200 [00:47<00:05,  4.07it/s] 90%|█████████ | 180/200 [00:47<00:04,  4.07it/s] 90%|█████████ | 181/200 [00:47<00:04,  4.07it/s] 91%|█████████ | 182/200 [00:48<00:04,  4.06it/s] 92%|█████████▏| 183/200 [00:48<00:04,  3.93it/s] 92%|█████████▏| 184/200 [00:48<00:04,  3.86it/s] 92%|█████████▎| 185/200 [00:48<00:03,  3.83it/s] 93%|█████████▎| 186/200 [00:49<00:03,  3.85it/s] 94%|█████████▎| 187/200 [00:49<00:03,  3.75it/s] 94%|█████████▍| 188/200 [00:49<00:03,  3.77it/s] 94%|█████████▍| 189/200 [00:50<00:02,  3.70it/s] 95%|█████████▌| 190/200 [00:50<00:02,  3.73it/s] 96%|█████████▌| 191/200 [00:50<00:02,  3.67it/s] 96%|█████████▌| 192/200 [00:50<00:02,  3.70it/s] 96%|█████████▋| 193/200 [00:51<00:01,  3.65it/s] 97%|█████████▋| 194/200 [00:51<00:01,  3.70it/s] 98%|█████████▊| 195/200 [00:51<00:01,  3.65it/s] 98%|█████████▊| 196/200 [00:51<00:01,  3.70it/s] 98%|█████████▊| 197/200 [00:52<00:00,  3.65it/s] 99%|█████████▉| 198/200 [00:52<00:00,  3.69it/s]100%|█████████▉| 199/200 [00:52<00:00,  3.63it/s]100%|██████████| 200/200 [00:53<00:00,  3.68it/s]accuracy:  0.575
100%|██████████| 200/200 [00:56<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 191
Training dataset size: 144, validation dataset size: 191
Training dataset size: 144, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:04:21,442] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:04:22,038] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:04:29,578] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<01:00,  2.62s/it]  8%|▊         | 2/24 [00:04<00:49,  2.27s/it] 12%|█▎        | 3/24 [00:07<00:51,  2.47s/it] 17%|█▋        | 4/24 [00:10<00:51,  2.57s/it] 21%|██        | 5/24 [00:13<00:52,  2.74s/it] 25%|██▌       | 6/24 [00:15<00:48,  2.70s/it] 29%|██▉       | 7/24 [00:17<00:42,  2.53s/it] 33%|███▎      | 8/24 [00:20<00:41,  2.59s/it] 38%|███▊      | 9/24 [00:22<00:37,  2.51s/it] 42%|████▏     | 10/24 [00:25<00:33,  2.42s/it]                                               {'loss': 1.4937, 'grad_norm': 16.42169189453125, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:25<00:33,  2.42s/it] 46%|████▌     | 11/24 [00:27<00:31,  2.42s/it] 50%|█████     | 12/24 [00:30<00:29,  2.47s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:32<00:26,  2.45s/it] 58%|█████▊    | 14/24 [00:34<00:24,  2.41s/it] 62%|██████▎   | 15/24 [00:37<00:22,  2.49s/it] 67%|██████▋   | 16/24 [00:39<00:18,  2.32s/it] 71%|███████   | 17/24 [00:42<00:17,  2.48s/it] 75%|███████▌  | 18/24 [00:44<00:14,  2.49s/it] 79%|███████▉  | 19/24 [00:47<00:12,  2.50s/it] 83%|████████▎ | 20/24 [00:49<00:09,  2.48s/it]                                               {'loss': 1.1357, 'grad_norm': 12.71138858795166, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:49<00:09,  2.48s/it] 88%|████████▊ | 21/24 [00:52<00:07,  2.56s/it] 92%|█████████▏| 22/24 [00:55<00:05,  2.73s/it] 96%|█████████▌| 23/24 [00:58<00:02,  2.72s/it]100%|██████████| 24/24 [01:00<00:00,  2.56s/it]                                               {'train_runtime': 61.2218, 'train_samples_per_second': 4.704, 'train_steps_per_second': 0.392, 'train_loss': 1.3227205673853557, 'epoch': 2.0}
100%|██████████| 24/24 [01:01<00:00,  2.56s/it]100%|██████████| 24/24 [01:01<00:00,  2.54s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:26,  3.23it/s]  1%|          | 2/279 [00:00<01:19,  3.47it/s]  1%|          | 3/279 [00:00<01:17,  3.55it/s]  1%|▏         | 4/279 [00:01<01:16,  3.57it/s]  2%|▏         | 5/279 [00:01<01:13,  3.73it/s]  2%|▏         | 6/279 [00:01<01:13,  3.69it/s]  3%|▎         | 7/279 [00:01<01:14,  3.68it/s]  3%|▎         | 8/279 [00:02<01:14,  3.66it/s]  3%|▎         | 9/279 [00:02<01:11,  3.76it/s]  4%|▎         | 10/279 [00:02<01:12,  3.69it/s]  4%|▍         | 11/279 [00:02<01:11,  3.74it/s]  4%|▍         | 12/279 [00:03<01:12,  3.69it/s]  5%|▍         | 13/279 [00:03<01:10,  3.75it/s]  5%|▌         | 14/279 [00:03<01:12,  3.68it/s]  5%|▌         | 15/279 [00:04<01:11,  3.70it/s]  6%|▌         | 16/279 [00:04<01:11,  3.70it/s]  6%|▌         | 17/279 [00:04<01:09,  3.75it/s]  6%|▋         | 18/279 [00:04<01:11,  3.67it/s]  7%|▋         | 19/279 [00:05<01:11,  3.62it/s]  7%|▋         | 20/279 [00:05<01:08,  3.77it/s]  8%|▊         | 21/279 [00:05<01:08,  3.76it/s]  8%|▊         | 22/279 [00:05<01:09,  3.71it/s]  8%|▊         | 23/279 [00:06<01:10,  3.65it/s]  9%|▊         | 24/279 [00:06<01:07,  3.78it/s]  9%|▉         | 25/279 [00:06<01:07,  3.75it/s]  9%|▉         | 26/279 [00:07<01:08,  3.70it/s] 10%|▉         | 27/279 [00:07<01:09,  3.64it/s] 10%|█         | 28/279 [00:07<01:06,  3.77it/s] 10%|█         | 29/279 [00:07<01:07,  3.72it/s] 11%|█         | 30/279 [00:08<01:07,  3.69it/s] 11%|█         | 31/279 [00:08<01:07,  3.67it/s] 11%|█▏        | 32/279 [00:08<01:05,  3.78it/s] 12%|█▏        | 33/279 [00:08<01:06,  3.73it/s] 12%|█▏        | 34/279 [00:09<01:06,  3.70it/s] 13%|█▎        | 35/279 [00:09<01:06,  3.68it/s] 13%|█▎        | 36/279 [00:09<01:04,  3.79it/s] 13%|█▎        | 37/279 [00:10<01:04,  3.73it/s] 14%|█▎        | 38/279 [00:10<01:05,  3.71it/s] 14%|█▍        | 39/279 [00:10<01:05,  3.67it/s] 14%|█▍        | 40/279 [00:10<01:03,  3.78it/s] 15%|█▍        | 41/279 [00:11<01:03,  3.73it/s] 15%|█▌        | 42/279 [00:11<01:04,  3.70it/s] 15%|█▌        | 43/279 [00:11<01:04,  3.67it/s] 16%|█▌        | 44/279 [00:11<01:02,  3.77it/s] 16%|█▌        | 45/279 [00:12<01:03,  3.71it/s] 16%|█▋        | 46/279 [00:12<01:02,  3.70it/s] 17%|█▋        | 47/279 [00:12<01:03,  3.67it/s] 17%|█▋        | 48/279 [00:12<01:01,  3.75it/s] 18%|█▊        | 49/279 [00:13<01:02,  3.68it/s] 18%|█▊        | 50/279 [00:13<01:01,  3.70it/s] 18%|█▊        | 51/279 [00:13<01:01,  3.71it/s] 19%|█▊        | 52/279 [00:14<01:00,  3.76it/s] 19%|█▉        | 53/279 [00:14<01:01,  3.69it/s] 19%|█▉        | 54/279 [00:14<01:00,  3.74it/s] 20%|█▉        | 55/279 [00:14<01:00,  3.70it/s] 20%|██        | 56/279 [00:15<00:59,  3.75it/s] 20%|██        | 57/279 [00:15<01:00,  3.67it/s] 21%|██        | 58/279 [00:15<01:00,  3.68it/s] 21%|██        | 59/279 [00:15<00:59,  3.70it/s] 22%|██▏       | 60/279 [00:16<00:58,  3.75it/s] 22%|██▏       | 61/279 [00:16<00:59,  3.67it/s] 22%|██▏       | 62/279 [00:16<00:58,  3.69it/s] 23%|██▎       | 63/279 [00:16<00:56,  3.80it/s] 23%|██▎       | 64/279 [00:17<00:55,  3.90it/s] 23%|██▎       | 65/279 [00:17<00:53,  3.97it/s] 24%|██▎       | 66/279 [00:17<00:52,  4.02it/s] 24%|██▍       | 67/279 [00:17<00:52,  4.06it/s] 24%|██▍       | 68/279 [00:18<00:51,  4.08it/s] 25%|██▍       | 69/279 [00:18<00:51,  4.10it/s] 25%|██▌       | 70/279 [00:18<00:50,  4.11it/s] 25%|██▌       | 71/279 [00:18<00:50,  4.10it/s] 26%|██▌       | 72/279 [00:19<00:50,  4.09it/s] 26%|██▌       | 73/279 [00:19<00:50,  4.10it/s] 27%|██▋       | 74/279 [00:19<00:49,  4.12it/s] 27%|██▋       | 75/279 [00:19<00:49,  4.13it/s] 27%|██▋       | 76/279 [00:20<00:49,  4.14it/s] 28%|██▊       | 77/279 [00:20<00:49,  4.06it/s] 28%|██▊       | 78/279 [00:20<00:50,  3.97it/s] 28%|██▊       | 79/279 [00:20<00:51,  3.88it/s] 29%|██▊       | 80/279 [00:21<00:51,  3.86it/s] 29%|██▉       | 81/279 [00:21<00:50,  3.93it/s] 29%|██▉       | 82/279 [00:21<00:50,  3.91it/s] 30%|██▉       | 83/279 [00:21<00:50,  3.86it/s] 30%|███       | 84/279 [00:22<00:51,  3.81it/s] 30%|███       | 85/279 [00:22<00:52,  3.71it/s] 31%|███       | 86/279 [00:22<00:50,  3.84it/s] 31%|███       | 87/279 [00:23<00:49,  3.88it/s] 32%|███▏      | 88/279 [00:23<00:49,  3.83it/s] 32%|███▏      | 89/279 [00:23<00:50,  3.79it/s] 32%|███▏      | 90/279 [00:23<00:51,  3.70it/s] 33%|███▎      | 91/279 [00:24<00:49,  3.82it/s] 33%|███▎      | 92/279 [00:24<00:48,  3.88it/s] 33%|███▎      | 93/279 [00:24<00:48,  3.83it/s] 34%|███▎      | 94/279 [00:24<00:48,  3.78it/s] 34%|███▍      | 95/279 [00:25<00:49,  3.69it/s] 34%|███▍      | 96/279 [00:25<00:48,  3.81it/s] 35%|███▍      | 97/279 [00:25<00:47,  3.87it/s] 35%|███▌      | 98/279 [00:25<00:47,  3.82it/s] 35%|███▌      | 99/279 [00:26<00:47,  3.76it/s] 36%|███▌      | 100/279 [00:26<00:48,  3.68it/s] 36%|███▌      | 101/279 [00:26<00:46,  3.81it/s] 37%|███▋      | 102/279 [00:26<00:45,  3.87it/s] 37%|███▋      | 103/279 [00:27<00:45,  3.83it/s] 37%|███▋      | 104/279 [00:27<00:46,  3.76it/s] 38%|███▊      | 105/279 [00:27<00:47,  3.68it/s] 38%|███▊      | 106/279 [00:28<00:45,  3.80it/s] 38%|███▊      | 107/279 [00:28<00:44,  3.85it/s] 39%|███▊      | 108/279 [00:28<00:44,  3.80it/s] 39%|███▉      | 109/279 [00:28<00:45,  3.76it/s] 39%|███▉      | 110/279 [00:29<00:45,  3.68it/s] 40%|███▉      | 111/279 [00:29<00:44,  3.81it/s] 40%|████      | 112/279 [00:29<00:43,  3.86it/s] 41%|████      | 113/279 [00:29<00:43,  3.81it/s] 41%|████      | 114/279 [00:30<00:43,  3.77it/s] 41%|████      | 115/279 [00:30<00:44,  3.68it/s] 42%|████▏     | 116/279 [00:30<00:42,  3.81it/s] 42%|████▏     | 117/279 [00:30<00:41,  3.88it/s] 42%|████▏     | 118/279 [00:31<00:42,  3.82it/s] 43%|████▎     | 119/279 [00:31<00:42,  3.77it/s] 43%|████▎     | 120/279 [00:31<00:43,  3.67it/s] 43%|████▎     | 121/279 [00:31<00:41,  3.80it/s] 44%|████▎     | 122/279 [00:32<00:40,  3.88it/s] 44%|████▍     | 123/279 [00:32<00:40,  3.84it/s] 44%|████▍     | 124/279 [00:32<00:40,  3.78it/s] 45%|████▍     | 125/279 [00:33<00:40,  3.79it/s] 45%|████▌     | 126/279 [00:33<00:41,  3.71it/s] 46%|████▌     | 127/279 [00:33<00:39,  3.82it/s] 46%|████▌     | 128/279 [00:33<00:39,  3.82it/s] 46%|████▌     | 129/279 [00:34<00:39,  3.77it/s] 47%|████▋     | 130/279 [00:34<00:39,  3.78it/s] 47%|████▋     | 131/279 [00:34<00:39,  3.71it/s] 47%|████▋     | 132/279 [00:34<00:38,  3.82it/s] 48%|████▊     | 133/279 [00:35<00:38,  3.83it/s] 48%|████▊     | 134/279 [00:35<00:38,  3.78it/s] 48%|████▊     | 135/279 [00:35<00:38,  3.78it/s] 49%|████▊     | 136/279 [00:35<00:38,  3.71it/s] 49%|████▉     | 137/279 [00:36<00:37,  3.83it/s] 49%|████▉     | 138/279 [00:36<00:37,  3.81it/s] 50%|████▉     | 139/279 [00:36<00:37,  3.75it/s] 50%|█████     | 140/279 [00:37<00:36,  3.78it/s] 51%|█████     | 141/279 [00:37<00:37,  3.71it/s] 51%|█████     | 142/279 [00:37<00:35,  3.81it/s] 51%|█████▏    | 143/279 [00:37<00:35,  3.81it/s] 52%|█████▏    | 144/279 [00:38<00:36,  3.75it/s] 52%|█████▏    | 145/279 [00:38<00:35,  3.76it/s] 52%|█████▏    | 146/279 [00:38<00:36,  3.69it/s] 53%|█████▎    | 147/279 [00:38<00:34,  3.80it/s] 53%|█████▎    | 148/279 [00:39<00:34,  3.79it/s] 53%|█████▎    | 149/279 [00:39<00:34,  3.73it/s] 54%|█████▍    | 150/279 [00:39<00:34,  3.72it/s] 54%|█████▍    | 151/279 [00:39<00:33,  3.79it/s] 54%|█████▍    | 152/279 [00:40<00:32,  3.86it/s] 55%|█████▍    | 153/279 [00:40<00:32,  3.93it/s] 55%|█████▌    | 154/279 [00:40<00:31,  3.98it/s] 56%|█████▌    | 155/279 [00:40<00:30,  4.01it/s] 56%|█████▌    | 156/279 [00:41<00:30,  4.03it/s] 56%|█████▋    | 157/279 [00:41<00:30,  4.05it/s] 57%|█████▋    | 158/279 [00:41<00:29,  4.06it/s] 57%|█████▋    | 159/279 [00:41<00:29,  4.06it/s] 57%|█████▋    | 160/279 [00:42<00:29,  4.06it/s] 58%|█████▊    | 161/279 [00:42<00:28,  4.07it/s] 58%|█████▊    | 162/279 [00:42<00:28,  4.08it/s] 58%|█████▊    | 163/279 [00:42<00:28,  4.07it/s] 59%|█████▉    | 164/279 [00:43<00:29,  3.96it/s] 59%|█████▉    | 165/279 [00:43<00:29,  3.87it/s] 59%|█████▉    | 166/279 [00:43<00:29,  3.81it/s] 60%|█████▉    | 167/279 [00:43<00:28,  3.88it/s] 60%|██████    | 168/279 [00:44<00:28,  3.87it/s] 61%|██████    | 169/279 [00:44<00:29,  3.77it/s] 61%|██████    | 170/279 [00:44<00:29,  3.70it/s] 61%|██████▏   | 171/279 [00:45<00:29,  3.61it/s] 62%|██████▏   | 172/279 [00:45<00:28,  3.74it/s] 62%|██████▏   | 173/279 [00:45<00:27,  3.83it/s] 62%|██████▏   | 174/279 [00:45<00:28,  3.73it/s] 63%|██████▎   | 175/279 [00:46<00:28,  3.69it/s] 63%|██████▎   | 176/279 [00:46<00:28,  3.66it/s] 63%|██████▎   | 177/279 [00:46<00:26,  3.78it/s] 64%|██████▍   | 178/279 [00:46<00:26,  3.87it/s] 64%|██████▍   | 179/279 [00:47<00:25,  3.93it/s] 65%|██████▍   | 180/279 [00:47<00:24,  3.97it/s] 65%|██████▍   | 181/279 [00:47<00:24,  4.01it/s] 65%|██████▌   | 182/279 [00:47<00:24,  4.04it/s] 66%|██████▌   | 183/279 [00:48<00:23,  4.04it/s] 66%|██████▌   | 184/279 [00:48<00:23,  4.05it/s] 66%|██████▋   | 185/279 [00:48<00:23,  4.07it/s] 67%|██████▋   | 186/279 [00:48<00:22,  4.07it/s] 67%|██████▋   | 187/279 [00:49<00:23,  3.93it/s] 67%|██████▋   | 188/279 [00:49<00:23,  3.86it/s] 68%|██████▊   | 189/279 [00:49<00:23,  3.84it/s] 68%|██████▊   | 190/279 [00:49<00:22,  3.92it/s] 68%|██████▊   | 191/279 [00:50<00:22,  3.89it/s] 69%|██████▉   | 192/279 [00:50<00:22,  3.86it/s] 69%|██████▉   | 193/279 [00:50<00:22,  3.77it/s] 70%|██████▉   | 194/279 [00:50<00:22,  3.80it/s] 70%|██████▉   | 195/279 [00:51<00:21,  3.85it/s] 70%|███████   | 196/279 [00:51<00:21,  3.79it/s] 71%|███████   | 197/279 [00:51<00:21,  3.77it/s] 71%|███████   | 198/279 [00:52<00:21,  3.71it/s] 71%|███████▏  | 199/279 [00:52<00:20,  3.82it/s] 72%|███████▏  | 200/279 [00:52<00:20,  3.83it/s] 72%|███████▏  | 201/279 [00:52<00:20,  3.81it/s] 72%|███████▏  | 202/279 [00:53<00:20,  3.74it/s] 73%|███████▎  | 203/279 [00:53<00:20,  3.77it/s] 73%|███████▎  | 204/279 [00:53<00:19,  3.82it/s] 73%|███████▎  | 205/279 [00:53<00:19,  3.77it/s] 74%|███████▍  | 206/279 [00:54<00:19,  3.76it/s] 74%|███████▍  | 207/279 [00:54<00:19,  3.70it/s] 75%|███████▍  | 208/279 [00:54<00:18,  3.81it/s] 75%|███████▍  | 209/279 [00:54<00:18,  3.80it/s] 75%|███████▌  | 210/279 [00:55<00:18,  3.78it/s] 76%|███████▌  | 211/279 [00:55<00:18,  3.72it/s] 76%|███████▌  | 212/279 [00:55<00:17,  3.76it/s] 76%|███████▋  | 213/279 [00:55<00:17,  3.86it/s] 77%|███████▋  | 214/279 [00:56<00:17,  3.82it/s] 77%|███████▋  | 215/279 [00:56<00:16,  3.77it/s] 77%|███████▋  | 216/279 [00:56<00:17,  3.66it/s] 78%|███████▊  | 217/279 [00:57<00:16,  3.78it/s] 78%|███████▊  | 218/279 [00:57<00:15,  3.84it/s] 78%|███████▊  | 219/279 [00:57<00:15,  3.78it/s] 79%|███████▉  | 220/279 [00:57<00:15,  3.77it/s] 79%|███████▉  | 221/279 [00:58<00:15,  3.70it/s] 80%|███████▉  | 222/279 [00:58<00:14,  3.80it/s] 80%|███████▉  | 223/279 [00:58<00:14,  3.82it/s] 80%|████████  | 224/279 [00:58<00:14,  3.79it/s] 81%|████████  | 225/279 [00:59<00:14,  3.76it/s] 81%|████████  | 226/279 [00:59<00:14,  3.69it/s] 81%|████████▏ | 227/279 [00:59<00:13,  3.81it/s] 82%|████████▏ | 228/279 [00:59<00:13,  3.80it/s] 82%|████████▏ | 229/279 [01:00<00:13,  3.79it/s] 82%|████████▏ | 230/279 [01:00<00:13,  3.72it/s] 83%|████████▎ | 231/279 [01:00<00:12,  3.76it/s] 83%|████████▎ | 232/279 [01:01<00:12,  3.83it/s] 84%|████████▎ | 233/279 [01:01<00:12,  3.79it/s] 84%|████████▍ | 234/279 [01:01<00:11,  3.76it/s] 84%|████████▍ | 235/279 [01:01<00:11,  3.70it/s] 85%|████████▍ | 236/279 [01:02<00:11,  3.81it/s] 85%|████████▍ | 237/279 [01:02<00:11,  3.80it/s] 85%|████████▌ | 238/279 [01:02<00:10,  3.78it/s] 86%|████████▌ | 239/279 [01:02<00:10,  3.71it/s] 86%|████████▌ | 240/279 [01:03<00:10,  3.76it/s] 86%|████████▋ | 241/279 [01:03<00:09,  3.86it/s] 87%|████████▋ | 242/279 [01:03<00:09,  3.82it/s] 87%|████████▋ | 243/279 [01:03<00:09,  3.77it/s] 87%|████████▋ | 244/279 [01:04<00:09,  3.69it/s] 88%|████████▊ | 245/279 [01:04<00:08,  3.79it/s] 88%|████████▊ | 246/279 [01:04<00:08,  3.81it/s] 89%|████████▊ | 247/279 [01:04<00:08,  3.77it/s] 89%|████████▉ | 248/279 [01:05<00:08,  3.75it/s] 89%|████████▉ | 249/279 [01:05<00:08,  3.70it/s] 90%|████████▉ | 250/279 [01:05<00:07,  3.80it/s] 90%|████████▉ | 251/279 [01:06<00:07,  3.81it/s] 90%|█████████ | 252/279 [01:06<00:07,  3.80it/s] 91%|█████████ | 253/279 [01:06<00:06,  3.72it/s] 91%|█████████ | 254/279 [01:06<00:06,  3.76it/s] 91%|█████████▏| 255/279 [01:07<00:06,  3.84it/s] 92%|█████████▏| 256/279 [01:07<00:06,  3.79it/s] 92%|█████████▏| 257/279 [01:07<00:05,  3.76it/s] 92%|█████████▏| 258/279 [01:07<00:05,  3.70it/s] 93%|█████████▎| 259/279 [01:08<00:05,  3.80it/s] 93%|█████████▎| 260/279 [01:08<00:04,  3.80it/s] 94%|█████████▎| 261/279 [01:08<00:04,  3.78it/s] 94%|█████████▍| 262/279 [01:08<00:04,  3.76it/s] 94%|█████████▍| 263/279 [01:09<00:04,  3.69it/s] 95%|█████████▍| 264/279 [01:09<00:03,  3.79it/s] 95%|█████████▍| 265/279 [01:09<00:03,  3.79it/s] 95%|█████████▌| 266/279 [01:10<00:03,  3.78it/s] 96%|█████████▌| 267/279 [01:10<00:03,  3.70it/s] 96%|█████████▌| 268/279 [01:10<00:02,  3.75it/s] 96%|█████████▋| 269/279 [01:10<00:02,  3.79it/s] 97%|█████████▋| 270/279 [01:11<00:02,  3.76it/s] 97%|█████████▋| 271/279 [01:11<00:02,  3.74it/s] 97%|█████████▋| 272/279 [01:11<00:01,  3.68it/s] 98%|█████████▊| 273/279 [01:11<00:01,  3.78it/s] 98%|█████████▊| 274/279 [01:12<00:01,  3.79it/s] 99%|█████████▊| 275/279 [01:12<00:01,  3.78it/s] 99%|█████████▉| 276/279 [01:12<00:00,  3.71it/s] 99%|█████████▉| 277/279 [01:12<00:00,  3.75it/s]100%|█████████▉| 278/279 [01:13<00:00,  3.83it/s]100%|██████████| 279/279 [01:13<00:00,  3.79it/s]accuracy:  0.5232974910394266
100%|██████████| 279/279 [01:17<00:00,  3.59it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 241
Training dataset size: 144, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:07:15,821] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it][93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:07:17,195] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:07:17,245] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:58,  2.52s/it]  8%|▊         | 2/24 [00:04<00:46,  2.10s/it] 12%|█▎        | 3/24 [00:06<00:46,  2.23s/it] 17%|█▋        | 4/24 [00:09<00:46,  2.31s/it] 21%|██        | 5/24 [00:11<00:42,  2.23s/it] 25%|██▌       | 6/24 [00:14<00:45,  2.54s/it] 29%|██▉       | 7/24 [00:16<00:41,  2.44s/it] 33%|███▎      | 8/24 [00:18<00:37,  2.35s/it] 38%|███▊      | 9/24 [00:20<00:33,  2.25s/it] 42%|████▏     | 10/24 [00:23<00:32,  2.35s/it]                                               {'loss': 0.3471, 'grad_norm': 0.6346194744110107, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:23<00:32,  2.35s/it] 46%|████▌     | 11/24 [00:25<00:30,  2.32s/it] 50%|█████     | 12/24 [00:28<00:29,  2.45s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:31<00:28,  2.56s/it] 58%|█████▊    | 14/24 [00:33<00:24,  2.47s/it] 62%|██████▎   | 15/24 [00:35<00:22,  2.45s/it] 67%|██████▋   | 16/24 [00:38<00:18,  2.37s/it] 71%|███████   | 17/24 [00:40<00:15,  2.25s/it] 75%|███████▌  | 18/24 [00:42<00:13,  2.32s/it] 79%|███████▉  | 19/24 [00:44<00:11,  2.26s/it] 83%|████████▎ | 20/24 [00:46<00:08,  2.21s/it]                                               {'loss': 0.4599, 'grad_norm': 2.2489609718322754, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:46<00:08,  2.21s/it] 88%|████████▊ | 21/24 [00:49<00:07,  2.41s/it] 92%|█████████▏| 22/24 [00:51<00:04,  2.37s/it] 96%|█████████▌| 23/24 [00:54<00:02,  2.34s/it]100%|██████████| 24/24 [00:57<00:00,  2.50s/it]                                               {'train_runtime': 57.6378, 'train_samples_per_second': 4.997, 'train_steps_per_second': 0.416, 'train_loss': 0.37074973434209824, 'epoch': 2.0}
100%|██████████| 24/24 [00:57<00:00,  2.50s/it]100%|██████████| 24/24 [00:57<00:00,  2.39s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:29,  3.04it/s]  1%|          | 2/272 [00:00<01:17,  3.49it/s]  1%|          | 3/272 [00:00<01:15,  3.55it/s]  1%|▏         | 4/272 [00:01<01:13,  3.66it/s]  2%|▏         | 5/272 [00:01<01:13,  3.64it/s]  2%|▏         | 6/272 [00:01<01:10,  3.80it/s]  3%|▎         | 7/272 [00:01<01:10,  3.76it/s]  3%|▎         | 8/272 [00:02<01:10,  3.75it/s]  3%|▎         | 9/272 [00:02<01:09,  3.80it/s]  4%|▎         | 10/272 [00:02<01:10,  3.74it/s]  4%|▍         | 11/272 [00:02<01:08,  3.82it/s]  4%|▍         | 12/272 [00:03<01:08,  3.78it/s]  5%|▍         | 13/272 [00:03<01:09,  3.75it/s]  5%|▌         | 14/272 [00:03<01:10,  3.68it/s]  6%|▌         | 15/272 [00:04<01:07,  3.81it/s]  6%|▌         | 16/272 [00:04<01:06,  3.83it/s]  6%|▋         | 17/272 [00:04<01:07,  3.75it/s]  7%|▋         | 18/272 [00:04<01:06,  3.79it/s]  7%|▋         | 19/272 [00:05<01:07,  3.72it/s]  7%|▋         | 20/272 [00:05<01:06,  3.79it/s]  8%|▊         | 21/272 [00:05<01:06,  3.76it/s]  8%|▊         | 22/272 [00:05<01:07,  3.73it/s]  8%|▊         | 23/272 [00:06<01:07,  3.67it/s]  9%|▉         | 24/272 [00:06<01:05,  3.79it/s]  9%|▉         | 25/272 [00:06<01:04,  3.81it/s] 10%|▉         | 26/272 [00:06<01:05,  3.74it/s] 10%|▉         | 27/272 [00:07<01:04,  3.78it/s] 10%|█         | 28/272 [00:07<01:05,  3.71it/s] 11%|█         | 29/272 [00:07<01:04,  3.79it/s] 11%|█         | 30/272 [00:08<01:04,  3.76it/s] 11%|█▏        | 31/272 [00:08<01:04,  3.73it/s] 12%|█▏        | 32/272 [00:08<01:05,  3.67it/s] 12%|█▏        | 33/272 [00:08<01:03,  3.79it/s] 12%|█▎        | 34/272 [00:09<01:02,  3.81it/s] 13%|█▎        | 35/272 [00:09<01:03,  3.75it/s] 13%|█▎        | 36/272 [00:09<01:02,  3.79it/s] 14%|█▎        | 37/272 [00:09<01:02,  3.73it/s] 14%|█▍        | 38/272 [00:10<01:01,  3.81it/s] 14%|█▍        | 39/272 [00:10<01:01,  3.78it/s] 15%|█▍        | 40/272 [00:10<01:01,  3.75it/s] 15%|█▌        | 41/272 [00:10<01:02,  3.68it/s] 15%|█▌        | 42/272 [00:11<01:00,  3.81it/s] 16%|█▌        | 43/272 [00:11<00:59,  3.83it/s] 16%|█▌        | 44/272 [00:11<01:00,  3.76it/s] 17%|█▋        | 45/272 [00:12<01:00,  3.76it/s] 17%|█▋        | 46/272 [00:12<01:00,  3.71it/s] 17%|█▋        | 47/272 [00:12<00:58,  3.82it/s] 18%|█▊        | 48/272 [00:12<00:58,  3.80it/s] 18%|█▊        | 49/272 [00:13<00:59,  3.74it/s] 18%|█▊        | 50/272 [00:13<00:58,  3.79it/s] 19%|█▉        | 51/272 [00:13<00:59,  3.73it/s] 19%|█▉        | 52/272 [00:13<00:57,  3.83it/s] 19%|█▉        | 53/272 [00:14<00:57,  3.78it/s] 20%|█▉        | 54/272 [00:14<00:58,  3.74it/s] 20%|██        | 55/272 [00:14<00:59,  3.67it/s] 21%|██        | 56/272 [00:14<00:56,  3.80it/s] 21%|██        | 57/272 [00:15<00:56,  3.82it/s] 21%|██▏       | 58/272 [00:15<00:57,  3.75it/s] 22%|██▏       | 59/272 [00:15<00:56,  3.76it/s] 22%|██▏       | 60/272 [00:16<00:57,  3.71it/s] 22%|██▏       | 61/272 [00:16<00:55,  3.83it/s] 23%|██▎       | 62/272 [00:16<00:54,  3.82it/s] 23%|██▎       | 63/272 [00:16<00:55,  3.76it/s] 24%|██▎       | 64/272 [00:17<00:54,  3.78it/s] 24%|██▍       | 65/272 [00:17<00:55,  3.72it/s] 24%|██▍       | 66/272 [00:17<00:53,  3.83it/s] 25%|██▍       | 67/272 [00:17<00:54,  3.78it/s] 25%|██▌       | 68/272 [00:18<00:54,  3.72it/s] 25%|██▌       | 69/272 [00:18<00:54,  3.73it/s] 26%|██▌       | 70/272 [00:18<00:53,  3.77it/s] 26%|██▌       | 71/272 [00:18<00:52,  3.82it/s] 26%|██▋       | 72/272 [00:19<00:53,  3.76it/s] 27%|██▋       | 73/272 [00:19<00:53,  3.74it/s] 27%|██▋       | 74/272 [00:19<00:53,  3.67it/s] 28%|██▊       | 75/272 [00:19<00:51,  3.80it/s] 28%|██▊       | 76/272 [00:20<00:51,  3.83it/s] 28%|██▊       | 77/272 [00:20<00:51,  3.76it/s] 29%|██▊       | 78/272 [00:20<00:51,  3.76it/s] 29%|██▉       | 79/272 [00:21<00:52,  3.68it/s] 29%|██▉       | 80/272 [00:21<00:50,  3.81it/s] 30%|██▉       | 81/272 [00:21<00:49,  3.82it/s] 30%|███       | 82/272 [00:21<00:50,  3.74it/s] 31%|███       | 83/272 [00:22<00:50,  3.76it/s] 31%|███       | 84/272 [00:22<00:50,  3.69it/s] 31%|███▏      | 85/272 [00:22<00:49,  3.80it/s] 32%|███▏      | 86/272 [00:22<00:49,  3.79it/s] 32%|███▏      | 87/272 [00:23<00:49,  3.74it/s] 32%|███▏      | 88/272 [00:23<00:48,  3.78it/s] 33%|███▎      | 89/272 [00:23<00:49,  3.71it/s] 33%|███▎      | 90/272 [00:23<00:47,  3.81it/s] 33%|███▎      | 91/272 [00:24<00:48,  3.76it/s] 34%|███▍      | 92/272 [00:24<00:48,  3.73it/s] 34%|███▍      | 93/272 [00:24<00:48,  3.66it/s] 35%|███▍      | 94/272 [00:25<00:46,  3.79it/s] 35%|███▍      | 95/272 [00:25<00:46,  3.82it/s] 35%|███▌      | 96/272 [00:25<00:46,  3.75it/s] 36%|███▌      | 97/272 [00:25<00:46,  3.75it/s] 36%|███▌      | 98/272 [00:26<00:47,  3.68it/s] 36%|███▋      | 99/272 [00:26<00:45,  3.80it/s] 37%|███▋      | 100/272 [00:26<00:45,  3.81it/s] 37%|███▋      | 101/272 [00:26<00:45,  3.75it/s] 38%|███▊      | 102/272 [00:27<00:45,  3.74it/s] 38%|███▊      | 103/272 [00:27<00:46,  3.67it/s] 38%|███▊      | 104/272 [00:27<00:44,  3.80it/s] 39%|███▊      | 105/272 [00:27<00:43,  3.82it/s] 39%|███▉      | 106/272 [00:28<00:44,  3.75it/s] 39%|███▉      | 107/272 [00:28<00:43,  3.76it/s] 40%|███▉      | 108/272 [00:28<00:44,  3.71it/s] 40%|████      | 109/272 [00:29<00:42,  3.82it/s] 40%|████      | 110/272 [00:29<00:43,  3.77it/s] 41%|████      | 111/272 [00:29<00:43,  3.73it/s] 41%|████      | 112/272 [00:29<00:43,  3.66it/s] 42%|████▏     | 113/272 [00:30<00:42,  3.78it/s] 42%|████▏     | 114/272 [00:30<00:41,  3.81it/s] 42%|████▏     | 115/272 [00:30<00:41,  3.74it/s] 43%|████▎     | 116/272 [00:30<00:41,  3.76it/s] 43%|████▎     | 117/272 [00:31<00:41,  3.70it/s] 43%|████▎     | 118/272 [00:31<00:40,  3.81it/s] 44%|████▍     | 119/272 [00:31<00:40,  3.79it/s] 44%|████▍     | 120/272 [00:31<00:40,  3.72it/s] 44%|████▍     | 121/272 [00:32<00:40,  3.76it/s] 45%|████▍     | 122/272 [00:32<00:40,  3.69it/s] 45%|████▌     | 123/272 [00:32<00:39,  3.79it/s] 46%|████▌     | 124/272 [00:33<00:39,  3.75it/s] 46%|████▌     | 125/272 [00:33<00:39,  3.71it/s] 46%|████▋     | 126/272 [00:33<00:40,  3.65it/s] 47%|████▋     | 127/272 [00:33<00:38,  3.78it/s] 47%|████▋     | 128/272 [00:34<00:37,  3.81it/s] 47%|████▋     | 129/272 [00:34<00:38,  3.74it/s] 48%|████▊     | 130/272 [00:34<00:38,  3.74it/s] 48%|████▊     | 131/272 [00:34<00:38,  3.67it/s] 49%|████▊     | 132/272 [00:35<00:36,  3.79it/s] 49%|████▉     | 133/272 [00:35<00:36,  3.80it/s] 49%|████▉     | 134/272 [00:35<00:37,  3.73it/s] 50%|████▉     | 135/272 [00:35<00:36,  3.75it/s] 50%|█████     | 136/272 [00:36<00:36,  3.69it/s] 50%|█████     | 137/272 [00:36<00:35,  3.79it/s] 51%|█████     | 138/272 [00:36<00:35,  3.75it/s] 51%|█████     | 139/272 [00:37<00:35,  3.71it/s] 51%|█████▏    | 140/272 [00:37<00:36,  3.65it/s] 52%|█████▏    | 141/272 [00:37<00:34,  3.79it/s] 52%|█████▏    | 142/272 [00:37<00:34,  3.81it/s] 53%|█████▎    | 143/272 [00:38<00:34,  3.76it/s] 53%|█████▎    | 144/272 [00:38<00:34,  3.73it/s] 53%|█████▎    | 145/272 [00:38<00:34,  3.66it/s] 54%|█████▎    | 146/272 [00:38<00:33,  3.77it/s] 54%|█████▍    | 147/272 [00:39<00:32,  3.79it/s] 54%|█████▍    | 148/272 [00:39<00:33,  3.72it/s] 55%|█████▍    | 149/272 [00:39<00:32,  3.76it/s] 55%|█████▌    | 150/272 [00:39<00:32,  3.70it/s] 56%|█████▌    | 151/272 [00:40<00:31,  3.79it/s] 56%|█████▌    | 152/272 [00:40<00:32,  3.75it/s] 56%|█████▋    | 153/272 [00:40<00:32,  3.71it/s] 57%|█████▋    | 154/272 [00:41<00:32,  3.65it/s] 57%|█████▋    | 155/272 [00:41<00:31,  3.77it/s] 57%|█████▋    | 156/272 [00:41<00:30,  3.77it/s] 58%|█████▊    | 157/272 [00:41<00:31,  3.71it/s] 58%|█████▊    | 158/272 [00:42<00:30,  3.73it/s] 58%|█████▊    | 159/272 [00:42<00:30,  3.69it/s] 59%|█████▉    | 160/272 [00:42<00:29,  3.76it/s] 59%|█████▉    | 161/272 [00:42<00:30,  3.69it/s] 60%|█████▉    | 162/272 [00:43<00:29,  3.70it/s] 60%|█████▉    | 163/272 [00:43<00:29,  3.64it/s] 60%|██████    | 164/272 [00:43<00:28,  3.76it/s] 61%|██████    | 165/272 [00:43<00:28,  3.78it/s] 61%|██████    | 166/272 [00:44<00:28,  3.71it/s] 61%|██████▏   | 167/272 [00:44<00:28,  3.72it/s] 62%|██████▏   | 168/272 [00:44<00:28,  3.70it/s] 62%|██████▏   | 169/272 [00:45<00:27,  3.77it/s] 62%|██████▎   | 170/272 [00:45<00:27,  3.74it/s] 63%|██████▎   | 171/272 [00:45<00:27,  3.70it/s] 63%|██████▎   | 172/272 [00:45<00:27,  3.65it/s] 64%|██████▎   | 173/272 [00:46<00:26,  3.76it/s] 64%|██████▍   | 174/272 [00:46<00:26,  3.77it/s] 64%|██████▍   | 175/272 [00:46<00:26,  3.71it/s] 65%|██████▍   | 176/272 [00:46<00:25,  3.74it/s] 65%|██████▌   | 177/272 [00:47<00:25,  3.68it/s] 65%|██████▌   | 178/272 [00:47<00:24,  3.77it/s] 66%|██████▌   | 179/272 [00:47<00:24,  3.73it/s] 66%|██████▌   | 180/272 [00:48<00:24,  3.70it/s] 67%|██████▋   | 181/272 [00:48<00:24,  3.64it/s] 67%|██████▋   | 182/272 [00:48<00:23,  3.76it/s] 67%|██████▋   | 183/272 [00:48<00:23,  3.78it/s] 68%|██████▊   | 184/272 [00:49<00:23,  3.72it/s] 68%|██████▊   | 185/272 [00:49<00:23,  3.71it/s] 68%|██████▊   | 186/272 [00:49<00:23,  3.67it/s] 69%|██████▉   | 187/272 [00:49<00:22,  3.79it/s] 69%|██████▉   | 188/272 [00:50<00:22,  3.74it/s] 69%|██████▉   | 189/272 [00:50<00:22,  3.71it/s] 70%|██████▉   | 190/272 [00:50<00:21,  3.75it/s] 70%|███████   | 191/272 [00:50<00:22,  3.68it/s] 71%|███████   | 192/272 [00:51<00:21,  3.75it/s] 71%|███████   | 193/272 [00:51<00:21,  3.70it/s] 71%|███████▏  | 194/272 [00:51<00:21,  3.70it/s] 72%|███████▏  | 195/272 [00:52<00:21,  3.63it/s] 72%|███████▏  | 196/272 [00:52<00:20,  3.76it/s] 72%|███████▏  | 197/272 [00:52<00:19,  3.78it/s] 73%|███████▎  | 198/272 [00:52<00:19,  3.71it/s] 73%|███████▎  | 199/272 [00:53<00:19,  3.71it/s] 74%|███████▎  | 200/272 [00:53<00:19,  3.64it/s] 74%|███████▍  | 201/272 [00:53<00:18,  3.77it/s] 74%|███████▍  | 202/272 [00:53<00:18,  3.79it/s] 75%|███████▍  | 203/272 [00:54<00:18,  3.72it/s] 75%|███████▌  | 204/272 [00:54<00:18,  3.72it/s] 75%|███████▌  | 205/272 [00:54<00:18,  3.64it/s] 76%|███████▌  | 206/272 [00:55<00:17,  3.77it/s] 76%|███████▌  | 207/272 [00:55<00:17,  3.79it/s] 76%|███████▋  | 208/272 [00:55<00:17,  3.73it/s] 77%|███████▋  | 209/272 [00:55<00:16,  3.72it/s] 77%|███████▋  | 210/272 [00:56<00:16,  3.65it/s] 78%|███████▊  | 211/272 [00:56<00:16,  3.77it/s] 78%|███████▊  | 212/272 [00:56<00:15,  3.80it/s] 78%|███████▊  | 213/272 [00:56<00:15,  3.73it/s] 79%|███████▊  | 214/272 [00:57<00:15,  3.72it/s] 79%|███████▉  | 215/272 [00:57<00:15,  3.65it/s] 79%|███████▉  | 216/272 [00:57<00:14,  3.76it/s] 80%|███████▉  | 217/272 [00:57<00:14,  3.77it/s] 80%|████████  | 218/272 [00:58<00:14,  3.70it/s] 81%|████████  | 219/272 [00:58<00:14,  3.74it/s] 81%|████████  | 220/272 [00:58<00:14,  3.67it/s] 81%|████████▏ | 221/272 [00:59<00:13,  3.77it/s] 82%|████████▏ | 222/272 [00:59<00:13,  3.72it/s] 82%|████████▏ | 223/272 [00:59<00:13,  3.69it/s] 82%|████████▏ | 224/272 [00:59<00:13,  3.63it/s] 83%|████████▎ | 225/272 [01:00<00:12,  3.76it/s] 83%|████████▎ | 226/272 [01:00<00:12,  3.77it/s] 83%|████████▎ | 227/272 [01:00<00:12,  3.71it/s] 84%|████████▍ | 228/272 [01:00<00:11,  3.71it/s] 84%|████████▍ | 229/272 [01:01<00:11,  3.65it/s] 85%|████████▍ | 230/272 [01:01<00:11,  3.77it/s] 85%|████████▍ | 231/272 [01:01<00:10,  3.77it/s] 85%|████████▌ | 232/272 [01:02<00:10,  3.70it/s] 86%|████████▌ | 233/272 [01:02<00:10,  3.73it/s] 86%|████████▌ | 234/272 [01:02<00:10,  3.67it/s] 86%|████████▋ | 235/272 [01:02<00:09,  3.79it/s] 87%|████████▋ | 236/272 [01:03<00:09,  3.72it/s] 87%|████████▋ | 237/272 [01:03<00:09,  3.72it/s] 88%|████████▊ | 238/272 [01:03<00:09,  3.72it/s] 88%|████████▊ | 239/272 [01:03<00:08,  3.82it/s] 88%|████████▊ | 240/272 [01:04<00:08,  3.89it/s] 89%|████████▊ | 241/272 [01:04<00:07,  3.94it/s] 89%|████████▉ | 242/272 [01:04<00:07,  3.98it/s] 89%|████████▉ | 243/272 [01:04<00:07,  4.01it/s] 90%|████████▉ | 244/272 [01:05<00:06,  4.02it/s] 90%|█████████ | 245/272 [01:05<00:06,  4.05it/s] 90%|█████████ | 246/272 [01:05<00:06,  4.06it/s] 91%|█████████ | 247/272 [01:05<00:06,  4.06it/s] 91%|█████████ | 248/272 [01:06<00:05,  4.06it/s] 92%|█████████▏| 249/272 [01:06<00:05,  4.08it/s] 92%|█████████▏| 250/272 [01:06<00:05,  4.09it/s] 92%|█████████▏| 251/272 [01:06<00:05,  4.10it/s] 93%|█████████▎| 252/272 [01:07<00:04,  4.09it/s] 93%|█████████▎| 253/272 [01:07<00:04,  4.08it/s] 93%|█████████▎| 254/272 [01:07<00:04,  4.09it/s] 94%|█████████▍| 255/272 [01:07<00:04,  4.10it/s] 94%|█████████▍| 256/272 [01:08<00:03,  4.10it/s] 94%|█████████▍| 257/272 [01:08<00:03,  4.10it/s] 95%|█████████▍| 258/272 [01:08<00:03,  4.09it/s] 95%|█████████▌| 259/272 [01:08<00:03,  3.96it/s] 96%|█████████▌| 260/272 [01:09<00:03,  3.86it/s] 96%|█████████▌| 261/272 [01:09<00:02,  3.86it/s] 96%|█████████▋| 262/272 [01:09<00:02,  3.90it/s] 97%|█████████▋| 263/272 [01:09<00:02,  3.82it/s] 97%|█████████▋| 264/272 [01:10<00:02,  3.83it/s] 97%|█████████▋| 265/272 [01:10<00:01,  3.74it/s] 98%|█████████▊| 266/272 [01:10<00:01,  3.79it/s] 98%|█████████▊| 267/272 [01:10<00:01,  3.75it/s] 99%|█████████▊| 268/272 [01:11<00:01,  3.79it/s] 99%|█████████▉| 269/272 [01:11<00:00,  3.71it/s] 99%|█████████▉| 270/272 [01:11<00:00,  3.77it/s]100%|█████████▉| 271/272 [01:11<00:00,  3.73it/s]100%|██████████| 272/272 [01:12<00:00,  3.77it/s]accuracy:  0.8639705882352942
100%|██████████| 272/272 [01:16<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.30s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[2025-03-12 04:09:58,447] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:09:58,807] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:09:59,192] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<01:06,  2.90s/it]  8%|▊         | 2/24 [00:05<01:01,  2.80s/it] 12%|█▎        | 3/24 [00:08<00:59,  2.84s/it] 17%|█▋        | 4/24 [00:11<00:55,  2.80s/it] 21%|██        | 5/24 [00:13<00:51,  2.69s/it] 25%|██▌       | 6/24 [00:16<00:47,  2.66s/it] 29%|██▉       | 7/24 [00:18<00:40,  2.41s/it] 33%|███▎      | 8/24 [00:20<00:37,  2.31s/it] 38%|███▊      | 9/24 [00:22<00:34,  2.32s/it] 42%|████▏     | 10/24 [00:25<00:35,  2.52s/it]                                               {'loss': 0.7497, 'grad_norm': 3.0498664379119873, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:25<00:35,  2.52s/it] 46%|████▌     | 11/24 [00:27<00:31,  2.39s/it] 50%|█████     | 12/24 [00:30<00:28,  2.38s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:32<00:26,  2.44s/it] 58%|█████▊    | 14/24 [00:34<00:23,  2.38s/it] 62%|██████▎   | 15/24 [00:37<00:21,  2.35s/it] 67%|██████▋   | 16/24 [00:39<00:19,  2.41s/it] 71%|███████   | 17/24 [00:42<00:16,  2.42s/it] 75%|███████▌  | 18/24 [00:44<00:14,  2.34s/it] 79%|███████▉  | 19/24 [00:46<00:11,  2.26s/it] 83%|████████▎ | 20/24 [00:48<00:08,  2.21s/it]                                               {'loss': 0.7393, 'grad_norm': 10.318486213684082, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:48<00:08,  2.21s/it] 88%|████████▊ | 21/24 [00:51<00:06,  2.33s/it] 92%|█████████▏| 22/24 [00:53<00:04,  2.27s/it] 96%|█████████▌| 23/24 [00:55<00:02,  2.40s/it]100%|██████████| 24/24 [00:58<00:00,  2.46s/it]                                               {'train_runtime': 59.1809, 'train_samples_per_second': 4.866, 'train_steps_per_second': 0.406, 'train_loss': 0.733628104130427, 'epoch': 2.0}
100%|██████████| 24/24 [00:58<00:00,  2.46s/it]100%|██████████| 24/24 [00:58<00:00,  2.46s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<00:58,  2.97it/s]  1%|          | 2/175 [00:00<00:52,  3.32it/s]  2%|▏         | 3/175 [00:00<00:48,  3.57it/s]  2%|▏         | 4/175 [00:01<00:47,  3.57it/s]  3%|▎         | 5/175 [00:01<00:45,  3.71it/s]  3%|▎         | 6/175 [00:01<00:46,  3.67it/s]  4%|▍         | 7/175 [00:01<00:45,  3.69it/s]  5%|▍         | 8/175 [00:02<00:45,  3.66it/s]  5%|▌         | 9/175 [00:02<00:43,  3.81it/s]  6%|▌         | 10/175 [00:02<00:43,  3.82it/s]  6%|▋         | 11/175 [00:02<00:43,  3.76it/s]  7%|▋         | 12/175 [00:03<00:42,  3.80it/s]  7%|▋         | 13/175 [00:03<00:43,  3.73it/s]  8%|▊         | 14/175 [00:03<00:42,  3.81it/s]  9%|▊         | 15/175 [00:04<00:42,  3.75it/s]  9%|▉         | 16/175 [00:04<00:42,  3.74it/s] 10%|▉         | 17/175 [00:04<00:42,  3.69it/s] 10%|█         | 18/175 [00:04<00:41,  3.80it/s] 11%|█         | 19/175 [00:05<00:41,  3.80it/s] 11%|█▏        | 20/175 [00:05<00:41,  3.74it/s] 12%|█▏        | 21/175 [00:05<00:40,  3.78it/s] 13%|█▎        | 22/175 [00:05<00:41,  3.71it/s] 13%|█▎        | 23/175 [00:06<00:40,  3.79it/s] 14%|█▎        | 24/175 [00:06<00:40,  3.75it/s] 14%|█▍        | 25/175 [00:06<00:40,  3.73it/s] 15%|█▍        | 26/175 [00:07<00:40,  3.69it/s] 15%|█▌        | 27/175 [00:07<00:38,  3.81it/s] 16%|█▌        | 28/175 [00:07<00:38,  3.81it/s] 17%|█▋        | 29/175 [00:07<00:38,  3.75it/s] 17%|█▋        | 30/175 [00:08<00:38,  3.79it/s] 18%|█▊        | 31/175 [00:08<00:38,  3.72it/s] 18%|█▊        | 32/175 [00:08<00:37,  3.82it/s] 19%|█▉        | 33/175 [00:08<00:37,  3.76it/s] 19%|█▉        | 34/175 [00:09<00:37,  3.73it/s] 20%|██        | 35/175 [00:09<00:38,  3.67it/s] 21%|██        | 36/175 [00:09<00:36,  3.81it/s] 21%|██        | 37/175 [00:09<00:36,  3.83it/s] 22%|██▏       | 38/175 [00:10<00:36,  3.76it/s] 22%|██▏       | 39/175 [00:10<00:35,  3.79it/s] 23%|██▎       | 40/175 [00:10<00:36,  3.73it/s] 23%|██▎       | 41/175 [00:10<00:35,  3.81it/s] 24%|██▍       | 42/175 [00:11<00:35,  3.73it/s] 25%|██▍       | 43/175 [00:11<00:35,  3.74it/s] 25%|██▌       | 44/175 [00:11<00:35,  3.69it/s] 26%|██▌       | 45/175 [00:12<00:34,  3.81it/s] 26%|██▋       | 46/175 [00:12<00:33,  3.82it/s] 27%|██▋       | 47/175 [00:12<00:34,  3.75it/s] 27%|██▋       | 48/175 [00:12<00:33,  3.79it/s] 28%|██▊       | 49/175 [00:13<00:33,  3.72it/s] 29%|██▊       | 50/175 [00:13<00:32,  3.80it/s] 29%|██▉       | 51/175 [00:13<00:33,  3.76it/s] 30%|██▉       | 52/175 [00:13<00:33,  3.72it/s] 30%|███       | 53/175 [00:14<00:33,  3.66it/s] 31%|███       | 54/175 [00:14<00:31,  3.79it/s] 31%|███▏      | 55/175 [00:14<00:31,  3.81it/s] 32%|███▏      | 56/175 [00:14<00:31,  3.75it/s] 33%|███▎      | 57/175 [00:15<00:31,  3.76it/s] 33%|███▎      | 58/175 [00:15<00:31,  3.71it/s] 34%|███▎      | 59/175 [00:15<00:30,  3.82it/s] 34%|███▍      | 60/175 [00:16<00:30,  3.77it/s] 35%|███▍      | 61/175 [00:16<00:30,  3.75it/s] 35%|███▌      | 62/175 [00:16<00:29,  3.79it/s] 36%|███▌      | 63/175 [00:16<00:30,  3.72it/s] 37%|███▋      | 64/175 [00:17<00:29,  3.82it/s] 37%|███▋      | 65/175 [00:17<00:29,  3.77it/s] 38%|███▊      | 66/175 [00:17<00:29,  3.73it/s] 38%|███▊      | 67/175 [00:17<00:29,  3.67it/s] 39%|███▉      | 68/175 [00:18<00:28,  3.80it/s] 39%|███▉      | 69/175 [00:18<00:27,  3.81it/s] 40%|████      | 70/175 [00:18<00:28,  3.74it/s] 41%|████      | 71/175 [00:18<00:27,  3.77it/s] 41%|████      | 72/175 [00:19<00:27,  3.71it/s] 42%|████▏     | 73/175 [00:19<00:26,  3.79it/s] 42%|████▏     | 74/175 [00:19<00:26,  3.76it/s] 43%|████▎     | 75/175 [00:20<00:26,  3.72it/s] 43%|████▎     | 76/175 [00:20<00:27,  3.66it/s] 44%|████▍     | 77/175 [00:20<00:25,  3.79it/s] 45%|████▍     | 78/175 [00:20<00:25,  3.81it/s] 45%|████▌     | 79/175 [00:21<00:25,  3.74it/s] 46%|████▌     | 80/175 [00:21<00:25,  3.76it/s] 46%|████▋     | 81/175 [00:21<00:25,  3.71it/s] 47%|████▋     | 82/175 [00:21<00:24,  3.81it/s] 47%|████▋     | 83/175 [00:22<00:24,  3.76it/s] 48%|████▊     | 84/175 [00:22<00:24,  3.72it/s] 49%|████▊     | 85/175 [00:22<00:24,  3.66it/s] 49%|████▉     | 86/175 [00:22<00:23,  3.78it/s] 50%|████▉     | 87/175 [00:23<00:23,  3.81it/s] 50%|█████     | 88/175 [00:23<00:23,  3.74it/s] 51%|█████     | 89/175 [00:23<00:22,  3.77it/s] 51%|█████▏    | 90/175 [00:24<00:22,  3.71it/s] 52%|█████▏    | 91/175 [00:24<00:22,  3.80it/s] 53%|█████▎    | 92/175 [00:24<00:22,  3.76it/s] 53%|█████▎    | 93/175 [00:24<00:22,  3.72it/s] 54%|█████▎    | 94/175 [00:25<00:22,  3.66it/s] 54%|█████▍    | 95/175 [00:25<00:21,  3.79it/s] 55%|█████▍    | 96/175 [00:25<00:20,  3.81it/s] 55%|█████▌    | 97/175 [00:25<00:20,  3.74it/s] 56%|█████▌    | 98/175 [00:26<00:20,  3.76it/s] 57%|█████▋    | 99/175 [00:26<00:20,  3.71it/s] 57%|█████▋    | 100/175 [00:26<00:19,  3.80it/s] 58%|█████▊    | 101/175 [00:26<00:19,  3.76it/s] 58%|█████▊    | 102/175 [00:27<00:19,  3.72it/s] 59%|█████▉    | 103/175 [00:27<00:19,  3.66it/s] 59%|█████▉    | 104/175 [00:27<00:18,  3.79it/s] 60%|██████    | 105/175 [00:28<00:18,  3.81it/s] 61%|██████    | 106/175 [00:28<00:18,  3.74it/s] 61%|██████    | 107/175 [00:28<00:18,  3.77it/s] 62%|██████▏   | 108/175 [00:28<00:18,  3.70it/s] 62%|██████▏   | 109/175 [00:29<00:17,  3.79it/s] 63%|██████▎   | 110/175 [00:29<00:17,  3.75it/s] 63%|██████▎   | 111/175 [00:29<00:17,  3.71it/s] 64%|██████▍   | 112/175 [00:29<00:17,  3.64it/s] 65%|██████▍   | 113/175 [00:30<00:16,  3.77it/s] 65%|██████▌   | 114/175 [00:30<00:16,  3.79it/s] 66%|██████▌   | 115/175 [00:30<00:16,  3.72it/s] 66%|██████▋   | 116/175 [00:30<00:15,  3.74it/s] 67%|██████▋   | 117/175 [00:31<00:15,  3.66it/s] 67%|██████▋   | 118/175 [00:31<00:15,  3.80it/s] 68%|██████▊   | 119/175 [00:31<00:14,  3.81it/s] 69%|██████▊   | 120/175 [00:32<00:14,  3.74it/s] 69%|██████▉   | 121/175 [00:32<00:14,  3.76it/s] 70%|██████▉   | 122/175 [00:32<00:14,  3.69it/s] 70%|███████   | 123/175 [00:32<00:13,  3.80it/s] 71%|███████   | 124/175 [00:33<00:13,  3.74it/s] 71%|███████▏  | 125/175 [00:33<00:13,  3.72it/s] 72%|███████▏  | 126/175 [00:33<00:13,  3.76it/s] 73%|███████▎  | 127/175 [00:33<00:12,  3.71it/s] 73%|███████▎  | 128/175 [00:34<00:12,  3.80it/s] 74%|███████▎  | 129/175 [00:34<00:12,  3.76it/s] 74%|███████▍  | 130/175 [00:34<00:12,  3.72it/s] 75%|███████▍  | 131/175 [00:34<00:12,  3.65it/s] 75%|███████▌  | 132/175 [00:35<00:11,  3.78it/s] 76%|███████▌  | 133/175 [00:35<00:11,  3.80it/s] 77%|███████▋  | 134/175 [00:35<00:10,  3.76it/s] 77%|███████▋  | 135/175 [00:36<00:10,  3.72it/s] 78%|███████▊  | 136/175 [00:36<00:10,  3.71it/s] 78%|███████▊  | 137/175 [00:36<00:09,  3.81it/s] 79%|███████▉  | 138/175 [00:36<00:09,  3.89it/s] 79%|███████▉  | 139/175 [00:37<00:09,  3.95it/s] 80%|████████  | 140/175 [00:37<00:08,  4.00it/s] 81%|████████  | 141/175 [00:37<00:08,  4.03it/s] 81%|████████  | 142/175 [00:37<00:08,  4.05it/s] 82%|████████▏ | 143/175 [00:38<00:07,  4.05it/s] 82%|████████▏ | 144/175 [00:38<00:07,  4.07it/s] 83%|████████▎ | 145/175 [00:38<00:07,  4.09it/s] 83%|████████▎ | 146/175 [00:38<00:07,  4.09it/s] 84%|████████▍ | 147/175 [00:39<00:06,  4.05it/s] 85%|████████▍ | 148/175 [00:39<00:06,  3.93it/s] 85%|████████▌ | 149/175 [00:39<00:06,  3.88it/s] 86%|████████▌ | 150/175 [00:39<00:06,  3.81it/s] 86%|████████▋ | 151/175 [00:40<00:06,  3.89it/s] 87%|████████▋ | 152/175 [00:40<00:05,  3.88it/s] 87%|████████▋ | 153/175 [00:40<00:05,  3.83it/s] 88%|████████▊ | 154/175 [00:40<00:05,  3.77it/s] 89%|████████▊ | 155/175 [00:41<00:05,  3.68it/s] 89%|████████▉ | 156/175 [00:41<00:05,  3.80it/s] 90%|████████▉ | 157/175 [00:41<00:04,  3.83it/s] 90%|█████████ | 158/175 [00:41<00:04,  3.78it/s] 91%|█████████ | 159/175 [00:42<00:04,  3.74it/s] 91%|█████████▏| 160/175 [00:42<00:04,  3.66it/s] 92%|█████████▏| 161/175 [00:42<00:03,  3.79it/s] 93%|█████████▎| 162/175 [00:42<00:03,  3.83it/s] 93%|█████████▎| 163/175 [00:43<00:03,  3.79it/s] 94%|█████████▎| 164/175 [00:43<00:02,  3.73it/s] 94%|█████████▍| 165/175 [00:43<00:02,  3.65it/s] 95%|█████████▍| 166/175 [00:44<00:02,  3.77it/s] 95%|█████████▌| 167/175 [00:44<00:02,  3.84it/s] 96%|█████████▌| 168/175 [00:44<00:01,  3.79it/s] 97%|█████████▋| 169/175 [00:44<00:01,  3.73it/s] 97%|█████████▋| 170/175 [00:45<00:01,  3.65it/s] 98%|█████████▊| 171/175 [00:45<00:01,  3.77it/s] 98%|█████████▊| 172/175 [00:45<00:00,  3.83it/s] 99%|█████████▉| 173/175 [00:45<00:00,  3.78it/s] 99%|█████████▉| 174/175 [00:46<00:00,  3.73it/s]100%|██████████| 175/175 [00:46<00:00,  3.65it/s]accuracy:  0.8285714285714286
100%|██████████| 175/175 [00:49<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 144, validation dataset size: 119
Training dataset size: 144, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 144, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 04:12:14,803] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:12:15,122] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 04:12:15,188] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/24 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  4%|▍         | 1/24 [00:02<00:56,  2.44s/it]  8%|▊         | 2/24 [00:04<00:52,  2.40s/it] 12%|█▎        | 3/24 [00:07<00:50,  2.42s/it] 17%|█▋        | 4/24 [00:09<00:49,  2.49s/it] 21%|██        | 5/24 [00:12<00:48,  2.55s/it] 25%|██▌       | 6/24 [00:15<00:46,  2.59s/it] 29%|██▉       | 7/24 [00:17<00:42,  2.49s/it] 33%|███▎      | 8/24 [00:19<00:39,  2.48s/it] 38%|███▊      | 9/24 [00:22<00:36,  2.41s/it] 42%|████▏     | 10/24 [00:24<00:34,  2.48s/it]                                               {'loss': 1.1048, 'grad_norm': 7.990067005157471, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 10/24 [00:24<00:34,  2.48s/it] 46%|████▌     | 11/24 [00:27<00:33,  2.55s/it] 50%|█████     | 12/24 [00:30<00:31,  2.63s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 54%|█████▍    | 13/24 [00:32<00:27,  2.53s/it] 58%|█████▊    | 14/24 [00:34<00:23,  2.36s/it] 62%|██████▎   | 15/24 [00:36<00:20,  2.26s/it] 67%|██████▋   | 16/24 [00:38<00:18,  2.29s/it] 71%|███████   | 17/24 [00:41<00:16,  2.42s/it] 75%|███████▌  | 18/24 [00:44<00:15,  2.50s/it] 79%|███████▉  | 19/24 [00:46<00:12,  2.48s/it] 83%|████████▎ | 20/24 [00:49<00:09,  2.46s/it]                                               {'loss': 0.9719, 'grad_norm': 6.0248122215271, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 20/24 [00:49<00:09,  2.46s/it] 88%|████████▊ | 21/24 [00:51<00:07,  2.51s/it] 92%|█████████▏| 22/24 [00:53<00:04,  2.37s/it] 96%|█████████▌| 23/24 [00:56<00:02,  2.54s/it]100%|██████████| 24/24 [00:59<00:00,  2.59s/it]                                               {'train_runtime': 60.2363, 'train_samples_per_second': 4.781, 'train_steps_per_second': 0.398, 'train_loss': 0.9882981479167938, 'epoch': 2.0}
100%|██████████| 24/24 [01:00<00:00,  2.59s/it]100%|██████████| 24/24 [01:00<00:00,  2.50s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:54,  2.79it/s]  1%|▏         | 2/153 [00:00<00:46,  3.24it/s]  2%|▏         | 3/153 [00:00<00:42,  3.51it/s]  3%|▎         | 4/153 [00:01<00:41,  3.61it/s]  3%|▎         | 5/153 [00:01<00:39,  3.79it/s]  4%|▍         | 6/153 [00:01<00:37,  3.92it/s]  5%|▍         | 7/153 [00:01<00:36,  4.00it/s]  5%|▌         | 8/153 [00:02<00:35,  4.06it/s]  6%|▌         | 9/153 [00:02<00:35,  4.09it/s]  7%|▋         | 10/153 [00:02<00:34,  4.12it/s]  7%|▋         | 11/153 [00:02<00:34,  4.13it/s]  8%|▊         | 12/153 [00:03<00:34,  4.14it/s]  8%|▊         | 13/153 [00:03<00:33,  4.14it/s]  9%|▉         | 14/153 [00:03<00:33,  4.15it/s] 10%|▉         | 15/153 [00:03<00:33,  4.16it/s] 10%|█         | 16/153 [00:04<00:32,  4.16it/s] 11%|█         | 17/153 [00:04<00:32,  4.16it/s] 12%|█▏        | 18/153 [00:04<00:32,  4.16it/s] 12%|█▏        | 19/153 [00:04<00:32,  4.17it/s] 13%|█▎        | 20/153 [00:04<00:31,  4.17it/s] 14%|█▎        | 21/153 [00:05<00:31,  4.16it/s] 14%|█▍        | 22/153 [00:05<00:31,  4.16it/s] 15%|█▌        | 23/153 [00:05<00:31,  4.16it/s] 16%|█▌        | 24/153 [00:05<00:31,  4.16it/s] 16%|█▋        | 25/153 [00:06<00:31,  4.11it/s] 17%|█▋        | 26/153 [00:06<00:31,  3.98it/s] 18%|█▊        | 27/153 [00:06<00:32,  3.85it/s] 18%|█▊        | 28/153 [00:06<00:31,  3.95it/s] 19%|█▉        | 29/153 [00:07<00:31,  3.94it/s] 20%|█▉        | 30/153 [00:07<00:32,  3.81it/s] 20%|██        | 31/153 [00:07<00:32,  3.78it/s] 21%|██        | 32/153 [00:08<00:32,  3.76it/s] 22%|██▏       | 33/153 [00:08<00:31,  3.84it/s] 22%|██▏       | 34/153 [00:08<00:31,  3.74it/s] 23%|██▎       | 35/153 [00:08<00:31,  3.75it/s] 24%|██▎       | 36/153 [00:09<00:31,  3.71it/s] 24%|██▍       | 37/153 [00:09<00:30,  3.82it/s] 25%|██▍       | 38/153 [00:09<00:30,  3.79it/s] 25%|██▌       | 39/153 [00:09<00:30,  3.74it/s] 26%|██▌       | 40/153 [00:10<00:30,  3.65it/s] 27%|██▋       | 41/153 [00:10<00:29,  3.79it/s] 27%|██▋       | 42/153 [00:10<00:28,  3.83it/s] 28%|██▊       | 43/153 [00:10<00:29,  3.75it/s] 29%|██▉       | 44/153 [00:11<00:29,  3.75it/s] 29%|██▉       | 45/153 [00:11<00:28,  3.74it/s] 30%|███       | 46/153 [00:11<00:27,  3.83it/s] 31%|███       | 47/153 [00:12<00:28,  3.77it/s] 31%|███▏      | 48/153 [00:12<00:28,  3.72it/s] 32%|███▏      | 49/153 [00:12<00:28,  3.68it/s] 33%|███▎      | 50/153 [00:12<00:27,  3.79it/s] 33%|███▎      | 51/153 [00:13<00:26,  3.82it/s] 34%|███▍      | 52/153 [00:13<00:27,  3.73it/s] 35%|███▍      | 53/153 [00:13<00:26,  3.77it/s] 35%|███▌      | 54/153 [00:13<00:26,  3.71it/s] 36%|███▌      | 55/153 [00:14<00:25,  3.82it/s] 37%|███▋      | 56/153 [00:14<00:25,  3.76it/s] 37%|███▋      | 57/153 [00:14<00:25,  3.72it/s] 38%|███▊      | 58/153 [00:14<00:25,  3.68it/s] 39%|███▊      | 59/153 [00:15<00:24,  3.80it/s] 39%|███▉      | 60/153 [00:15<00:24,  3.80it/s] 40%|███▉      | 61/153 [00:15<00:24,  3.73it/s] 41%|████      | 62/153 [00:16<00:24,  3.70it/s] 41%|████      | 63/153 [00:16<00:23,  3.76it/s] 42%|████▏     | 64/153 [00:16<00:23,  3.82it/s] 42%|████▏     | 65/153 [00:16<00:23,  3.73it/s] 43%|████▎     | 66/153 [00:17<00:23,  3.74it/s] 44%|████▍     | 67/153 [00:17<00:23,  3.69it/s] 44%|████▍     | 68/153 [00:17<00:22,  3.80it/s] 45%|████▌     | 69/153 [00:17<00:22,  3.82it/s] 46%|████▌     | 70/153 [00:18<00:22,  3.72it/s] 46%|████▋     | 71/153 [00:18<00:21,  3.77it/s] 47%|████▋     | 72/153 [00:18<00:21,  3.70it/s] 48%|████▊     | 73/153 [00:18<00:21,  3.80it/s] 48%|████▊     | 74/153 [00:19<00:21,  3.74it/s] 49%|████▉     | 75/153 [00:19<00:21,  3.70it/s] 50%|████▉     | 76/153 [00:19<00:21,  3.66it/s] 50%|█████     | 77/153 [00:20<00:20,  3.79it/s] 51%|█████     | 78/153 [00:20<00:19,  3.81it/s] 52%|█████▏    | 79/153 [00:20<00:19,  3.72it/s] 52%|█████▏    | 80/153 [00:20<00:19,  3.77it/s] 53%|█████▎    | 81/153 [00:21<00:19,  3.71it/s] 54%|█████▎    | 82/153 [00:21<00:18,  3.82it/s] 54%|█████▍    | 83/153 [00:21<00:18,  3.75it/s] 55%|█████▍    | 84/153 [00:21<00:18,  3.71it/s] 56%|█████▌    | 85/153 [00:22<00:18,  3.67it/s] 56%|█████▌    | 86/153 [00:22<00:17,  3.80it/s] 57%|█████▋    | 87/153 [00:22<00:17,  3.84it/s] 58%|█████▊    | 88/153 [00:22<00:17,  3.73it/s] 58%|█████▊    | 89/153 [00:23<00:17,  3.76it/s] 59%|█████▉    | 90/153 [00:23<00:17,  3.70it/s] 59%|█████▉    | 91/153 [00:23<00:16,  3.81it/s] 60%|██████    | 92/153 [00:24<00:16,  3.74it/s] 61%|██████    | 93/153 [00:24<00:16,  3.70it/s] 61%|██████▏   | 94/153 [00:24<00:16,  3.67it/s] 62%|██████▏   | 95/153 [00:24<00:15,  3.79it/s] 63%|██████▎   | 96/153 [00:25<00:14,  3.82it/s] 63%|██████▎   | 97/153 [00:25<00:15,  3.72it/s] 64%|██████▍   | 98/153 [00:25<00:14,  3.76it/s] 65%|██████▍   | 99/153 [00:25<00:14,  3.71it/s] 65%|██████▌   | 100/153 [00:26<00:13,  3.81it/s] 66%|██████▌   | 101/153 [00:26<00:13,  3.74it/s] 67%|██████▋   | 102/153 [00:26<00:13,  3.70it/s] 67%|██████▋   | 103/153 [00:27<00:13,  3.66it/s] 68%|██████▊   | 104/153 [00:27<00:12,  3.79it/s] 69%|██████▊   | 105/153 [00:27<00:12,  3.79it/s] 69%|██████▉   | 106/153 [00:27<00:12,  3.71it/s] 70%|██████▉   | 107/153 [00:28<00:12,  3.70it/s] 71%|███████   | 108/153 [00:28<00:12,  3.73it/s] 71%|███████   | 109/153 [00:28<00:11,  3.79it/s] 72%|███████▏  | 110/153 [00:28<00:11,  3.71it/s] 73%|███████▎  | 111/153 [00:29<00:11,  3.76it/s] 73%|███████▎  | 112/153 [00:29<00:11,  3.70it/s] 74%|███████▍  | 113/153 [00:29<00:10,  3.81it/s] 75%|███████▍  | 114/153 [00:29<00:10,  3.74it/s] 75%|███████▌  | 115/153 [00:30<00:10,  3.70it/s] 76%|███████▌  | 116/153 [00:30<00:10,  3.66it/s] 76%|███████▋  | 117/153 [00:30<00:09,  3.78it/s] 77%|███████▋  | 118/153 [00:30<00:09,  3.79it/s] 78%|███████▊  | 119/153 [00:31<00:09,  3.70it/s] 78%|███████▊  | 120/153 [00:31<00:08,  3.70it/s] 79%|███████▉  | 121/153 [00:31<00:08,  3.71it/s] 80%|███████▉  | 122/153 [00:32<00:08,  3.78it/s] 80%|████████  | 123/153 [00:32<00:08,  3.70it/s] 81%|████████  | 124/153 [00:32<00:07,  3.72it/s] 82%|████████▏ | 125/153 [00:32<00:07,  3.76it/s] 82%|████████▏ | 126/153 [00:33<00:07,  3.85it/s] 83%|████████▎ | 127/153 [00:33<00:06,  3.92it/s] 84%|████████▎ | 128/153 [00:33<00:06,  3.97it/s] 84%|████████▍ | 129/153 [00:33<00:06,  4.00it/s] 85%|████████▍ | 130/153 [00:34<00:05,  4.02it/s] 86%|████████▌ | 131/153 [00:34<00:05,  4.04it/s] 86%|████████▋ | 132/153 [00:34<00:05,  4.06it/s] 87%|████████▋ | 133/153 [00:34<00:04,  4.06it/s] 88%|████████▊ | 134/153 [00:35<00:04,  4.06it/s] 88%|████████▊ | 135/153 [00:35<00:04,  4.07it/s] 89%|████████▉ | 136/153 [00:35<00:04,  4.08it/s] 90%|████████▉ | 137/153 [00:35<00:03,  4.08it/s] 90%|█████████ | 138/153 [00:36<00:03,  4.08it/s] 91%|█████████ | 139/153 [00:36<00:03,  4.08it/s] 92%|█████████▏| 140/153 [00:36<00:03,  4.09it/s] 92%|█████████▏| 141/153 [00:36<00:02,  4.08it/s] 93%|█████████▎| 142/153 [00:37<00:02,  4.07it/s] 93%|█████████▎| 143/153 [00:37<00:02,  4.08it/s] 94%|█████████▍| 144/153 [00:37<00:02,  4.09it/s] 95%|█████████▍| 145/153 [00:37<00:01,  4.08it/s] 95%|█████████▌| 146/153 [00:38<00:01,  4.07it/s] 96%|█████████▌| 147/153 [00:38<00:01,  4.08it/s] 97%|█████████▋| 148/153 [00:38<00:01,  4.08it/s] 97%|█████████▋| 149/153 [00:38<00:00,  4.07it/s] 98%|█████████▊| 150/153 [00:39<00:00,  4.08it/s] 99%|█████████▊| 151/153 [00:39<00:00,  4.09it/s] 99%|█████████▉| 152/153 [00:39<00:00,  4.09it/s]100%|██████████| 153/153 [00:39<00:00,  4.09it/s]accuracy:  0.6535947712418301
100%|██████████| 153/153 [00:42<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.59s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:14:26,775] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:14:27,780] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:14:29,522] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:10,  2.28s/it]  6%|▋         | 2/32 [00:04<01:09,  2.32s/it]  9%|▉         | 3/32 [00:07<01:11,  2.47s/it] 12%|█▎        | 4/32 [00:09<01:07,  2.43s/it] 16%|█▌        | 5/32 [00:12<01:05,  2.41s/it] 19%|█▉        | 6/32 [00:14<01:02,  2.41s/it] 22%|██▏       | 7/32 [00:16<01:00,  2.43s/it] 25%|██▌       | 8/32 [00:18<00:53,  2.25s/it] 28%|██▊       | 9/32 [00:21<00:53,  2.34s/it] 31%|███▏      | 10/32 [00:23<00:52,  2.37s/it]                                               {'loss': 1.1254, 'grad_norm': 5.645589351654053, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:23<00:52,  2.37s/it] 34%|███▍      | 11/32 [00:25<00:46,  2.22s/it] 38%|███▊      | 12/32 [00:27<00:41,  2.10s/it] 41%|████      | 13/32 [00:29<00:39,  2.08s/it] 44%|████▍     | 14/32 [00:31<00:37,  2.09s/it] 47%|████▋     | 15/32 [00:33<00:33,  1.99s/it] 50%|█████     | 16/32 [00:35<00:33,  2.07s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:38<00:33,  2.23s/it] 56%|█████▋    | 18/32 [00:40<00:32,  2.31s/it] 59%|█████▉    | 19/32 [00:42<00:29,  2.30s/it] 62%|██████▎   | 20/32 [00:45<00:28,  2.35s/it]                                               {'loss': 0.9942, 'grad_norm': 7.3164191246032715, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:45<00:28,  2.35s/it] 66%|██████▌   | 21/32 [00:48<00:27,  2.46s/it] 69%|██████▉   | 22/32 [00:50<00:24,  2.47s/it] 72%|███████▏  | 23/32 [00:53<00:22,  2.46s/it] 75%|███████▌  | 24/32 [00:55<00:19,  2.46s/it] 78%|███████▊  | 25/32 [00:57<00:16,  2.40s/it] 81%|████████▏ | 26/32 [01:00<00:15,  2.56s/it] 84%|████████▍ | 27/32 [01:03<00:12,  2.51s/it] 88%|████████▊ | 28/32 [01:05<00:10,  2.60s/it] 91%|█████████ | 29/32 [01:07<00:07,  2.42s/it] 94%|█████████▍| 30/32 [01:10<00:05,  2.51s/it]                                               {'loss': 0.7984, 'grad_norm': 5.9602861404418945, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:10<00:05,  2.51s/it] 97%|█████████▋| 31/32 [01:13<00:02,  2.64s/it]100%|██████████| 32/32 [01:15<00:00,  2.54s/it]                                               {'train_runtime': 76.5268, 'train_samples_per_second': 5.018, 'train_steps_per_second': 0.418, 'train_loss': 0.967054195702076, 'epoch': 2.0}
100%|██████████| 32/32 [01:16<00:00,  2.54s/it]100%|██████████| 32/32 [01:16<00:00,  2.39s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:09,  3.04it/s]  1%|          | 2/212 [00:00<01:02,  3.35it/s]  1%|▏         | 3/212 [00:00<00:58,  3.58it/s]  2%|▏         | 4/212 [00:01<00:57,  3.59it/s]  2%|▏         | 5/212 [00:01<00:55,  3.72it/s]  3%|▎         | 6/212 [00:01<00:56,  3.68it/s]  3%|▎         | 7/212 [00:01<00:54,  3.74it/s]  4%|▍         | 8/212 [00:02<00:55,  3.70it/s]  4%|▍         | 9/212 [00:02<00:53,  3.79it/s]  5%|▍         | 10/212 [00:02<00:54,  3.71it/s]  5%|▌         | 11/212 [00:02<00:53,  3.76it/s]  6%|▌         | 12/212 [00:03<00:53,  3.71it/s]  6%|▌         | 13/212 [00:03<00:52,  3.78it/s]  7%|▋         | 14/212 [00:03<00:53,  3.73it/s]  7%|▋         | 15/212 [00:04<00:52,  3.76it/s]  8%|▊         | 16/212 [00:04<00:52,  3.71it/s]  8%|▊         | 17/212 [00:04<00:51,  3.79it/s]  8%|▊         | 18/212 [00:04<00:52,  3.72it/s]  9%|▉         | 19/212 [00:05<00:51,  3.77it/s]  9%|▉         | 20/212 [00:05<00:51,  3.73it/s] 10%|▉         | 21/212 [00:05<00:50,  3.81it/s] 10%|█         | 22/212 [00:05<00:50,  3.75it/s] 11%|█         | 23/212 [00:06<00:49,  3.79it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.73it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.81it/s] 12%|█▏        | 26/212 [00:06<00:49,  3.73it/s] 13%|█▎        | 27/212 [00:07<00:49,  3.77it/s] 13%|█▎        | 28/212 [00:07<00:49,  3.71it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.78it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.72it/s] 15%|█▍        | 31/212 [00:08<00:48,  3.77it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.72it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.80it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.74it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.77it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.72it/s] 17%|█▋        | 37/212 [00:09<00:45,  3.81it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.75it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.75it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.70it/s] 19%|█▉        | 41/212 [00:10<00:44,  3.80it/s] 20%|█▉        | 42/212 [00:11<00:45,  3.74it/s] 20%|██        | 43/212 [00:11<00:45,  3.71it/s] 21%|██        | 44/212 [00:11<00:45,  3.66it/s] 21%|██        | 45/212 [00:12<00:44,  3.78it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.73it/s] 22%|██▏       | 47/212 [00:12<00:44,  3.70it/s] 23%|██▎       | 48/212 [00:12<00:44,  3.65it/s] 23%|██▎       | 49/212 [00:13<00:43,  3.77it/s] 24%|██▎       | 50/212 [00:13<00:43,  3.73it/s] 24%|██▍       | 51/212 [00:13<00:43,  3.70it/s] 25%|██▍       | 52/212 [00:13<00:43,  3.65it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.78it/s] 25%|██▌       | 54/212 [00:14<00:41,  3.77it/s] 26%|██▌       | 55/212 [00:14<00:42,  3.71it/s] 26%|██▋       | 56/212 [00:15<00:41,  3.76it/s] 27%|██▋       | 57/212 [00:15<00:41,  3.71it/s] 27%|██▋       | 58/212 [00:15<00:41,  3.75it/s] 28%|██▊       | 59/212 [00:15<00:41,  3.69it/s] 28%|██▊       | 60/212 [00:16<00:40,  3.74it/s] 29%|██▉       | 61/212 [00:16<00:41,  3.68it/s] 29%|██▉       | 62/212 [00:16<00:40,  3.75it/s] 30%|██▉       | 63/212 [00:16<00:40,  3.69it/s] 30%|███       | 64/212 [00:17<00:39,  3.74it/s] 31%|███       | 65/212 [00:17<00:39,  3.69it/s] 31%|███       | 66/212 [00:17<00:38,  3.75it/s] 32%|███▏      | 67/212 [00:17<00:39,  3.69it/s] 32%|███▏      | 68/212 [00:18<00:38,  3.75it/s] 33%|███▎      | 69/212 [00:18<00:38,  3.70it/s] 33%|███▎      | 70/212 [00:18<00:37,  3.76it/s] 33%|███▎      | 71/212 [00:19<00:38,  3.69it/s] 34%|███▍      | 72/212 [00:19<00:37,  3.74it/s] 34%|███▍      | 73/212 [00:19<00:37,  3.69it/s] 35%|███▍      | 74/212 [00:19<00:36,  3.75it/s] 35%|███▌      | 75/212 [00:20<00:37,  3.69it/s] 36%|███▌      | 76/212 [00:20<00:36,  3.74it/s] 36%|███▋      | 77/212 [00:20<00:36,  3.68it/s] 37%|███▋      | 78/212 [00:20<00:35,  3.76it/s] 37%|███▋      | 79/212 [00:21<00:35,  3.70it/s] 38%|███▊      | 80/212 [00:21<00:35,  3.74it/s] 38%|███▊      | 81/212 [00:21<00:35,  3.68it/s] 39%|███▊      | 82/212 [00:22<00:34,  3.74it/s] 39%|███▉      | 83/212 [00:22<00:35,  3.69it/s] 40%|███▉      | 84/212 [00:22<00:34,  3.73it/s] 40%|████      | 85/212 [00:22<00:34,  3.68it/s] 41%|████      | 86/212 [00:23<00:33,  3.74it/s] 41%|████      | 87/212 [00:23<00:33,  3.68it/s] 42%|████▏     | 88/212 [00:23<00:33,  3.73it/s] 42%|████▏     | 89/212 [00:23<00:33,  3.67it/s] 42%|████▏     | 90/212 [00:24<00:32,  3.73it/s] 43%|████▎     | 91/212 [00:24<00:32,  3.68it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.72it/s] 44%|████▍     | 93/212 [00:24<00:32,  3.67it/s] 44%|████▍     | 94/212 [00:25<00:31,  3.74it/s] 45%|████▍     | 95/212 [00:25<00:31,  3.68it/s] 45%|████▌     | 96/212 [00:25<00:31,  3.72it/s] 46%|████▌     | 97/212 [00:26<00:31,  3.67it/s] 46%|████▌     | 98/212 [00:26<00:30,  3.73it/s] 47%|████▋     | 99/212 [00:26<00:30,  3.67it/s] 47%|████▋     | 100/212 [00:26<00:30,  3.72it/s] 48%|████▊     | 101/212 [00:27<00:30,  3.67it/s] 48%|████▊     | 102/212 [00:27<00:29,  3.73it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.67it/s] 49%|████▉     | 104/212 [00:27<00:29,  3.72it/s] 50%|████▉     | 105/212 [00:28<00:29,  3.68it/s] 50%|█████     | 106/212 [00:28<00:28,  3.75it/s] 50%|█████     | 107/212 [00:28<00:28,  3.68it/s] 51%|█████     | 108/212 [00:29<00:27,  3.73it/s] 51%|█████▏    | 109/212 [00:29<00:28,  3.68it/s] 52%|█████▏    | 110/212 [00:29<00:27,  3.75it/s] 52%|█████▏    | 111/212 [00:29<00:27,  3.68it/s] 53%|█████▎    | 112/212 [00:30<00:26,  3.72it/s] 53%|█████▎    | 113/212 [00:30<00:26,  3.67it/s] 54%|█████▍    | 114/212 [00:30<00:26,  3.74it/s] 54%|█████▍    | 115/212 [00:30<00:26,  3.68it/s] 55%|█████▍    | 116/212 [00:31<00:25,  3.72it/s] 55%|█████▌    | 117/212 [00:31<00:25,  3.67it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.76it/s] 56%|█████▌    | 119/212 [00:32<00:25,  3.69it/s] 57%|█████▋    | 120/212 [00:32<00:24,  3.72it/s] 57%|█████▋    | 121/212 [00:32<00:24,  3.67it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.75it/s] 58%|█████▊    | 123/212 [00:33<00:24,  3.68it/s] 58%|█████▊    | 124/212 [00:33<00:23,  3.72it/s] 59%|█████▉    | 125/212 [00:33<00:23,  3.67it/s] 59%|█████▉    | 126/212 [00:33<00:22,  3.75it/s] 60%|█████▉    | 127/212 [00:34<00:23,  3.68it/s] 60%|██████    | 128/212 [00:34<00:22,  3.72it/s] 61%|██████    | 129/212 [00:34<00:22,  3.67it/s] 61%|██████▏   | 130/212 [00:34<00:21,  3.75it/s] 62%|██████▏   | 131/212 [00:35<00:22,  3.68it/s] 62%|██████▏   | 132/212 [00:35<00:21,  3.72it/s] 63%|██████▎   | 133/212 [00:35<00:21,  3.67it/s] 63%|██████▎   | 134/212 [00:36<00:20,  3.76it/s] 64%|██████▎   | 135/212 [00:36<00:20,  3.69it/s] 64%|██████▍   | 136/212 [00:36<00:20,  3.73it/s] 65%|██████▍   | 137/212 [00:36<00:20,  3.67it/s] 65%|██████▌   | 138/212 [00:37<00:19,  3.73it/s] 66%|██████▌   | 139/212 [00:37<00:19,  3.67it/s] 66%|██████▌   | 140/212 [00:37<00:19,  3.71it/s] 67%|██████▋   | 141/212 [00:37<00:19,  3.67it/s] 67%|██████▋   | 142/212 [00:38<00:18,  3.76it/s] 67%|██████▋   | 143/212 [00:38<00:18,  3.69it/s] 68%|██████▊   | 144/212 [00:38<00:18,  3.75it/s] 68%|██████▊   | 145/212 [00:39<00:18,  3.67it/s] 69%|██████▉   | 146/212 [00:39<00:17,  3.80it/s] 69%|██████▉   | 147/212 [00:39<00:17,  3.81it/s] 70%|██████▉   | 148/212 [00:39<00:17,  3.72it/s] 70%|███████   | 149/212 [00:40<00:16,  3.76it/s] 71%|███████   | 150/212 [00:40<00:16,  3.70it/s] 71%|███████   | 151/212 [00:40<00:16,  3.75it/s] 72%|███████▏  | 152/212 [00:40<00:16,  3.69it/s] 72%|███████▏  | 153/212 [00:41<00:15,  3.73it/s] 73%|███████▎  | 154/212 [00:41<00:15,  3.68it/s] 73%|███████▎  | 155/212 [00:41<00:15,  3.75it/s] 74%|███████▎  | 156/212 [00:41<00:15,  3.69it/s] 74%|███████▍  | 157/212 [00:42<00:14,  3.73it/s] 75%|███████▍  | 158/212 [00:42<00:14,  3.67it/s] 75%|███████▌  | 159/212 [00:42<00:14,  3.75it/s] 75%|███████▌  | 160/212 [00:43<00:14,  3.68it/s] 76%|███████▌  | 161/212 [00:43<00:13,  3.73it/s] 76%|███████▋  | 162/212 [00:43<00:13,  3.68it/s] 77%|███████▋  | 163/212 [00:43<00:13,  3.74it/s] 77%|███████▋  | 164/212 [00:44<00:13,  3.68it/s] 78%|███████▊  | 165/212 [00:44<00:12,  3.72it/s] 78%|███████▊  | 166/212 [00:44<00:12,  3.67it/s] 79%|███████▉  | 167/212 [00:44<00:12,  3.74it/s] 79%|███████▉  | 168/212 [00:45<00:11,  3.68it/s] 80%|███████▉  | 169/212 [00:45<00:11,  3.73it/s] 80%|████████  | 170/212 [00:45<00:11,  3.67it/s] 81%|████████  | 171/212 [00:46<00:10,  3.74it/s] 81%|████████  | 172/212 [00:46<00:10,  3.68it/s] 82%|████████▏ | 173/212 [00:46<00:10,  3.73it/s] 82%|████████▏ | 174/212 [00:46<00:10,  3.68it/s] 83%|████████▎ | 175/212 [00:47<00:09,  3.75it/s] 83%|████████▎ | 176/212 [00:47<00:09,  3.68it/s] 83%|████████▎ | 177/212 [00:47<00:09,  3.73it/s] 84%|████████▍ | 178/212 [00:47<00:09,  3.68it/s] 84%|████████▍ | 179/212 [00:48<00:08,  3.75it/s] 85%|████████▍ | 180/212 [00:48<00:08,  3.68it/s] 85%|████████▌ | 181/212 [00:48<00:08,  3.72it/s] 86%|████████▌ | 182/212 [00:48<00:08,  3.67it/s] 86%|████████▋ | 183/212 [00:49<00:07,  3.74it/s] 87%|████████▋ | 184/212 [00:49<00:07,  3.67it/s] 87%|████████▋ | 185/212 [00:49<00:07,  3.72it/s] 88%|████████▊ | 186/212 [00:50<00:07,  3.67it/s] 88%|████████▊ | 187/212 [00:50<00:06,  3.74it/s] 89%|████████▊ | 188/212 [00:50<00:06,  3.68it/s] 89%|████████▉ | 189/212 [00:50<00:06,  3.73it/s] 90%|████████▉ | 190/212 [00:51<00:05,  3.68it/s] 90%|█████████ | 191/212 [00:51<00:05,  3.80it/s] 91%|█████████ | 192/212 [00:51<00:05,  3.74it/s] 91%|█████████ | 193/212 [00:51<00:05,  3.70it/s] 92%|█████████▏| 194/212 [00:52<00:04,  3.64it/s] 92%|█████████▏| 195/212 [00:52<00:04,  3.77it/s] 92%|█████████▏| 196/212 [00:52<00:04,  3.73it/s] 93%|█████████▎| 197/212 [00:53<00:04,  3.69it/s] 93%|█████████▎| 198/212 [00:53<00:03,  3.70it/s] 94%|█████████▍| 199/212 [00:53<00:03,  3.70it/s] 94%|█████████▍| 200/212 [00:53<00:03,  3.73it/s] 95%|█████████▍| 201/212 [00:54<00:03,  3.67it/s] 95%|█████████▌| 202/212 [00:54<00:02,  3.72it/s] 96%|█████████▌| 203/212 [00:54<00:02,  3.68it/s] 96%|█████████▌| 204/212 [00:54<00:02,  3.72it/s] 97%|█████████▋| 205/212 [00:55<00:01,  3.68it/s] 97%|█████████▋| 206/212 [00:55<00:01,  3.71it/s] 98%|█████████▊| 207/212 [00:55<00:01,  3.66it/s] 98%|█████████▊| 208/212 [00:55<00:01,  3.73it/s] 99%|█████████▊| 209/212 [00:56<00:00,  3.67it/s] 99%|█████████▉| 210/212 [00:56<00:00,  3.71it/s]100%|█████████▉| 211/212 [00:56<00:00,  3.65it/s]100%|██████████| 212/212 [00:57<00:00,  3.73it/s]accuracy:  0.6698113207547169
100%|██████████| 212/212 [01:00<00:00,  3.52it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 135
Training dataset size: 192, validation dataset size: 135
Training dataset size: 192, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 04:17:13,059] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 04:17:13,205] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:17:15,197] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:18,  2.53s/it]  6%|▋         | 2/32 [00:04<01:07,  2.25s/it]  9%|▉         | 3/32 [00:07<01:09,  2.39s/it] 12%|█▎        | 4/32 [00:10<01:16,  2.73s/it] 16%|█▌        | 5/32 [00:12<01:11,  2.65s/it] 19%|█▉        | 6/32 [00:15<01:09,  2.66s/it] 22%|██▏       | 7/32 [00:18<01:05,  2.61s/it] 25%|██▌       | 8/32 [00:20<01:00,  2.52s/it] 28%|██▊       | 9/32 [00:22<00:55,  2.40s/it] 31%|███▏      | 10/32 [00:24<00:51,  2.33s/it]                                               {'loss': 0.5931, 'grad_norm': 7.431161880493164, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:24<00:51,  2.33s/it] 34%|███▍      | 11/32 [00:27<00:52,  2.50s/it] 38%|███▊      | 12/32 [00:30<00:49,  2.47s/it] 41%|████      | 13/32 [00:32<00:45,  2.41s/it] 44%|████▍     | 14/32 [00:35<00:45,  2.52s/it] 47%|████▋     | 15/32 [00:37<00:41,  2.45s/it] 50%|█████     | 16/32 [00:39<00:38,  2.44s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:41<00:35,  2.36s/it] 56%|█████▋    | 18/32 [00:44<00:35,  2.53s/it] 59%|█████▉    | 19/32 [00:47<00:31,  2.46s/it] 62%|██████▎   | 20/32 [00:49<00:29,  2.50s/it]                                               {'loss': 0.475, 'grad_norm': 6.067366600036621, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:49<00:29,  2.50s/it] 66%|██████▌   | 21/32 [00:51<00:26,  2.40s/it] 69%|██████▉   | 22/32 [00:54<00:23,  2.31s/it] 72%|███████▏  | 23/32 [00:56<00:20,  2.24s/it] 75%|███████▌  | 24/32 [00:58<00:19,  2.39s/it] 78%|███████▊  | 25/32 [01:01<00:16,  2.41s/it] 81%|████████▏ | 26/32 [01:03<00:14,  2.48s/it] 84%|████████▍ | 27/32 [01:06<00:12,  2.52s/it] 88%|████████▊ | 28/32 [01:09<00:10,  2.50s/it] 91%|█████████ | 29/32 [01:11<00:07,  2.46s/it] 94%|█████████▍| 30/32 [01:13<00:04,  2.45s/it]                                               {'loss': 0.5365, 'grad_norm': 4.544320583343506, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:13<00:04,  2.45s/it] 97%|█████████▋| 31/32 [01:16<00:02,  2.41s/it]100%|██████████| 32/32 [01:18<00:00,  2.49s/it]                                               {'train_runtime': 79.4708, 'train_samples_per_second': 4.832, 'train_steps_per_second': 0.403, 'train_loss': 0.5203822664916515, 'epoch': 2.0}
100%|██████████| 32/32 [01:19<00:00,  2.49s/it]100%|██████████| 32/32 [01:19<00:00,  2.48s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:50,  3.19it/s]  1%|          | 2/163 [00:00<00:45,  3.53it/s]  2%|▏         | 3/163 [00:00<00:44,  3.62it/s]  2%|▏         | 4/163 [00:01<00:44,  3.58it/s]  3%|▎         | 5/163 [00:01<00:41,  3.77it/s]  4%|▎         | 6/163 [00:01<00:41,  3.80it/s]  4%|▍         | 7/163 [00:01<00:41,  3.77it/s]  5%|▍         | 8/163 [00:02<00:41,  3.73it/s]  6%|▌         | 9/163 [00:02<00:40,  3.79it/s]  6%|▌         | 10/163 [00:02<00:39,  3.83it/s]  7%|▋         | 11/163 [00:02<00:40,  3.79it/s]  7%|▋         | 12/163 [00:03<00:39,  3.78it/s]  8%|▊         | 13/163 [00:03<00:39,  3.76it/s]  9%|▊         | 14/163 [00:03<00:39,  3.82it/s]  9%|▉         | 15/163 [00:04<00:39,  3.79it/s] 10%|▉         | 16/163 [00:04<00:39,  3.77it/s] 10%|█         | 17/163 [00:04<00:38,  3.77it/s] 11%|█         | 18/163 [00:04<00:37,  3.82it/s] 12%|█▏        | 19/163 [00:05<00:37,  3.79it/s] 12%|█▏        | 20/163 [00:05<00:38,  3.75it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.78it/s] 13%|█▎        | 22/163 [00:05<00:36,  3.83it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.79it/s] 15%|█▍        | 24/163 [00:06<00:36,  3.77it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.78it/s] 16%|█▌        | 26/163 [00:06<00:35,  3.85it/s] 17%|█▋        | 27/163 [00:07<00:35,  3.81it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.76it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.79it/s] 18%|█▊        | 30/163 [00:07<00:34,  3.82it/s] 19%|█▉        | 31/163 [00:08<00:34,  3.79it/s] 20%|█▉        | 32/163 [00:08<00:34,  3.77it/s] 20%|██        | 33/163 [00:08<00:34,  3.77it/s] 21%|██        | 34/163 [00:09<00:33,  3.82it/s] 21%|██▏       | 35/163 [00:09<00:33,  3.79it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.74it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.78it/s] 23%|██▎       | 38/163 [00:10<00:32,  3.84it/s] 24%|██▍       | 39/163 [00:10<00:32,  3.80it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.78it/s] 25%|██▌       | 41/163 [00:10<00:32,  3.77it/s] 26%|██▌       | 42/163 [00:11<00:31,  3.83it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.79it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.73it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.79it/s] 28%|██▊       | 46/163 [00:12<00:30,  3.83it/s] 29%|██▉       | 47/163 [00:12<00:30,  3.78it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.73it/s] 30%|███       | 49/163 [00:12<00:30,  3.79it/s] 31%|███       | 50/163 [00:13<00:29,  3.84it/s] 31%|███▏      | 51/163 [00:13<00:29,  3.80it/s] 32%|███▏      | 52/163 [00:13<00:29,  3.74it/s] 33%|███▎      | 53/163 [00:14<00:29,  3.78it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.82it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.78it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.72it/s] 35%|███▍      | 57/163 [00:15<00:28,  3.78it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.82it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.78it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.79it/s] 37%|███▋      | 61/163 [00:16<00:27,  3.75it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.81it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.78it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.72it/s] 40%|███▉      | 65/163 [00:17<00:25,  3.78it/s] 40%|████      | 66/163 [00:17<00:25,  3.82it/s] 41%|████      | 67/163 [00:17<00:25,  3.79it/s] 42%|████▏     | 68/163 [00:18<00:25,  3.71it/s] 42%|████▏     | 69/163 [00:18<00:24,  3.77it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.81it/s] 44%|████▎     | 71/163 [00:18<00:24,  3.77it/s] 44%|████▍     | 72/163 [00:19<00:24,  3.71it/s] 45%|████▍     | 73/163 [00:19<00:23,  3.78it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.82it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.78it/s] 47%|████▋     | 76/163 [00:20<00:23,  3.71it/s] 47%|████▋     | 77/163 [00:20<00:22,  3.77it/s] 48%|████▊     | 78/163 [00:20<00:22,  3.80it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.76it/s] 49%|████▉     | 80/163 [00:21<00:22,  3.71it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.77it/s] 50%|█████     | 82/163 [00:21<00:21,  3.81it/s] 51%|█████     | 83/163 [00:21<00:21,  3.77it/s] 52%|█████▏    | 84/163 [00:22<00:21,  3.70it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.76it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.78it/s] 53%|█████▎    | 87/163 [00:23<00:20,  3.77it/s] 54%|█████▍    | 88/163 [00:23<00:20,  3.70it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.76it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.79it/s] 56%|█████▌    | 91/163 [00:24<00:19,  3.76it/s] 56%|█████▋    | 92/163 [00:24<00:18,  3.75it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.75it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.80it/s] 58%|█████▊    | 95/163 [00:25<00:18,  3.76it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.74it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.75it/s] 60%|██████    | 98/163 [00:25<00:17,  3.80it/s] 61%|██████    | 99/163 [00:26<00:17,  3.76it/s] 61%|██████▏   | 100/163 [00:26<00:17,  3.69it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.75it/s] 63%|██████▎   | 102/163 [00:27<00:16,  3.78it/s] 63%|██████▎   | 103/163 [00:27<00:16,  3.75it/s] 64%|██████▍   | 104/163 [00:27<00:16,  3.64it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.77it/s] 65%|██████▌   | 106/163 [00:28<00:15,  3.80it/s] 66%|██████▌   | 107/163 [00:28<00:14,  3.76it/s] 66%|██████▋   | 108/163 [00:28<00:15,  3.65it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.78it/s] 67%|██████▋   | 110/163 [00:29<00:13,  3.80it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.77it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.66it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.79it/s] 70%|██████▉   | 114/163 [00:30<00:12,  3.80it/s] 71%|███████   | 115/163 [00:30<00:12,  3.77it/s] 71%|███████   | 116/163 [00:30<00:12,  3.75it/s] 72%|███████▏  | 117/163 [00:31<00:12,  3.75it/s] 72%|███████▏  | 118/163 [00:31<00:11,  3.80it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.76it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.70it/s] 74%|███████▍  | 121/163 [00:32<00:11,  3.76it/s] 75%|███████▍  | 122/163 [00:32<00:10,  3.83it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.78it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.72it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.77it/s] 77%|███████▋  | 126/163 [00:33<00:09,  3.81it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.76it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.69it/s] 79%|███████▉  | 129/163 [00:34<00:09,  3.75it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.78it/s] 80%|████████  | 131/163 [00:34<00:08,  3.76it/s] 81%|████████  | 132/163 [00:35<00:08,  3.70it/s] 82%|████████▏ | 133/163 [00:35<00:08,  3.75it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.78it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.74it/s] 83%|████████▎ | 136/163 [00:36<00:07,  3.70it/s] 84%|████████▍ | 137/163 [00:36<00:06,  3.74it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.78it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.75it/s] 86%|████████▌ | 140/163 [00:37<00:06,  3.69it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.74it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.78it/s] 88%|████████▊ | 143/163 [00:37<00:05,  3.75it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.69it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.75it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.79it/s] 90%|█████████ | 147/163 [00:39<00:04,  3.75it/s] 91%|█████████ | 148/163 [00:39<00:04,  3.69it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.75it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.80it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.75it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.71it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.75it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.80it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.75it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.69it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.74it/s] 97%|█████████▋| 158/163 [00:41<00:01,  3.78it/s] 98%|█████████▊| 159/163 [00:42<00:01,  3.74it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.69it/s] 99%|█████████▉| 161/163 [00:42<00:00,  3.75it/s] 99%|█████████▉| 162/163 [00:43<00:00,  3.80it/s]100%|██████████| 163/163 [00:43<00:00,  3.75it/s]accuracy:  0.8773006134969326
100%|██████████| 163/163 [00:45<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 198
Training dataset size: 192, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:19:46,402] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:19:46,674] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:19:48,478] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:17,  2.50s/it]  6%|▋         | 2/32 [00:05<01:15,  2.50s/it]  9%|▉         | 3/32 [00:07<01:13,  2.52s/it] 12%|█▎        | 4/32 [00:10<01:13,  2.63s/it] 16%|█▌        | 5/32 [00:12<01:10,  2.60s/it] 19%|█▉        | 6/32 [00:15<01:08,  2.62s/it] 22%|██▏       | 7/32 [00:18<01:09,  2.77s/it] 25%|██▌       | 8/32 [00:21<01:05,  2.74s/it] 28%|██▊       | 9/32 [00:24<01:02,  2.73s/it] 31%|███▏      | 10/32 [00:26<00:58,  2.66s/it]                                               {'loss': 0.8133, 'grad_norm': 1.897670030593872, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:26<00:58,  2.66s/it] 34%|███▍      | 11/32 [00:28<00:53,  2.57s/it] 38%|███▊      | 12/32 [00:31<00:52,  2.63s/it] 41%|████      | 13/32 [00:34<00:48,  2.57s/it] 44%|████▍     | 14/32 [00:36<00:46,  2.60s/it] 47%|████▋     | 15/32 [00:38<00:42,  2.49s/it] 50%|█████     | 16/32 [00:41<00:40,  2.55s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:44<00:37,  2.49s/it] 56%|█████▋    | 18/32 [00:46<00:35,  2.54s/it] 59%|█████▉    | 19/32 [00:48<00:30,  2.37s/it] 62%|██████▎   | 20/32 [00:50<00:27,  2.32s/it]                                               {'loss': 0.6986, 'grad_norm': 5.035240650177002, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:50<00:27,  2.32s/it] 66%|██████▌   | 21/32 [00:53<00:25,  2.30s/it] 69%|██████▉   | 22/32 [00:55<00:23,  2.39s/it] 72%|███████▏  | 23/32 [00:58<00:22,  2.48s/it] 75%|███████▌  | 24/32 [01:01<00:20,  2.53s/it] 78%|███████▊  | 25/32 [01:03<00:17,  2.49s/it] 81%|████████▏ | 26/32 [01:05<00:14,  2.46s/it] 84%|████████▍ | 27/32 [01:07<00:11,  2.32s/it] 88%|████████▊ | 28/32 [01:10<00:09,  2.40s/it] 91%|█████████ | 29/32 [01:12<00:06,  2.26s/it] 94%|█████████▍| 30/32 [01:14<00:04,  2.25s/it]                                               {'loss': 0.573, 'grad_norm': 1.397289514541626, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:14<00:04,  2.25s/it] 97%|█████████▋| 31/32 [01:17<00:02,  2.37s/it]100%|██████████| 32/32 [01:20<00:00,  2.61s/it]                                               {'train_runtime': 81.142, 'train_samples_per_second': 4.732, 'train_steps_per_second': 0.394, 'train_loss': 0.6918470151722431, 'epoch': 2.0}
100%|██████████| 32/32 [01:20<00:00,  2.61s/it]100%|██████████| 32/32 [01:20<00:00,  2.53s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:16,  3.02it/s]  1%|          | 2/232 [00:00<01:08,  3.37it/s]  1%|▏         | 3/232 [00:00<01:05,  3.52it/s]  2%|▏         | 4/232 [00:01<01:02,  3.64it/s]  2%|▏         | 5/232 [00:01<01:00,  3.72it/s]  3%|▎         | 6/232 [00:01<01:00,  3.71it/s]  3%|▎         | 7/232 [00:01<01:00,  3.71it/s]  3%|▎         | 8/232 [00:02<00:59,  3.74it/s]  4%|▍         | 9/232 [00:02<00:58,  3.81it/s]  4%|▍         | 10/232 [00:02<00:58,  3.77it/s]  5%|▍         | 11/232 [00:02<00:59,  3.73it/s]  5%|▌         | 12/232 [00:03<00:58,  3.77it/s]  6%|▌         | 13/232 [00:03<00:57,  3.82it/s]  6%|▌         | 14/232 [00:03<00:57,  3.78it/s]  6%|▋         | 15/232 [00:04<00:57,  3.75it/s]  7%|▋         | 16/232 [00:04<00:57,  3.77it/s]  7%|▋         | 17/232 [00:04<00:56,  3.81it/s]  8%|▊         | 18/232 [00:04<00:56,  3.78it/s]  8%|▊         | 19/232 [00:05<00:56,  3.77it/s]  9%|▊         | 20/232 [00:05<00:56,  3.77it/s]  9%|▉         | 21/232 [00:05<00:55,  3.81it/s]  9%|▉         | 22/232 [00:05<00:55,  3.77it/s] 10%|▉         | 23/232 [00:06<00:55,  3.76it/s] 10%|█         | 24/232 [00:06<00:55,  3.77it/s] 11%|█         | 25/232 [00:06<00:54,  3.82it/s] 11%|█         | 26/232 [00:06<00:54,  3.78it/s] 12%|█▏        | 27/232 [00:07<00:54,  3.77it/s] 12%|█▏        | 28/232 [00:07<00:54,  3.77it/s] 12%|█▎        | 29/232 [00:07<00:53,  3.82it/s] 13%|█▎        | 30/232 [00:08<00:53,  3.77it/s] 13%|█▎        | 31/232 [00:08<00:53,  3.73it/s] 14%|█▍        | 32/232 [00:08<00:53,  3.77it/s] 14%|█▍        | 33/232 [00:08<00:52,  3.82it/s] 15%|█▍        | 34/232 [00:09<00:52,  3.78it/s] 15%|█▌        | 35/232 [00:09<00:52,  3.76it/s] 16%|█▌        | 36/232 [00:09<00:51,  3.77it/s] 16%|█▌        | 37/232 [00:09<00:51,  3.81it/s] 16%|█▋        | 38/232 [00:10<00:51,  3.77it/s] 17%|█▋        | 39/232 [00:10<00:51,  3.73it/s] 17%|█▋        | 40/232 [00:10<00:51,  3.76it/s] 18%|█▊        | 41/232 [00:10<00:50,  3.81it/s] 18%|█▊        | 42/232 [00:11<00:50,  3.77it/s] 19%|█▊        | 43/232 [00:11<00:50,  3.73it/s] 19%|█▉        | 44/232 [00:11<00:49,  3.78it/s] 19%|█▉        | 45/232 [00:11<00:49,  3.81it/s] 20%|█▉        | 46/232 [00:12<00:49,  3.77it/s] 20%|██        | 47/232 [00:12<00:49,  3.73it/s] 21%|██        | 48/232 [00:12<00:48,  3.78it/s] 21%|██        | 49/232 [00:13<00:47,  3.83it/s] 22%|██▏       | 50/232 [00:13<00:48,  3.78it/s] 22%|██▏       | 51/232 [00:13<00:48,  3.76it/s] 22%|██▏       | 52/232 [00:13<00:47,  3.77it/s] 23%|██▎       | 53/232 [00:14<00:46,  3.82it/s] 23%|██▎       | 54/232 [00:14<00:47,  3.77it/s] 24%|██▎       | 55/232 [00:14<00:46,  3.82it/s] 24%|██▍       | 56/232 [00:14<00:46,  3.75it/s] 25%|██▍       | 57/232 [00:15<00:45,  3.82it/s] 25%|██▌       | 58/232 [00:15<00:46,  3.77it/s] 25%|██▌       | 59/232 [00:15<00:45,  3.77it/s] 26%|██▌       | 60/232 [00:15<00:45,  3.76it/s] 26%|██▋       | 61/232 [00:16<00:44,  3.81it/s] 27%|██▋       | 62/232 [00:16<00:45,  3.77it/s] 27%|██▋       | 63/232 [00:16<00:44,  3.77it/s] 28%|██▊       | 64/232 [00:17<00:44,  3.76it/s] 28%|██▊       | 65/232 [00:17<00:43,  3.82it/s] 28%|██▊       | 66/232 [00:17<00:43,  3.78it/s] 29%|██▉       | 67/232 [00:17<00:43,  3.78it/s] 29%|██▉       | 68/232 [00:18<00:43,  3.75it/s] 30%|██▉       | 69/232 [00:18<00:42,  3.80it/s] 30%|███       | 70/232 [00:18<00:43,  3.76it/s] 31%|███       | 71/232 [00:18<00:42,  3.79it/s] 31%|███       | 72/232 [00:19<00:42,  3.75it/s] 31%|███▏      | 73/232 [00:19<00:41,  3.82it/s] 32%|███▏      | 74/232 [00:19<00:41,  3.77it/s] 32%|███▏      | 75/232 [00:19<00:41,  3.76it/s] 33%|███▎      | 76/232 [00:20<00:41,  3.75it/s] 33%|███▎      | 77/232 [00:20<00:40,  3.81it/s] 34%|███▎      | 78/232 [00:20<00:40,  3.76it/s] 34%|███▍      | 79/232 [00:20<00:40,  3.78it/s] 34%|███▍      | 80/232 [00:21<00:40,  3.75it/s] 35%|███▍      | 81/232 [00:21<00:39,  3.79it/s] 35%|███▌      | 82/232 [00:21<00:40,  3.75it/s] 36%|███▌      | 83/232 [00:22<00:39,  3.73it/s] 36%|███▌      | 84/232 [00:22<00:39,  3.72it/s] 37%|███▋      | 85/232 [00:22<00:38,  3.78it/s] 37%|███▋      | 86/232 [00:22<00:39,  3.74it/s] 38%|███▊      | 87/232 [00:23<00:38,  3.73it/s] 38%|███▊      | 88/232 [00:23<00:38,  3.74it/s] 38%|███▊      | 89/232 [00:23<00:37,  3.78it/s] 39%|███▉      | 90/232 [00:23<00:38,  3.74it/s] 39%|███▉      | 91/232 [00:24<00:37,  3.73it/s] 40%|███▉      | 92/232 [00:24<00:37,  3.74it/s] 40%|████      | 93/232 [00:24<00:36,  3.78it/s] 41%|████      | 94/232 [00:24<00:36,  3.74it/s] 41%|████      | 95/232 [00:25<00:36,  3.73it/s] 41%|████▏     | 96/232 [00:25<00:36,  3.73it/s] 42%|████▏     | 97/232 [00:25<00:35,  3.78it/s] 42%|████▏     | 98/232 [00:26<00:35,  3.75it/s] 43%|████▎     | 99/232 [00:26<00:35,  3.75it/s] 43%|████▎     | 100/232 [00:26<00:35,  3.75it/s] 44%|████▎     | 101/232 [00:26<00:34,  3.82it/s] 44%|████▍     | 102/232 [00:27<00:34,  3.78it/s] 44%|████▍     | 103/232 [00:27<00:34,  3.77it/s] 45%|████▍     | 104/232 [00:27<00:34,  3.75it/s] 45%|████▌     | 105/232 [00:27<00:33,  3.79it/s] 46%|████▌     | 106/232 [00:28<00:33,  3.76it/s] 46%|████▌     | 107/232 [00:28<00:33,  3.75it/s] 47%|████▋     | 108/232 [00:28<00:33,  3.75it/s] 47%|████▋     | 109/232 [00:28<00:32,  3.81it/s] 47%|████▋     | 110/232 [00:29<00:32,  3.76it/s] 48%|████▊     | 111/232 [00:29<00:32,  3.75it/s] 48%|████▊     | 112/232 [00:29<00:31,  3.75it/s] 49%|████▊     | 113/232 [00:30<00:31,  3.80it/s] 49%|████▉     | 114/232 [00:30<00:31,  3.75it/s] 50%|████▉     | 115/232 [00:30<00:31,  3.73it/s] 50%|█████     | 116/232 [00:30<00:30,  3.75it/s] 50%|█████     | 117/232 [00:31<00:30,  3.80it/s] 51%|█████     | 118/232 [00:31<00:30,  3.75it/s] 51%|█████▏    | 119/232 [00:31<00:30,  3.72it/s] 52%|█████▏    | 120/232 [00:31<00:29,  3.75it/s] 52%|█████▏    | 121/232 [00:32<00:29,  3.80it/s] 53%|█████▎    | 122/232 [00:32<00:29,  3.76it/s] 53%|█████▎    | 123/232 [00:32<00:28,  3.76it/s] 53%|█████▎    | 124/232 [00:32<00:28,  3.74it/s] 54%|█████▍    | 125/232 [00:33<00:28,  3.80it/s] 54%|█████▍    | 126/232 [00:33<00:28,  3.75it/s] 55%|█████▍    | 127/232 [00:33<00:28,  3.73it/s] 55%|█████▌    | 128/232 [00:34<00:27,  3.83it/s] 56%|█████▌    | 129/232 [00:34<00:26,  3.92it/s] 56%|█████▌    | 130/232 [00:34<00:25,  3.98it/s] 56%|█████▋    | 131/232 [00:34<00:25,  4.03it/s] 57%|█████▋    | 132/232 [00:34<00:24,  4.06it/s] 57%|█████▋    | 133/232 [00:35<00:24,  4.08it/s] 58%|█████▊    | 134/232 [00:35<00:23,  4.10it/s] 58%|█████▊    | 135/232 [00:35<00:23,  4.10it/s] 59%|█████▊    | 136/232 [00:35<00:23,  4.10it/s] 59%|█████▉    | 137/232 [00:36<00:23,  4.10it/s] 59%|█████▉    | 138/232 [00:36<00:22,  4.11it/s] 60%|█████▉    | 139/232 [00:36<00:22,  4.12it/s] 60%|██████    | 140/232 [00:36<00:22,  4.13it/s] 61%|██████    | 141/232 [00:37<00:22,  4.14it/s] 61%|██████    | 142/232 [00:37<00:21,  4.13it/s] 62%|██████▏   | 143/232 [00:37<00:21,  4.13it/s] 62%|██████▏   | 144/232 [00:37<00:21,  4.12it/s] 62%|██████▎   | 145/232 [00:38<00:21,  4.11it/s] 63%|██████▎   | 146/232 [00:38<00:20,  4.12it/s] 63%|██████▎   | 147/232 [00:38<00:20,  4.12it/s] 64%|██████▍   | 148/232 [00:38<00:20,  4.12it/s] 64%|██████▍   | 149/232 [00:39<00:20,  4.11it/s] 65%|██████▍   | 150/232 [00:39<00:19,  4.11it/s] 65%|██████▌   | 151/232 [00:39<00:19,  4.11it/s] 66%|██████▌   | 152/232 [00:39<00:19,  4.11it/s] 66%|██████▌   | 153/232 [00:40<00:19,  4.11it/s] 66%|██████▋   | 154/232 [00:40<00:18,  4.12it/s] 67%|██████▋   | 155/232 [00:40<00:18,  4.12it/s] 67%|██████▋   | 156/232 [00:40<00:18,  4.11it/s] 68%|██████▊   | 157/232 [00:41<00:18,  4.10it/s] 68%|██████▊   | 158/232 [00:41<00:18,  4.11it/s] 69%|██████▊   | 159/232 [00:41<00:17,  4.12it/s] 69%|██████▉   | 160/232 [00:41<00:17,  4.12it/s] 69%|██████▉   | 161/232 [00:42<00:17,  4.12it/s] 70%|██████▉   | 162/232 [00:42<00:17,  4.12it/s] 70%|███████   | 163/232 [00:42<00:16,  4.11it/s] 71%|███████   | 164/232 [00:42<00:16,  4.11it/s] 71%|███████   | 165/232 [00:42<00:16,  4.11it/s] 72%|███████▏  | 166/232 [00:43<00:16,  4.12it/s] 72%|███████▏  | 167/232 [00:43<00:15,  4.12it/s] 72%|███████▏  | 168/232 [00:43<00:15,  4.11it/s] 73%|███████▎  | 169/232 [00:43<00:15,  4.11it/s] 73%|███████▎  | 170/232 [00:44<00:15,  4.10it/s] 74%|███████▎  | 171/232 [00:44<00:14,  4.11it/s] 74%|███████▍  | 172/232 [00:44<00:14,  4.11it/s] 75%|███████▍  | 173/232 [00:44<00:14,  4.11it/s] 75%|███████▌  | 174/232 [00:45<00:14,  4.11it/s] 75%|███████▌  | 175/232 [00:45<00:13,  4.10it/s] 76%|███████▌  | 176/232 [00:45<00:13,  4.11it/s] 76%|███████▋  | 177/232 [00:45<00:13,  4.11it/s] 77%|███████▋  | 178/232 [00:46<00:13,  4.11it/s] 77%|███████▋  | 179/232 [00:46<00:13,  4.03it/s] 78%|███████▊  | 180/232 [00:46<00:13,  3.93it/s] 78%|███████▊  | 181/232 [00:46<00:13,  3.80it/s] 78%|███████▊  | 182/232 [00:47<00:12,  3.89it/s] 79%|███████▉  | 183/232 [00:47<00:12,  3.87it/s] 79%|███████▉  | 184/232 [00:47<00:12,  3.74it/s] 80%|███████▉  | 185/232 [00:48<00:12,  3.66it/s] 80%|████████  | 186/232 [00:48<00:12,  3.77it/s] 81%|████████  | 187/232 [00:48<00:12,  3.70it/s] 81%|████████  | 188/232 [00:48<00:11,  3.68it/s] 81%|████████▏ | 189/232 [00:49<00:11,  3.65it/s] 82%|████████▏ | 190/232 [00:49<00:11,  3.77it/s] 82%|████████▏ | 191/232 [00:49<00:11,  3.67it/s] 83%|████████▎ | 192/232 [00:49<00:10,  3.70it/s] 83%|████████▎ | 193/232 [00:50<00:10,  3.74it/s] 84%|████████▎ | 194/232 [00:50<00:09,  3.84it/s] 84%|████████▍ | 195/232 [00:50<00:09,  3.92it/s] 84%|████████▍ | 196/232 [00:50<00:09,  3.97it/s] 85%|████████▍ | 197/232 [00:51<00:08,  4.00it/s] 85%|████████▌ | 198/232 [00:51<00:08,  4.03it/s] 86%|████████▌ | 199/232 [00:51<00:08,  4.06it/s] 86%|████████▌ | 200/232 [00:51<00:07,  4.07it/s] 87%|████████▋ | 201/232 [00:52<00:07,  4.07it/s] 87%|████████▋ | 202/232 [00:52<00:07,  4.07it/s] 88%|████████▊ | 203/232 [00:52<00:07,  4.08it/s] 88%|████████▊ | 204/232 [00:52<00:06,  4.09it/s] 88%|████████▊ | 205/232 [00:53<00:06,  4.10it/s] 89%|████████▉ | 206/232 [00:53<00:06,  4.09it/s] 89%|████████▉ | 207/232 [00:53<00:06,  4.09it/s] 90%|████████▉ | 208/232 [00:53<00:05,  4.09it/s] 90%|█████████ | 209/232 [00:54<00:05,  4.10it/s] 91%|█████████ | 210/232 [00:54<00:05,  4.10it/s] 91%|█████████ | 211/232 [00:54<00:05,  4.09it/s] 91%|█████████▏| 212/232 [00:54<00:04,  4.09it/s] 92%|█████████▏| 213/232 [00:55<00:04,  4.10it/s] 92%|█████████▏| 214/232 [00:55<00:04,  4.11it/s] 93%|█████████▎| 215/232 [00:55<00:04,  4.10it/s] 93%|█████████▎| 216/232 [00:55<00:03,  4.02it/s] 94%|█████████▎| 217/232 [00:56<00:03,  3.92it/s] 94%|█████████▍| 218/232 [00:56<00:03,  3.81it/s] 94%|█████████▍| 219/232 [00:56<00:03,  3.89it/s] 95%|█████████▍| 220/232 [00:56<00:03,  3.96it/s] 95%|█████████▌| 221/232 [00:57<00:02,  3.82it/s] 96%|█████████▌| 222/232 [00:57<00:02,  3.75it/s] 96%|█████████▌| 223/232 [00:57<00:02,  3.75it/s] 97%|█████████▋| 224/232 [00:57<00:02,  3.66it/s] 97%|█████████▋| 225/232 [00:58<00:01,  3.80it/s] 97%|█████████▋| 226/232 [00:58<00:01,  3.87it/s] 98%|█████████▊| 227/232 [00:58<00:01,  3.78it/s] 98%|█████████▊| 228/232 [00:59<00:01,  3.71it/s] 99%|█████████▊| 229/232 [00:59<00:00,  3.70it/s] 99%|█████████▉| 230/232 [00:59<00:00,  3.70it/s]100%|█████████▉| 231/232 [00:59<00:00,  3.81it/s]100%|██████████| 232/232 [01:00<00:00,  3.80it/s]accuracy:  0.8706896551724138
100%|██████████| 232/232 [01:03<00:00,  3.65it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 164
Training dataset size: 192, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.75s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[2025-03-12 04:22:39,251] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:22:40,078] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:22:42,396] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:25,  2.75s/it]  6%|▋         | 2/32 [00:05<01:18,  2.63s/it]  9%|▉         | 3/32 [00:07<01:13,  2.53s/it] 12%|█▎        | 4/32 [00:10<01:08,  2.45s/it] 16%|█▌        | 5/32 [00:12<01:04,  2.38s/it] 19%|█▉        | 6/32 [00:14<01:04,  2.48s/it] 22%|██▏       | 7/32 [00:17<01:02,  2.49s/it] 25%|██▌       | 8/32 [00:19<00:57,  2.38s/it] 28%|██▊       | 9/32 [00:21<00:53,  2.32s/it] 31%|███▏      | 10/32 [00:23<00:49,  2.25s/it]                                               {'loss': 1.0732, 'grad_norm': 10.335193634033203, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:23<00:49,  2.25s/it] 34%|███▍      | 11/32 [00:25<00:45,  2.17s/it] 38%|███▊      | 12/32 [00:27<00:43,  2.15s/it] 41%|████      | 13/32 [00:30<00:42,  2.21s/it] 44%|████▍     | 14/32 [00:33<00:42,  2.36s/it] 47%|████▋     | 15/32 [00:35<00:39,  2.31s/it] 50%|█████     | 16/32 [00:37<00:37,  2.33s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:40<00:37,  2.47s/it] 56%|█████▋    | 18/32 [00:42<00:33,  2.36s/it] 59%|█████▉    | 19/32 [00:44<00:30,  2.36s/it] 62%|██████▎   | 20/32 [00:47<00:28,  2.37s/it]                                               {'loss': 0.8944, 'grad_norm': 11.126995086669922, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:47<00:28,  2.37s/it] 66%|██████▌   | 21/32 [00:49<00:25,  2.31s/it] 69%|██████▉   | 22/32 [00:52<00:25,  2.52s/it] 72%|███████▏  | 23/32 [00:54<00:22,  2.47s/it] 75%|███████▌  | 24/32 [00:56<00:18,  2.34s/it] 78%|███████▊  | 25/32 [00:59<00:17,  2.49s/it] 81%|████████▏ | 26/32 [01:02<00:16,  2.70s/it] 84%|████████▍ | 27/32 [01:05<00:13,  2.71s/it] 88%|████████▊ | 28/32 [01:07<00:10,  2.54s/it] 91%|█████████ | 29/32 [01:09<00:06,  2.33s/it] 94%|█████████▍| 30/32 [01:12<00:04,  2.39s/it]                                               {'loss': 0.8839, 'grad_norm': 7.408307075500488, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:12<00:04,  2.39s/it] 97%|█████████▋| 31/32 [01:14<00:02,  2.52s/it]100%|██████████| 32/32 [01:17<00:00,  2.49s/it]                                               {'train_runtime': 77.9693, 'train_samples_per_second': 4.925, 'train_steps_per_second': 0.41, 'train_loss': 0.9443578571081161, 'epoch': 2.0}
100%|██████████| 32/32 [01:17<00:00,  2.49s/it]100%|██████████| 32/32 [01:17<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:24,  2.93it/s]  1%|          | 2/248 [00:00<01:09,  3.52it/s]  1%|          | 3/248 [00:00<01:08,  3.60it/s]  2%|▏         | 4/248 [00:01<01:07,  3.63it/s]  2%|▏         | 5/248 [00:01<01:06,  3.63it/s]  2%|▏         | 6/248 [00:01<01:04,  3.77it/s]  3%|▎         | 7/248 [00:01<01:04,  3.73it/s]  3%|▎         | 8/248 [00:02<01:04,  3.75it/s]  4%|▎         | 9/248 [00:02<01:04,  3.71it/s]  4%|▍         | 10/248 [00:02<01:02,  3.83it/s]  4%|▍         | 11/248 [00:02<01:02,  3.79it/s]  5%|▍         | 12/248 [00:03<01:02,  3.75it/s]  5%|▌         | 13/248 [00:03<01:02,  3.77it/s]  6%|▌         | 14/248 [00:03<01:00,  3.86it/s]  6%|▌         | 15/248 [00:03<00:59,  3.94it/s]  6%|▋         | 16/248 [00:04<00:58,  4.00it/s]  7%|▋         | 17/248 [00:04<00:57,  4.04it/s]  7%|▋         | 18/248 [00:04<00:56,  4.08it/s]  8%|▊         | 19/248 [00:04<00:55,  4.10it/s]  8%|▊         | 20/248 [00:05<00:55,  4.12it/s]  8%|▊         | 21/248 [00:05<00:54,  4.13it/s]  9%|▉         | 22/248 [00:05<00:54,  4.14it/s]  9%|▉         | 23/248 [00:05<00:54,  4.15it/s] 10%|▉         | 24/248 [00:06<00:53,  4.16it/s] 10%|█         | 25/248 [00:06<00:53,  4.16it/s] 10%|█         | 26/248 [00:06<00:53,  4.16it/s] 11%|█         | 27/248 [00:06<00:53,  4.16it/s] 11%|█▏        | 28/248 [00:07<00:52,  4.16it/s] 12%|█▏        | 29/248 [00:07<00:52,  4.16it/s] 12%|█▏        | 30/248 [00:07<00:52,  4.16it/s] 12%|█▎        | 31/248 [00:07<00:52,  4.16it/s] 13%|█▎        | 32/248 [00:08<00:51,  4.16it/s] 13%|█▎        | 33/248 [00:08<00:51,  4.16it/s] 14%|█▎        | 34/248 [00:08<00:51,  4.16it/s] 14%|█▍        | 35/248 [00:08<00:51,  4.15it/s] 15%|█▍        | 36/248 [00:09<00:51,  4.14it/s] 15%|█▍        | 37/248 [00:09<00:51,  4.13it/s] 15%|█▌        | 38/248 [00:09<00:50,  4.12it/s] 16%|█▌        | 39/248 [00:09<00:50,  4.13it/s] 16%|█▌        | 40/248 [00:10<00:50,  4.14it/s] 17%|█▋        | 41/248 [00:10<00:50,  4.14it/s] 17%|█▋        | 42/248 [00:10<00:49,  4.14it/s] 17%|█▋        | 43/248 [00:10<00:49,  4.14it/s] 18%|█▊        | 44/248 [00:10<00:49,  4.14it/s] 18%|█▊        | 45/248 [00:11<00:49,  4.14it/s] 19%|█▊        | 46/248 [00:11<00:49,  4.07it/s] 19%|█▉        | 47/248 [00:11<00:50,  3.94it/s] 19%|█▉        | 48/248 [00:12<00:52,  3.83it/s] 20%|█▉        | 49/248 [00:12<00:50,  3.91it/s] 20%|██        | 50/248 [00:12<00:51,  3.82it/s] 21%|██        | 51/248 [00:12<00:52,  3.75it/s] 21%|██        | 52/248 [00:13<00:52,  3.70it/s] 21%|██▏       | 53/248 [00:13<00:51,  3.82it/s] 22%|██▏       | 54/248 [00:13<00:51,  3.74it/s] 22%|██▏       | 55/248 [00:13<00:52,  3.70it/s] 23%|██▎       | 56/248 [00:14<00:52,  3.67it/s] 23%|██▎       | 57/248 [00:14<00:50,  3.77it/s] 23%|██▎       | 58/248 [00:14<00:51,  3.68it/s] 24%|██▍       | 59/248 [00:14<00:50,  3.73it/s] 24%|██▍       | 60/248 [00:15<00:51,  3.69it/s] 25%|██▍       | 61/248 [00:15<00:49,  3.74it/s] 25%|██▌       | 62/248 [00:15<00:50,  3.67it/s] 25%|██▌       | 63/248 [00:16<00:50,  3.67it/s] 26%|██▌       | 64/248 [00:16<00:48,  3.79it/s] 26%|██▌       | 65/248 [00:16<00:47,  3.89it/s] 27%|██▋       | 66/248 [00:16<00:45,  3.96it/s] 27%|██▋       | 67/248 [00:17<00:45,  4.02it/s] 27%|██▋       | 68/248 [00:17<00:44,  4.05it/s] 28%|██▊       | 69/248 [00:17<00:43,  4.07it/s] 28%|██▊       | 70/248 [00:17<00:43,  4.08it/s] 29%|██▊       | 71/248 [00:18<00:43,  4.08it/s] 29%|██▉       | 72/248 [00:18<00:43,  4.09it/s] 29%|██▉       | 73/248 [00:18<00:42,  4.10it/s] 30%|██▉       | 74/248 [00:18<00:42,  4.11it/s] 30%|███       | 75/248 [00:18<00:42,  4.11it/s] 31%|███       | 76/248 [00:19<00:41,  4.11it/s] 31%|███       | 77/248 [00:19<00:41,  4.10it/s] 31%|███▏      | 78/248 [00:19<00:41,  4.11it/s] 32%|███▏      | 79/248 [00:19<00:41,  4.11it/s] 32%|███▏      | 80/248 [00:20<00:40,  4.12it/s] 33%|███▎      | 81/248 [00:20<00:40,  4.13it/s] 33%|███▎      | 82/248 [00:20<00:40,  4.12it/s] 33%|███▎      | 83/248 [00:20<00:40,  4.03it/s] 34%|███▍      | 84/248 [00:21<00:41,  3.92it/s] 34%|███▍      | 85/248 [00:21<00:41,  3.89it/s] 35%|███▍      | 86/248 [00:21<00:41,  3.94it/s] 35%|███▌      | 87/248 [00:21<00:41,  3.85it/s] 35%|███▌      | 88/248 [00:22<00:41,  3.82it/s] 36%|███▌      | 89/248 [00:22<00:42,  3.74it/s] 36%|███▋      | 90/248 [00:22<00:41,  3.84it/s] 37%|███▋      | 91/248 [00:23<00:41,  3.79it/s] 37%|███▋      | 92/248 [00:23<00:41,  3.77it/s] 38%|███▊      | 93/248 [00:23<00:41,  3.71it/s] 38%|███▊      | 94/248 [00:23<00:40,  3.82it/s] 38%|███▊      | 95/248 [00:24<00:40,  3.79it/s] 39%|███▊      | 96/248 [00:24<00:40,  3.75it/s] 39%|███▉      | 97/248 [00:24<00:40,  3.71it/s] 40%|███▉      | 98/248 [00:24<00:39,  3.81it/s] 40%|███▉      | 99/248 [00:25<00:39,  3.78it/s] 40%|████      | 100/248 [00:25<00:39,  3.75it/s] 41%|████      | 101/248 [00:25<00:39,  3.70it/s] 41%|████      | 102/248 [00:25<00:38,  3.82it/s] 42%|████▏     | 103/248 [00:26<00:38,  3.78it/s] 42%|████▏     | 104/248 [00:26<00:38,  3.75it/s] 42%|████▏     | 105/248 [00:26<00:38,  3.70it/s] 43%|████▎     | 106/248 [00:27<00:37,  3.81it/s] 43%|████▎     | 107/248 [00:27<00:37,  3.78it/s] 44%|████▎     | 108/248 [00:27<00:37,  3.75it/s] 44%|████▍     | 109/248 [00:27<00:37,  3.69it/s] 44%|████▍     | 110/248 [00:28<00:36,  3.79it/s] 45%|████▍     | 111/248 [00:28<00:36,  3.75it/s] 45%|████▌     | 112/248 [00:28<00:36,  3.73it/s] 46%|████▌     | 113/248 [00:28<00:36,  3.68it/s] 46%|████▌     | 114/248 [00:29<00:35,  3.79it/s] 46%|████▋     | 115/248 [00:29<00:35,  3.74it/s] 47%|████▋     | 116/248 [00:29<00:35,  3.74it/s] 47%|████▋     | 117/248 [00:30<00:35,  3.68it/s] 48%|████▊     | 118/248 [00:30<00:34,  3.79it/s] 48%|████▊     | 119/248 [00:30<00:34,  3.75it/s] 48%|████▊     | 120/248 [00:30<00:34,  3.73it/s] 49%|████▉     | 121/248 [00:31<00:34,  3.68it/s] 49%|████▉     | 122/248 [00:31<00:33,  3.80it/s] 50%|████▉     | 123/248 [00:31<00:33,  3.76it/s] 50%|█████     | 124/248 [00:31<00:33,  3.74it/s] 50%|█████     | 125/248 [00:32<00:33,  3.68it/s] 51%|█████     | 126/248 [00:32<00:32,  3.79it/s] 51%|█████     | 127/248 [00:32<00:32,  3.75it/s] 52%|█████▏    | 128/248 [00:32<00:32,  3.73it/s] 52%|█████▏    | 129/248 [00:33<00:32,  3.68it/s] 52%|█████▏    | 130/248 [00:33<00:31,  3.79it/s] 53%|█████▎    | 131/248 [00:33<00:31,  3.77it/s] 53%|█████▎    | 132/248 [00:34<00:31,  3.74it/s] 54%|█████▎    | 133/248 [00:34<00:31,  3.68it/s] 54%|█████▍    | 134/248 [00:34<00:30,  3.79it/s] 54%|█████▍    | 135/248 [00:34<00:30,  3.74it/s] 55%|█████▍    | 136/248 [00:35<00:30,  3.73it/s] 55%|█████▌    | 137/248 [00:35<00:30,  3.68it/s] 56%|█████▌    | 138/248 [00:35<00:28,  3.80it/s] 56%|█████▌    | 139/248 [00:35<00:28,  3.77it/s] 56%|█████▋    | 140/248 [00:36<00:28,  3.74it/s] 57%|█████▋    | 141/248 [00:36<00:29,  3.68it/s] 57%|█████▋    | 142/248 [00:36<00:27,  3.79it/s] 58%|█████▊    | 143/248 [00:36<00:28,  3.75it/s] 58%|█████▊    | 144/248 [00:37<00:27,  3.72it/s] 58%|█████▊    | 145/248 [00:37<00:27,  3.68it/s] 59%|█████▉    | 146/248 [00:37<00:26,  3.79it/s] 59%|█████▉    | 147/248 [00:38<00:26,  3.77it/s] 60%|█████▉    | 148/248 [00:38<00:26,  3.72it/s] 60%|██████    | 149/248 [00:38<00:27,  3.65it/s] 60%|██████    | 150/248 [00:38<00:25,  3.78it/s] 61%|██████    | 151/248 [00:39<00:25,  3.76it/s] 61%|██████▏   | 152/248 [00:39<00:25,  3.74it/s] 62%|██████▏   | 153/248 [00:39<00:25,  3.65it/s] 62%|██████▏   | 154/248 [00:39<00:24,  3.77it/s] 62%|██████▎   | 155/248 [00:40<00:24,  3.78it/s] 63%|██████▎   | 156/248 [00:40<00:24,  3.74it/s] 63%|██████▎   | 157/248 [00:40<00:24,  3.65it/s] 64%|██████▎   | 158/248 [00:40<00:23,  3.77it/s] 64%|██████▍   | 159/248 [00:41<00:23,  3.78it/s] 65%|██████▍   | 160/248 [00:41<00:23,  3.74it/s] 65%|██████▍   | 161/248 [00:41<00:23,  3.64it/s] 65%|██████▌   | 162/248 [00:42<00:22,  3.76it/s] 66%|██████▌   | 163/248 [00:42<00:22,  3.78it/s] 66%|██████▌   | 164/248 [00:42<00:22,  3.74it/s] 67%|██████▋   | 165/248 [00:42<00:22,  3.74it/s] 67%|██████▋   | 166/248 [00:43<00:21,  3.84it/s] 67%|██████▋   | 167/248 [00:43<00:20,  3.90it/s] 68%|██████▊   | 168/248 [00:43<00:20,  3.95it/s] 68%|██████▊   | 169/248 [00:43<00:19,  4.00it/s] 69%|██████▊   | 170/248 [00:44<00:19,  4.02it/s] 69%|██████▉   | 171/248 [00:44<00:19,  4.03it/s] 69%|██████▉   | 172/248 [00:44<00:18,  4.05it/s] 70%|██████▉   | 173/248 [00:44<00:18,  4.07it/s] 70%|███████   | 174/248 [00:45<00:18,  4.07it/s] 71%|███████   | 175/248 [00:45<00:18,  4.03it/s] 71%|███████   | 176/248 [00:45<00:18,  3.92it/s] 71%|███████▏  | 177/248 [00:45<00:18,  3.86it/s] 72%|███████▏  | 178/248 [00:46<00:18,  3.81it/s] 72%|███████▏  | 179/248 [00:46<00:17,  3.88it/s] 73%|███████▎  | 180/248 [00:46<00:17,  3.92it/s] 73%|███████▎  | 181/248 [00:46<00:17,  3.85it/s] 73%|███████▎  | 182/248 [00:47<00:17,  3.79it/s] 74%|███████▍  | 183/248 [00:47<00:17,  3.69it/s] 74%|███████▍  | 184/248 [00:47<00:16,  3.80it/s] 75%|███████▍  | 185/248 [00:47<00:16,  3.86it/s] 75%|███████▌  | 186/248 [00:48<00:16,  3.81it/s] 75%|███████▌  | 187/248 [00:48<00:16,  3.75it/s] 76%|███████▌  | 188/248 [00:48<00:16,  3.65it/s] 76%|███████▌  | 189/248 [00:49<00:15,  3.77it/s] 77%|███████▋  | 190/248 [00:49<00:15,  3.85it/s] 77%|███████▋  | 191/248 [00:49<00:14,  3.80it/s] 77%|███████▋  | 192/248 [00:49<00:14,  3.76it/s] 78%|███████▊  | 193/248 [00:50<00:14,  3.77it/s] 78%|███████▊  | 194/248 [00:50<00:14,  3.70it/s] 79%|███████▊  | 195/248 [00:50<00:13,  3.81it/s] 79%|███████▉  | 196/248 [00:50<00:13,  3.77it/s] 79%|███████▉  | 197/248 [00:51<00:13,  3.74it/s] 80%|███████▉  | 198/248 [00:51<00:13,  3.77it/s] 80%|████████  | 199/248 [00:51<00:13,  3.69it/s] 81%|████████  | 200/248 [00:51<00:12,  3.79it/s] 81%|████████  | 201/248 [00:52<00:12,  3.79it/s] 81%|████████▏ | 202/248 [00:52<00:12,  3.75it/s] 82%|████████▏ | 203/248 [00:52<00:11,  3.76it/s] 82%|████████▏ | 204/248 [00:53<00:11,  3.69it/s] 83%|████████▎ | 205/248 [00:53<00:11,  3.80it/s] 83%|████████▎ | 206/248 [00:53<00:11,  3.79it/s] 83%|████████▎ | 207/248 [00:53<00:10,  3.75it/s] 84%|████████▍ | 208/248 [00:54<00:10,  3.76it/s] 84%|████████▍ | 209/248 [00:54<00:10,  3.68it/s] 85%|████████▍ | 210/248 [00:54<00:10,  3.79it/s] 85%|████████▌ | 211/248 [00:54<00:09,  3.81it/s] 85%|████████▌ | 212/248 [00:55<00:09,  3.74it/s] 86%|████████▌ | 213/248 [00:55<00:09,  3.72it/s] 86%|████████▋ | 214/248 [00:55<00:09,  3.64it/s] 87%|████████▋ | 215/248 [00:55<00:08,  3.77it/s] 87%|████████▋ | 216/248 [00:56<00:08,  3.83it/s] 88%|████████▊ | 217/248 [00:56<00:08,  3.78it/s] 88%|████████▊ | 218/248 [00:56<00:08,  3.73it/s] 88%|████████▊ | 219/248 [00:57<00:07,  3.65it/s] 89%|████████▊ | 220/248 [00:57<00:07,  3.77it/s] 89%|████████▉ | 221/248 [00:57<00:07,  3.81it/s] 90%|████████▉ | 222/248 [00:57<00:06,  3.78it/s] 90%|████████▉ | 223/248 [00:58<00:06,  3.74it/s] 90%|█████████ | 224/248 [00:58<00:06,  3.65it/s] 91%|█████████ | 225/248 [00:58<00:06,  3.77it/s] 91%|█████████ | 226/248 [00:58<00:05,  3.82it/s] 92%|█████████▏| 227/248 [00:59<00:05,  3.78it/s] 92%|█████████▏| 228/248 [00:59<00:05,  3.73it/s] 92%|█████████▏| 229/248 [00:59<00:05,  3.65it/s] 93%|█████████▎| 230/248 [00:59<00:04,  3.78it/s] 93%|█████████▎| 231/248 [01:00<00:04,  3.84it/s] 94%|█████████▎| 232/248 [01:00<00:04,  3.78it/s] 94%|█████████▍| 233/248 [01:00<00:04,  3.73it/s] 94%|█████████▍| 234/248 [01:01<00:03,  3.64it/s] 95%|█████████▍| 235/248 [01:01<00:03,  3.76it/s] 95%|█████████▌| 236/248 [01:01<00:03,  3.82it/s] 96%|█████████▌| 237/248 [01:01<00:02,  3.78it/s] 96%|█████████▌| 238/248 [01:02<00:02,  3.73it/s] 96%|█████████▋| 239/248 [01:02<00:02,  3.65it/s] 97%|█████████▋| 240/248 [01:02<00:02,  3.77it/s] 97%|█████████▋| 241/248 [01:02<00:01,  3.81it/s] 98%|█████████▊| 242/248 [01:03<00:01,  3.77it/s] 98%|█████████▊| 243/248 [01:03<00:01,  3.73it/s] 98%|█████████▊| 244/248 [01:03<00:01,  3.65it/s] 99%|█████████▉| 245/248 [01:03<00:00,  3.77it/s] 99%|█████████▉| 246/248 [01:04<00:00,  3.82it/s]100%|█████████▉| 247/248 [01:04<00:00,  3.78it/s]100%|██████████| 248/248 [01:04<00:00,  3.73it/s]accuracy:  0.657258064516129
100%|██████████| 248/248 [01:08<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 236
Training dataset size: 192, validation dataset size: 236
Training dataset size: 192, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
training start
[2025-03-12 04:25:35,600] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 04:25:35,748] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:25:35,799] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:18,  2.54s/it]  6%|▋         | 2/32 [00:05<01:27,  2.91s/it]  9%|▉         | 3/32 [00:07<01:15,  2.62s/it] 12%|█▎        | 4/32 [00:10<01:08,  2.43s/it] 16%|█▌        | 5/32 [00:12<01:04,  2.39s/it] 19%|█▉        | 6/32 [00:14<01:00,  2.33s/it] 22%|██▏       | 7/32 [00:16<00:56,  2.25s/it] 25%|██▌       | 8/32 [00:19<00:55,  2.32s/it] 28%|██▊       | 9/32 [00:21<00:52,  2.28s/it] 31%|███▏      | 10/32 [00:23<00:51,  2.33s/it]                                               {'loss': 1.0043, 'grad_norm': 16.322824478149414, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:23<00:51,  2.33s/it] 34%|███▍      | 11/32 [00:25<00:46,  2.22s/it] 38%|███▊      | 12/32 [00:27<00:43,  2.18s/it] 41%|████      | 13/32 [00:30<00:42,  2.24s/it] 44%|████▍     | 14/32 [00:32<00:40,  2.25s/it] 47%|████▋     | 15/32 [00:34<00:38,  2.26s/it] 50%|█████     | 16/32 [00:36<00:34,  2.15s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:39<00:36,  2.42s/it] 56%|█████▋    | 18/32 [00:42<00:35,  2.53s/it] 59%|█████▉    | 19/32 [00:45<00:33,  2.61s/it] 62%|██████▎   | 20/32 [00:47<00:29,  2.45s/it]                                               {'loss': 0.9714, 'grad_norm': 8.426201820373535, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:47<00:29,  2.45s/it] 66%|██████▌   | 21/32 [00:49<00:26,  2.41s/it] 69%|██████▉   | 22/32 [00:51<00:23,  2.30s/it] 72%|███████▏  | 23/32 [00:54<00:20,  2.28s/it] 75%|███████▌  | 24/32 [00:56<00:18,  2.25s/it] 78%|███████▊  | 25/32 [00:58<00:16,  2.38s/it] 81%|████████▏ | 26/32 [01:00<00:13,  2.26s/it] 84%|████████▍ | 27/32 [01:03<00:11,  2.29s/it] 88%|████████▊ | 28/32 [01:05<00:08,  2.23s/it] 91%|█████████ | 29/32 [01:07<00:06,  2.19s/it] 94%|█████████▍| 30/32 [01:09<00:04,  2.18s/it]                                               {'loss': 0.6371, 'grad_norm': 7.730414867401123, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:09<00:04,  2.18s/it] 97%|█████████▋| 31/32 [01:11<00:02,  2.20s/it]100%|██████████| 32/32 [01:14<00:00,  2.24s/it]                                               {'train_runtime': 74.8626, 'train_samples_per_second': 5.129, 'train_steps_per_second': 0.427, 'train_loss': 0.8848086893558502, 'epoch': 2.0}
100%|██████████| 32/32 [01:14<00:00,  2.24s/it]100%|██████████| 32/32 [01:14<00:00,  2.33s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<01:59,  3.02it/s]  1%|          | 2/361 [00:00<01:40,  3.58it/s]  1%|          | 3/361 [00:00<01:33,  3.82it/s]  1%|          | 4/361 [00:01<01:30,  3.94it/s]  1%|▏         | 5/361 [00:01<01:28,  4.02it/s]  2%|▏         | 6/361 [00:01<01:27,  4.06it/s]  2%|▏         | 7/361 [00:01<01:26,  4.09it/s]  2%|▏         | 8/361 [00:02<01:25,  4.11it/s]  2%|▏         | 9/361 [00:02<01:25,  4.13it/s]  3%|▎         | 10/361 [00:02<01:24,  4.13it/s]  3%|▎         | 11/361 [00:02<01:25,  4.12it/s]  3%|▎         | 12/361 [00:03<01:27,  3.99it/s]  4%|▎         | 13/361 [00:03<01:29,  3.90it/s]  4%|▍         | 14/361 [00:03<01:29,  3.88it/s]  4%|▍         | 15/361 [00:03<01:29,  3.89it/s]  4%|▍         | 16/361 [00:04<01:30,  3.79it/s]  5%|▍         | 17/361 [00:04<01:30,  3.82it/s]  5%|▍         | 18/361 [00:04<01:31,  3.75it/s]  5%|▌         | 19/361 [00:04<01:29,  3.80it/s]  6%|▌         | 20/361 [00:05<01:31,  3.72it/s]  6%|▌         | 21/361 [00:05<01:30,  3.76it/s]  6%|▌         | 22/361 [00:05<01:31,  3.70it/s]  6%|▋         | 23/361 [00:05<01:30,  3.75it/s]  7%|▋         | 24/361 [00:06<01:31,  3.69it/s]  7%|▋         | 25/361 [00:06<01:30,  3.69it/s]  7%|▋         | 26/361 [00:06<01:30,  3.72it/s]  7%|▋         | 27/361 [00:07<01:29,  3.75it/s]  8%|▊         | 28/361 [00:07<01:30,  3.69it/s]  8%|▊         | 29/361 [00:07<01:29,  3.70it/s]  8%|▊         | 30/361 [00:07<01:28,  3.73it/s]  9%|▊         | 31/361 [00:08<01:27,  3.76it/s]  9%|▉         | 32/361 [00:08<01:28,  3.70it/s]  9%|▉         | 33/361 [00:08<01:28,  3.69it/s]  9%|▉         | 34/361 [00:08<01:27,  3.74it/s] 10%|▉         | 35/361 [00:09<01:26,  3.76it/s] 10%|▉         | 36/361 [00:09<01:27,  3.70it/s] 10%|█         | 37/361 [00:09<01:27,  3.70it/s] 11%|█         | 38/361 [00:09<01:26,  3.73it/s] 11%|█         | 39/361 [00:10<01:25,  3.77it/s] 11%|█         | 40/361 [00:10<01:26,  3.70it/s] 11%|█▏        | 41/361 [00:10<01:25,  3.74it/s] 12%|█▏        | 42/361 [00:11<01:26,  3.69it/s] 12%|█▏        | 43/361 [00:11<01:25,  3.74it/s] 12%|█▏        | 44/361 [00:11<01:26,  3.68it/s] 12%|█▏        | 45/361 [00:11<01:24,  3.73it/s] 13%|█▎        | 46/361 [00:12<01:25,  3.68it/s] 13%|█▎        | 47/361 [00:12<01:24,  3.74it/s] 13%|█▎        | 48/361 [00:12<01:25,  3.68it/s] 14%|█▎        | 49/361 [00:12<01:23,  3.73it/s] 14%|█▍        | 50/361 [00:13<01:24,  3.68it/s] 14%|█▍        | 51/361 [00:13<01:22,  3.74it/s] 14%|█▍        | 52/361 [00:13<01:24,  3.68it/s] 15%|█▍        | 53/361 [00:14<01:22,  3.73it/s] 15%|█▍        | 54/361 [00:14<01:23,  3.67it/s] 15%|█▌        | 55/361 [00:14<01:21,  3.74it/s] 16%|█▌        | 56/361 [00:14<01:22,  3.68it/s] 16%|█▌        | 57/361 [00:15<01:21,  3.73it/s] 16%|█▌        | 58/361 [00:15<01:22,  3.68it/s] 16%|█▋        | 59/361 [00:15<01:20,  3.77it/s] 17%|█▋        | 60/361 [00:15<01:21,  3.71it/s] 17%|█▋        | 61/361 [00:16<01:20,  3.71it/s] 17%|█▋        | 62/361 [00:16<01:21,  3.67it/s] 17%|█▋        | 63/361 [00:16<01:18,  3.78it/s] 18%|█▊        | 64/361 [00:16<01:19,  3.73it/s] 18%|█▊        | 65/361 [00:17<01:19,  3.70it/s] 18%|█▊        | 66/361 [00:17<01:20,  3.65it/s] 19%|█▊        | 67/361 [00:17<01:17,  3.78it/s] 19%|█▉        | 68/361 [00:18<01:18,  3.72it/s] 19%|█▉        | 69/361 [00:18<01:19,  3.69it/s] 19%|█▉        | 70/361 [00:18<01:19,  3.64it/s] 20%|█▉        | 71/361 [00:18<01:16,  3.77it/s] 20%|█▉        | 72/361 [00:19<01:17,  3.72it/s] 20%|██        | 73/361 [00:19<01:18,  3.68it/s] 20%|██        | 74/361 [00:19<01:19,  3.63it/s] 21%|██        | 75/361 [00:19<01:15,  3.77it/s] 21%|██        | 76/361 [00:20<01:16,  3.73it/s] 21%|██▏       | 77/361 [00:20<01:16,  3.70it/s] 22%|██▏       | 78/361 [00:20<01:17,  3.65it/s] 22%|██▏       | 79/361 [00:21<01:14,  3.76it/s] 22%|██▏       | 80/361 [00:21<01:14,  3.77it/s] 22%|██▏       | 81/361 [00:21<01:15,  3.71it/s] 23%|██▎       | 82/361 [00:21<01:14,  3.77it/s] 23%|██▎       | 83/361 [00:22<01:14,  3.71it/s] 23%|██▎       | 84/361 [00:22<01:13,  3.76it/s] 24%|██▎       | 85/361 [00:22<01:14,  3.70it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.74it/s] 24%|██▍       | 87/361 [00:23<01:14,  3.69it/s] 24%|██▍       | 88/361 [00:23<01:12,  3.75it/s] 25%|██▍       | 89/361 [00:23<01:13,  3.68it/s] 25%|██▍       | 90/361 [00:23<01:12,  3.73it/s] 25%|██▌       | 91/361 [00:24<01:13,  3.68it/s] 25%|██▌       | 92/361 [00:24<01:11,  3.74it/s] 26%|██▌       | 93/361 [00:24<01:12,  3.68it/s] 26%|██▌       | 94/361 [00:25<01:11,  3.73it/s] 26%|██▋       | 95/361 [00:25<01:12,  3.68it/s] 27%|██▋       | 96/361 [00:25<01:11,  3.72it/s] 27%|██▋       | 97/361 [00:25<01:11,  3.67it/s] 27%|██▋       | 98/361 [00:26<01:10,  3.73it/s] 27%|██▋       | 99/361 [00:26<01:11,  3.68it/s] 28%|██▊       | 100/361 [00:26<01:10,  3.71it/s] 28%|██▊       | 101/361 [00:26<01:11,  3.66it/s] 28%|██▊       | 102/361 [00:27<01:09,  3.74it/s] 29%|██▊       | 103/361 [00:27<01:09,  3.70it/s] 29%|██▉       | 104/361 [00:27<01:08,  3.74it/s] 29%|██▉       | 105/361 [00:28<01:09,  3.68it/s] 29%|██▉       | 106/361 [00:28<01:08,  3.75it/s] 30%|██▉       | 107/361 [00:28<01:08,  3.68it/s] 30%|██▉       | 108/361 [00:28<01:07,  3.74it/s] 30%|███       | 109/361 [00:29<01:08,  3.68it/s] 30%|███       | 110/361 [00:29<01:07,  3.73it/s] 31%|███       | 111/361 [00:29<01:08,  3.67it/s] 31%|███       | 112/361 [00:29<01:07,  3.71it/s] 31%|███▏      | 113/361 [00:30<01:07,  3.67it/s] 32%|███▏      | 114/361 [00:30<01:06,  3.73it/s] 32%|███▏      | 115/361 [00:30<01:06,  3.68it/s] 32%|███▏      | 116/361 [00:31<01:05,  3.75it/s] 32%|███▏      | 117/361 [00:31<01:06,  3.68it/s] 33%|███▎      | 118/361 [00:31<01:05,  3.72it/s] 33%|███▎      | 119/361 [00:31<01:05,  3.68it/s] 33%|███▎      | 120/361 [00:32<01:04,  3.75it/s] 34%|███▎      | 121/361 [00:32<01:05,  3.68it/s] 34%|███▍      | 122/361 [00:32<01:04,  3.73it/s] 34%|███▍      | 123/361 [00:32<01:04,  3.68it/s] 34%|███▍      | 124/361 [00:33<01:03,  3.74it/s] 35%|███▍      | 125/361 [00:33<01:04,  3.67it/s] 35%|███▍      | 126/361 [00:33<01:03,  3.72it/s] 35%|███▌      | 127/361 [00:33<01:03,  3.67it/s] 35%|███▌      | 128/361 [00:34<01:02,  3.74it/s] 36%|███▌      | 129/361 [00:34<01:03,  3.68it/s] 36%|███▌      | 130/361 [00:34<01:02,  3.72it/s] 36%|███▋      | 131/361 [00:35<01:02,  3.67it/s] 37%|███▋      | 132/361 [00:35<01:01,  3.74it/s] 37%|███▋      | 133/361 [00:35<01:01,  3.68it/s] 37%|███▋      | 134/361 [00:35<01:01,  3.71it/s] 37%|███▋      | 135/361 [00:36<01:00,  3.76it/s] 38%|███▊      | 136/361 [00:36<00:58,  3.86it/s] 38%|███▊      | 137/361 [00:36<00:57,  3.92it/s] 38%|███▊      | 138/361 [00:36<00:56,  3.96it/s] 39%|███▊      | 139/361 [00:37<00:55,  4.00it/s] 39%|███▉      | 140/361 [00:37<00:54,  4.04it/s] 39%|███▉      | 141/361 [00:37<00:54,  4.06it/s] 39%|███▉      | 142/361 [00:37<00:53,  4.06it/s] 40%|███▉      | 143/361 [00:38<00:53,  4.07it/s] 40%|███▉      | 144/361 [00:38<00:53,  4.08it/s] 40%|████      | 145/361 [00:38<00:53,  4.01it/s] 40%|████      | 146/361 [00:38<00:54,  3.93it/s] 41%|████      | 147/361 [00:39<00:55,  3.86it/s] 41%|████      | 148/361 [00:39<00:55,  3.85it/s] 41%|████▏     | 149/361 [00:39<00:54,  3.92it/s] 42%|████▏     | 150/361 [00:39<00:54,  3.87it/s] 42%|████▏     | 151/361 [00:40<00:55,  3.78it/s] 42%|████▏     | 152/361 [00:40<00:55,  3.80it/s] 42%|████▏     | 153/361 [00:40<00:56,  3.71it/s] 43%|████▎     | 154/361 [00:40<00:54,  3.81it/s] 43%|████▎     | 155/361 [00:41<00:54,  3.76it/s] 43%|████▎     | 156/361 [00:41<00:54,  3.75it/s] 43%|████▎     | 157/361 [00:41<00:53,  3.78it/s] 44%|████▍     | 158/361 [00:42<00:54,  3.71it/s] 44%|████▍     | 159/361 [00:42<00:52,  3.82it/s] 44%|████▍     | 160/361 [00:42<00:52,  3.81it/s] 45%|████▍     | 161/361 [00:42<00:53,  3.75it/s] 45%|████▍     | 162/361 [00:43<00:52,  3.77it/s] 45%|████▌     | 163/361 [00:43<00:53,  3.70it/s] 45%|████▌     | 164/361 [00:43<00:51,  3.80it/s] 46%|████▌     | 165/361 [00:43<00:52,  3.74it/s] 46%|████▌     | 166/361 [00:44<00:52,  3.73it/s] 46%|████▋     | 167/361 [00:44<00:51,  3.76it/s] 47%|████▋     | 168/361 [00:44<00:52,  3.69it/s] 47%|████▋     | 169/361 [00:44<00:50,  3.80it/s] 47%|████▋     | 170/361 [00:45<00:50,  3.78it/s] 47%|████▋     | 171/361 [00:45<00:51,  3.72it/s] 48%|████▊     | 172/361 [00:45<00:50,  3.75it/s] 48%|████▊     | 173/361 [00:46<00:50,  3.70it/s] 48%|████▊     | 174/361 [00:46<00:49,  3.81it/s] 48%|████▊     | 175/361 [00:46<00:48,  3.81it/s] 49%|████▉     | 176/361 [00:46<00:49,  3.74it/s] 49%|████▉     | 177/361 [00:47<00:48,  3.78it/s] 49%|████▉     | 178/361 [00:47<00:49,  3.70it/s] 50%|████▉     | 179/361 [00:47<00:47,  3.81it/s] 50%|████▉     | 180/361 [00:47<00:47,  3.78it/s] 50%|█████     | 181/361 [00:48<00:48,  3.73it/s] 50%|█████     | 182/361 [00:48<00:47,  3.76it/s] 51%|█████     | 183/361 [00:48<00:48,  3.70it/s] 51%|█████     | 184/361 [00:48<00:46,  3.81it/s] 51%|█████     | 185/361 [00:49<00:46,  3.82it/s] 52%|█████▏    | 186/361 [00:49<00:46,  3.75it/s] 52%|█████▏    | 187/361 [00:49<00:46,  3.74it/s] 52%|█████▏    | 188/361 [00:50<00:47,  3.67it/s] 52%|█████▏    | 189/361 [00:50<00:45,  3.79it/s] 53%|█████▎    | 190/361 [00:50<00:44,  3.85it/s] 53%|█████▎    | 191/361 [00:50<00:44,  3.80it/s] 53%|█████▎    | 192/361 [00:51<00:45,  3.75it/s] 53%|█████▎    | 193/361 [00:51<00:45,  3.68it/s] 54%|█████▎    | 194/361 [00:51<00:44,  3.78it/s] 54%|█████▍    | 195/361 [00:51<00:43,  3.84it/s] 54%|█████▍    | 196/361 [00:52<00:43,  3.78it/s] 55%|█████▍    | 197/361 [00:52<00:43,  3.73it/s] 55%|█████▍    | 198/361 [00:52<00:44,  3.68it/s] 55%|█████▌    | 199/361 [00:52<00:42,  3.77it/s] 55%|█████▌    | 200/361 [00:53<00:41,  3.83it/s] 56%|█████▌    | 201/361 [00:53<00:42,  3.78it/s] 56%|█████▌    | 202/361 [00:53<00:42,  3.74it/s] 56%|█████▌    | 203/361 [00:54<00:42,  3.69it/s] 57%|█████▋    | 204/361 [00:54<00:41,  3.77it/s] 57%|█████▋    | 205/361 [00:54<00:41,  3.80it/s] 57%|█████▋    | 206/361 [00:54<00:41,  3.77it/s] 57%|█████▋    | 207/361 [00:55<00:41,  3.72it/s] 58%|█████▊    | 208/361 [00:55<00:41,  3.69it/s] 58%|█████▊    | 209/361 [00:55<00:40,  3.75it/s] 58%|█████▊    | 210/361 [00:55<00:39,  3.81it/s] 58%|█████▊    | 211/361 [00:56<00:40,  3.75it/s] 59%|█████▊    | 212/361 [00:56<00:39,  3.76it/s] 59%|█████▉    | 213/361 [00:56<00:39,  3.74it/s] 59%|█████▉    | 214/361 [00:56<00:38,  3.83it/s] 60%|█████▉    | 215/361 [00:57<00:37,  3.91it/s] 60%|█████▉    | 216/361 [00:57<00:36,  3.96it/s] 60%|██████    | 217/361 [00:57<00:36,  3.98it/s] 60%|██████    | 218/361 [00:57<00:35,  4.01it/s] 61%|██████    | 219/361 [00:58<00:35,  4.03it/s] 61%|██████    | 220/361 [00:58<00:34,  4.04it/s] 61%|██████    | 221/361 [00:58<00:34,  4.04it/s] 61%|██████▏   | 222/361 [00:58<00:34,  4.06it/s] 62%|██████▏   | 223/361 [00:59<00:34,  3.98it/s] 62%|██████▏   | 224/361 [00:59<00:35,  3.88it/s] 62%|██████▏   | 225/361 [00:59<00:35,  3.83it/s] 63%|██████▎   | 226/361 [00:59<00:34,  3.87it/s] 63%|██████▎   | 227/361 [01:00<00:35,  3.80it/s] 63%|██████▎   | 228/361 [01:00<00:35,  3.77it/s] 63%|██████▎   | 229/361 [01:00<00:35,  3.70it/s] 64%|██████▎   | 230/361 [01:01<00:34,  3.80it/s] 64%|██████▍   | 231/361 [01:01<00:34,  3.74it/s] 64%|██████▍   | 232/361 [01:01<00:34,  3.72it/s] 65%|██████▍   | 233/361 [01:01<00:34,  3.66it/s] 65%|██████▍   | 234/361 [01:02<00:33,  3.77it/s] 65%|██████▌   | 235/361 [01:02<00:33,  3.78it/s] 65%|██████▌   | 236/361 [01:02<00:33,  3.75it/s] 66%|██████▌   | 237/361 [01:02<00:33,  3.70it/s] 66%|██████▌   | 238/361 [01:03<00:32,  3.75it/s] 66%|██████▌   | 239/361 [01:03<00:32,  3.77it/s] 66%|██████▋   | 240/361 [01:03<00:32,  3.74it/s] 67%|██████▋   | 241/361 [01:03<00:32,  3.70it/s] 67%|██████▋   | 242/361 [01:04<00:31,  3.74it/s] 67%|██████▋   | 243/361 [01:04<00:31,  3.79it/s] 68%|██████▊   | 244/361 [01:04<00:31,  3.73it/s] 68%|██████▊   | 245/361 [01:05<00:31,  3.70it/s] 68%|██████▊   | 246/361 [01:05<00:30,  3.80it/s] 68%|██████▊   | 247/361 [01:05<00:29,  3.87it/s] 69%|██████▊   | 248/361 [01:05<00:28,  3.94it/s] 69%|██████▉   | 249/361 [01:06<00:28,  3.97it/s] 69%|██████▉   | 250/361 [01:06<00:27,  4.00it/s] 70%|██████▉   | 251/361 [01:06<00:27,  4.02it/s] 70%|██████▉   | 252/361 [01:06<00:27,  4.04it/s] 70%|███████   | 253/361 [01:07<00:26,  4.04it/s] 70%|███████   | 254/361 [01:07<00:26,  4.05it/s] 71%|███████   | 255/361 [01:07<00:26,  4.06it/s] 71%|███████   | 256/361 [01:07<00:25,  4.05it/s] 71%|███████   | 257/361 [01:08<00:25,  4.05it/s] 71%|███████▏  | 258/361 [01:08<00:25,  4.05it/s] 72%|███████▏  | 259/361 [01:08<00:25,  4.05it/s] 72%|███████▏  | 260/361 [01:08<00:24,  4.06it/s] 72%|███████▏  | 261/361 [01:08<00:24,  4.06it/s] 73%|███████▎  | 262/361 [01:09<00:24,  4.05it/s] 73%|███████▎  | 263/361 [01:09<00:24,  4.06it/s] 73%|███████▎  | 264/361 [01:09<00:23,  4.06it/s] 73%|███████▎  | 265/361 [01:09<00:23,  4.06it/s] 74%|███████▎  | 266/361 [01:10<00:23,  4.05it/s] 74%|███████▍  | 267/361 [01:10<00:23,  4.06it/s] 74%|███████▍  | 268/361 [01:10<00:22,  4.06it/s] 75%|███████▍  | 269/361 [01:10<00:22,  4.05it/s] 75%|███████▍  | 270/361 [01:11<00:22,  4.06it/s] 75%|███████▌  | 271/361 [01:11<00:22,  4.05it/s] 75%|███████▌  | 272/361 [01:11<00:21,  4.05it/s] 76%|███████▌  | 273/361 [01:11<00:22,  3.92it/s] 76%|███████▌  | 274/361 [01:12<00:22,  3.82it/s] 76%|███████▌  | 275/361 [01:12<00:22,  3.80it/s] 76%|███████▋  | 276/361 [01:12<00:22,  3.84it/s] 77%|███████▋  | 277/361 [01:13<00:22,  3.73it/s] 77%|███████▋  | 278/361 [01:13<00:22,  3.65it/s] 77%|███████▋  | 279/361 [01:13<00:22,  3.59it/s] 78%|███████▊  | 280/361 [01:13<00:21,  3.73it/s] 78%|███████▊  | 281/361 [01:14<00:21,  3.74it/s] 78%|███████▊  | 282/361 [01:14<00:21,  3.61it/s] 78%|███████▊  | 283/361 [01:14<00:21,  3.67it/s] 79%|███████▊  | 284/361 [01:14<00:21,  3.62it/s] 79%|███████▉  | 285/361 [01:15<00:20,  3.70it/s] 79%|███████▉  | 286/361 [01:15<00:20,  3.64it/s] 80%|███████▉  | 287/361 [01:15<00:20,  3.60it/s] 80%|███████▉  | 288/361 [01:16<00:20,  3.55it/s] 80%|████████  | 289/361 [01:16<00:19,  3.69it/s] 80%|████████  | 290/361 [01:16<00:19,  3.71it/s] 81%|████████  | 291/361 [01:16<00:19,  3.60it/s] 81%|████████  | 292/361 [01:17<00:18,  3.66it/s] 81%|████████  | 293/361 [01:17<00:18,  3.61it/s] 81%|████████▏ | 294/361 [01:17<00:18,  3.69it/s] 82%|████████▏ | 295/361 [01:18<00:18,  3.62it/s] 82%|████████▏ | 296/361 [01:18<00:18,  3.58it/s] 82%|████████▏ | 297/361 [01:18<00:18,  3.54it/s] 83%|████████▎ | 298/361 [01:18<00:17,  3.68it/s] 83%|████████▎ | 299/361 [01:19<00:16,  3.69it/s] 83%|████████▎ | 300/361 [01:19<00:17,  3.58it/s] 83%|████████▎ | 301/361 [01:19<00:16,  3.60it/s] 84%|████████▎ | 302/361 [01:19<00:16,  3.62it/s] 84%|████████▍ | 303/361 [01:20<00:15,  3.69it/s] 84%|████████▍ | 304/361 [01:20<00:15,  3.63it/s] 84%|████████▍ | 305/361 [01:20<00:15,  3.58it/s] 85%|████████▍ | 306/361 [01:21<00:15,  3.54it/s] 85%|████████▌ | 307/361 [01:21<00:14,  3.68it/s] 85%|████████▌ | 308/361 [01:21<00:14,  3.68it/s] 86%|████████▌ | 309/361 [01:21<00:14,  3.58it/s] 86%|████████▌ | 310/361 [01:22<00:13,  3.64it/s] 86%|████████▌ | 311/361 [01:22<00:13,  3.60it/s] 86%|████████▋ | 312/361 [01:22<00:13,  3.67it/s] 87%|████████▋ | 313/361 [01:22<00:13,  3.61it/s] 87%|████████▋ | 314/361 [01:23<00:13,  3.57it/s] 87%|████████▋ | 315/361 [01:23<00:13,  3.53it/s] 88%|████████▊ | 316/361 [01:23<00:12,  3.68it/s] 88%|████████▊ | 317/361 [01:24<00:11,  3.68it/s] 88%|████████▊ | 318/361 [01:24<00:12,  3.58it/s] 88%|████████▊ | 319/361 [01:24<00:11,  3.65it/s] 89%|████████▊ | 320/361 [01:24<00:11,  3.62it/s] 89%|████████▉ | 321/361 [01:25<00:10,  3.69it/s] 89%|████████▉ | 322/361 [01:25<00:10,  3.62it/s] 89%|████████▉ | 323/361 [01:25<00:10,  3.58it/s] 90%|████████▉ | 324/361 [01:26<00:10,  3.54it/s] 90%|█████████ | 325/361 [01:26<00:09,  3.68it/s] 90%|█████████ | 326/361 [01:26<00:09,  3.70it/s] 91%|█████████ | 327/361 [01:26<00:09,  3.58it/s] 91%|█████████ | 328/361 [01:27<00:09,  3.65it/s] 91%|█████████ | 329/361 [01:27<00:08,  3.61it/s] 91%|█████████▏| 330/361 [01:27<00:08,  3.67it/s] 92%|█████████▏| 331/361 [01:27<00:08,  3.62it/s] 92%|█████████▏| 332/361 [01:28<00:08,  3.57it/s] 92%|█████████▏| 333/361 [01:28<00:07,  3.53it/s] 93%|█████████▎| 334/361 [01:28<00:07,  3.68it/s] 93%|█████████▎| 335/361 [01:29<00:07,  3.71it/s] 93%|█████████▎| 336/361 [01:29<00:06,  3.59it/s] 93%|█████████▎| 337/361 [01:29<00:06,  3.65it/s] 94%|█████████▎| 338/361 [01:29<00:06,  3.61it/s] 94%|█████████▍| 339/361 [01:30<00:05,  3.68it/s] 94%|█████████▍| 340/361 [01:30<00:05,  3.62it/s] 94%|█████████▍| 341/361 [01:30<00:05,  3.58it/s] 95%|█████████▍| 342/361 [01:31<00:05,  3.54it/s] 95%|█████████▌| 343/361 [01:31<00:04,  3.68it/s] 95%|█████████▌| 344/361 [01:31<00:04,  3.70it/s] 96%|█████████▌| 345/361 [01:31<00:04,  3.59it/s] 96%|█████████▌| 346/361 [01:32<00:04,  3.66it/s] 96%|█████████▌| 347/361 [01:32<00:03,  3.62it/s] 96%|█████████▋| 348/361 [01:32<00:03,  3.68it/s] 97%|█████████▋| 349/361 [01:32<00:03,  3.63it/s] 97%|█████████▋| 350/361 [01:33<00:03,  3.58it/s] 97%|█████████▋| 351/361 [01:33<00:02,  3.54it/s] 98%|█████████▊| 352/361 [01:33<00:02,  3.69it/s] 98%|█████████▊| 353/361 [01:33<00:02,  3.71it/s] 98%|█████████▊| 354/361 [01:34<00:01,  3.60it/s] 98%|█████████▊| 355/361 [01:34<00:01,  3.60it/s] 99%|█████████▊| 356/361 [01:34<00:01,  3.73it/s] 99%|█████████▉| 357/361 [01:35<00:01,  3.82it/s] 99%|█████████▉| 358/361 [01:35<00:00,  3.88it/s] 99%|█████████▉| 359/361 [01:35<00:00,  3.94it/s]100%|█████████▉| 360/361 [01:35<00:00,  3.96it/s]100%|██████████| 361/361 [01:36<00:00,  3.99it/s]accuracy:  0.6011080332409973
100%|██████████| 361/361 [01:41<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 188
Training dataset size: 192, validation dataset size: 188
Training dataset size: 192, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.97s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:28:58,475] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 04:28:58,509] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:28:59,574] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:03<01:47,  3.47s/it]  6%|▋         | 2/32 [00:06<01:28,  2.94s/it]  9%|▉         | 3/32 [00:08<01:18,  2.69s/it] 12%|█▎        | 4/32 [00:10<01:12,  2.60s/it] 16%|█▌        | 5/32 [00:13<01:11,  2.66s/it] 19%|█▉        | 6/32 [00:16<01:08,  2.63s/it] 22%|██▏       | 7/32 [00:18<01:05,  2.63s/it] 25%|██▌       | 8/32 [00:21<01:01,  2.56s/it] 28%|██▊       | 9/32 [00:24<01:00,  2.64s/it] 31%|███▏      | 10/32 [00:26<00:57,  2.60s/it]                                               {'loss': 0.9517, 'grad_norm': 7.958449840545654, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:26<00:57,  2.60s/it] 34%|███▍      | 11/32 [00:29<00:55,  2.65s/it] 38%|███▊      | 12/32 [00:32<00:56,  2.81s/it] 41%|████      | 13/32 [00:35<00:55,  2.94s/it] 44%|████▍     | 14/32 [00:38<00:49,  2.77s/it] 47%|████▋     | 15/32 [00:40<00:47,  2.78s/it] 50%|█████     | 16/32 [00:43<00:43,  2.74s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:46<00:43,  2.91s/it] 56%|█████▋    | 18/32 [00:50<00:42,  3.02s/it] 59%|█████▉    | 19/32 [00:53<00:38,  3.00s/it] 62%|██████▎   | 20/32 [00:55<00:34,  2.89s/it]                                               {'loss': 0.715, 'grad_norm': 6.17309045791626, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:55<00:34,  2.89s/it] 66%|██████▌   | 21/32 [00:58<00:30,  2.77s/it] 69%|██████▉   | 22/32 [01:01<00:28,  2.86s/it] 72%|███████▏  | 23/32 [01:04<00:26,  2.92s/it] 75%|███████▌  | 24/32 [01:07<00:23,  2.95s/it] 78%|███████▊  | 25/32 [01:10<00:19,  2.85s/it] 81%|████████▏ | 26/32 [01:13<00:17,  3.00s/it] 84%|████████▍ | 27/32 [01:15<00:13,  2.76s/it] 88%|████████▊ | 28/32 [01:19<00:11,  2.95s/it] 91%|█████████ | 29/32 [01:21<00:08,  2.84s/it] 94%|█████████▍| 30/32 [01:24<00:05,  2.91s/it]                                               {'loss': 0.4883, 'grad_norm': 5.310972690582275, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:24<00:05,  2.91s/it] 97%|█████████▋| 31/32 [01:27<00:02,  2.94s/it]100%|██████████| 32/32 [01:30<00:00,  2.79s/it]                                               {'train_runtime': 90.8299, 'train_samples_per_second': 4.228, 'train_steps_per_second': 0.352, 'train_loss': 0.7373572885990143, 'epoch': 2.0}
100%|██████████| 32/32 [01:30<00:00,  2.79s/it]100%|██████████| 32/32 [01:30<00:00,  2.83s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:14,  2.82it/s]  1%|          | 2/212 [00:00<01:04,  3.26it/s]  1%|▏         | 3/212 [00:00<01:00,  3.44it/s]  2%|▏         | 4/212 [00:01<00:59,  3.50it/s]  2%|▏         | 5/212 [00:01<00:55,  3.70it/s]  3%|▎         | 6/212 [00:01<00:55,  3.68it/s]  3%|▎         | 7/212 [00:01<00:55,  3.69it/s]  4%|▍         | 8/212 [00:02<00:55,  3.66it/s]  4%|▍         | 9/212 [00:02<00:53,  3.79it/s]  5%|▍         | 10/212 [00:02<00:53,  3.74it/s]  5%|▌         | 11/212 [00:03<00:54,  3.72it/s]  6%|▌         | 12/212 [00:03<00:54,  3.69it/s]  6%|▌         | 13/212 [00:03<00:52,  3.81it/s]  7%|▋         | 14/212 [00:03<00:52,  3.76it/s]  7%|▋         | 15/212 [00:04<00:52,  3.74it/s]  8%|▊         | 16/212 [00:04<00:53,  3.69it/s]  8%|▊         | 17/212 [00:04<00:51,  3.82it/s]  8%|▊         | 18/212 [00:04<00:51,  3.78it/s]  9%|▉         | 19/212 [00:05<00:51,  3.74it/s]  9%|▉         | 20/212 [00:05<00:51,  3.69it/s] 10%|▉         | 21/212 [00:05<00:50,  3.81it/s] 10%|█         | 22/212 [00:05<00:50,  3.75it/s] 11%|█         | 23/212 [00:06<00:50,  3.73it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.70it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.81it/s] 12%|█▏        | 26/212 [00:07<00:49,  3.77it/s] 13%|█▎        | 27/212 [00:07<00:49,  3.74it/s] 13%|█▎        | 28/212 [00:07<00:49,  3.70it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.80it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.76it/s] 15%|█▍        | 31/212 [00:08<00:48,  3.74it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.68it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.79it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.75it/s] 17%|█▋        | 35/212 [00:09<00:47,  3.72it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.68it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.80it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.75it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.73it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.69it/s] 19%|█▉        | 41/212 [00:11<00:44,  3.81it/s] 20%|█▉        | 42/212 [00:11<00:45,  3.76it/s] 20%|██        | 43/212 [00:11<00:45,  3.72it/s] 21%|██        | 44/212 [00:11<00:45,  3.69it/s] 21%|██        | 45/212 [00:12<00:43,  3.82it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.77it/s] 22%|██▏       | 47/212 [00:12<00:44,  3.73it/s] 23%|██▎       | 48/212 [00:12<00:44,  3.68it/s] 23%|██▎       | 49/212 [00:13<00:42,  3.79it/s] 24%|██▎       | 50/212 [00:13<00:43,  3.73it/s] 24%|██▍       | 51/212 [00:13<00:43,  3.71it/s] 25%|██▍       | 52/212 [00:13<00:43,  3.67it/s] 25%|██▌       | 53/212 [00:14<00:41,  3.79it/s] 25%|██▌       | 54/212 [00:14<00:42,  3.75it/s] 26%|██▌       | 55/212 [00:14<00:42,  3.73it/s] 26%|██▋       | 56/212 [00:15<00:42,  3.68it/s] 27%|██▋       | 57/212 [00:15<00:40,  3.80it/s] 27%|██▋       | 58/212 [00:15<00:41,  3.75it/s] 28%|██▊       | 59/212 [00:15<00:41,  3.72it/s] 28%|██▊       | 60/212 [00:16<00:41,  3.68it/s] 29%|██▉       | 61/212 [00:16<00:39,  3.81it/s] 29%|██▉       | 62/212 [00:16<00:39,  3.75it/s] 30%|██▉       | 63/212 [00:16<00:39,  3.73it/s] 30%|███       | 64/212 [00:17<00:40,  3.68it/s] 31%|███       | 65/212 [00:17<00:38,  3.79it/s] 31%|███       | 66/212 [00:17<00:39,  3.73it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.77it/s] 32%|███▏      | 68/212 [00:18<00:38,  3.77it/s] 33%|███▎      | 69/212 [00:18<00:36,  3.88it/s] 33%|███▎      | 70/212 [00:18<00:35,  3.96it/s] 33%|███▎      | 71/212 [00:18<00:35,  4.01it/s] 34%|███▍      | 72/212 [00:19<00:34,  4.04it/s] 34%|███▍      | 73/212 [00:19<00:34,  4.05it/s] 35%|███▍      | 74/212 [00:19<00:33,  4.07it/s] 35%|███▌      | 75/212 [00:19<00:33,  4.10it/s] 36%|███▌      | 76/212 [00:20<00:33,  4.11it/s] 36%|███▋      | 77/212 [00:20<00:32,  4.12it/s] 37%|███▋      | 78/212 [00:20<00:32,  4.14it/s] 37%|███▋      | 79/212 [00:20<00:32,  4.14it/s] 38%|███▊      | 80/212 [00:21<00:31,  4.15it/s] 38%|███▊      | 81/212 [00:21<00:31,  4.15it/s] 39%|███▊      | 82/212 [00:21<00:31,  4.15it/s] 39%|███▉      | 83/212 [00:21<00:31,  4.15it/s] 40%|███▉      | 84/212 [00:22<00:30,  4.15it/s] 40%|████      | 85/212 [00:22<00:30,  4.15it/s] 41%|████      | 86/212 [00:22<00:30,  4.14it/s] 41%|████      | 87/212 [00:22<00:30,  4.14it/s] 42%|████▏     | 88/212 [00:23<00:30,  4.13it/s] 42%|████▏     | 89/212 [00:23<00:30,  4.09it/s] 42%|████▏     | 90/212 [00:23<00:30,  3.97it/s] 43%|████▎     | 91/212 [00:23<00:31,  3.88it/s] 43%|████▎     | 92/212 [00:24<00:30,  3.96it/s] 44%|████▍     | 93/212 [00:24<00:30,  3.88it/s] 44%|████▍     | 94/212 [00:24<00:30,  3.82it/s] 45%|████▍     | 95/212 [00:24<00:31,  3.75it/s] 45%|████▌     | 96/212 [00:25<00:30,  3.85it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.79it/s] 46%|████▌     | 98/212 [00:25<00:30,  3.79it/s] 47%|████▋     | 99/212 [00:25<00:30,  3.72it/s] 47%|████▋     | 100/212 [00:26<00:29,  3.83it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.78it/s] 48%|████▊     | 102/212 [00:26<00:29,  3.76it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.71it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.80it/s] 50%|████▉     | 105/212 [00:27<00:28,  3.75it/s] 50%|█████     | 106/212 [00:27<00:28,  3.71it/s] 50%|█████     | 107/212 [00:28<00:27,  3.75it/s] 51%|█████     | 108/212 [00:28<00:27,  3.78it/s] 51%|█████▏    | 109/212 [00:28<00:27,  3.76it/s] 52%|█████▏    | 110/212 [00:28<00:27,  3.66it/s] 52%|█████▏    | 111/212 [00:29<00:26,  3.79it/s] 53%|█████▎    | 112/212 [00:29<00:26,  3.77it/s] 53%|█████▎    | 113/212 [00:29<00:26,  3.75it/s] 54%|█████▍    | 114/212 [00:29<00:26,  3.70it/s] 54%|█████▍    | 115/212 [00:30<00:25,  3.80it/s] 55%|█████▍    | 116/212 [00:30<00:25,  3.76it/s] 55%|█████▌    | 117/212 [00:30<00:25,  3.75it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.71it/s] 56%|█████▌    | 119/212 [00:31<00:24,  3.82it/s] 57%|█████▋    | 120/212 [00:31<00:24,  3.78it/s] 57%|█████▋    | 121/212 [00:31<00:24,  3.76it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.72it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.82it/s] 58%|█████▊    | 124/212 [00:32<00:23,  3.77it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.75it/s] 59%|█████▉    | 126/212 [00:33<00:23,  3.71it/s] 60%|█████▉    | 127/212 [00:33<00:22,  3.82it/s] 60%|██████    | 128/212 [00:33<00:22,  3.78it/s] 61%|██████    | 129/212 [00:33<00:22,  3.75it/s] 61%|██████▏   | 130/212 [00:34<00:22,  3.71it/s] 62%|██████▏   | 131/212 [00:34<00:21,  3.82it/s] 62%|██████▏   | 132/212 [00:34<00:21,  3.77it/s] 63%|██████▎   | 133/212 [00:35<00:21,  3.75it/s] 63%|██████▎   | 134/212 [00:35<00:21,  3.70it/s] 64%|██████▎   | 135/212 [00:35<00:20,  3.81it/s] 64%|██████▍   | 136/212 [00:35<00:20,  3.77it/s] 65%|██████▍   | 137/212 [00:36<00:19,  3.76it/s] 65%|██████▌   | 138/212 [00:36<00:19,  3.70it/s] 66%|██████▌   | 139/212 [00:36<00:19,  3.81it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.76it/s] 67%|██████▋   | 141/212 [00:37<00:18,  3.76it/s] 67%|██████▋   | 142/212 [00:37<00:18,  3.71it/s] 67%|██████▋   | 143/212 [00:37<00:18,  3.80it/s] 68%|██████▊   | 144/212 [00:37<00:18,  3.76it/s] 68%|██████▊   | 145/212 [00:38<00:17,  3.75it/s] 69%|██████▉   | 146/212 [00:38<00:17,  3.70it/s] 69%|██████▉   | 147/212 [00:38<00:17,  3.81it/s] 70%|██████▉   | 148/212 [00:39<00:16,  3.77it/s] 70%|███████   | 149/212 [00:39<00:16,  3.75it/s] 71%|███████   | 150/212 [00:39<00:16,  3.69it/s] 71%|███████   | 151/212 [00:39<00:16,  3.78it/s] 72%|███████▏  | 152/212 [00:40<00:15,  3.75it/s] 72%|███████▏  | 153/212 [00:40<00:15,  3.77it/s] 73%|███████▎  | 154/212 [00:40<00:15,  3.72it/s] 73%|███████▎  | 155/212 [00:40<00:15,  3.76it/s] 74%|███████▎  | 156/212 [00:41<00:15,  3.73it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.71it/s] 75%|███████▍  | 158/212 [00:41<00:14,  3.82it/s] 75%|███████▌  | 159/212 [00:41<00:13,  3.90it/s] 75%|███████▌  | 160/212 [00:42<00:13,  3.97it/s] 76%|███████▌  | 161/212 [00:42<00:12,  4.01it/s] 76%|███████▋  | 162/212 [00:42<00:12,  4.04it/s] 77%|███████▋  | 163/212 [00:42<00:12,  4.05it/s] 77%|███████▋  | 164/212 [00:43<00:11,  4.07it/s] 78%|███████▊  | 165/212 [00:43<00:11,  4.08it/s] 78%|███████▊  | 166/212 [00:43<00:11,  4.10it/s] 79%|███████▉  | 167/212 [00:43<00:10,  4.10it/s] 79%|███████▉  | 168/212 [00:44<00:10,  4.10it/s] 80%|███████▉  | 169/212 [00:44<00:10,  4.10it/s] 80%|████████  | 170/212 [00:44<00:10,  4.10it/s] 81%|████████  | 171/212 [00:44<00:09,  4.11it/s] 81%|████████  | 172/212 [00:45<00:09,  4.12it/s] 82%|████████▏ | 173/212 [00:45<00:09,  4.11it/s] 82%|████████▏ | 174/212 [00:45<00:09,  4.11it/s] 83%|████████▎ | 175/212 [00:45<00:09,  4.10it/s] 83%|████████▎ | 176/212 [00:46<00:08,  4.11it/s] 83%|████████▎ | 177/212 [00:46<00:08,  4.11it/s] 84%|████████▍ | 178/212 [00:46<00:08,  4.12it/s] 84%|████████▍ | 179/212 [00:46<00:08,  4.11it/s] 85%|████████▍ | 180/212 [00:47<00:07,  4.10it/s] 85%|████████▌ | 181/212 [00:47<00:07,  4.10it/s] 86%|████████▌ | 182/212 [00:47<00:07,  4.11it/s] 86%|████████▋ | 183/212 [00:47<00:07,  4.12it/s] 87%|████████▋ | 184/212 [00:48<00:06,  4.11it/s] 87%|████████▋ | 185/212 [00:48<00:06,  4.11it/s] 88%|████████▊ | 186/212 [00:48<00:06,  4.10it/s] 88%|████████▊ | 187/212 [00:48<00:06,  4.10it/s] 89%|████████▊ | 188/212 [00:48<00:05,  4.11it/s] 89%|████████▉ | 189/212 [00:49<00:05,  4.12it/s] 90%|████████▉ | 190/212 [00:49<00:05,  4.12it/s] 90%|█████████ | 191/212 [00:49<00:05,  4.11it/s] 91%|█████████ | 192/212 [00:49<00:05,  3.97it/s] 91%|█████████ | 193/212 [00:50<00:04,  3.88it/s] 92%|█████████▏| 194/212 [00:50<00:04,  3.85it/s] 92%|█████████▏| 195/212 [00:50<00:04,  3.87it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.74it/s] 93%|█████████▎| 197/212 [00:51<00:04,  3.73it/s] 93%|█████████▎| 198/212 [00:51<00:03,  3.72it/s] 94%|█████████▍| 199/212 [00:51<00:03,  3.74it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.67it/s] 95%|█████████▍| 201/212 [00:52<00:03,  3.60it/s] 95%|█████████▌| 202/212 [00:52<00:02,  3.74it/s] 96%|█████████▌| 203/212 [00:52<00:02,  3.68it/s] 96%|█████████▌| 204/212 [00:53<00:02,  3.66it/s] 97%|█████████▋| 205/212 [00:53<00:01,  3.64it/s] 97%|█████████▋| 206/212 [00:53<00:01,  3.72it/s] 98%|█████████▊| 207/212 [00:54<00:01,  3.65it/s] 98%|█████████▊| 208/212 [00:54<00:01,  3.66it/s] 99%|█████████▊| 209/212 [00:54<00:00,  3.68it/s] 99%|█████████▉| 210/212 [00:54<00:00,  3.71it/s]100%|█████████▉| 211/212 [00:55<00:00,  3.65it/s]100%|██████████| 212/212 [00:55<00:00,  3.58it/s]accuracy:  0.8632075471698113
100%|██████████| 212/212 [00:58<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 134
Training dataset size: 192, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 04:31:57,598] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 04:31:57,715] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:31:57,949] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:27,  2.82s/it]  6%|▋         | 2/32 [00:05<01:21,  2.71s/it]  9%|▉         | 3/32 [00:07<01:15,  2.61s/it] 12%|█▎        | 4/32 [00:10<01:16,  2.73s/it] 16%|█▌        | 5/32 [00:13<01:16,  2.84s/it] 19%|█▉        | 6/32 [00:16<01:13,  2.83s/it] 22%|██▏       | 7/32 [00:19<01:09,  2.80s/it] 25%|██▌       | 8/32 [00:22<01:07,  2.83s/it] 28%|██▊       | 9/32 [00:25<01:04,  2.81s/it] 31%|███▏      | 10/32 [00:27<01:00,  2.76s/it]                                               {'loss': 0.8338, 'grad_norm': 4.60763692855835, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:27<01:00,  2.76s/it] 34%|███▍      | 11/32 [00:30<00:56,  2.69s/it] 38%|███▊      | 12/32 [00:32<00:52,  2.60s/it] 41%|████      | 13/32 [00:35<00:50,  2.68s/it] 44%|████▍     | 14/32 [00:38<00:48,  2.69s/it] 47%|████▋     | 15/32 [00:40<00:44,  2.59s/it] 50%|█████     | 16/32 [00:42<00:40,  2.53s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:45<00:37,  2.49s/it] 56%|█████▋    | 18/32 [00:47<00:35,  2.52s/it] 59%|█████▉    | 19/32 [00:50<00:31,  2.42s/it] 62%|██████▎   | 20/32 [00:53<00:31,  2.61s/it]                                               {'loss': 0.6733, 'grad_norm': 3.4816970825195312, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:53<00:31,  2.61s/it] 66%|██████▌   | 21/32 [00:55<00:28,  2.59s/it] 69%|██████▉   | 22/32 [00:57<00:24,  2.48s/it] 72%|███████▏  | 23/32 [01:00<00:22,  2.54s/it] 75%|███████▌  | 24/32 [01:03<00:21,  2.64s/it] 78%|███████▊  | 25/32 [01:06<00:18,  2.70s/it] 81%|████████▏ | 26/32 [01:09<00:16,  2.74s/it] 84%|████████▍ | 27/32 [01:11<00:13,  2.64s/it] 88%|████████▊ | 28/32 [01:14<00:11,  2.79s/it] 91%|█████████ | 29/32 [01:17<00:08,  2.92s/it] 94%|█████████▍| 30/32 [01:20<00:05,  2.82s/it]                                               {'loss': 0.5681, 'grad_norm': 4.19384241104126, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:20<00:05,  2.82s/it] 97%|█████████▋| 31/32 [01:23<00:02,  2.84s/it]100%|██████████| 32/32 [01:26<00:00,  2.76s/it]                                               {'train_runtime': 86.6548, 'train_samples_per_second': 4.431, 'train_steps_per_second': 0.369, 'train_loss': 0.6924997940659523, 'epoch': 2.0}
100%|██████████| 32/32 [01:26<00:00,  2.76s/it]100%|██████████| 32/32 [01:26<00:00,  2.70s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:54,  2.95it/s]  1%|          | 2/163 [00:00<00:49,  3.25it/s]  2%|▏         | 3/163 [00:00<00:44,  3.62it/s]  2%|▏         | 4/163 [00:01<00:42,  3.71it/s]  3%|▎         | 5/163 [00:01<00:42,  3.72it/s]  4%|▎         | 6/163 [00:01<00:43,  3.63it/s]  4%|▍         | 7/163 [00:01<00:41,  3.79it/s]  5%|▍         | 8/163 [00:02<00:40,  3.81it/s]  6%|▌         | 9/163 [00:02<00:40,  3.78it/s]  6%|▌         | 10/163 [00:02<00:41,  3.73it/s]  7%|▋         | 11/163 [00:02<00:40,  3.79it/s]  7%|▋         | 12/163 [00:03<00:39,  3.82it/s]  8%|▊         | 13/163 [00:03<00:39,  3.79it/s]  9%|▊         | 14/163 [00:03<00:39,  3.73it/s]  9%|▉         | 15/163 [00:04<00:39,  3.79it/s] 10%|▉         | 16/163 [00:04<00:38,  3.82it/s] 10%|█         | 17/163 [00:04<00:38,  3.79it/s] 11%|█         | 18/163 [00:04<00:38,  3.74it/s] 12%|█▏        | 19/163 [00:05<00:38,  3.78it/s] 12%|█▏        | 20/163 [00:05<00:37,  3.81it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.79it/s] 13%|█▎        | 22/163 [00:05<00:37,  3.74it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.79it/s] 15%|█▍        | 24/163 [00:06<00:36,  3.82it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.79it/s] 16%|█▌        | 26/163 [00:06<00:36,  3.72it/s] 17%|█▋        | 27/163 [00:07<00:35,  3.81it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.82it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.78it/s] 18%|█▊        | 30/163 [00:08<00:36,  3.68it/s] 19%|█▉        | 31/163 [00:08<00:34,  3.81it/s] 20%|█▉        | 32/163 [00:08<00:34,  3.81it/s] 20%|██        | 33/163 [00:08<00:34,  3.78it/s] 21%|██        | 34/163 [00:09<00:34,  3.69it/s] 21%|██▏       | 35/163 [00:09<00:33,  3.82it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.83it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.79it/s] 23%|██▎       | 38/163 [00:10<00:33,  3.69it/s] 24%|██▍       | 39/163 [00:10<00:32,  3.82it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.81it/s] 25%|██▌       | 41/163 [00:10<00:32,  3.78it/s] 26%|██▌       | 42/163 [00:11<00:32,  3.69it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.81it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.80it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.78it/s] 28%|██▊       | 46/163 [00:12<00:31,  3.69it/s] 29%|██▉       | 47/163 [00:12<00:30,  3.82it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.83it/s] 30%|███       | 49/163 [00:13<00:30,  3.79it/s] 31%|███       | 50/163 [00:13<00:29,  3.78it/s] 31%|███▏      | 51/163 [00:13<00:29,  3.78it/s] 32%|███▏      | 52/163 [00:13<00:29,  3.82it/s] 33%|███▎      | 53/163 [00:14<00:29,  3.79it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.83it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.75it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.80it/s] 35%|███▍      | 57/163 [00:15<00:28,  3.77it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.81it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.73it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.81it/s] 37%|███▋      | 61/163 [00:16<00:26,  3.78it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.81it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.73it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.78it/s] 40%|███▉      | 65/163 [00:17<00:26,  3.75it/s] 40%|████      | 66/163 [00:17<00:25,  3.79it/s] 41%|████      | 67/163 [00:17<00:25,  3.73it/s] 42%|████▏     | 68/163 [00:18<00:25,  3.79it/s] 42%|████▏     | 69/163 [00:18<00:25,  3.75it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.78it/s] 44%|████▎     | 71/163 [00:18<00:24,  3.74it/s] 44%|████▍     | 72/163 [00:19<00:23,  3.80it/s] 45%|████▍     | 73/163 [00:19<00:23,  3.75it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.78it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.74it/s] 47%|████▋     | 76/163 [00:20<00:22,  3.79it/s] 47%|████▋     | 77/163 [00:20<00:22,  3.76it/s] 48%|████▊     | 78/163 [00:20<00:22,  3.76it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.74it/s] 49%|████▉     | 80/163 [00:21<00:22,  3.77it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.75it/s] 50%|█████     | 82/163 [00:21<00:21,  3.74it/s] 51%|█████     | 83/163 [00:22<00:21,  3.72it/s] 52%|█████▏    | 84/163 [00:22<00:20,  3.78it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.74it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.78it/s] 53%|█████▎    | 87/163 [00:23<00:20,  3.73it/s] 54%|█████▍    | 88/163 [00:23<00:19,  3.80it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.76it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.80it/s] 56%|█████▌    | 91/163 [00:24<00:19,  3.73it/s] 56%|█████▋    | 92/163 [00:24<00:18,  3.79it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.75it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.79it/s] 58%|█████▊    | 95/163 [00:25<00:18,  3.73it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.77it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.75it/s] 60%|██████    | 98/163 [00:26<00:17,  3.79it/s] 61%|██████    | 99/163 [00:26<00:17,  3.72it/s] 61%|██████▏   | 100/163 [00:26<00:16,  3.78it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.75it/s] 63%|██████▎   | 102/163 [00:27<00:16,  3.79it/s] 63%|██████▎   | 103/163 [00:27<00:16,  3.71it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.78it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.76it/s] 65%|██████▌   | 106/163 [00:28<00:15,  3.78it/s] 66%|██████▌   | 107/163 [00:28<00:15,  3.73it/s] 66%|██████▋   | 108/163 [00:28<00:14,  3.78it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.74it/s] 67%|██████▋   | 110/163 [00:29<00:14,  3.74it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.74it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.78it/s] 69%|██████▉   | 113/163 [00:30<00:13,  3.75it/s] 70%|██████▉   | 114/163 [00:30<00:13,  3.73it/s] 71%|███████   | 115/163 [00:30<00:12,  3.73it/s] 71%|███████   | 116/163 [00:30<00:12,  3.76it/s] 72%|███████▏  | 117/163 [00:31<00:12,  3.73it/s] 72%|███████▏  | 118/163 [00:31<00:12,  3.69it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.75it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.78it/s] 74%|███████▍  | 121/163 [00:32<00:11,  3.75it/s] 75%|███████▍  | 122/163 [00:32<00:11,  3.65it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.78it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.78it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.75it/s] 77%|███████▋  | 126/163 [00:33<00:10,  3.65it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.78it/s] 79%|███████▊  | 128/163 [00:34<00:09,  3.78it/s] 79%|███████▉  | 129/163 [00:34<00:09,  3.75it/s] 80%|███████▉  | 130/163 [00:34<00:09,  3.65it/s] 80%|████████  | 131/163 [00:34<00:08,  3.78it/s] 81%|████████  | 132/163 [00:35<00:08,  3.80it/s] 82%|████████▏ | 133/163 [00:35<00:07,  3.77it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.70it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.76it/s] 83%|████████▎ | 136/163 [00:36<00:07,  3.78it/s] 84%|████████▍ | 137/163 [00:36<00:06,  3.75it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.65it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.76it/s] 86%|████████▌ | 140/163 [00:37<00:06,  3.78it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.75it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.64it/s] 88%|████████▊ | 143/163 [00:38<00:05,  3.77it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.78it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.75it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.65it/s] 90%|█████████ | 147/163 [00:39<00:04,  3.77it/s] 91%|█████████ | 148/163 [00:39<00:03,  3.76it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.73it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.65it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.77it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.76it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.72it/s] 94%|█████████▍| 154/163 [00:41<00:02,  3.67it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.78it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.73it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.71it/s] 97%|█████████▋| 158/163 [00:42<00:01,  3.67it/s] 98%|█████████▊| 159/163 [00:42<00:01,  3.79it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.74it/s] 99%|█████████▉| 161/163 [00:42<00:00,  3.73it/s] 99%|█████████▉| 162/163 [00:43<00:00,  3.68it/s]100%|██████████| 163/163 [00:43<00:00,  3.79it/s]accuracy:  0.7914110429447853
100%|██████████| 163/163 [00:45<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:34:37,463] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:34:38,703] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:34:40,645] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:03<01:39,  3.22s/it]  6%|▋         | 2/32 [00:05<01:23,  2.80s/it]  9%|▉         | 3/32 [00:08<01:21,  2.82s/it] 12%|█▎        | 4/32 [00:11<01:15,  2.69s/it] 16%|█▌        | 5/32 [00:13<01:08,  2.55s/it] 19%|█▉        | 6/32 [00:15<01:03,  2.45s/it] 22%|██▏       | 7/32 [00:18<01:02,  2.49s/it] 25%|██▌       | 8/32 [00:20<00:59,  2.49s/it] 28%|██▊       | 9/32 [00:23<00:57,  2.49s/it] 31%|███▏      | 10/32 [00:25<00:55,  2.54s/it]                                               {'loss': 0.316, 'grad_norm': 6.218859672546387, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:25<00:55,  2.54s/it] 34%|███▍      | 11/32 [00:28<00:51,  2.44s/it] 38%|███▊      | 12/32 [00:30<00:47,  2.35s/it] 41%|████      | 13/32 [00:32<00:45,  2.40s/it] 44%|████▍     | 14/32 [00:35<00:45,  2.54s/it] 47%|████▋     | 15/32 [00:38<00:43,  2.57s/it] 50%|█████     | 16/32 [00:40<00:40,  2.53s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:43<00:40,  2.70s/it] 56%|█████▋    | 18/32 [00:46<00:37,  2.65s/it] 59%|█████▉    | 19/32 [00:48<00:33,  2.57s/it] 62%|██████▎   | 20/32 [00:51<00:31,  2.62s/it]                                               {'loss': 0.4595, 'grad_norm': 8.654107093811035, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:51<00:31,  2.62s/it] 66%|██████▌   | 21/32 [00:53<00:28,  2.56s/it] 69%|██████▉   | 22/32 [00:56<00:26,  2.61s/it] 72%|███████▏  | 23/32 [00:58<00:22,  2.53s/it] 75%|███████▌  | 24/32 [01:01<00:20,  2.54s/it] 78%|███████▊  | 25/32 [01:04<00:17,  2.56s/it] 81%|████████▏ | 26/32 [01:06<00:14,  2.42s/it] 84%|████████▍ | 27/32 [01:08<00:11,  2.36s/it] 88%|████████▊ | 28/32 [01:10<00:09,  2.36s/it] 91%|█████████ | 29/32 [01:13<00:07,  2.42s/it] 94%|█████████▍| 30/32 [01:15<00:04,  2.44s/it]                                               {'loss': 0.3151, 'grad_norm': 6.1904988288879395, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:15<00:04,  2.44s/it] 97%|█████████▋| 31/32 [01:18<00:02,  2.51s/it]100%|██████████| 32/32 [01:21<00:00,  2.59s/it]                                               {'train_runtime': 81.9132, 'train_samples_per_second': 4.688, 'train_steps_per_second': 0.391, 'train_loss': 0.40486887097358704, 'epoch': 2.0}
100%|██████████| 32/32 [01:21<00:00,  2.59s/it]100%|██████████| 32/32 [01:21<00:00,  2.55s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:12,  2.98it/s]  1%|          | 2/217 [00:00<01:02,  3.45it/s]  1%|▏         | 3/217 [00:00<00:59,  3.58it/s]  2%|▏         | 4/217 [00:01<00:58,  3.64it/s]  2%|▏         | 5/217 [00:01<00:56,  3.75it/s]  3%|▎         | 6/217 [00:01<00:55,  3.80it/s]  3%|▎         | 7/217 [00:01<00:56,  3.73it/s]  4%|▎         | 8/217 [00:02<00:55,  3.76it/s]  4%|▍         | 9/217 [00:02<00:55,  3.73it/s]  5%|▍         | 10/217 [00:02<00:54,  3.78it/s]  5%|▌         | 11/217 [00:02<00:55,  3.70it/s]  6%|▌         | 12/217 [00:03<00:55,  3.71it/s]  6%|▌         | 13/217 [00:03<00:54,  3.74it/s]  6%|▋         | 14/217 [00:03<00:53,  3.77it/s]  7%|▋         | 15/217 [00:04<00:54,  3.72it/s]  7%|▋         | 16/217 [00:04<00:53,  3.73it/s]  8%|▊         | 17/217 [00:04<00:53,  3.72it/s]  8%|▊         | 18/217 [00:04<00:52,  3.76it/s]  9%|▉         | 19/217 [00:05<00:53,  3.70it/s]  9%|▉         | 20/217 [00:05<00:53,  3.71it/s] 10%|▉         | 21/217 [00:05<00:51,  3.83it/s] 10%|█         | 22/217 [00:05<00:49,  3.91it/s] 11%|█         | 23/217 [00:06<00:48,  3.98it/s] 11%|█         | 24/217 [00:06<00:47,  4.02it/s] 12%|█▏        | 25/217 [00:06<00:47,  4.05it/s] 12%|█▏        | 26/217 [00:06<00:46,  4.08it/s] 12%|█▏        | 27/217 [00:07<00:46,  4.10it/s] 13%|█▎        | 28/217 [00:07<00:45,  4.12it/s] 13%|█▎        | 29/217 [00:07<00:45,  4.13it/s] 14%|█▍        | 30/217 [00:07<00:45,  4.13it/s] 14%|█▍        | 31/217 [00:08<00:44,  4.14it/s] 15%|█▍        | 32/217 [00:08<00:44,  4.15it/s] 15%|█▌        | 33/217 [00:08<00:44,  4.15it/s] 16%|█▌        | 34/217 [00:08<00:44,  4.16it/s] 16%|█▌        | 35/217 [00:09<00:43,  4.16it/s] 17%|█▋        | 36/217 [00:09<00:43,  4.16it/s] 17%|█▋        | 37/217 [00:09<00:43,  4.16it/s] 18%|█▊        | 38/217 [00:09<00:43,  4.16it/s] 18%|█▊        | 39/217 [00:09<00:42,  4.16it/s] 18%|█▊        | 40/217 [00:10<00:42,  4.16it/s] 19%|█▉        | 41/217 [00:10<00:42,  4.16it/s] 19%|█▉        | 42/217 [00:10<00:42,  4.15it/s] 20%|█▉        | 43/217 [00:10<00:42,  4.14it/s] 20%|██        | 44/217 [00:11<00:41,  4.14it/s] 21%|██        | 45/217 [00:11<00:41,  4.13it/s] 21%|██        | 46/217 [00:11<00:41,  4.13it/s] 22%|██▏       | 47/217 [00:11<00:41,  4.13it/s] 22%|██▏       | 48/217 [00:12<00:40,  4.14it/s] 23%|██▎       | 49/217 [00:12<00:40,  4.14it/s] 23%|██▎       | 50/217 [00:12<00:40,  4.15it/s] 24%|██▎       | 51/217 [00:12<00:39,  4.15it/s] 24%|██▍       | 52/217 [00:13<00:39,  4.16it/s] 24%|██▍       | 53/217 [00:13<00:39,  4.16it/s] 25%|██▍       | 54/217 [00:13<00:39,  4.16it/s] 25%|██▌       | 55/217 [00:13<00:40,  4.03it/s] 26%|██▌       | 56/217 [00:14<00:40,  3.93it/s] 26%|██▋       | 57/217 [00:14<00:41,  3.85it/s] 27%|██▋       | 58/217 [00:14<00:40,  3.90it/s] 27%|██▋       | 59/217 [00:14<00:41,  3.80it/s] 28%|██▊       | 60/217 [00:15<00:41,  3.76it/s] 28%|██▊       | 61/217 [00:15<00:42,  3.71it/s] 29%|██▊       | 62/217 [00:15<00:40,  3.80it/s] 29%|██▉       | 63/217 [00:16<00:41,  3.75it/s] 29%|██▉       | 64/217 [00:16<00:41,  3.71it/s] 30%|██▉       | 65/217 [00:16<00:41,  3.67it/s] 30%|███       | 66/217 [00:16<00:40,  3.77it/s] 31%|███       | 67/217 [00:17<00:40,  3.71it/s] 31%|███▏      | 68/217 [00:17<00:40,  3.70it/s] 32%|███▏      | 69/217 [00:17<00:40,  3.66it/s] 32%|███▏      | 70/217 [00:17<00:39,  3.76it/s] 33%|███▎      | 71/217 [00:18<00:39,  3.70it/s] 33%|███▎      | 72/217 [00:18<00:39,  3.70it/s] 34%|███▎      | 73/217 [00:18<00:39,  3.66it/s] 34%|███▍      | 74/217 [00:18<00:37,  3.77it/s] 35%|███▍      | 75/217 [00:19<00:38,  3.73it/s] 35%|███▌      | 76/217 [00:19<00:38,  3.70it/s] 35%|███▌      | 77/217 [00:19<00:38,  3.67it/s] 36%|███▌      | 78/217 [00:20<00:37,  3.74it/s] 36%|███▋      | 79/217 [00:20<00:37,  3.68it/s] 37%|███▋      | 80/217 [00:20<00:36,  3.73it/s] 37%|███▋      | 81/217 [00:20<00:36,  3.68it/s] 38%|███▊      | 82/217 [00:21<00:36,  3.74it/s] 38%|███▊      | 83/217 [00:21<00:36,  3.68it/s] 39%|███▊      | 84/217 [00:21<00:35,  3.73it/s] 39%|███▉      | 85/217 [00:21<00:35,  3.68it/s] 40%|███▉      | 86/217 [00:22<00:35,  3.73it/s] 40%|████      | 87/217 [00:22<00:35,  3.68it/s] 41%|████      | 88/217 [00:22<00:34,  3.72it/s] 41%|████      | 89/217 [00:23<00:34,  3.68it/s] 41%|████▏     | 90/217 [00:23<00:34,  3.73it/s] 42%|████▏     | 91/217 [00:23<00:34,  3.67it/s] 42%|████▏     | 92/217 [00:23<00:33,  3.73it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.68it/s] 43%|████▎     | 94/217 [00:24<00:32,  3.74it/s] 44%|████▍     | 95/217 [00:24<00:33,  3.68it/s] 44%|████▍     | 96/217 [00:24<00:32,  3.73it/s] 45%|████▍     | 97/217 [00:25<00:32,  3.68it/s] 45%|████▌     | 98/217 [00:25<00:31,  3.73it/s] 46%|████▌     | 99/217 [00:25<00:32,  3.68it/s] 46%|████▌     | 100/217 [00:25<00:31,  3.72it/s] 47%|████▋     | 101/217 [00:26<00:31,  3.68it/s] 47%|████▋     | 102/217 [00:26<00:30,  3.75it/s] 47%|████▋     | 103/217 [00:26<00:31,  3.68it/s] 48%|████▊     | 104/217 [00:27<00:30,  3.72it/s] 48%|████▊     | 105/217 [00:27<00:30,  3.68it/s] 49%|████▉     | 106/217 [00:27<00:29,  3.74it/s] 49%|████▉     | 107/217 [00:27<00:29,  3.68it/s] 50%|████▉     | 108/217 [00:28<00:29,  3.72it/s] 50%|█████     | 109/217 [00:28<00:29,  3.68it/s] 51%|█████     | 110/217 [00:28<00:28,  3.73it/s] 51%|█████     | 111/217 [00:28<00:28,  3.67it/s] 52%|█████▏    | 112/217 [00:29<00:28,  3.72it/s] 52%|█████▏    | 113/217 [00:29<00:28,  3.68it/s] 53%|█████▎    | 114/217 [00:29<00:27,  3.74it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.68it/s] 53%|█████▎    | 116/217 [00:30<00:27,  3.72it/s] 54%|█████▍    | 117/217 [00:30<00:27,  3.68it/s] 54%|█████▍    | 118/217 [00:30<00:26,  3.70it/s] 55%|█████▍    | 119/217 [00:31<00:26,  3.65it/s] 55%|█████▌    | 120/217 [00:31<00:26,  3.68it/s] 56%|█████▌    | 121/217 [00:31<00:26,  3.68it/s] 56%|█████▌    | 122/217 [00:31<00:25,  3.72it/s] 57%|█████▋    | 123/217 [00:32<00:25,  3.65it/s] 57%|█████▋    | 124/217 [00:32<00:25,  3.71it/s] 58%|█████▊    | 125/217 [00:32<00:25,  3.66it/s] 58%|█████▊    | 126/217 [00:33<00:24,  3.71it/s] 59%|█████▊    | 127/217 [00:33<00:24,  3.66it/s] 59%|█████▉    | 128/217 [00:33<00:23,  3.71it/s] 59%|█████▉    | 129/217 [00:33<00:24,  3.67it/s] 60%|█████▉    | 130/217 [00:34<00:23,  3.73it/s] 60%|██████    | 131/217 [00:34<00:23,  3.67it/s] 61%|██████    | 132/217 [00:34<00:22,  3.72it/s] 61%|██████▏   | 133/217 [00:34<00:22,  3.68it/s] 62%|██████▏   | 134/217 [00:35<00:22,  3.73it/s] 62%|██████▏   | 135/217 [00:35<00:22,  3.67it/s] 63%|██████▎   | 136/217 [00:35<00:21,  3.73it/s] 63%|██████▎   | 137/217 [00:36<00:21,  3.68it/s] 64%|██████▎   | 138/217 [00:36<00:21,  3.72it/s] 64%|██████▍   | 139/217 [00:36<00:21,  3.67it/s] 65%|██████▍   | 140/217 [00:36<00:20,  3.73it/s] 65%|██████▍   | 141/217 [00:37<00:20,  3.69it/s] 65%|██████▌   | 142/217 [00:37<00:20,  3.73it/s] 66%|██████▌   | 143/217 [00:37<00:20,  3.67it/s] 66%|██████▋   | 144/217 [00:37<00:19,  3.73it/s] 67%|██████▋   | 145/217 [00:38<00:19,  3.69it/s] 67%|██████▋   | 146/217 [00:38<00:19,  3.72it/s] 68%|██████▊   | 147/217 [00:38<00:19,  3.66it/s] 68%|██████▊   | 148/217 [00:38<00:18,  3.72it/s] 69%|██████▊   | 149/217 [00:39<00:18,  3.69it/s] 69%|██████▉   | 150/217 [00:39<00:17,  3.72it/s] 70%|██████▉   | 151/217 [00:39<00:18,  3.66it/s] 70%|███████   | 152/217 [00:40<00:17,  3.73it/s] 71%|███████   | 153/217 [00:40<00:17,  3.67it/s] 71%|███████   | 154/217 [00:40<00:16,  3.71it/s] 71%|███████▏  | 155/217 [00:40<00:16,  3.65it/s] 72%|███████▏  | 156/217 [00:41<00:16,  3.71it/s] 72%|███████▏  | 157/217 [00:41<00:16,  3.66it/s] 73%|███████▎  | 158/217 [00:41<00:15,  3.71it/s] 73%|███████▎  | 159/217 [00:41<00:15,  3.66it/s] 74%|███████▎  | 160/217 [00:42<00:15,  3.72it/s] 74%|███████▍  | 161/217 [00:42<00:15,  3.67it/s] 75%|███████▍  | 162/217 [00:42<00:14,  3.67it/s] 75%|███████▌  | 163/217 [00:43<00:14,  3.63it/s] 76%|███████▌  | 164/217 [00:43<00:14,  3.61it/s] 76%|███████▌  | 165/217 [00:43<00:14,  3.69it/s] 76%|███████▋  | 166/217 [00:43<00:13,  3.70it/s] 77%|███████▋  | 167/217 [00:44<00:13,  3.65it/s] 77%|███████▋  | 168/217 [00:44<00:13,  3.67it/s] 78%|███████▊  | 169/217 [00:44<00:13,  3.68it/s] 78%|███████▊  | 170/217 [00:44<00:12,  3.71it/s] 79%|███████▉  | 171/217 [00:45<00:12,  3.65it/s] 79%|███████▉  | 172/217 [00:45<00:12,  3.67it/s] 80%|███████▉  | 173/217 [00:45<00:12,  3.66it/s] 80%|████████  | 174/217 [00:46<00:11,  3.70it/s] 81%|████████  | 175/217 [00:46<00:11,  3.65it/s] 81%|████████  | 176/217 [00:46<00:11,  3.71it/s] 82%|████████▏ | 177/217 [00:46<00:10,  3.68it/s] 82%|████████▏ | 178/217 [00:47<00:10,  3.71it/s] 82%|████████▏ | 179/217 [00:47<00:10,  3.65it/s] 83%|████████▎ | 180/217 [00:47<00:10,  3.65it/s] 83%|████████▎ | 181/217 [00:47<00:09,  3.77it/s] 84%|████████▍ | 182/217 [00:48<00:09,  3.86it/s] 84%|████████▍ | 183/217 [00:48<00:08,  3.92it/s] 85%|████████▍ | 184/217 [00:48<00:08,  3.97it/s] 85%|████████▌ | 185/217 [00:48<00:07,  4.01it/s] 86%|████████▌ | 186/217 [00:49<00:07,  4.03it/s] 86%|████████▌ | 187/217 [00:49<00:07,  4.04it/s] 87%|████████▋ | 188/217 [00:49<00:07,  4.06it/s] 87%|████████▋ | 189/217 [00:49<00:06,  4.07it/s] 88%|████████▊ | 190/217 [00:50<00:06,  4.07it/s] 88%|████████▊ | 191/217 [00:50<00:06,  4.07it/s] 88%|████████▊ | 192/217 [00:50<00:06,  4.07it/s] 89%|████████▉ | 193/217 [00:50<00:05,  4.08it/s] 89%|████████▉ | 194/217 [00:51<00:05,  4.07it/s] 90%|████████▉ | 195/217 [00:51<00:05,  4.07it/s] 90%|█████████ | 196/217 [00:51<00:05,  4.08it/s] 91%|█████████ | 197/217 [00:51<00:04,  4.09it/s] 91%|█████████ | 198/217 [00:52<00:04,  4.08it/s] 92%|█████████▏| 199/217 [00:52<00:04,  4.08it/s] 92%|█████████▏| 200/217 [00:52<00:04,  4.08it/s] 93%|█████████▎| 201/217 [00:52<00:04,  3.99it/s] 93%|█████████▎| 202/217 [00:53<00:03,  3.88it/s] 94%|█████████▎| 203/217 [00:53<00:03,  3.85it/s] 94%|█████████▍| 204/217 [00:53<00:03,  3.91it/s] 94%|█████████▍| 205/217 [00:53<00:03,  3.81it/s] 95%|█████████▍| 206/217 [00:54<00:02,  3.77it/s] 95%|█████████▌| 207/217 [00:54<00:02,  3.71it/s] 96%|█████████▌| 208/217 [00:54<00:02,  3.81it/s] 96%|█████████▋| 209/217 [00:54<00:02,  3.76it/s] 97%|█████████▋| 210/217 [00:55<00:01,  3.74it/s] 97%|█████████▋| 211/217 [00:55<00:01,  3.68it/s] 98%|█████████▊| 212/217 [00:55<00:01,  3.79it/s] 98%|█████████▊| 213/217 [00:56<00:01,  3.77it/s] 99%|█████████▊| 214/217 [00:56<00:00,  3.73it/s] 99%|█████████▉| 215/217 [00:56<00:00,  3.67it/s]100%|█████████▉| 216/217 [00:56<00:00,  3.76it/s]100%|██████████| 217/217 [00:57<00:00,  3.73it/s]accuracy:  0.8525345622119815
100%|██████████| 217/217 [01:00<00:00,  3.59it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 103
Training dataset size: 192, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.78s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:37:30,788] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:37:31,390] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:37:32,027] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:27,  2.84s/it]  6%|▋         | 2/32 [00:04<01:04,  2.15s/it]  9%|▉         | 3/32 [00:06<01:05,  2.24s/it] 12%|█▎        | 4/32 [00:09<01:04,  2.30s/it] 16%|█▌        | 5/32 [00:11<01:05,  2.44s/it] 19%|█▉        | 6/32 [00:13<00:59,  2.28s/it] 22%|██▏       | 7/32 [00:16<00:58,  2.32s/it] 25%|██▌       | 8/32 [00:18<00:55,  2.30s/it] 28%|██▊       | 9/32 [00:21<00:53,  2.34s/it] 31%|███▏      | 10/32 [00:23<00:54,  2.47s/it]                                               {'loss': 0.9623, 'grad_norm': 9.896484375, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:23<00:54,  2.47s/it] 34%|███▍      | 11/32 [00:26<00:51,  2.45s/it] 38%|███▊      | 12/32 [00:28<00:49,  2.50s/it] 41%|████      | 13/32 [00:31<00:46,  2.46s/it] 44%|████▍     | 14/32 [00:32<00:38,  2.16s/it] 47%|████▋     | 15/32 [00:34<00:37,  2.22s/it] 50%|█████     | 16/32 [00:37<00:35,  2.22s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:39<00:32,  2.14s/it] 56%|█████▋    | 18/32 [00:41<00:31,  2.27s/it] 59%|█████▉    | 19/32 [00:43<00:27,  2.15s/it] 62%|██████▎   | 20/32 [00:45<00:24,  2.07s/it]                                               {'loss': 0.6336, 'grad_norm': 7.356968879699707, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:45<00:24,  2.07s/it] 66%|██████▌   | 21/32 [00:48<00:24,  2.27s/it] 69%|██████▉   | 22/32 [00:50<00:22,  2.20s/it] 72%|███████▏  | 23/32 [00:52<00:21,  2.35s/it] 75%|███████▌  | 24/32 [00:55<00:18,  2.36s/it] 78%|███████▊  | 25/32 [00:57<00:15,  2.17s/it] 81%|████████▏ | 26/32 [00:59<00:13,  2.25s/it] 84%|████████▍ | 27/32 [01:01<00:11,  2.27s/it] 88%|████████▊ | 28/32 [01:04<00:09,  2.27s/it] 91%|█████████ | 29/32 [01:06<00:07,  2.35s/it] 94%|█████████▍| 30/32 [01:09<00:04,  2.36s/it]                                               {'loss': 0.5788, 'grad_norm': 4.750221252441406, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:09<00:04,  2.36s/it] 97%|█████████▋| 31/32 [01:11<00:02,  2.28s/it]100%|██████████| 32/32 [01:12<00:00,  2.10s/it]                                               {'train_runtime': 73.4367, 'train_samples_per_second': 5.229, 'train_steps_per_second': 0.436, 'train_loss': 0.7043726071715355, 'epoch': 2.0}
100%|██████████| 32/32 [01:13<00:00,  2.10s/it]100%|██████████| 32/32 [01:13<00:00,  2.29s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:50,  2.93it/s]  1%|▏         | 2/150 [00:00<00:44,  3.30it/s]  2%|▏         | 3/150 [00:00<00:40,  3.64it/s]  3%|▎         | 4/150 [00:01<00:40,  3.61it/s]  3%|▎         | 5/150 [00:01<00:40,  3.59it/s]  4%|▍         | 6/150 [00:01<00:40,  3.56it/s]  5%|▍         | 7/150 [00:01<00:38,  3.74it/s]  5%|▌         | 8/150 [00:02<00:37,  3.79it/s]  6%|▌         | 9/150 [00:02<00:38,  3.66it/s]  7%|▋         | 10/150 [00:02<00:37,  3.70it/s]  7%|▋         | 11/150 [00:03<00:37,  3.68it/s]  8%|▊         | 12/150 [00:03<00:36,  3.80it/s]  9%|▊         | 13/150 [00:03<00:36,  3.72it/s]  9%|▉         | 14/150 [00:03<00:37,  3.67it/s] 10%|█         | 15/150 [00:04<00:37,  3.63it/s] 11%|█         | 16/150 [00:04<00:35,  3.77it/s] 11%|█▏        | 17/150 [00:04<00:35,  3.79it/s] 12%|█▏        | 18/150 [00:04<00:35,  3.67it/s] 13%|█▎        | 19/150 [00:05<00:35,  3.72it/s] 13%|█▎        | 20/150 [00:05<00:35,  3.69it/s] 14%|█▍        | 21/150 [00:05<00:34,  3.78it/s] 15%|█▍        | 22/150 [00:05<00:34,  3.73it/s] 15%|█▌        | 23/150 [00:06<00:34,  3.68it/s] 16%|█▌        | 24/150 [00:06<00:34,  3.63it/s] 17%|█▋        | 25/150 [00:06<00:33,  3.77it/s] 17%|█▋        | 26/150 [00:07<00:32,  3.80it/s] 18%|█▊        | 27/150 [00:07<00:33,  3.67it/s] 19%|█▊        | 28/150 [00:07<00:32,  3.72it/s] 19%|█▉        | 29/150 [00:07<00:32,  3.68it/s] 20%|██        | 30/150 [00:08<00:31,  3.80it/s] 21%|██        | 31/150 [00:08<00:31,  3.73it/s] 21%|██▏       | 32/150 [00:08<00:32,  3.67it/s] 22%|██▏       | 33/150 [00:08<00:32,  3.62it/s] 23%|██▎       | 34/150 [00:09<00:30,  3.76it/s] 23%|██▎       | 35/150 [00:09<00:30,  3.78it/s] 24%|██▍       | 36/150 [00:09<00:31,  3.67it/s] 25%|██▍       | 37/150 [00:10<00:30,  3.72it/s] 25%|██▌       | 38/150 [00:10<00:30,  3.68it/s] 26%|██▌       | 39/150 [00:10<00:29,  3.77it/s] 27%|██▋       | 40/150 [00:10<00:29,  3.71it/s] 27%|██▋       | 41/150 [00:11<00:29,  3.67it/s] 28%|██▊       | 42/150 [00:11<00:29,  3.62it/s] 29%|██▊       | 43/150 [00:11<00:28,  3.77it/s] 29%|██▉       | 44/150 [00:11<00:28,  3.78it/s] 30%|███       | 45/150 [00:12<00:28,  3.67it/s] 31%|███       | 46/150 [00:12<00:27,  3.72it/s] 31%|███▏      | 47/150 [00:12<00:28,  3.67it/s] 32%|███▏      | 48/150 [00:12<00:27,  3.77it/s] 33%|███▎      | 49/150 [00:13<00:27,  3.72it/s] 33%|███▎      | 50/150 [00:13<00:27,  3.67it/s] 34%|███▍      | 51/150 [00:13<00:27,  3.61it/s] 35%|███▍      | 52/150 [00:14<00:26,  3.76it/s] 35%|███▌      | 53/150 [00:14<00:25,  3.79it/s] 36%|███▌      | 54/150 [00:14<00:26,  3.67it/s] 37%|███▋      | 55/150 [00:14<00:25,  3.73it/s] 37%|███▋      | 56/150 [00:15<00:25,  3.69it/s] 38%|███▊      | 57/150 [00:15<00:24,  3.78it/s] 39%|███▊      | 58/150 [00:15<00:24,  3.73it/s] 39%|███▉      | 59/150 [00:15<00:24,  3.66it/s] 40%|████      | 60/150 [00:16<00:24,  3.62it/s] 41%|████      | 61/150 [00:16<00:23,  3.76it/s] 41%|████▏     | 62/150 [00:16<00:23,  3.78it/s] 42%|████▏     | 63/150 [00:17<00:23,  3.66it/s] 43%|████▎     | 64/150 [00:17<00:23,  3.68it/s] 43%|████▎     | 65/150 [00:17<00:23,  3.64it/s] 44%|████▍     | 66/150 [00:17<00:22,  3.78it/s] 45%|████▍     | 67/150 [00:18<00:21,  3.78it/s] 45%|████▌     | 68/150 [00:18<00:22,  3.66it/s] 46%|████▌     | 69/150 [00:18<00:21,  3.72it/s] 47%|████▋     | 70/150 [00:18<00:21,  3.67it/s] 47%|████▋     | 71/150 [00:19<00:20,  3.77it/s] 48%|████▊     | 72/150 [00:19<00:21,  3.71it/s] 49%|████▊     | 73/150 [00:19<00:21,  3.65it/s] 49%|████▉     | 74/150 [00:20<00:21,  3.61it/s] 50%|█████     | 75/150 [00:20<00:19,  3.75it/s] 51%|█████     | 76/150 [00:20<00:19,  3.79it/s] 51%|█████▏    | 77/150 [00:20<00:19,  3.66it/s] 52%|█████▏    | 78/150 [00:21<00:19,  3.70it/s] 53%|█████▎    | 79/150 [00:21<00:19,  3.65it/s] 53%|█████▎    | 80/150 [00:21<00:18,  3.74it/s] 54%|█████▍    | 81/150 [00:21<00:18,  3.69it/s] 55%|█████▍    | 82/150 [00:22<00:18,  3.63it/s] 55%|█████▌    | 83/150 [00:22<00:18,  3.59it/s] 56%|█████▌    | 84/150 [00:22<00:17,  3.74it/s] 57%|█████▋    | 85/150 [00:22<00:17,  3.78it/s] 57%|█████▋    | 86/150 [00:23<00:17,  3.71it/s] 58%|█████▊    | 87/150 [00:23<00:17,  3.66it/s] 59%|█████▊    | 88/150 [00:23<00:17,  3.61it/s] 59%|█████▉    | 89/150 [00:24<00:16,  3.74it/s] 60%|██████    | 90/150 [00:24<00:15,  3.78it/s] 61%|██████    | 91/150 [00:24<00:15,  3.70it/s] 61%|██████▏   | 92/150 [00:24<00:15,  3.65it/s] 62%|██████▏   | 93/150 [00:25<00:15,  3.60it/s] 63%|██████▎   | 94/150 [00:25<00:14,  3.75it/s] 63%|██████▎   | 95/150 [00:25<00:14,  3.78it/s] 64%|██████▍   | 96/150 [00:25<00:14,  3.65it/s] 65%|██████▍   | 97/150 [00:26<00:14,  3.67it/s] 65%|██████▌   | 98/150 [00:26<00:14,  3.62it/s] 66%|██████▌   | 99/150 [00:26<00:13,  3.75it/s] 67%|██████▋   | 100/150 [00:27<00:13,  3.76it/s] 67%|██████▋   | 101/150 [00:27<00:13,  3.64it/s] 68%|██████▊   | 102/150 [00:27<00:13,  3.69it/s] 69%|██████▊   | 103/150 [00:27<00:12,  3.66it/s] 69%|██████▉   | 104/150 [00:28<00:12,  3.75it/s] 70%|███████   | 105/150 [00:28<00:12,  3.70it/s] 71%|███████   | 106/150 [00:28<00:12,  3.64it/s] 71%|███████▏  | 107/150 [00:28<00:11,  3.59it/s] 72%|███████▏  | 108/150 [00:29<00:11,  3.74it/s] 73%|███████▎  | 109/150 [00:29<00:10,  3.76it/s] 73%|███████▎  | 110/150 [00:29<00:11,  3.63it/s] 74%|███████▍  | 111/150 [00:30<00:10,  3.69it/s] 75%|███████▍  | 112/150 [00:30<00:10,  3.64it/s] 75%|███████▌  | 113/150 [00:30<00:09,  3.72it/s] 76%|███████▌  | 114/150 [00:30<00:09,  3.66it/s] 77%|███████▋  | 115/150 [00:31<00:09,  3.62it/s] 77%|███████▋  | 116/150 [00:31<00:09,  3.58it/s] 78%|███████▊  | 117/150 [00:31<00:08,  3.72it/s] 79%|███████▊  | 118/150 [00:31<00:08,  3.74it/s] 79%|███████▉  | 119/150 [00:32<00:08,  3.63it/s] 80%|████████  | 120/150 [00:32<00:08,  3.68it/s] 81%|████████  | 121/150 [00:32<00:07,  3.65it/s] 81%|████████▏ | 122/150 [00:33<00:07,  3.74it/s] 82%|████████▏ | 123/150 [00:33<00:07,  3.68it/s] 83%|████████▎ | 124/150 [00:33<00:07,  3.63it/s] 83%|████████▎ | 125/150 [00:33<00:06,  3.59it/s] 84%|████████▍ | 126/150 [00:34<00:06,  3.72it/s] 85%|████████▍ | 127/150 [00:34<00:06,  3.75it/s] 85%|████████▌ | 128/150 [00:34<00:06,  3.64it/s] 86%|████████▌ | 129/150 [00:34<00:05,  3.68it/s] 87%|████████▋ | 130/150 [00:35<00:05,  3.64it/s] 87%|████████▋ | 131/150 [00:35<00:05,  3.73it/s] 88%|████████▊ | 132/150 [00:35<00:04,  3.68it/s] 89%|████████▊ | 133/150 [00:36<00:04,  3.62it/s] 89%|████████▉ | 134/150 [00:36<00:04,  3.58it/s] 90%|█████████ | 135/150 [00:36<00:04,  3.73it/s] 91%|█████████ | 136/150 [00:36<00:03,  3.74it/s] 91%|█████████▏| 137/150 [00:37<00:03,  3.62it/s] 92%|█████████▏| 138/150 [00:37<00:03,  3.69it/s] 93%|█████████▎| 139/150 [00:37<00:03,  3.65it/s] 93%|█████████▎| 140/150 [00:37<00:02,  3.73it/s] 94%|█████████▍| 141/150 [00:38<00:02,  3.68it/s] 95%|█████████▍| 142/150 [00:38<00:02,  3.63it/s] 95%|█████████▌| 143/150 [00:38<00:01,  3.58it/s] 96%|█████████▌| 144/150 [00:39<00:01,  3.72it/s] 97%|█████████▋| 145/150 [00:39<00:01,  3.74it/s] 97%|█████████▋| 146/150 [00:39<00:01,  3.62it/s] 98%|█████████▊| 147/150 [00:39<00:00,  3.68it/s] 99%|█████████▊| 148/150 [00:40<00:00,  3.64it/s] 99%|█████████▉| 149/150 [00:40<00:00,  3.74it/s]100%|██████████| 150/150 [00:40<00:00,  3.69it/s]accuracy:  0.62
100%|██████████| 150/150 [00:43<00:00,  3.49it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 145
Training dataset size: 192, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:39:54,194] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 04:39:54,215] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:40:00,354] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:03<01:35,  3.08s/it]  6%|▋         | 2/32 [00:05<01:21,  2.72s/it]  9%|▉         | 3/32 [00:08<01:17,  2.68s/it] 12%|█▎        | 4/32 [00:10<01:12,  2.58s/it] 16%|█▌        | 5/32 [00:12<01:03,  2.35s/it] 19%|█▉        | 6/32 [00:14<00:59,  2.30s/it] 22%|██▏       | 7/32 [00:17<00:58,  2.36s/it] 25%|██▌       | 8/32 [00:19<00:59,  2.47s/it] 28%|██▊       | 9/32 [00:21<00:53,  2.32s/it] 31%|███▏      | 10/32 [00:24<00:53,  2.43s/it]                                               {'loss': 1.5723, 'grad_norm': 16.005231857299805, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:24<00:53,  2.43s/it] 34%|███▍      | 11/32 [00:26<00:50,  2.40s/it] 38%|███▊      | 12/32 [00:29<00:49,  2.48s/it] 41%|████      | 13/32 [00:32<00:46,  2.47s/it] 44%|████▍     | 14/32 [00:33<00:41,  2.31s/it] 47%|████▋     | 15/32 [00:36<00:40,  2.39s/it] 50%|█████     | 16/32 [00:39<00:39,  2.47s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:42<00:38,  2.59s/it] 56%|█████▋    | 18/32 [00:44<00:36,  2.62s/it] 59%|█████▉    | 19/32 [00:47<00:32,  2.52s/it] 62%|██████▎   | 20/32 [00:49<00:28,  2.42s/it]                                               {'loss': 1.177, 'grad_norm': 9.379318237304688, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:49<00:28,  2.42s/it] 66%|██████▌   | 21/32 [00:51<00:27,  2.52s/it] 69%|██████▉   | 22/32 [00:54<00:26,  2.65s/it] 72%|███████▏  | 23/32 [00:57<00:22,  2.49s/it] 75%|███████▌  | 24/32 [00:59<00:20,  2.51s/it] 78%|███████▊  | 25/32 [01:01<00:17,  2.46s/it] 81%|████████▏ | 26/32 [01:03<00:14,  2.34s/it] 84%|████████▍ | 27/32 [01:06<00:11,  2.35s/it] 88%|████████▊ | 28/32 [01:08<00:09,  2.34s/it] 91%|█████████ | 29/32 [01:11<00:07,  2.37s/it] 94%|█████████▍| 30/32 [01:13<00:04,  2.26s/it]                                               {'loss': 0.9783, 'grad_norm': 8.093913078308105, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:13<00:04,  2.26s/it] 97%|█████████▋| 31/32 [01:15<00:02,  2.19s/it]100%|██████████| 32/32 [01:18<00:00,  2.39s/it]                                               {'train_runtime': 78.6462, 'train_samples_per_second': 4.883, 'train_steps_per_second': 0.407, 'train_loss': 1.2318675518035889, 'epoch': 2.0}
100%|██████████| 32/32 [01:18<00:00,  2.39s/it]100%|██████████| 32/32 [01:18<00:00,  2.45s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:11,  2.80it/s]  1%|          | 2/200 [00:00<01:00,  3.27it/s]  2%|▏         | 3/200 [00:00<00:57,  3.42it/s]  2%|▏         | 4/200 [00:01<00:53,  3.67it/s]  2%|▎         | 5/200 [00:01<00:52,  3.73it/s]  3%|▎         | 6/200 [00:01<00:52,  3.71it/s]  4%|▎         | 7/200 [00:01<00:51,  3.78it/s]  4%|▍         | 8/200 [00:02<00:51,  3.72it/s]  4%|▍         | 9/200 [00:02<00:49,  3.83it/s]  5%|▌         | 10/200 [00:02<00:50,  3.79it/s]  6%|▌         | 11/200 [00:02<00:50,  3.76it/s]  6%|▌         | 12/200 [00:03<00:50,  3.72it/s]  6%|▋         | 13/200 [00:03<00:48,  3.84it/s]  7%|▋         | 14/200 [00:03<00:48,  3.84it/s]  8%|▊         | 15/200 [00:04<00:48,  3.79it/s]  8%|▊         | 16/200 [00:04<00:48,  3.78it/s]  8%|▊         | 17/200 [00:04<00:47,  3.89it/s]  9%|▉         | 18/200 [00:04<00:45,  3.97it/s] 10%|▉         | 19/200 [00:05<00:44,  4.02it/s] 10%|█         | 20/200 [00:05<00:44,  4.06it/s] 10%|█         | 21/200 [00:05<00:43,  4.10it/s] 11%|█         | 22/200 [00:05<00:43,  4.11it/s] 12%|█▏        | 23/200 [00:05<00:42,  4.13it/s] 12%|█▏        | 24/200 [00:06<00:42,  4.14it/s] 12%|█▎        | 25/200 [00:06<00:42,  4.15it/s] 13%|█▎        | 26/200 [00:06<00:41,  4.16it/s] 14%|█▎        | 27/200 [00:06<00:41,  4.16it/s] 14%|█▍        | 28/200 [00:07<00:41,  4.17it/s] 14%|█▍        | 29/200 [00:07<00:41,  4.17it/s] 15%|█▌        | 30/200 [00:07<00:40,  4.17it/s] 16%|█▌        | 31/200 [00:07<00:40,  4.17it/s] 16%|█▌        | 32/200 [00:08<00:40,  4.17it/s] 16%|█▋        | 33/200 [00:08<00:40,  4.17it/s] 17%|█▋        | 34/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 35/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 36/200 [00:09<00:39,  4.17it/s] 18%|█▊        | 37/200 [00:09<00:39,  4.14it/s] 19%|█▉        | 38/200 [00:09<00:40,  4.01it/s] 20%|█▉        | 39/200 [00:09<00:41,  3.91it/s] 20%|██        | 40/200 [00:10<00:41,  3.88it/s] 20%|██        | 41/200 [00:10<00:40,  3.93it/s] 21%|██        | 42/200 [00:10<00:41,  3.84it/s] 22%|██▏       | 43/200 [00:10<00:41,  3.78it/s] 22%|██▏       | 44/200 [00:11<00:41,  3.73it/s] 22%|██▎       | 45/200 [00:11<00:40,  3.84it/s] 23%|██▎       | 46/200 [00:11<00:40,  3.78it/s] 24%|██▎       | 47/200 [00:12<00:41,  3.73it/s] 24%|██▍       | 48/200 [00:12<00:41,  3.67it/s] 24%|██▍       | 49/200 [00:12<00:39,  3.80it/s] 25%|██▌       | 50/200 [00:12<00:39,  3.81it/s] 26%|██▌       | 51/200 [00:13<00:40,  3.72it/s] 26%|██▌       | 52/200 [00:13<00:39,  3.78it/s] 26%|██▋       | 53/200 [00:13<00:39,  3.71it/s] 27%|██▋       | 54/200 [00:13<00:38,  3.79it/s] 28%|██▊       | 55/200 [00:14<00:38,  3.72it/s] 28%|██▊       | 56/200 [00:14<00:38,  3.75it/s] 28%|██▊       | 57/200 [00:14<00:38,  3.71it/s] 29%|██▉       | 58/200 [00:14<00:37,  3.82it/s] 30%|██▉       | 59/200 [00:15<00:37,  3.76it/s] 30%|███       | 60/200 [00:15<00:37,  3.73it/s] 30%|███       | 61/200 [00:15<00:37,  3.68it/s] 31%|███       | 62/200 [00:16<00:36,  3.81it/s] 32%|███▏      | 63/200 [00:16<00:35,  3.81it/s] 32%|███▏      | 64/200 [00:16<00:36,  3.73it/s] 32%|███▎      | 65/200 [00:16<00:36,  3.71it/s] 33%|███▎      | 66/200 [00:17<00:35,  3.76it/s] 34%|███▎      | 67/200 [00:17<00:35,  3.79it/s] 34%|███▍      | 68/200 [00:17<00:35,  3.71it/s] 34%|███▍      | 69/200 [00:17<00:34,  3.74it/s] 35%|███▌      | 70/200 [00:18<00:35,  3.69it/s] 36%|███▌      | 71/200 [00:18<00:34,  3.77it/s] 36%|███▌      | 72/200 [00:18<00:34,  3.70it/s] 36%|███▋      | 73/200 [00:18<00:34,  3.73it/s] 37%|███▋      | 74/200 [00:19<00:34,  3.68it/s] 38%|███▊      | 75/200 [00:19<00:33,  3.77it/s] 38%|███▊      | 76/200 [00:19<00:33,  3.70it/s] 38%|███▊      | 77/200 [00:20<00:33,  3.71it/s] 39%|███▉      | 78/200 [00:20<00:33,  3.67it/s] 40%|███▉      | 79/200 [00:20<00:31,  3.79it/s] 40%|████      | 80/200 [00:20<00:32,  3.75it/s] 40%|████      | 81/200 [00:21<00:32,  3.71it/s] 41%|████      | 82/200 [00:21<00:32,  3.66it/s] 42%|████▏     | 83/200 [00:21<00:31,  3.76it/s] 42%|████▏     | 84/200 [00:21<00:30,  3.86it/s] 42%|████▎     | 85/200 [00:22<00:29,  3.94it/s] 43%|████▎     | 86/200 [00:22<00:28,  3.98it/s] 44%|████▎     | 87/200 [00:22<00:29,  3.86it/s] 44%|████▍     | 88/200 [00:22<00:29,  3.78it/s] 44%|████▍     | 89/200 [00:23<00:29,  3.72it/s] 45%|████▌     | 90/200 [00:23<00:28,  3.83it/s] 46%|████▌     | 91/200 [00:23<00:28,  3.81it/s] 46%|████▌     | 92/200 [00:23<00:29,  3.72it/s] 46%|████▋     | 93/200 [00:24<00:29,  3.66it/s] 47%|████▋     | 94/200 [00:24<00:28,  3.78it/s] 48%|████▊     | 95/200 [00:24<00:27,  3.81it/s] 48%|████▊     | 96/200 [00:25<00:27,  3.72it/s] 48%|████▊     | 97/200 [00:25<00:27,  3.69it/s] 49%|████▉     | 98/200 [00:25<00:27,  3.76it/s] 50%|████▉     | 99/200 [00:25<00:26,  3.79it/s] 50%|█████     | 100/200 [00:26<00:27,  3.70it/s] 50%|█████     | 101/200 [00:26<00:26,  3.74it/s] 51%|█████     | 102/200 [00:26<00:26,  3.69it/s] 52%|█████▏    | 103/200 [00:26<00:25,  3.79it/s] 52%|█████▏    | 104/200 [00:27<00:25,  3.74it/s] 52%|█████▎    | 105/200 [00:27<00:25,  3.70it/s] 53%|█████▎    | 106/200 [00:27<00:25,  3.66it/s] 54%|█████▎    | 107/200 [00:28<00:24,  3.79it/s] 54%|█████▍    | 108/200 [00:28<00:24,  3.75it/s] 55%|█████▍    | 109/200 [00:28<00:24,  3.70it/s] 55%|█████▌    | 110/200 [00:28<00:24,  3.63it/s] 56%|█████▌    | 111/200 [00:29<00:23,  3.76it/s] 56%|█████▌    | 112/200 [00:29<00:23,  3.79it/s] 56%|█████▋    | 113/200 [00:29<00:23,  3.69it/s] 57%|█████▋    | 114/200 [00:29<00:23,  3.69it/s] 57%|█████▊    | 115/200 [00:30<00:22,  3.81it/s] 58%|█████▊    | 116/200 [00:30<00:21,  3.89it/s] 58%|█████▊    | 117/200 [00:30<00:21,  3.94it/s] 59%|█████▉    | 118/200 [00:30<00:20,  3.99it/s] 60%|█████▉    | 119/200 [00:31<00:20,  4.03it/s] 60%|██████    | 120/200 [00:31<00:19,  4.06it/s] 60%|██████    | 121/200 [00:31<00:19,  4.07it/s] 61%|██████    | 122/200 [00:31<00:19,  4.08it/s] 62%|██████▏   | 123/200 [00:32<00:18,  4.08it/s] 62%|██████▏   | 124/200 [00:32<00:18,  4.09it/s] 62%|██████▎   | 125/200 [00:32<00:18,  4.10it/s] 63%|██████▎   | 126/200 [00:32<00:18,  4.10it/s] 64%|██████▎   | 127/200 [00:33<00:17,  4.09it/s] 64%|██████▍   | 128/200 [00:33<00:17,  4.09it/s] 64%|██████▍   | 129/200 [00:33<00:17,  4.10it/s] 65%|██████▌   | 130/200 [00:33<00:17,  4.11it/s] 66%|██████▌   | 131/200 [00:34<00:16,  4.10it/s] 66%|██████▌   | 132/200 [00:34<00:16,  4.10it/s] 66%|██████▋   | 133/200 [00:34<00:16,  4.09it/s] 67%|██████▋   | 134/200 [00:34<00:16,  4.10it/s] 68%|██████▊   | 135/200 [00:35<00:15,  4.09it/s] 68%|██████▊   | 136/200 [00:35<00:16,  3.96it/s] 68%|██████▊   | 137/200 [00:35<00:16,  3.84it/s] 69%|██████▉   | 138/200 [00:35<00:15,  3.92it/s] 70%|██████▉   | 139/200 [00:36<00:15,  3.93it/s] 70%|███████   | 140/200 [00:36<00:15,  3.83it/s] 70%|███████   | 141/200 [00:36<00:15,  3.82it/s] 71%|███████   | 142/200 [00:36<00:15,  3.73it/s] 72%|███████▏  | 143/200 [00:37<00:15,  3.79it/s] 72%|███████▏  | 144/200 [00:37<00:14,  3.75it/s] 72%|███████▎  | 145/200 [00:37<00:14,  3.75it/s] 73%|███████▎  | 146/200 [00:37<00:14,  3.69it/s] 74%|███████▎  | 147/200 [00:38<00:13,  3.80it/s] 74%|███████▍  | 148/200 [00:38<00:13,  3.75it/s] 74%|███████▍  | 149/200 [00:38<00:13,  3.71it/s] 75%|███████▌  | 150/200 [00:39<00:13,  3.66it/s] 76%|███████▌  | 151/200 [00:39<00:12,  3.77it/s] 76%|███████▌  | 152/200 [00:39<00:12,  3.74it/s] 76%|███████▋  | 153/200 [00:39<00:12,  3.70it/s] 77%|███████▋  | 154/200 [00:40<00:12,  3.65it/s] 78%|███████▊  | 155/200 [00:40<00:11,  3.77it/s] 78%|███████▊  | 156/200 [00:40<00:11,  3.75it/s] 78%|███████▊  | 157/200 [00:40<00:11,  3.74it/s] 79%|███████▉  | 158/200 [00:41<00:11,  3.73it/s] 80%|███████▉  | 159/200 [00:41<00:11,  3.71it/s] 80%|████████  | 160/200 [00:41<00:10,  3.76it/s] 80%|████████  | 161/200 [00:41<00:10,  3.73it/s] 81%|████████  | 162/200 [00:42<00:10,  3.76it/s] 82%|████████▏ | 163/200 [00:42<00:10,  3.69it/s] 82%|████████▏ | 164/200 [00:42<00:09,  3.75it/s] 82%|████████▎ | 165/200 [00:43<00:09,  3.72it/s] 83%|████████▎ | 166/200 [00:43<00:09,  3.73it/s] 84%|████████▎ | 167/200 [00:43<00:08,  3.67it/s] 84%|████████▍ | 168/200 [00:43<00:08,  3.78it/s] 84%|████████▍ | 169/200 [00:44<00:08,  3.73it/s] 85%|████████▌ | 170/200 [00:44<00:08,  3.70it/s] 86%|████████▌ | 171/200 [00:44<00:07,  3.64it/s] 86%|████████▌ | 172/200 [00:44<00:07,  3.76it/s] 86%|████████▋ | 173/200 [00:45<00:07,  3.74it/s] 87%|████████▋ | 174/200 [00:45<00:07,  3.70it/s] 88%|████████▊ | 175/200 [00:45<00:06,  3.64it/s] 88%|████████▊ | 176/200 [00:46<00:06,  3.76it/s] 88%|████████▊ | 177/200 [00:46<00:06,  3.77it/s] 89%|████████▉ | 178/200 [00:46<00:05,  3.73it/s] 90%|████████▉ | 179/200 [00:46<00:05,  3.76it/s] 90%|█████████ | 180/200 [00:47<00:05,  3.71it/s] 90%|█████████ | 181/200 [00:47<00:05,  3.74it/s] 91%|█████████ | 182/200 [00:47<00:04,  3.71it/s] 92%|█████████▏| 183/200 [00:47<00:04,  3.74it/s] 92%|█████████▏| 184/200 [00:48<00:04,  3.68it/s] 92%|█████████▎| 185/200 [00:48<00:03,  3.76it/s] 93%|█████████▎| 186/200 [00:48<00:03,  3.72it/s] 94%|█████████▎| 187/200 [00:48<00:03,  3.73it/s] 94%|█████████▍| 188/200 [00:49<00:03,  3.67it/s] 94%|█████████▍| 189/200 [00:49<00:02,  3.77it/s] 95%|█████████▌| 190/200 [00:49<00:02,  3.73it/s] 96%|█████████▌| 191/200 [00:50<00:02,  3.70it/s] 96%|█████████▌| 192/200 [00:50<00:02,  3.65it/s] 96%|█████████▋| 193/200 [00:50<00:01,  3.77it/s] 97%|█████████▋| 194/200 [00:50<00:01,  3.73it/s] 98%|█████████▊| 195/200 [00:51<00:01,  3.69it/s] 98%|█████████▊| 196/200 [00:51<00:01,  3.65it/s] 98%|█████████▊| 197/200 [00:51<00:00,  3.77it/s] 99%|█████████▉| 198/200 [00:51<00:00,  3.85it/s]100%|█████████▉| 199/200 [00:52<00:00,  3.78it/s]100%|██████████| 200/200 [00:52<00:00,  3.74it/s]accuracy:  0.575
100%|██████████| 200/200 [00:55<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 191
Training dataset size: 192, validation dataset size: 191
Training dataset size: 192, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.21s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 04:42:40,393] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:42:40,700] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.35s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:42:42,111] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:22,  2.68s/it]  6%|▋         | 2/32 [00:05<01:21,  2.73s/it]  9%|▉         | 3/32 [00:08<01:25,  2.96s/it] 12%|█▎        | 4/32 [00:10<01:15,  2.70s/it] 16%|█▌        | 5/32 [00:13<01:13,  2.73s/it] 19%|█▉        | 6/32 [00:16<01:13,  2.83s/it] 22%|██▏       | 7/32 [00:18<01:05,  2.62s/it] 25%|██▌       | 8/32 [00:21<00:58,  2.46s/it] 28%|██▊       | 9/32 [00:24<01:00,  2.63s/it] 31%|███▏      | 10/32 [00:26<00:56,  2.58s/it]                                               {'loss': 1.3436, 'grad_norm': 10.22135066986084, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:26<00:56,  2.58s/it] 34%|███▍      | 11/32 [00:28<00:53,  2.53s/it] 38%|███▊      | 12/32 [00:32<00:54,  2.72s/it] 41%|████      | 13/32 [00:34<00:52,  2.74s/it] 44%|████▍     | 14/32 [00:37<00:48,  2.67s/it] 47%|████▋     | 15/32 [00:40<00:44,  2.64s/it] 50%|█████     | 16/32 [00:42<00:41,  2.57s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:45<00:40,  2.71s/it] 56%|█████▋    | 18/32 [00:48<00:38,  2.72s/it] 59%|█████▉    | 19/32 [00:51<00:36,  2.81s/it] 62%|██████▎   | 20/32 [00:53<00:33,  2.77s/it]                                               {'loss': 1.0229, 'grad_norm': 14.656479835510254, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:53<00:33,  2.77s/it] 66%|██████▌   | 21/32 [00:56<00:29,  2.71s/it] 69%|██████▉   | 22/32 [00:59<00:27,  2.72s/it] 72%|███████▏  | 23/32 [01:01<00:23,  2.57s/it] 75%|███████▌  | 24/32 [01:03<00:19,  2.43s/it] 78%|███████▊  | 25/32 [01:05<00:16,  2.34s/it] 81%|████████▏ | 26/32 [01:08<00:15,  2.52s/it] 84%|████████▍ | 27/32 [01:11<00:12,  2.51s/it] 88%|████████▊ | 28/32 [01:13<00:10,  2.55s/it] 91%|█████████ | 29/32 [01:16<00:08,  2.67s/it] 94%|█████████▍| 30/32 [01:19<00:05,  2.75s/it]                                               {'loss': 0.8918, 'grad_norm': 10.912971496582031, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:19<00:05,  2.75s/it] 97%|█████████▋| 31/32 [01:22<00:02,  2.73s/it]100%|██████████| 32/32 [01:24<00:00,  2.59s/it]                                               {'train_runtime': 85.2195, 'train_samples_per_second': 4.506, 'train_steps_per_second': 0.376, 'train_loss': 1.1283551454544067, 'epoch': 2.0}
100%|██████████| 32/32 [01:25<00:00,  2.59s/it]100%|██████████| 32/32 [01:25<00:00,  2.66s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:53,  2.46it/s]  1%|          | 2/279 [00:00<01:28,  3.15it/s]  1%|          | 3/279 [00:00<01:21,  3.38it/s]  1%|▏         | 4/279 [00:01<01:16,  3.59it/s]  2%|▏         | 5/279 [00:01<01:15,  3.61it/s]  2%|▏         | 6/279 [00:01<01:13,  3.71it/s]  3%|▎         | 7/279 [00:01<01:13,  3.70it/s]  3%|▎         | 8/279 [00:02<01:14,  3.65it/s]  3%|▎         | 9/279 [00:02<01:11,  3.78it/s]  4%|▎         | 10/279 [00:02<01:10,  3.79it/s]  4%|▍         | 11/279 [00:03<01:11,  3.76it/s]  4%|▍         | 12/279 [00:03<01:12,  3.68it/s]  5%|▍         | 13/279 [00:03<01:09,  3.81it/s]  5%|▌         | 14/279 [00:03<01:09,  3.82it/s]  5%|▌         | 15/279 [00:04<01:09,  3.78it/s]  6%|▌         | 16/279 [00:04<01:11,  3.69it/s]  6%|▌         | 17/279 [00:04<01:08,  3.81it/s]  6%|▋         | 18/279 [00:04<01:08,  3.82it/s]  7%|▋         | 19/279 [00:05<01:08,  3.79it/s]  7%|▋         | 20/279 [00:05<01:09,  3.71it/s]  8%|▊         | 21/279 [00:05<01:07,  3.83it/s]  8%|▊         | 22/279 [00:05<01:07,  3.83it/s]  8%|▊         | 23/279 [00:06<01:07,  3.79it/s]  9%|▊         | 24/279 [00:06<01:08,  3.70it/s]  9%|▉         | 25/279 [00:06<01:06,  3.83it/s]  9%|▉         | 26/279 [00:07<01:06,  3.80it/s] 10%|▉         | 27/279 [00:07<01:06,  3.78it/s] 10%|█         | 28/279 [00:07<01:07,  3.72it/s] 10%|█         | 29/279 [00:07<01:05,  3.84it/s] 11%|█         | 30/279 [00:08<01:05,  3.79it/s] 11%|█         | 31/279 [00:08<01:05,  3.77it/s] 11%|█▏        | 32/279 [00:08<01:06,  3.72it/s] 12%|█▏        | 33/279 [00:08<01:04,  3.82it/s] 12%|█▏        | 34/279 [00:09<01:04,  3.77it/s] 13%|█▎        | 35/279 [00:09<01:04,  3.75it/s] 13%|█▎        | 36/279 [00:09<01:05,  3.71it/s] 13%|█▎        | 37/279 [00:09<01:03,  3.82it/s] 14%|█▎        | 38/279 [00:10<01:03,  3.78it/s] 14%|█▍        | 39/279 [00:10<01:03,  3.76it/s] 14%|█▍        | 40/279 [00:10<01:04,  3.71it/s] 15%|█▍        | 41/279 [00:10<01:02,  3.84it/s] 15%|█▌        | 42/279 [00:11<01:02,  3.79it/s] 15%|█▌        | 43/279 [00:11<01:02,  3.78it/s] 16%|█▌        | 44/279 [00:11<01:03,  3.72it/s] 16%|█▌        | 45/279 [00:12<01:00,  3.84it/s] 16%|█▋        | 46/279 [00:12<01:01,  3.78it/s] 17%|█▋        | 47/279 [00:12<01:01,  3.77it/s] 17%|█▋        | 48/279 [00:12<01:02,  3.71it/s] 18%|█▊        | 49/279 [00:13<01:00,  3.83it/s] 18%|█▊        | 50/279 [00:13<01:00,  3.78it/s] 18%|█▊        | 51/279 [00:13<01:00,  3.76it/s] 19%|█▊        | 52/279 [00:13<01:01,  3.71it/s] 19%|█▉        | 53/279 [00:14<00:59,  3.83it/s] 19%|█▉        | 54/279 [00:14<00:59,  3.79it/s] 20%|█▉        | 55/279 [00:14<00:59,  3.76it/s] 20%|██        | 56/279 [00:14<01:00,  3.71it/s] 20%|██        | 57/279 [00:15<00:57,  3.83it/s] 21%|██        | 58/279 [00:15<00:58,  3.80it/s] 21%|██        | 59/279 [00:15<00:58,  3.77it/s] 22%|██▏       | 60/279 [00:16<00:58,  3.72it/s] 22%|██▏       | 61/279 [00:16<00:56,  3.84it/s] 22%|██▏       | 62/279 [00:16<00:57,  3.79it/s] 23%|██▎       | 63/279 [00:16<00:57,  3.77it/s] 23%|██▎       | 64/279 [00:17<00:57,  3.72it/s] 23%|██▎       | 65/279 [00:17<00:55,  3.84it/s] 24%|██▎       | 66/279 [00:17<00:54,  3.89it/s] 24%|██▍       | 67/279 [00:17<00:55,  3.83it/s] 24%|██▍       | 68/279 [00:18<00:54,  3.85it/s] 25%|██▍       | 69/279 [00:18<00:55,  3.76it/s] 25%|██▌       | 70/279 [00:18<00:54,  3.80it/s] 25%|██▌       | 71/279 [00:18<00:55,  3.76it/s] 26%|██▌       | 72/279 [00:19<00:54,  3.81it/s] 26%|██▌       | 73/279 [00:19<00:54,  3.75it/s] 27%|██▋       | 74/279 [00:19<00:53,  3.81it/s] 27%|██▋       | 75/279 [00:19<00:54,  3.77it/s] 27%|██▋       | 76/279 [00:20<00:53,  3.81it/s] 28%|██▊       | 77/279 [00:20<00:54,  3.73it/s] 28%|██▊       | 78/279 [00:20<00:53,  3.78it/s] 28%|██▊       | 79/279 [00:21<00:53,  3.75it/s] 29%|██▊       | 80/279 [00:21<00:52,  3.79it/s] 29%|██▉       | 81/279 [00:21<00:53,  3.73it/s] 29%|██▉       | 82/279 [00:21<00:52,  3.79it/s] 30%|██▉       | 83/279 [00:22<00:52,  3.75it/s] 30%|███       | 84/279 [00:22<00:51,  3.79it/s] 30%|███       | 85/279 [00:22<00:51,  3.73it/s] 31%|███       | 86/279 [00:22<00:50,  3.79it/s] 31%|███       | 87/279 [00:23<00:51,  3.74it/s] 32%|███▏      | 88/279 [00:23<00:51,  3.74it/s] 32%|███▏      | 89/279 [00:23<00:50,  3.73it/s] 32%|███▏      | 90/279 [00:23<00:49,  3.79it/s] 33%|███▎      | 91/279 [00:24<00:50,  3.75it/s] 33%|███▎      | 92/279 [00:24<00:50,  3.73it/s] 33%|███▎      | 93/279 [00:24<00:49,  3.74it/s] 34%|███▎      | 94/279 [00:25<00:48,  3.78it/s] 34%|███▍      | 95/279 [00:25<00:49,  3.75it/s] 34%|███▍      | 96/279 [00:25<00:48,  3.80it/s] 35%|███▍      | 97/279 [00:25<00:48,  3.73it/s] 35%|███▌      | 98/279 [00:26<00:48,  3.77it/s] 35%|███▌      | 99/279 [00:26<00:48,  3.73it/s] 36%|███▌      | 100/279 [00:26<00:49,  3.65it/s] 36%|███▌      | 101/279 [00:26<00:47,  3.78it/s] 37%|███▋      | 102/279 [00:27<00:46,  3.78it/s] 37%|███▋      | 103/279 [00:27<00:46,  3.75it/s] 37%|███▋      | 104/279 [00:27<00:47,  3.66it/s] 38%|███▊      | 105/279 [00:27<00:45,  3.79it/s] 38%|███▊      | 106/279 [00:28<00:45,  3.79it/s] 38%|███▊      | 107/279 [00:28<00:45,  3.74it/s] 39%|███▊      | 108/279 [00:28<00:46,  3.66it/s] 39%|███▉      | 109/279 [00:29<00:44,  3.78it/s] 39%|███▉      | 110/279 [00:29<00:44,  3.79it/s] 40%|███▉      | 111/279 [00:29<00:44,  3.75it/s] 40%|████      | 112/279 [00:29<00:45,  3.66it/s] 41%|████      | 113/279 [00:30<00:43,  3.79it/s] 41%|████      | 114/279 [00:30<00:43,  3.78it/s] 41%|████      | 115/279 [00:30<00:43,  3.75it/s] 42%|████▏     | 116/279 [00:30<00:44,  3.66it/s] 42%|████▏     | 117/279 [00:31<00:42,  3.78it/s] 42%|████▏     | 118/279 [00:31<00:42,  3.79it/s] 43%|████▎     | 119/279 [00:31<00:42,  3.74it/s] 43%|████▎     | 120/279 [00:31<00:43,  3.65it/s] 43%|████▎     | 121/279 [00:32<00:42,  3.76it/s] 44%|████▎     | 122/279 [00:32<00:42,  3.72it/s] 44%|████▍     | 123/279 [00:32<00:42,  3.71it/s] 44%|████▍     | 124/279 [00:33<00:42,  3.67it/s] 45%|████▍     | 125/279 [00:33<00:40,  3.79it/s] 45%|████▌     | 126/279 [00:33<00:40,  3.75it/s] 46%|████▌     | 127/279 [00:33<00:40,  3.73it/s] 46%|████▌     | 128/279 [00:34<00:41,  3.68it/s] 46%|████▌     | 129/279 [00:34<00:39,  3.80it/s] 47%|████▋     | 130/279 [00:34<00:39,  3.76it/s] 47%|████▋     | 131/279 [00:34<00:39,  3.74it/s] 47%|████▋     | 132/279 [00:35<00:39,  3.69it/s] 48%|████▊     | 133/279 [00:35<00:38,  3.80it/s] 48%|████▊     | 134/279 [00:35<00:38,  3.75it/s] 48%|████▊     | 135/279 [00:35<00:38,  3.77it/s] 49%|████▊     | 136/279 [00:36<00:38,  3.76it/s] 49%|████▉     | 137/279 [00:36<00:36,  3.87it/s] 49%|████▉     | 138/279 [00:36<00:35,  3.94it/s] 50%|████▉     | 139/279 [00:36<00:35,  3.99it/s] 50%|█████     | 140/279 [00:37<00:34,  4.01it/s] 51%|█████     | 141/279 [00:37<00:34,  4.04it/s] 51%|█████     | 142/279 [00:37<00:33,  4.07it/s] 51%|█████▏    | 143/279 [00:37<00:33,  4.09it/s] 52%|█████▏    | 144/279 [00:38<00:32,  4.10it/s] 52%|█████▏    | 145/279 [00:38<00:32,  4.10it/s] 52%|█████▏    | 146/279 [00:38<00:32,  4.09it/s] 53%|█████▎    | 147/279 [00:38<00:32,  4.09it/s] 53%|█████▎    | 148/279 [00:39<00:31,  4.10it/s] 53%|█████▎    | 149/279 [00:39<00:31,  4.11it/s] 54%|█████▍    | 150/279 [00:39<00:31,  4.11it/s] 54%|█████▍    | 151/279 [00:39<00:31,  4.10it/s] 54%|█████▍    | 152/279 [00:40<00:31,  4.09it/s] 55%|█████▍    | 153/279 [00:40<00:30,  4.10it/s] 55%|█████▌    | 154/279 [00:40<00:30,  4.10it/s] 56%|█████▌    | 155/279 [00:40<00:30,  4.10it/s] 56%|█████▌    | 156/279 [00:41<00:30,  4.09it/s] 56%|█████▋    | 157/279 [00:41<00:29,  4.09it/s] 57%|█████▋    | 158/279 [00:41<00:29,  4.10it/s] 57%|█████▋    | 159/279 [00:41<00:29,  4.10it/s] 57%|█████▋    | 160/279 [00:42<00:29,  4.09it/s] 58%|█████▊    | 161/279 [00:42<00:28,  4.08it/s] 58%|█████▊    | 162/279 [00:42<00:28,  4.07it/s] 58%|█████▊    | 163/279 [00:42<00:29,  3.94it/s] 59%|█████▉    | 164/279 [00:43<00:29,  3.83it/s] 59%|█████▉    | 165/279 [00:43<00:29,  3.91it/s] 59%|█████▉    | 166/279 [00:43<00:29,  3.84it/s] 60%|█████▉    | 167/279 [00:43<00:29,  3.77it/s] 60%|██████    | 168/279 [00:44<00:29,  3.70it/s] 61%|██████    | 169/279 [00:44<00:28,  3.81it/s] 61%|██████    | 170/279 [00:44<00:28,  3.77it/s] 61%|██████▏   | 171/279 [00:45<00:29,  3.72it/s] 62%|██████▏   | 172/279 [00:45<00:29,  3.67it/s] 62%|██████▏   | 173/279 [00:45<00:28,  3.78it/s] 62%|██████▏   | 174/279 [00:45<00:28,  3.73it/s] 63%|██████▎   | 175/279 [00:46<00:28,  3.69it/s] 63%|██████▎   | 176/279 [00:46<00:28,  3.65it/s] 63%|██████▎   | 177/279 [00:46<00:27,  3.77it/s] 64%|██████▍   | 178/279 [00:46<00:27,  3.72it/s] 64%|██████▍   | 179/279 [00:47<00:27,  3.69it/s] 65%|██████▍   | 180/279 [00:47<00:27,  3.65it/s] 65%|██████▍   | 181/279 [00:47<00:25,  3.77it/s] 65%|██████▌   | 182/279 [00:47<00:26,  3.73it/s] 66%|██████▌   | 183/279 [00:48<00:25,  3.70it/s] 66%|██████▌   | 184/279 [00:48<00:25,  3.66it/s] 66%|██████▋   | 185/279 [00:48<00:24,  3.79it/s] 67%|██████▋   | 186/279 [00:49<00:24,  3.74it/s] 67%|██████▋   | 187/279 [00:49<00:24,  3.71it/s] 67%|██████▋   | 188/279 [00:49<00:24,  3.66it/s] 68%|██████▊   | 189/279 [00:49<00:23,  3.78it/s] 68%|██████▊   | 190/279 [00:50<00:23,  3.74it/s] 68%|██████▊   | 191/279 [00:50<00:23,  3.70it/s] 69%|██████▉   | 192/279 [00:50<00:23,  3.65it/s] 69%|██████▉   | 193/279 [00:50<00:22,  3.77it/s] 70%|██████▉   | 194/279 [00:51<00:22,  3.74it/s] 70%|██████▉   | 195/279 [00:51<00:22,  3.70it/s] 70%|███████   | 196/279 [00:51<00:22,  3.64it/s] 71%|███████   | 197/279 [00:52<00:21,  3.76it/s] 71%|███████   | 198/279 [00:52<00:21,  3.73it/s] 71%|███████▏  | 199/279 [00:52<00:21,  3.69it/s] 72%|███████▏  | 200/279 [00:52<00:21,  3.65it/s] 72%|███████▏  | 201/279 [00:53<00:20,  3.76it/s] 72%|███████▏  | 202/279 [00:53<00:20,  3.72it/s] 73%|███████▎  | 203/279 [00:53<00:20,  3.68it/s] 73%|███████▎  | 204/279 [00:53<00:20,  3.64it/s] 73%|███████▎  | 205/279 [00:54<00:19,  3.76it/s] 74%|███████▍  | 206/279 [00:54<00:19,  3.73it/s] 74%|███████▍  | 207/279 [00:54<00:19,  3.69it/s] 75%|███████▍  | 208/279 [00:54<00:19,  3.64it/s] 75%|███████▍  | 209/279 [00:55<00:18,  3.76it/s] 75%|███████▌  | 210/279 [00:55<00:18,  3.72it/s] 76%|███████▌  | 211/279 [00:55<00:18,  3.69it/s] 76%|███████▌  | 212/279 [00:56<00:18,  3.64it/s] 76%|███████▋  | 213/279 [00:56<00:17,  3.76it/s] 77%|███████▋  | 214/279 [00:56<00:17,  3.73it/s] 77%|███████▋  | 215/279 [00:56<00:17,  3.69it/s] 77%|███████▋  | 216/279 [00:57<00:17,  3.64it/s] 78%|███████▊  | 217/279 [00:57<00:16,  3.76it/s] 78%|███████▊  | 218/279 [00:57<00:16,  3.73it/s] 78%|███████▊  | 219/279 [00:57<00:16,  3.69it/s] 79%|███████▉  | 220/279 [00:58<00:16,  3.64it/s] 79%|███████▉  | 221/279 [00:58<00:15,  3.76it/s] 80%|███████▉  | 222/279 [00:58<00:15,  3.74it/s] 80%|███████▉  | 223/279 [00:59<00:15,  3.71it/s] 80%|████████  | 224/279 [00:59<00:15,  3.62it/s] 81%|████████  | 225/279 [00:59<00:14,  3.75it/s] 81%|████████  | 226/279 [00:59<00:14,  3.73it/s] 81%|████████▏ | 227/279 [01:00<00:14,  3.69it/s] 82%|████████▏ | 228/279 [01:00<00:13,  3.64it/s] 82%|████████▏ | 229/279 [01:00<00:13,  3.76it/s] 82%|████████▏ | 230/279 [01:00<00:13,  3.73it/s] 83%|████████▎ | 231/279 [01:01<00:13,  3.69it/s] 83%|████████▎ | 232/279 [01:01<00:12,  3.64it/s] 84%|████████▎ | 233/279 [01:01<00:12,  3.76it/s] 84%|████████▍ | 234/279 [01:01<00:12,  3.73it/s] 84%|████████▍ | 235/279 [01:02<00:11,  3.69it/s] 85%|████████▍ | 236/279 [01:02<00:11,  3.65it/s] 85%|████████▍ | 237/279 [01:02<00:11,  3.76it/s] 85%|████████▌ | 238/279 [01:03<00:11,  3.72it/s] 86%|████████▌ | 239/279 [01:03<00:10,  3.69it/s] 86%|████████▌ | 240/279 [01:03<00:10,  3.64it/s] 86%|████████▋ | 241/279 [01:03<00:10,  3.76it/s] 87%|████████▋ | 242/279 [01:04<00:09,  3.72it/s] 87%|████████▋ | 243/279 [01:04<00:09,  3.69it/s] 87%|████████▋ | 244/279 [01:04<00:09,  3.64it/s] 88%|████████▊ | 245/279 [01:04<00:09,  3.76it/s] 88%|████████▊ | 246/279 [01:05<00:08,  3.73it/s] 89%|████████▊ | 247/279 [01:05<00:08,  3.69it/s] 89%|████████▉ | 248/279 [01:05<00:08,  3.65it/s] 89%|████████▉ | 249/279 [01:06<00:07,  3.76it/s] 90%|████████▉ | 250/279 [01:06<00:07,  3.74it/s] 90%|████████▉ | 251/279 [01:06<00:07,  3.69it/s] 90%|█████████ | 252/279 [01:06<00:07,  3.64it/s] 91%|█████████ | 253/279 [01:07<00:06,  3.76it/s] 91%|█████████ | 254/279 [01:07<00:06,  3.72it/s] 91%|█████████▏| 255/279 [01:07<00:06,  3.68it/s] 92%|█████████▏| 256/279 [01:07<00:06,  3.64it/s] 92%|█████████▏| 257/279 [01:08<00:05,  3.76it/s] 92%|█████████▏| 258/279 [01:08<00:05,  3.74it/s] 93%|█████████▎| 259/279 [01:08<00:05,  3.71it/s] 93%|█████████▎| 260/279 [01:09<00:05,  3.70it/s] 94%|█████████▎| 261/279 [01:09<00:04,  3.72it/s] 94%|█████████▍| 262/279 [01:09<00:04,  3.74it/s] 94%|█████████▍| 263/279 [01:09<00:04,  3.70it/s] 95%|█████████▍| 264/279 [01:10<00:04,  3.69it/s] 95%|█████████▍| 265/279 [01:10<00:03,  3.71it/s] 95%|█████████▌| 266/279 [01:10<00:03,  3.73it/s] 96%|█████████▌| 267/279 [01:10<00:03,  3.69it/s] 96%|█████████▌| 268/279 [01:11<00:02,  3.69it/s] 96%|█████████▋| 269/279 [01:11<00:02,  3.69it/s] 97%|█████████▋| 270/279 [01:11<00:02,  3.73it/s] 97%|█████████▋| 271/279 [01:11<00:02,  3.68it/s] 97%|█████████▋| 272/279 [01:12<00:01,  3.68it/s] 98%|█████████▊| 273/279 [01:12<00:01,  3.68it/s] 98%|█████████▊| 274/279 [01:12<00:01,  3.73it/s] 99%|█████████▊| 275/279 [01:13<00:01,  3.69it/s] 99%|█████████▉| 276/279 [01:13<00:00,  3.68it/s] 99%|█████████▉| 277/279 [01:13<00:00,  3.69it/s]100%|█████████▉| 278/279 [01:13<00:00,  3.73it/s]100%|██████████| 279/279 [01:14<00:00,  3.69it/s]accuracy:  0.5197132616487455
100%|██████████| 279/279 [01:18<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 241
Training dataset size: 192, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 192, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.53s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:45:53,172] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it][93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 04:45:54,215] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 04:45:54,344] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:07,  2.18s/it]  6%|▋         | 2/32 [00:04<01:06,  2.21s/it]  9%|▉         | 3/32 [00:07<01:11,  2.48s/it] 12%|█▎        | 4/32 [00:09<01:11,  2.54s/it] 16%|█▌        | 5/32 [00:12<01:08,  2.53s/it] 19%|█▉        | 6/32 [00:14<01:05,  2.53s/it] 22%|██▏       | 7/32 [00:17<01:03,  2.55s/it] 25%|██▌       | 8/32 [00:19<00:57,  2.40s/it] 28%|██▊       | 9/32 [00:21<00:55,  2.41s/it] 31%|███▏      | 10/32 [00:24<00:53,  2.45s/it]                                               {'loss': 0.3343, 'grad_norm': 1.1027390956878662, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:24<00:53,  2.45s/it] 34%|███▍      | 11/32 [00:26<00:51,  2.44s/it] 38%|███▊      | 12/32 [00:28<00:45,  2.26s/it] 41%|████      | 13/32 [00:31<00:45,  2.38s/it] 44%|████▍     | 14/32 [00:33<00:41,  2.30s/it] 47%|████▋     | 15/32 [00:36<00:42,  2.49s/it] 50%|█████     | 16/32 [00:38<00:37,  2.36s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:41<00:36,  2.43s/it] 56%|█████▋    | 18/32 [00:43<00:34,  2.46s/it] 59%|█████▉    | 19/32 [00:46<00:32,  2.48s/it] 62%|██████▎   | 20/32 [00:48<00:29,  2.50s/it]                                               {'loss': 0.3538, 'grad_norm': 1.1433966159820557, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:48<00:29,  2.50s/it] 66%|██████▌   | 21/32 [00:50<00:26,  2.41s/it] 69%|██████▉   | 22/32 [00:53<00:23,  2.36s/it] 72%|███████▏  | 23/32 [00:55<00:22,  2.49s/it] 75%|███████▌  | 24/32 [00:58<00:19,  2.45s/it] 78%|███████▊  | 25/32 [01:00<00:16,  2.40s/it] 81%|████████▏ | 26/32 [01:02<00:13,  2.31s/it] 84%|████████▍ | 27/32 [01:05<00:11,  2.33s/it] 88%|████████▊ | 28/32 [01:07<00:09,  2.35s/it] 91%|█████████ | 29/32 [01:10<00:07,  2.45s/it] 94%|█████████▍| 30/32 [01:12<00:04,  2.48s/it]                                               {'loss': 0.1789, 'grad_norm': 5.027912616729736, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:12<00:04,  2.48s/it] 97%|█████████▋| 31/32 [01:15<00:02,  2.63s/it]100%|██████████| 32/32 [01:17<00:00,  2.53s/it]                                               {'train_runtime': 78.6786, 'train_samples_per_second': 4.881, 'train_steps_per_second': 0.407, 'train_loss': 0.2809368036687374, 'epoch': 2.0}
100%|██████████| 32/32 [01:18<00:00,  2.53s/it]100%|██████████| 32/32 [01:18<00:00,  2.45s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:33,  2.89it/s]  1%|          | 2/272 [00:00<01:18,  3.43it/s]  1%|          | 3/272 [00:00<01:16,  3.53it/s]  1%|▏         | 4/272 [00:01<01:12,  3.69it/s]  2%|▏         | 5/272 [00:01<01:12,  3.66it/s]  2%|▏         | 6/272 [00:01<01:11,  3.74it/s]  3%|▎         | 7/272 [00:01<01:11,  3.72it/s]  3%|▎         | 8/272 [00:02<01:09,  3.79it/s]  3%|▎         | 9/272 [00:02<01:10,  3.72it/s]  4%|▎         | 10/272 [00:02<01:09,  3.78it/s]  4%|▍         | 11/272 [00:02<01:09,  3.73it/s]  4%|▍         | 12/272 [00:03<01:08,  3.80it/s]  5%|▍         | 13/272 [00:03<01:09,  3.74it/s]  5%|▌         | 14/272 [00:03<01:08,  3.78it/s]  6%|▌         | 15/272 [00:04<01:08,  3.74it/s]  6%|▌         | 16/272 [00:04<01:08,  3.76it/s]  6%|▋         | 17/272 [00:04<01:08,  3.75it/s]  7%|▋         | 18/272 [00:04<01:06,  3.79it/s]  7%|▋         | 19/272 [00:05<01:07,  3.75it/s]  7%|▋         | 20/272 [00:05<01:07,  3.76it/s]  8%|▊         | 21/272 [00:05<01:04,  3.87it/s]  8%|▊         | 22/272 [00:05<01:03,  3.95it/s]  8%|▊         | 23/272 [00:06<01:02,  4.01it/s]  9%|▉         | 24/272 [00:06<01:01,  4.05it/s]  9%|▉         | 25/272 [00:06<01:00,  4.08it/s] 10%|▉         | 26/272 [00:06<00:59,  4.11it/s] 10%|▉         | 27/272 [00:07<00:59,  4.13it/s] 10%|█         | 28/272 [00:07<00:59,  4.13it/s] 11%|█         | 29/272 [00:07<00:58,  4.15it/s] 11%|█         | 30/272 [00:07<00:58,  4.16it/s] 11%|█▏        | 31/272 [00:08<00:57,  4.17it/s] 12%|█▏        | 32/272 [00:08<00:57,  4.17it/s] 12%|█▏        | 33/272 [00:08<00:57,  4.17it/s] 12%|█▎        | 34/272 [00:08<00:57,  4.17it/s] 13%|█▎        | 35/272 [00:08<00:56,  4.17it/s] 13%|█▎        | 36/272 [00:09<00:56,  4.17it/s] 14%|█▎        | 37/272 [00:09<00:56,  4.17it/s] 14%|█▍        | 38/272 [00:09<00:56,  4.17it/s] 14%|█▍        | 39/272 [00:09<00:55,  4.17it/s] 15%|█▍        | 40/272 [00:10<00:55,  4.17it/s] 15%|█▌        | 41/272 [00:10<00:55,  4.17it/s] 15%|█▌        | 42/272 [00:10<00:55,  4.17it/s] 16%|█▌        | 43/272 [00:10<00:54,  4.17it/s] 16%|█▌        | 44/272 [00:11<00:54,  4.16it/s] 17%|█▋        | 45/272 [00:11<00:54,  4.17it/s] 17%|█▋        | 46/272 [00:11<00:54,  4.17it/s] 17%|█▋        | 47/272 [00:11<00:54,  4.16it/s] 18%|█▊        | 48/272 [00:12<00:53,  4.16it/s] 18%|█▊        | 49/272 [00:12<00:53,  4.16it/s] 18%|█▊        | 50/272 [00:12<00:53,  4.17it/s] 19%|█▉        | 51/272 [00:12<00:53,  4.17it/s] 19%|█▉        | 52/272 [00:13<00:52,  4.17it/s] 19%|█▉        | 53/272 [00:13<00:52,  4.17it/s] 20%|█▉        | 54/272 [00:13<00:52,  4.17it/s] 20%|██        | 55/272 [00:13<00:52,  4.17it/s] 21%|██        | 56/272 [00:14<00:51,  4.17it/s] 21%|██        | 57/272 [00:14<00:52,  4.08it/s] 21%|██▏       | 58/272 [00:14<00:53,  3.98it/s] 22%|██▏       | 59/272 [00:14<00:54,  3.90it/s] 22%|██▏       | 60/272 [00:15<00:54,  3.88it/s] 22%|██▏       | 61/272 [00:15<00:54,  3.88it/s] 23%|██▎       | 62/272 [00:15<00:55,  3.78it/s] 23%|██▎       | 63/272 [00:15<00:54,  3.81it/s] 24%|██▎       | 64/272 [00:16<00:55,  3.74it/s] 24%|██▍       | 65/272 [00:16<00:54,  3.79it/s] 24%|██▍       | 66/272 [00:16<00:55,  3.72it/s] 25%|██▍       | 67/272 [00:16<00:54,  3.77it/s] 25%|██▌       | 68/272 [00:17<00:54,  3.72it/s] 25%|██▌       | 69/272 [00:17<00:53,  3.79it/s] 26%|██▌       | 70/272 [00:17<00:54,  3.71it/s] 26%|██▌       | 71/272 [00:18<00:53,  3.76it/s] 26%|██▋       | 72/272 [00:18<00:54,  3.70it/s] 27%|██▋       | 73/272 [00:18<00:53,  3.75it/s] 27%|██▋       | 74/272 [00:18<00:53,  3.69it/s] 28%|██▊       | 75/272 [00:19<00:52,  3.74it/s] 28%|██▊       | 76/272 [00:19<00:53,  3.69it/s] 28%|██▊       | 77/272 [00:19<00:52,  3.75it/s] 29%|██▊       | 78/272 [00:19<00:52,  3.68it/s] 29%|██▉       | 79/272 [00:20<00:51,  3.73it/s] 29%|██▉       | 80/272 [00:20<00:52,  3.69it/s] 30%|██▉       | 81/272 [00:20<00:50,  3.76it/s] 30%|███       | 82/272 [00:20<00:51,  3.69it/s] 31%|███       | 83/272 [00:21<00:50,  3.75it/s] 31%|███       | 84/272 [00:21<00:50,  3.70it/s] 31%|███▏      | 85/272 [00:21<00:49,  3.77it/s] 32%|███▏      | 86/272 [00:22<00:50,  3.69it/s] 32%|███▏      | 87/272 [00:22<00:49,  3.74it/s] 32%|███▏      | 88/272 [00:22<00:49,  3.69it/s] 33%|███▎      | 89/272 [00:22<00:48,  3.75it/s] 33%|███▎      | 90/272 [00:23<00:49,  3.68it/s] 33%|███▎      | 91/272 [00:23<00:48,  3.74it/s] 34%|███▍      | 92/272 [00:23<00:48,  3.69it/s] 34%|███▍      | 93/272 [00:23<00:47,  3.75it/s] 35%|███▍      | 94/272 [00:24<00:48,  3.67it/s] 35%|███▍      | 95/272 [00:24<00:47,  3.73it/s] 35%|███▌      | 96/272 [00:24<00:47,  3.68it/s] 36%|███▌      | 97/272 [00:25<00:46,  3.75it/s] 36%|███▌      | 98/272 [00:25<00:47,  3.68it/s] 36%|███▋      | 99/272 [00:25<00:46,  3.73it/s] 37%|███▋      | 100/272 [00:25<00:46,  3.68it/s] 37%|███▋      | 101/272 [00:26<00:45,  3.75it/s] 38%|███▊      | 102/272 [00:26<00:46,  3.67it/s] 38%|███▊      | 103/272 [00:26<00:45,  3.68it/s] 38%|███▊      | 104/272 [00:26<00:44,  3.79it/s] 39%|███▊      | 105/272 [00:27<00:43,  3.88it/s] 39%|███▉      | 106/272 [00:27<00:42,  3.95it/s] 39%|███▉      | 107/272 [00:27<00:41,  4.00it/s] 40%|███▉      | 108/272 [00:27<00:40,  4.04it/s] 40%|████      | 109/272 [00:28<00:40,  4.07it/s] 40%|████      | 110/272 [00:28<00:39,  4.09it/s] 41%|████      | 111/272 [00:28<00:39,  4.10it/s] 41%|████      | 112/272 [00:28<00:39,  4.10it/s] 42%|████▏     | 113/272 [00:29<00:38,  4.10it/s] 42%|████▏     | 114/272 [00:29<00:38,  4.11it/s] 42%|████▏     | 115/272 [00:29<00:38,  4.11it/s] 43%|████▎     | 116/272 [00:29<00:37,  4.12it/s] 43%|████▎     | 117/272 [00:30<00:37,  4.13it/s] 43%|████▎     | 118/272 [00:30<00:37,  4.13it/s] 44%|████▍     | 119/272 [00:30<00:37,  4.13it/s] 44%|████▍     | 120/272 [00:30<00:36,  4.12it/s] 44%|████▍     | 121/272 [00:31<00:36,  4.12it/s] 45%|████▍     | 122/272 [00:31<00:36,  4.12it/s] 45%|████▌     | 123/272 [00:31<00:36,  4.13it/s] 46%|████▌     | 124/272 [00:31<00:35,  4.13it/s] 46%|████▌     | 125/272 [00:31<00:35,  4.14it/s] 46%|████▋     | 126/272 [00:32<00:35,  4.14it/s] 47%|████▋     | 127/272 [00:32<00:35,  4.14it/s] 47%|████▋     | 128/272 [00:32<00:34,  4.14it/s] 47%|████▋     | 129/272 [00:32<00:34,  4.13it/s] 48%|████▊     | 130/272 [00:33<00:34,  4.12it/s] 48%|████▊     | 131/272 [00:33<00:34,  4.12it/s] 49%|████▊     | 132/272 [00:33<00:33,  4.12it/s] 49%|████▉     | 133/272 [00:33<00:33,  4.13it/s] 49%|████▉     | 134/272 [00:34<00:33,  4.13it/s] 50%|████▉     | 135/272 [00:34<00:33,  4.13it/s] 50%|█████     | 136/272 [00:34<00:32,  4.13it/s] 50%|█████     | 137/272 [00:34<00:32,  4.12it/s] 51%|█████     | 138/272 [00:35<00:32,  4.11it/s] 51%|█████     | 139/272 [00:35<00:32,  4.11it/s] 51%|█████▏    | 140/272 [00:35<00:32,  4.12it/s] 52%|█████▏    | 141/272 [00:35<00:31,  4.12it/s] 52%|█████▏    | 142/272 [00:36<00:31,  4.13it/s] 53%|█████▎    | 143/272 [00:36<00:31,  4.13it/s] 53%|█████▎    | 144/272 [00:36<00:31,  4.13it/s] 53%|█████▎    | 145/272 [00:36<00:31,  4.04it/s] 54%|█████▎    | 146/272 [00:37<00:31,  3.95it/s] 54%|█████▍    | 147/272 [00:37<00:32,  3.87it/s] 54%|█████▍    | 148/272 [00:37<00:32,  3.86it/s] 55%|█████▍    | 149/272 [00:37<00:31,  3.86it/s] 55%|█████▌    | 150/272 [00:38<00:32,  3.78it/s] 56%|█████▌    | 151/272 [00:38<00:31,  3.80it/s] 56%|█████▌    | 152/272 [00:38<00:32,  3.74it/s] 56%|█████▋    | 153/272 [00:38<00:31,  3.80it/s] 57%|█████▋    | 154/272 [00:39<00:31,  3.73it/s] 57%|█████▋    | 155/272 [00:39<00:31,  3.76it/s] 57%|█████▋    | 156/272 [00:39<00:31,  3.70it/s] 58%|█████▊    | 157/272 [00:40<00:30,  3.78it/s] 58%|█████▊    | 158/272 [00:40<00:30,  3.74it/s] 58%|█████▊    | 159/272 [00:40<00:30,  3.71it/s] 59%|█████▉    | 160/272 [00:40<00:30,  3.65it/s] 59%|█████▉    | 161/272 [00:41<00:29,  3.78it/s] 60%|█████▉    | 162/272 [00:41<00:29,  3.75it/s] 60%|█████▉    | 163/272 [00:41<00:29,  3.71it/s] 60%|██████    | 164/272 [00:41<00:29,  3.69it/s] 61%|██████    | 165/272 [00:42<00:28,  3.76it/s] 61%|██████    | 166/272 [00:42<00:27,  3.79it/s] 61%|██████▏   | 167/272 [00:42<00:28,  3.71it/s] 62%|██████▏   | 168/272 [00:42<00:27,  3.76it/s] 62%|██████▏   | 169/272 [00:43<00:27,  3.70it/s] 62%|██████▎   | 170/272 [00:43<00:27,  3.75it/s] 63%|██████▎   | 171/272 [00:43<00:27,  3.69it/s] 63%|██████▎   | 172/272 [00:44<00:26,  3.73it/s] 64%|██████▎   | 173/272 [00:44<00:26,  3.69it/s] 64%|██████▍   | 174/272 [00:44<00:26,  3.76it/s] 64%|██████▍   | 175/272 [00:44<00:26,  3.69it/s] 65%|██████▍   | 176/272 [00:45<00:25,  3.73it/s] 65%|██████▌   | 177/272 [00:45<00:25,  3.68it/s] 65%|██████▌   | 178/272 [00:45<00:25,  3.75it/s] 66%|██████▌   | 179/272 [00:45<00:25,  3.68it/s] 66%|██████▌   | 180/272 [00:46<00:24,  3.73it/s] 67%|██████▋   | 181/272 [00:46<00:24,  3.68it/s] 67%|██████▋   | 182/272 [00:46<00:24,  3.74it/s] 67%|██████▋   | 183/272 [00:47<00:24,  3.68it/s] 68%|██████▊   | 184/272 [00:47<00:23,  3.72it/s] 68%|██████▊   | 185/272 [00:47<00:23,  3.67it/s] 68%|██████▊   | 186/272 [00:47<00:22,  3.75it/s] 69%|██████▉   | 187/272 [00:48<00:23,  3.69it/s] 69%|██████▉   | 188/272 [00:48<00:22,  3.73it/s] 69%|██████▉   | 189/272 [00:48<00:22,  3.68it/s] 70%|██████▉   | 190/272 [00:48<00:21,  3.75it/s] 70%|███████   | 191/272 [00:49<00:21,  3.69it/s] 71%|███████   | 192/272 [00:49<00:21,  3.72it/s] 71%|███████   | 193/272 [00:49<00:21,  3.68it/s] 71%|███████▏  | 194/272 [00:50<00:20,  3.75it/s] 72%|███████▏  | 195/272 [00:50<00:20,  3.70it/s] 72%|███████▏  | 196/272 [00:50<00:20,  3.74it/s] 72%|███████▏  | 197/272 [00:50<00:20,  3.69it/s] 73%|███████▎  | 198/272 [00:51<00:19,  3.77it/s] 73%|███████▎  | 199/272 [00:51<00:19,  3.72it/s] 74%|███████▎  | 200/272 [00:51<00:19,  3.69it/s] 74%|███████▍  | 201/272 [00:51<00:19,  3.65it/s] 74%|███████▍  | 202/272 [00:52<00:18,  3.76it/s] 75%|███████▍  | 203/272 [00:52<00:18,  3.71it/s] 75%|███████▌  | 204/272 [00:52<00:18,  3.69it/s] 75%|███████▌  | 205/272 [00:52<00:18,  3.64it/s] 76%|███████▌  | 206/272 [00:53<00:17,  3.77it/s] 76%|███████▌  | 207/272 [00:53<00:17,  3.76it/s] 76%|███████▋  | 208/272 [00:53<00:17,  3.69it/s] 77%|███████▋  | 209/272 [00:54<00:16,  3.75it/s] 77%|███████▋  | 210/272 [00:54<00:16,  3.68it/s] 78%|███████▊  | 211/272 [00:54<00:16,  3.73it/s] 78%|███████▊  | 212/272 [00:54<00:16,  3.67it/s] 78%|███████▊  | 213/272 [00:55<00:15,  3.71it/s] 79%|███████▊  | 214/272 [00:55<00:15,  3.67it/s] 79%|███████▉  | 215/272 [00:55<00:15,  3.74it/s] 79%|███████▉  | 216/272 [00:55<00:15,  3.67it/s] 80%|███████▉  | 217/272 [00:56<00:14,  3.72it/s] 80%|████████  | 218/272 [00:56<00:14,  3.67it/s] 81%|████████  | 219/272 [00:56<00:14,  3.74it/s] 81%|████████  | 220/272 [00:57<00:14,  3.68it/s] 81%|████████▏ | 221/272 [00:57<00:13,  3.72it/s] 82%|████████▏ | 222/272 [00:57<00:13,  3.68it/s] 82%|████████▏ | 223/272 [00:57<00:13,  3.75it/s] 82%|████████▏ | 224/272 [00:58<00:13,  3.69it/s] 83%|████████▎ | 225/272 [00:58<00:12,  3.72it/s] 83%|████████▎ | 226/272 [00:58<00:12,  3.68it/s] 83%|████████▎ | 227/272 [00:58<00:12,  3.74it/s] 84%|████████▍ | 228/272 [00:59<00:11,  3.68it/s] 84%|████████▍ | 229/272 [00:59<00:11,  3.72it/s] 85%|████████▍ | 230/272 [00:59<00:11,  3.67it/s] 85%|████████▍ | 231/272 [00:59<00:10,  3.74it/s] 85%|████████▌ | 232/272 [01:00<00:10,  3.68it/s] 86%|████████▌ | 233/272 [01:00<00:10,  3.72it/s] 86%|████████▌ | 234/272 [01:00<00:10,  3.67it/s] 86%|████████▋ | 235/272 [01:01<00:09,  3.75it/s] 87%|████████▋ | 236/272 [01:01<00:09,  3.69it/s] 87%|████████▋ | 237/272 [01:01<00:09,  3.72it/s] 88%|████████▊ | 238/272 [01:01<00:09,  3.67it/s] 88%|████████▊ | 239/272 [01:02<00:08,  3.75it/s] 88%|████████▊ | 240/272 [01:02<00:08,  3.69it/s] 89%|████████▊ | 241/272 [01:02<00:08,  3.72it/s] 89%|████████▉ | 242/272 [01:02<00:08,  3.67it/s] 89%|████████▉ | 243/272 [01:03<00:07,  3.74it/s] 90%|████████▉ | 244/272 [01:03<00:07,  3.68it/s] 90%|█████████ | 245/272 [01:03<00:07,  3.72it/s] 90%|█████████ | 246/272 [01:04<00:07,  3.67it/s] 91%|█████████ | 247/272 [01:04<00:06,  3.74it/s] 91%|█████████ | 248/272 [01:04<00:06,  3.68it/s] 92%|█████████▏| 249/272 [01:04<00:06,  3.72it/s] 92%|█████████▏| 250/272 [01:05<00:05,  3.67it/s] 92%|█████████▏| 251/272 [01:05<00:05,  3.74it/s] 93%|█████████▎| 252/272 [01:05<00:05,  3.67it/s] 93%|█████████▎| 253/272 [01:05<00:05,  3.71it/s] 93%|█████████▎| 254/272 [01:06<00:04,  3.67it/s] 94%|█████████▍| 255/272 [01:06<00:04,  3.74it/s] 94%|█████████▍| 256/272 [01:06<00:04,  3.67it/s] 94%|█████████▍| 257/272 [01:07<00:04,  3.72it/s] 95%|█████████▍| 258/272 [01:07<00:03,  3.67it/s] 95%|█████████▌| 259/272 [01:07<00:03,  3.76it/s] 96%|█████████▌| 260/272 [01:07<00:03,  3.69it/s] 96%|█████████▌| 261/272 [01:08<00:02,  3.72it/s] 96%|█████████▋| 262/272 [01:08<00:02,  3.67it/s] 97%|█████████▋| 263/272 [01:08<00:02,  3.75it/s] 97%|█████████▋| 264/272 [01:08<00:02,  3.68it/s] 97%|█████████▋| 265/272 [01:09<00:01,  3.71it/s] 98%|█████████▊| 266/272 [01:09<00:01,  3.66it/s] 98%|█████████▊| 267/272 [01:09<00:01,  3.75it/s] 99%|█████████▊| 268/272 [01:09<00:01,  3.70it/s] 99%|█████████▉| 269/272 [01:10<00:00,  3.68it/s] 99%|█████████▉| 270/272 [01:10<00:00,  3.63it/s]100%|█████████▉| 271/272 [01:10<00:00,  3.76it/s]100%|██████████| 272/272 [01:11<00:00,  3.71it/s]accuracy:  0.9007352941176471
100%|██████████| 272/272 [01:15<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 149
Training dataset size: 192, validation dataset size: 149
Training dataset size: 192, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[2025-03-12 04:48:55,751] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:48:55,938] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.58s/it][93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.45s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:48:57,738] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:21,  2.64s/it]  6%|▋         | 2/32 [00:05<01:15,  2.51s/it]  9%|▉         | 3/32 [00:07<01:09,  2.39s/it] 12%|█▎        | 4/32 [00:10<01:10,  2.53s/it] 16%|█▌        | 5/32 [00:12<01:05,  2.42s/it] 19%|█▉        | 6/32 [00:15<01:05,  2.53s/it] 22%|██▏       | 7/32 [00:17<01:01,  2.46s/it] 25%|██▌       | 8/32 [00:20<01:01,  2.58s/it] 28%|██▊       | 9/32 [00:23<01:02,  2.71s/it] 31%|███▏      | 10/32 [00:25<00:57,  2.61s/it]                                               {'loss': 0.721, 'grad_norm': 3.6622633934020996, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:25<00:57,  2.61s/it] 34%|███▍      | 11/32 [00:28<00:56,  2.68s/it] 38%|███▊      | 12/32 [00:30<00:50,  2.52s/it] 41%|████      | 13/32 [00:33<00:50,  2.66s/it] 44%|████▍     | 14/32 [00:35<00:46,  2.60s/it] 47%|████▋     | 15/32 [00:38<00:41,  2.45s/it] 50%|█████     | 16/32 [00:40<00:39,  2.50s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:43<00:39,  2.64s/it] 56%|█████▋    | 18/32 [00:45<00:35,  2.53s/it] 59%|█████▉    | 19/32 [00:48<00:33,  2.60s/it] 62%|██████▎   | 20/32 [00:50<00:29,  2.46s/it]                                               {'loss': 0.5926, 'grad_norm': 1.9354194402694702, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:50<00:29,  2.46s/it] 66%|██████▌   | 21/32 [00:53<00:26,  2.45s/it] 69%|██████▉   | 22/32 [00:55<00:24,  2.47s/it] 72%|███████▏  | 23/32 [00:57<00:21,  2.38s/it] 75%|███████▌  | 24/32 [01:00<00:19,  2.41s/it] 78%|███████▊  | 25/32 [01:03<00:17,  2.55s/it] 81%|████████▏ | 26/32 [01:06<00:16,  2.72s/it] 84%|████████▍ | 27/32 [01:09<00:13,  2.78s/it] 88%|████████▊ | 28/32 [01:11<00:10,  2.59s/it] 91%|█████████ | 29/32 [01:13<00:07,  2.55s/it] 94%|█████████▍| 30/32 [01:16<00:05,  2.57s/it]                                               {'loss': 0.454, 'grad_norm': 1.497419834136963, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:16<00:05,  2.57s/it] 97%|█████████▋| 31/32 [01:19<00:02,  2.72s/it]100%|██████████| 32/32 [01:21<00:00,  2.59s/it]                                               {'train_runtime': 82.5325, 'train_samples_per_second': 4.653, 'train_steps_per_second': 0.388, 'train_loss': 0.633974701166153, 'epoch': 2.0}
100%|██████████| 32/32 [01:22<00:00,  2.59s/it]100%|██████████| 32/32 [01:22<00:00,  2.57s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<00:56,  3.08it/s]  1%|          | 2/175 [00:00<00:49,  3.53it/s]  2%|▏         | 3/175 [00:00<00:47,  3.63it/s]  2%|▏         | 4/175 [00:01<00:45,  3.73it/s]  3%|▎         | 5/175 [00:01<00:46,  3.69it/s]  3%|▎         | 6/175 [00:01<00:44,  3.83it/s]  4%|▍         | 7/175 [00:01<00:44,  3.81it/s]  5%|▍         | 8/175 [00:02<00:43,  3.80it/s]  5%|▌         | 9/175 [00:02<00:43,  3.84it/s]  6%|▌         | 10/175 [00:02<00:43,  3.76it/s]  6%|▋         | 11/175 [00:02<00:42,  3.87it/s]  7%|▋         | 12/175 [00:03<00:42,  3.85it/s]  7%|▋         | 13/175 [00:03<00:42,  3.82it/s]  8%|▊         | 14/175 [00:03<00:42,  3.80it/s]  9%|▊         | 15/175 [00:03<00:41,  3.89it/s]  9%|▉         | 16/175 [00:04<00:40,  3.96it/s] 10%|▉         | 17/175 [00:04<00:39,  4.02it/s] 10%|█         | 18/175 [00:04<00:38,  4.06it/s] 11%|█         | 19/175 [00:04<00:38,  4.09it/s] 11%|█▏        | 20/175 [00:05<00:37,  4.11it/s] 12%|█▏        | 21/175 [00:05<00:37,  4.12it/s] 13%|█▎        | 22/175 [00:05<00:36,  4.14it/s] 13%|█▎        | 23/175 [00:05<00:36,  4.15it/s] 14%|█▎        | 24/175 [00:06<00:36,  4.16it/s] 14%|█▍        | 25/175 [00:06<00:36,  4.16it/s] 15%|█▍        | 26/175 [00:06<00:35,  4.16it/s] 15%|█▌        | 27/175 [00:06<00:35,  4.16it/s] 16%|█▌        | 28/175 [00:07<00:35,  4.17it/s] 17%|█▋        | 29/175 [00:07<00:35,  4.16it/s] 17%|█▋        | 30/175 [00:07<00:34,  4.16it/s] 18%|█▊        | 31/175 [00:07<00:34,  4.16it/s] 18%|█▊        | 32/175 [00:08<00:34,  4.16it/s] 19%|█▉        | 33/175 [00:08<00:34,  4.17it/s] 19%|█▉        | 34/175 [00:08<00:33,  4.17it/s] 20%|██        | 35/175 [00:08<00:33,  4.17it/s] 21%|██        | 36/175 [00:09<00:34,  4.02it/s] 21%|██        | 37/175 [00:09<00:35,  3.92it/s] 22%|██▏       | 38/175 [00:09<00:34,  3.93it/s] 22%|██▏       | 39/175 [00:09<00:34,  3.97it/s] 23%|██▎       | 40/175 [00:10<00:34,  3.88it/s] 23%|██▎       | 41/175 [00:10<00:35,  3.82it/s] 24%|██▍       | 42/175 [00:10<00:35,  3.76it/s] 25%|██▍       | 43/175 [00:10<00:34,  3.87it/s] 25%|██▌       | 44/175 [00:11<00:33,  3.87it/s] 26%|██▌       | 45/175 [00:11<00:34,  3.81it/s] 26%|██▋       | 46/175 [00:11<00:33,  3.86it/s] 27%|██▋       | 47/175 [00:11<00:33,  3.77it/s] 27%|██▋       | 48/175 [00:12<00:33,  3.85it/s] 28%|██▊       | 49/175 [00:12<00:33,  3.80it/s] 29%|██▊       | 50/175 [00:12<00:33,  3.78it/s] 29%|██▉       | 51/175 [00:12<00:33,  3.72it/s] 30%|██▉       | 52/175 [00:13<00:31,  3.85it/s] 30%|███       | 53/175 [00:13<00:31,  3.85it/s] 31%|███       | 54/175 [00:13<00:31,  3.80it/s] 31%|███▏      | 55/175 [00:14<00:31,  3.84it/s] 32%|███▏      | 56/175 [00:14<00:31,  3.76it/s] 33%|███▎      | 57/175 [00:14<00:30,  3.85it/s] 33%|███▎      | 58/175 [00:14<00:30,  3.80it/s] 34%|███▎      | 59/175 [00:15<00:30,  3.77it/s] 34%|███▍      | 60/175 [00:15<00:31,  3.71it/s] 35%|███▍      | 61/175 [00:15<00:29,  3.83it/s] 35%|███▌      | 62/175 [00:15<00:29,  3.83it/s] 36%|███▌      | 63/175 [00:16<00:29,  3.78it/s] 37%|███▋      | 64/175 [00:16<00:28,  3.83it/s] 37%|███▋      | 65/175 [00:16<00:29,  3.76it/s] 38%|███▊      | 66/175 [00:16<00:28,  3.84it/s] 38%|███▊      | 67/175 [00:17<00:28,  3.78it/s] 39%|███▉      | 68/175 [00:17<00:28,  3.77it/s] 39%|███▉      | 69/175 [00:17<00:28,  3.70it/s] 40%|████      | 70/175 [00:17<00:27,  3.81it/s] 41%|████      | 71/175 [00:18<00:27,  3.82it/s] 41%|████      | 72/175 [00:18<00:27,  3.78it/s] 42%|████▏     | 73/175 [00:18<00:26,  3.82it/s] 42%|████▏     | 74/175 [00:19<00:26,  3.75it/s] 43%|████▎     | 75/175 [00:19<00:26,  3.83it/s] 43%|████▎     | 76/175 [00:19<00:26,  3.78it/s] 44%|████▍     | 77/175 [00:19<00:26,  3.76it/s] 45%|████▍     | 78/175 [00:20<00:26,  3.70it/s] 45%|████▌     | 79/175 [00:20<00:25,  3.82it/s] 46%|████▌     | 80/175 [00:20<00:24,  3.82it/s] 46%|████▋     | 81/175 [00:20<00:24,  3.78it/s] 47%|████▋     | 82/175 [00:21<00:24,  3.82it/s] 47%|████▋     | 83/175 [00:21<00:24,  3.74it/s] 48%|████▊     | 84/175 [00:21<00:23,  3.83it/s] 49%|████▊     | 85/175 [00:21<00:23,  3.78it/s] 49%|████▉     | 86/175 [00:22<00:23,  3.74it/s] 50%|████▉     | 87/175 [00:22<00:23,  3.69it/s] 50%|█████     | 88/175 [00:22<00:22,  3.80it/s] 51%|█████     | 89/175 [00:22<00:22,  3.81it/s] 51%|█████▏    | 90/175 [00:23<00:22,  3.77it/s] 52%|█████▏    | 91/175 [00:23<00:22,  3.81it/s] 53%|█████▎    | 92/175 [00:23<00:22,  3.74it/s] 53%|█████▎    | 93/175 [00:24<00:21,  3.83it/s] 54%|█████▎    | 94/175 [00:24<00:21,  3.76it/s] 54%|█████▍    | 95/175 [00:24<00:21,  3.76it/s] 55%|█████▍    | 96/175 [00:24<00:21,  3.69it/s] 55%|█████▌    | 97/175 [00:25<00:20,  3.81it/s] 56%|█████▌    | 98/175 [00:25<00:20,  3.82it/s] 57%|█████▋    | 99/175 [00:25<00:20,  3.77it/s] 57%|█████▋    | 100/175 [00:25<00:19,  3.80it/s] 58%|█████▊    | 101/175 [00:26<00:19,  3.72it/s] 58%|█████▊    | 102/175 [00:26<00:19,  3.80it/s] 59%|█████▉    | 103/175 [00:26<00:19,  3.75it/s] 59%|█████▉    | 104/175 [00:26<00:19,  3.74it/s] 60%|██████    | 105/175 [00:27<00:19,  3.68it/s] 61%|██████    | 106/175 [00:27<00:18,  3.80it/s] 61%|██████    | 107/175 [00:27<00:17,  3.79it/s] 62%|██████▏   | 108/175 [00:28<00:17,  3.75it/s] 62%|██████▏   | 109/175 [00:28<00:17,  3.80it/s] 63%|██████▎   | 110/175 [00:28<00:17,  3.73it/s] 63%|██████▎   | 111/175 [00:28<00:16,  3.80it/s] 64%|██████▍   | 112/175 [00:29<00:16,  3.75it/s] 65%|██████▍   | 113/175 [00:29<00:16,  3.74it/s] 65%|██████▌   | 114/175 [00:29<00:16,  3.69it/s] 66%|██████▌   | 115/175 [00:29<00:15,  3.81it/s] 66%|██████▋   | 116/175 [00:30<00:15,  3.80it/s] 67%|██████▋   | 117/175 [00:30<00:15,  3.77it/s] 67%|██████▋   | 118/175 [00:30<00:14,  3.81it/s] 68%|██████▊   | 119/175 [00:30<00:15,  3.73it/s] 69%|██████▊   | 120/175 [00:31<00:14,  3.79it/s] 69%|██████▉   | 121/175 [00:31<00:14,  3.76it/s] 70%|██████▉   | 122/175 [00:31<00:14,  3.76it/s] 70%|███████   | 123/175 [00:32<00:14,  3.70it/s] 71%|███████   | 124/175 [00:32<00:13,  3.82it/s] 71%|███████▏  | 125/175 [00:32<00:13,  3.77it/s] 72%|███████▏  | 126/175 [00:32<00:13,  3.74it/s] 73%|███████▎  | 127/175 [00:33<00:13,  3.68it/s] 73%|███████▎  | 128/175 [00:33<00:12,  3.80it/s] 74%|███████▎  | 129/175 [00:33<00:12,  3.81it/s] 74%|███████▍  | 130/175 [00:33<00:11,  3.77it/s] 75%|███████▍  | 131/175 [00:34<00:11,  3.80it/s] 75%|███████▌  | 132/175 [00:34<00:11,  3.73it/s] 76%|███████▌  | 133/175 [00:34<00:11,  3.79it/s] 77%|███████▋  | 134/175 [00:34<00:10,  3.75it/s] 77%|███████▋  | 135/175 [00:35<00:10,  3.73it/s] 78%|███████▊  | 136/175 [00:35<00:10,  3.68it/s] 78%|███████▊  | 137/175 [00:35<00:10,  3.80it/s] 79%|███████▉  | 138/175 [00:36<00:09,  3.77it/s] 79%|███████▉  | 139/175 [00:36<00:09,  3.74it/s] 80%|████████  | 140/175 [00:36<00:09,  3.67it/s] 81%|████████  | 141/175 [00:36<00:08,  3.80it/s] 81%|████████  | 142/175 [00:37<00:08,  3.81it/s] 82%|████████▏ | 143/175 [00:37<00:08,  3.77it/s] 82%|████████▏ | 144/175 [00:37<00:08,  3.80it/s] 83%|████████▎ | 145/175 [00:37<00:08,  3.72it/s] 83%|████████▎ | 146/175 [00:38<00:07,  3.80it/s] 84%|████████▍ | 147/175 [00:38<00:07,  3.75it/s] 85%|████████▍ | 148/175 [00:38<00:07,  3.73it/s] 85%|████████▌ | 149/175 [00:38<00:07,  3.67it/s] 86%|████████▌ | 150/175 [00:39<00:06,  3.79it/s] 86%|████████▋ | 151/175 [00:39<00:06,  3.79it/s] 87%|████████▋ | 152/175 [00:39<00:06,  3.76it/s] 87%|████████▋ | 153/175 [00:40<00:05,  3.77it/s] 88%|████████▊ | 154/175 [00:40<00:05,  3.73it/s] 89%|████████▊ | 155/175 [00:40<00:05,  3.80it/s] 89%|████████▉ | 156/175 [00:40<00:05,  3.75it/s] 90%|████████▉ | 157/175 [00:41<00:04,  3.76it/s] 90%|█████████ | 158/175 [00:41<00:04,  3.70it/s] 91%|█████████ | 159/175 [00:41<00:04,  3.81it/s] 91%|█████████▏| 160/175 [00:41<00:03,  3.76it/s] 92%|█████████▏| 161/175 [00:42<00:03,  3.72it/s] 93%|█████████▎| 162/175 [00:42<00:03,  3.67it/s] 93%|█████████▎| 163/175 [00:42<00:03,  3.79it/s] 94%|█████████▎| 164/175 [00:42<00:02,  3.81it/s] 94%|█████████▍| 165/175 [00:43<00:02,  3.76it/s] 95%|█████████▍| 166/175 [00:43<00:02,  3.79it/s] 95%|█████████▌| 167/175 [00:43<00:02,  3.71it/s] 96%|█████████▌| 168/175 [00:44<00:01,  3.79it/s] 97%|█████████▋| 169/175 [00:44<00:01,  3.74it/s] 97%|█████████▋| 170/175 [00:44<00:01,  3.73it/s] 98%|█████████▊| 171/175 [00:44<00:01,  3.67it/s] 98%|█████████▊| 172/175 [00:45<00:00,  3.79it/s] 99%|█████████▉| 173/175 [00:45<00:00,  3.77it/s] 99%|█████████▉| 174/175 [00:45<00:00,  3.76it/s]100%|██████████| 175/175 [00:45<00:00,  3.79it/s]accuracy:  0.8228571428571428
100%|██████████| 175/175 [00:48<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 192, validation dataset size: 119
Training dataset size: 192, validation dataset size: 119
Training dataset size: 192, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:51:40,369] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 04:51:40,511] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 04:51:40,594] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/32 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/32 [00:02<01:31,  2.96s/it]  6%|▋         | 2/32 [00:05<01:14,  2.50s/it]  9%|▉         | 3/32 [00:07<01:13,  2.52s/it] 12%|█▎        | 4/32 [00:10<01:11,  2.54s/it] 16%|█▌        | 5/32 [00:12<01:03,  2.34s/it] 19%|█▉        | 6/32 [00:15<01:06,  2.55s/it] 22%|██▏       | 7/32 [00:17<01:03,  2.54s/it] 25%|██▌       | 8/32 [00:20<01:00,  2.54s/it] 28%|██▊       | 9/32 [00:22<00:58,  2.54s/it] 31%|███▏      | 10/32 [00:25<00:57,  2.60s/it]                                               {'loss': 1.3553, 'grad_norm': 5.146208763122559, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 10/32 [00:25<00:57,  2.60s/it] 34%|███▍      | 11/32 [00:28<00:55,  2.66s/it] 38%|███▊      | 12/32 [00:31<00:54,  2.72s/it] 41%|████      | 13/32 [00:34<00:52,  2.77s/it] 44%|████▍     | 14/32 [00:36<00:49,  2.73s/it] 47%|████▋     | 15/32 [00:39<00:44,  2.62s/it] 50%|█████     | 16/32 [00:41<00:40,  2.53s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 53%|█████▎    | 17/32 [00:44<00:39,  2.66s/it] 56%|█████▋    | 18/32 [00:47<00:38,  2.73s/it] 59%|█████▉    | 19/32 [00:50<00:35,  2.76s/it] 62%|██████▎   | 20/32 [00:52<00:32,  2.74s/it]                                               {'loss': 1.1624, 'grad_norm': 10.116219520568848, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 20/32 [00:52<00:32,  2.74s/it] 66%|██████▌   | 21/32 [00:55<00:29,  2.69s/it] 69%|██████▉   | 22/32 [00:58<00:27,  2.78s/it] 72%|███████▏  | 23/32 [01:00<00:24,  2.75s/it] 75%|███████▌  | 24/32 [01:03<00:21,  2.65s/it] 78%|███████▊  | 25/32 [01:06<00:19,  2.74s/it] 81%|████████▏ | 26/32 [01:08<00:15,  2.64s/it] 84%|████████▍ | 27/32 [01:11<00:12,  2.57s/it] 88%|████████▊ | 28/32 [01:13<00:10,  2.64s/it] 91%|█████████ | 29/32 [01:16<00:08,  2.69s/it] 94%|█████████▍| 30/32 [01:19<00:05,  2.73s/it]                                               {'loss': 0.8791, 'grad_norm': 9.720551490783691, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 30/32 [01:19<00:05,  2.73s/it] 97%|█████████▋| 31/32 [01:21<00:02,  2.56s/it]100%|██████████| 32/32 [01:24<00:00,  2.48s/it]                                               {'train_runtime': 84.7202, 'train_samples_per_second': 4.533, 'train_steps_per_second': 0.378, 'train_loss': 1.0993886962532997, 'epoch': 2.0}
100%|██████████| 32/32 [01:24<00:00,  2.48s/it]100%|██████████| 32/32 [01:24<00:00,  2.64s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:47,  3.23it/s]  1%|▏         | 2/153 [00:00<00:40,  3.72it/s]  2%|▏         | 3/153 [00:00<00:38,  3.90it/s]  3%|▎         | 4/153 [00:01<00:37,  3.99it/s]  3%|▎         | 5/153 [00:01<00:36,  4.04it/s]  4%|▍         | 6/153 [00:01<00:36,  4.07it/s]  5%|▍         | 7/153 [00:01<00:35,  4.10it/s]  5%|▌         | 8/153 [00:01<00:35,  4.11it/s]  6%|▌         | 9/153 [00:02<00:35,  4.07it/s]  7%|▋         | 10/153 [00:02<00:35,  3.98it/s]  7%|▋         | 11/153 [00:02<00:36,  3.89it/s]  8%|▊         | 12/153 [00:03<00:36,  3.89it/s]  8%|▊         | 13/153 [00:03<00:35,  3.91it/s]  9%|▉         | 14/153 [00:03<00:36,  3.79it/s] 10%|▉         | 15/153 [00:03<00:36,  3.81it/s] 10%|█         | 16/153 [00:04<00:36,  3.74it/s] 11%|█         | 17/153 [00:04<00:35,  3.80it/s] 12%|█▏        | 18/153 [00:04<00:36,  3.72it/s] 12%|█▏        | 19/153 [00:04<00:35,  3.77it/s] 13%|█▎        | 20/153 [00:05<00:35,  3.73it/s] 14%|█▎        | 21/153 [00:05<00:34,  3.79it/s] 14%|█▍        | 22/153 [00:05<00:35,  3.70it/s] 15%|█▌        | 23/153 [00:05<00:34,  3.75it/s] 16%|█▌        | 24/153 [00:06<00:34,  3.71it/s] 16%|█▋        | 25/153 [00:06<00:33,  3.78it/s] 17%|█▋        | 26/153 [00:06<00:34,  3.71it/s] 18%|█▊        | 27/153 [00:07<00:33,  3.76it/s] 18%|█▊        | 28/153 [00:07<00:33,  3.70it/s] 19%|█▉        | 29/153 [00:07<00:32,  3.78it/s] 20%|█▉        | 30/153 [00:07<00:33,  3.70it/s] 20%|██        | 31/153 [00:08<00:32,  3.75it/s] 21%|██        | 32/153 [00:08<00:32,  3.71it/s] 22%|██▏       | 33/153 [00:08<00:31,  3.79it/s] 22%|██▏       | 34/153 [00:08<00:32,  3.71it/s] 23%|██▎       | 35/153 [00:09<00:31,  3.77it/s] 24%|██▎       | 36/153 [00:09<00:31,  3.71it/s] 24%|██▍       | 37/153 [00:09<00:30,  3.76it/s] 25%|██▍       | 38/153 [00:10<00:31,  3.69it/s] 25%|██▌       | 39/153 [00:10<00:30,  3.76it/s] 26%|██▌       | 40/153 [00:10<00:30,  3.71it/s] 27%|██▋       | 41/153 [00:10<00:29,  3.78it/s] 27%|██▋       | 42/153 [00:11<00:30,  3.70it/s] 28%|██▊       | 43/153 [00:11<00:29,  3.75it/s] 29%|██▉       | 44/153 [00:11<00:29,  3.69it/s] 29%|██▉       | 45/153 [00:11<00:28,  3.77it/s] 30%|███       | 46/153 [00:12<00:28,  3.70it/s] 31%|███       | 47/153 [00:12<00:28,  3.74it/s] 31%|███▏      | 48/153 [00:12<00:27,  3.79it/s] 32%|███▏      | 49/153 [00:12<00:26,  3.88it/s] 33%|███▎      | 50/153 [00:13<00:26,  3.94it/s] 33%|███▎      | 51/153 [00:13<00:25,  3.99it/s] 34%|███▍      | 52/153 [00:13<00:25,  4.03it/s] 35%|███▍      | 53/153 [00:13<00:24,  4.06it/s] 35%|███▌      | 54/153 [00:14<00:24,  4.09it/s] 36%|███▌      | 55/153 [00:14<00:23,  4.11it/s] 37%|███▋      | 56/153 [00:14<00:23,  4.13it/s] 37%|███▋      | 57/153 [00:14<00:23,  4.14it/s] 38%|███▊      | 58/153 [00:15<00:22,  4.14it/s] 39%|███▊      | 59/153 [00:15<00:22,  4.14it/s] 39%|███▉      | 60/153 [00:15<00:22,  4.14it/s] 40%|███▉      | 61/153 [00:15<00:22,  4.14it/s] 41%|████      | 62/153 [00:16<00:22,  4.13it/s] 41%|████      | 63/153 [00:16<00:21,  4.12it/s] 42%|████▏     | 64/153 [00:16<00:21,  4.05it/s] 42%|████▏     | 65/153 [00:16<00:22,  3.95it/s] 43%|████▎     | 66/153 [00:17<00:22,  3.88it/s] 44%|████▍     | 67/153 [00:17<00:22,  3.86it/s] 44%|████▍     | 68/153 [00:17<00:21,  3.93it/s] 45%|████▌     | 69/153 [00:17<00:21,  3.98it/s] 46%|████▌     | 70/153 [00:18<00:20,  4.02it/s] 46%|████▋     | 71/153 [00:18<00:20,  4.06it/s] 47%|████▋     | 72/153 [00:18<00:19,  4.07it/s] 48%|████▊     | 73/153 [00:18<00:19,  4.06it/s] 48%|████▊     | 74/153 [00:19<00:20,  3.94it/s] 49%|████▉     | 75/153 [00:19<00:20,  3.85it/s] 50%|████▉     | 76/153 [00:19<00:20,  3.80it/s] 50%|█████     | 77/153 [00:19<00:19,  3.88it/s] 51%|█████     | 78/153 [00:20<00:18,  3.95it/s] 52%|█████▏    | 79/153 [00:20<00:18,  4.00it/s] 52%|█████▏    | 80/153 [00:20<00:18,  4.03it/s] 53%|█████▎    | 81/153 [00:20<00:17,  4.05it/s] 54%|█████▎    | 82/153 [00:21<00:17,  4.06it/s] 54%|█████▍    | 83/153 [00:21<00:17,  4.07it/s] 55%|█████▍    | 84/153 [00:21<00:16,  4.08it/s] 56%|█████▌    | 85/153 [00:21<00:16,  4.09it/s] 56%|█████▌    | 86/153 [00:22<00:16,  4.09it/s] 57%|█████▋    | 87/153 [00:22<00:16,  4.09it/s] 58%|█████▊    | 88/153 [00:22<00:15,  4.09it/s] 58%|█████▊    | 89/153 [00:22<00:15,  4.10it/s] 59%|█████▉    | 90/153 [00:23<00:15,  4.11it/s] 59%|█████▉    | 91/153 [00:23<00:15,  4.11it/s] 60%|██████    | 92/153 [00:23<00:15,  3.99it/s] 61%|██████    | 93/153 [00:23<00:15,  3.90it/s] 61%|██████▏   | 94/153 [00:24<00:15,  3.81it/s] 62%|██████▏   | 95/153 [00:24<00:14,  3.90it/s] 63%|██████▎   | 96/153 [00:24<00:14,  3.96it/s] 63%|██████▎   | 97/153 [00:24<00:14,  3.91it/s] 64%|██████▍   | 98/153 [00:25<00:14,  3.84it/s] 65%|██████▍   | 99/153 [00:25<00:14,  3.84it/s] 65%|██████▌   | 100/153 [00:25<00:14,  3.76it/s] 66%|██████▌   | 101/153 [00:25<00:13,  3.86it/s] 67%|██████▋   | 102/153 [00:26<00:13,  3.85it/s] 67%|██████▋   | 103/153 [00:26<00:13,  3.81it/s] 68%|██████▊   | 104/153 [00:26<00:13,  3.76it/s] 69%|██████▊   | 105/153 [00:26<00:12,  3.73it/s] 69%|██████▉   | 106/153 [00:27<00:12,  3.78it/s] 70%|██████▉   | 107/153 [00:27<00:11,  3.86it/s] 71%|███████   | 108/153 [00:27<00:11,  3.81it/s] 71%|███████   | 109/153 [00:28<00:11,  3.75it/s] 72%|███████▏  | 110/153 [00:28<00:11,  3.73it/s] 73%|███████▎  | 111/153 [00:28<00:11,  3.78it/s] 73%|███████▎  | 112/153 [00:28<00:10,  3.87it/s] 74%|███████▍  | 113/153 [00:29<00:10,  3.81it/s] 75%|███████▍  | 114/153 [00:29<00:10,  3.77it/s] 75%|███████▌  | 115/153 [00:29<00:10,  3.80it/s] 76%|███████▌  | 116/153 [00:29<00:09,  3.72it/s] 76%|███████▋  | 117/153 [00:30<00:09,  3.82it/s] 77%|███████▋  | 118/153 [00:30<00:09,  3.82it/s] 78%|███████▊  | 119/153 [00:30<00:09,  3.77it/s] 78%|███████▊  | 120/153 [00:30<00:08,  3.73it/s] 79%|███████▉  | 121/153 [00:31<00:08,  3.69it/s] 80%|███████▉  | 122/153 [00:31<00:08,  3.77it/s] 80%|████████  | 123/153 [00:31<00:07,  3.84it/s] 81%|████████  | 124/153 [00:31<00:07,  3.79it/s] 82%|████████▏ | 125/153 [00:32<00:07,  3.74it/s] 82%|████████▏ | 126/153 [00:32<00:07,  3.71it/s] 83%|████████▎ | 127/153 [00:32<00:06,  3.77it/s] 84%|████████▎ | 128/153 [00:33<00:06,  3.85it/s] 84%|████████▍ | 129/153 [00:33<00:06,  3.80it/s] 85%|████████▍ | 130/153 [00:33<00:06,  3.74it/s] 86%|████████▌ | 131/153 [00:33<00:05,  3.70it/s] 86%|████████▋ | 132/153 [00:34<00:05,  3.77it/s] 87%|████████▋ | 133/153 [00:34<00:05,  3.84it/s] 88%|████████▊ | 134/153 [00:34<00:05,  3.78it/s] 88%|████████▊ | 135/153 [00:34<00:04,  3.73it/s] 89%|████████▉ | 136/153 [00:35<00:04,  3.68it/s] 90%|████████▉ | 137/153 [00:35<00:04,  3.77it/s] 90%|█████████ | 138/153 [00:35<00:03,  3.84it/s] 91%|█████████ | 139/153 [00:35<00:03,  3.79it/s] 92%|█████████▏| 140/153 [00:36<00:03,  3.74it/s] 92%|█████████▏| 141/153 [00:36<00:03,  3.69it/s] 93%|█████████▎| 142/153 [00:36<00:02,  3.79it/s] 93%|█████████▎| 143/153 [00:37<00:02,  3.86it/s] 94%|█████████▍| 144/153 [00:37<00:02,  3.80it/s] 95%|█████████▍| 145/153 [00:37<00:02,  3.75it/s] 95%|█████████▌| 146/153 [00:37<00:01,  3.69it/s] 96%|█████████▌| 147/153 [00:38<00:01,  3.78it/s] 97%|█████████▋| 148/153 [00:38<00:01,  3.83it/s] 97%|█████████▋| 149/153 [00:38<00:01,  3.78it/s] 98%|█████████▊| 150/153 [00:38<00:00,  3.74it/s] 99%|█████████▊| 151/153 [00:39<00:00,  3.68it/s] 99%|█████████▉| 152/153 [00:39<00:00,  3.78it/s]100%|██████████| 153/153 [00:39<00:00,  3.87it/s]accuracy:  0.6535947712418301
100%|██████████| 153/153 [00:42<00:00,  3.63it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 152
Training dataset size: 240, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 240, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:54:13,408] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:54:13,643] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:54:17,171] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:54,  2.93s/it]  5%|▌         | 2/40 [00:05<01:42,  2.70s/it]  8%|▊         | 3/40 [00:07<01:29,  2.41s/it] 10%|█         | 4/40 [00:10<01:30,  2.50s/it] 12%|█▎        | 5/40 [00:12<01:30,  2.57s/it] 15%|█▌        | 6/40 [00:14<01:21,  2.39s/it] 18%|█▊        | 7/40 [00:17<01:18,  2.39s/it] 20%|██        | 8/40 [00:19<01:18,  2.47s/it] 22%|██▎       | 9/40 [00:21<01:11,  2.30s/it] 25%|██▌       | 10/40 [00:24<01:12,  2.41s/it]                                               {'loss': 1.1265, 'grad_norm': 13.099422454833984, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:24<01:12,  2.41s/it] 28%|██▊       | 11/40 [00:26<01:09,  2.38s/it] 30%|███       | 12/40 [00:29<01:09,  2.48s/it] 32%|███▎      | 13/40 [00:32<01:08,  2.54s/it] 35%|███▌      | 14/40 [00:34<01:02,  2.42s/it] 38%|███▊      | 15/40 [00:36<00:56,  2.27s/it] 40%|████      | 16/40 [00:38<00:54,  2.29s/it] 42%|████▎     | 17/40 [00:40<00:50,  2.18s/it] 45%|████▌     | 18/40 [00:42<00:47,  2.16s/it] 48%|████▊     | 19/40 [00:44<00:46,  2.21s/it] 50%|█████     | 20/40 [00:46<00:42,  2.11s/it]                                               {'loss': 0.9806, 'grad_norm': 9.665929794311523, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:46<00:42,  2.11s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:49<00:42,  2.25s/it] 55%|█████▌    | 22/40 [00:51<00:41,  2.28s/it] 57%|█████▊    | 23/40 [00:54<00:41,  2.43s/it] 60%|██████    | 24/40 [00:57<00:39,  2.47s/it] 62%|██████▎   | 25/40 [00:59<00:35,  2.35s/it] 65%|██████▌   | 26/40 [01:01<00:34,  2.44s/it] 68%|██████▊   | 27/40 [01:04<00:30,  2.36s/it] 70%|███████   | 28/40 [01:06<00:27,  2.32s/it] 72%|███████▎  | 29/40 [01:08<00:25,  2.28s/it] 75%|███████▌  | 30/40 [01:10<00:23,  2.34s/it]                                               {'loss': 0.9021, 'grad_norm': 5.054182529449463, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:10<00:23,  2.34s/it] 78%|███████▊  | 31/40 [01:13<00:21,  2.39s/it] 80%|████████  | 32/40 [01:16<00:19,  2.45s/it] 82%|████████▎ | 33/40 [01:18<00:16,  2.32s/it] 85%|████████▌ | 34/40 [01:20<00:14,  2.44s/it] 88%|████████▊ | 35/40 [01:22<00:11,  2.38s/it] 90%|█████████ | 36/40 [01:25<00:09,  2.42s/it] 92%|█████████▎| 37/40 [01:27<00:07,  2.44s/it] 95%|█████████▌| 38/40 [01:30<00:04,  2.34s/it] 98%|█████████▊| 39/40 [01:32<00:02,  2.26s/it]100%|██████████| 40/40 [01:34<00:00,  2.23s/it]                                               {'loss': 0.6357, 'grad_norm': 3.2774155139923096, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.23s/it]                                               {'train_runtime': 94.9551, 'train_samples_per_second': 5.055, 'train_steps_per_second': 0.421, 'train_loss': 0.9112454056739807, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.23s/it]100%|██████████| 40/40 [01:34<00:00,  2.37s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:07,  3.15it/s]  1%|          | 2/212 [00:00<01:01,  3.44it/s]  1%|▏         | 3/212 [00:00<00:57,  3.65it/s]  2%|▏         | 4/212 [00:01<00:57,  3.63it/s]  2%|▏         | 5/212 [00:01<00:55,  3.73it/s]  3%|▎         | 6/212 [00:01<00:55,  3.71it/s]  3%|▎         | 7/212 [00:01<00:56,  3.66it/s]  4%|▍         | 8/212 [00:02<00:53,  3.81it/s]  4%|▍         | 9/212 [00:02<00:53,  3.80it/s]  5%|▍         | 10/212 [00:02<00:53,  3.77it/s]  5%|▌         | 11/212 [00:02<00:54,  3.70it/s]  6%|▌         | 12/212 [00:03<00:52,  3.82it/s]  6%|▌         | 13/212 [00:03<00:52,  3.79it/s]  7%|▋         | 14/212 [00:03<00:52,  3.78it/s]  7%|▋         | 15/212 [00:04<00:52,  3.72it/s]  8%|▊         | 16/212 [00:04<00:51,  3.82it/s]  8%|▊         | 17/212 [00:04<00:51,  3.77it/s]  8%|▊         | 18/212 [00:04<00:50,  3.82it/s]  9%|▉         | 19/212 [00:05<00:51,  3.76it/s]  9%|▉         | 20/212 [00:05<00:50,  3.81it/s] 10%|▉         | 21/212 [00:05<00:50,  3.76it/s] 10%|█         | 22/212 [00:05<00:51,  3.69it/s] 11%|█         | 23/212 [00:06<00:49,  3.82it/s] 11%|█▏        | 24/212 [00:06<00:49,  3.80it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.77it/s] 12%|█▏        | 26/212 [00:06<00:50,  3.72it/s] 13%|█▎        | 27/212 [00:07<00:48,  3.84it/s] 13%|█▎        | 28/212 [00:07<00:48,  3.79it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.77it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.72it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.83it/s] 15%|█▌        | 32/212 [00:08<00:47,  3.79it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.77it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.72it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.83it/s] 17%|█▋        | 36/212 [00:09<00:46,  3.78it/s] 17%|█▋        | 37/212 [00:09<00:45,  3.82it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.74it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.79it/s] 19%|█▉        | 40/212 [00:10<00:45,  3.76it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.74it/s] 20%|█▉        | 42/212 [00:11<00:44,  3.85it/s] 20%|██        | 43/212 [00:11<00:43,  3.92it/s] 21%|██        | 44/212 [00:11<00:42,  3.98it/s] 21%|██        | 45/212 [00:11<00:41,  4.03it/s] 22%|██▏       | 46/212 [00:12<00:40,  4.06it/s] 22%|██▏       | 47/212 [00:12<00:40,  4.09it/s] 23%|██▎       | 48/212 [00:12<00:39,  4.11it/s] 23%|██▎       | 49/212 [00:12<00:39,  4.12it/s] 24%|██▎       | 50/212 [00:13<00:39,  4.13it/s] 24%|██▍       | 51/212 [00:13<00:38,  4.14it/s] 25%|██▍       | 52/212 [00:13<00:38,  4.15it/s] 25%|██▌       | 53/212 [00:13<00:38,  4.15it/s] 25%|██▌       | 54/212 [00:14<00:38,  4.14it/s] 26%|██▌       | 55/212 [00:14<00:37,  4.14it/s] 26%|██▋       | 56/212 [00:14<00:37,  4.13it/s] 27%|██▋       | 57/212 [00:14<00:37,  4.13it/s] 27%|██▋       | 58/212 [00:15<00:37,  4.13it/s] 28%|██▊       | 59/212 [00:15<00:37,  4.14it/s] 28%|██▊       | 60/212 [00:15<00:36,  4.14it/s] 29%|██▉       | 61/212 [00:15<00:36,  4.15it/s] 29%|██▉       | 62/212 [00:15<00:36,  4.16it/s] 30%|██▉       | 63/212 [00:16<00:35,  4.16it/s] 30%|███       | 64/212 [00:16<00:35,  4.16it/s] 31%|███       | 65/212 [00:16<00:35,  4.16it/s] 31%|███       | 66/212 [00:16<00:35,  4.16it/s] 32%|███▏      | 67/212 [00:17<00:34,  4.16it/s] 32%|███▏      | 68/212 [00:17<00:34,  4.17it/s] 33%|███▎      | 69/212 [00:17<00:34,  4.16it/s] 33%|███▎      | 70/212 [00:17<00:35,  4.02it/s] 33%|███▎      | 71/212 [00:18<00:36,  3.91it/s] 34%|███▍      | 72/212 [00:18<00:35,  3.90it/s] 34%|███▍      | 73/212 [00:18<00:35,  3.96it/s] 35%|███▍      | 74/212 [00:18<00:36,  3.83it/s] 35%|███▌      | 75/212 [00:19<00:36,  3.74it/s] 36%|███▌      | 76/212 [00:19<00:37,  3.66it/s] 36%|███▋      | 77/212 [00:19<00:35,  3.78it/s] 37%|███▋      | 78/212 [00:20<00:35,  3.80it/s] 37%|███▋      | 79/212 [00:20<00:36,  3.68it/s] 38%|███▊      | 80/212 [00:20<00:35,  3.73it/s] 38%|███▊      | 81/212 [00:20<00:35,  3.67it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.77it/s] 39%|███▉      | 83/212 [00:21<00:34,  3.69it/s] 40%|███▉      | 84/212 [00:21<00:35,  3.64it/s] 40%|████      | 85/212 [00:22<00:35,  3.60it/s] 41%|████      | 86/212 [00:22<00:33,  3.74it/s] 41%|████      | 87/212 [00:22<00:33,  3.77it/s] 42%|████▏     | 88/212 [00:22<00:33,  3.65it/s] 42%|████▏     | 89/212 [00:23<00:33,  3.70it/s] 42%|████▏     | 90/212 [00:23<00:33,  3.65it/s] 43%|████▎     | 91/212 [00:23<00:32,  3.77it/s] 43%|████▎     | 92/212 [00:23<00:32,  3.69it/s] 44%|████▍     | 93/212 [00:24<00:32,  3.64it/s] 44%|████▍     | 94/212 [00:24<00:32,  3.59it/s] 45%|████▍     | 95/212 [00:24<00:31,  3.74it/s] 45%|████▌     | 96/212 [00:24<00:30,  3.76it/s] 46%|████▌     | 97/212 [00:25<00:31,  3.65it/s] 46%|████▌     | 98/212 [00:25<00:30,  3.69it/s] 47%|████▋     | 99/212 [00:25<00:30,  3.65it/s] 47%|████▋     | 100/212 [00:26<00:29,  3.75it/s] 48%|████▊     | 101/212 [00:26<00:30,  3.69it/s] 48%|████▊     | 102/212 [00:26<00:30,  3.63it/s] 49%|████▊     | 103/212 [00:26<00:30,  3.59it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.74it/s] 50%|████▉     | 105/212 [00:27<00:28,  3.78it/s] 50%|█████     | 106/212 [00:27<00:29,  3.65it/s] 50%|█████     | 107/212 [00:27<00:28,  3.70it/s] 51%|█████     | 108/212 [00:28<00:28,  3.66it/s] 51%|█████▏    | 109/212 [00:28<00:27,  3.76it/s] 52%|█████▏    | 110/212 [00:28<00:27,  3.69it/s] 52%|█████▏    | 111/212 [00:29<00:27,  3.64it/s] 53%|█████▎    | 112/212 [00:29<00:27,  3.60it/s] 53%|█████▎    | 113/212 [00:29<00:26,  3.74it/s] 54%|█████▍    | 114/212 [00:29<00:25,  3.77it/s] 54%|█████▍    | 115/212 [00:30<00:26,  3.65it/s] 55%|█████▍    | 116/212 [00:30<00:26,  3.69it/s] 55%|█████▌    | 117/212 [00:30<00:26,  3.65it/s] 56%|█████▌    | 118/212 [00:30<00:24,  3.77it/s] 56%|█████▌    | 119/212 [00:31<00:25,  3.69it/s] 57%|█████▋    | 120/212 [00:31<00:25,  3.63it/s] 57%|█████▋    | 121/212 [00:31<00:25,  3.59it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.74it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.77it/s] 58%|█████▊    | 124/212 [00:32<00:24,  3.64it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.70it/s] 59%|█████▉    | 126/212 [00:33<00:23,  3.66it/s] 60%|█████▉    | 127/212 [00:33<00:22,  3.77it/s] 60%|██████    | 128/212 [00:33<00:22,  3.69it/s] 61%|██████    | 129/212 [00:33<00:22,  3.64it/s] 61%|██████▏   | 130/212 [00:34<00:22,  3.59it/s] 62%|██████▏   | 131/212 [00:34<00:21,  3.74it/s] 62%|██████▏   | 132/212 [00:34<00:21,  3.78it/s] 63%|██████▎   | 133/212 [00:34<00:21,  3.70it/s] 63%|██████▎   | 134/212 [00:35<00:21,  3.65it/s] 64%|██████▎   | 135/212 [00:35<00:21,  3.60it/s] 64%|██████▍   | 136/212 [00:35<00:20,  3.74it/s] 65%|██████▍   | 137/212 [00:36<00:19,  3.77it/s] 65%|██████▌   | 138/212 [00:36<00:20,  3.65it/s] 66%|██████▌   | 139/212 [00:36<00:19,  3.69it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.65it/s] 67%|██████▋   | 141/212 [00:37<00:18,  3.75it/s] 67%|██████▋   | 142/212 [00:37<00:19,  3.68it/s] 67%|██████▋   | 143/212 [00:37<00:18,  3.63it/s] 68%|██████▊   | 144/212 [00:38<00:18,  3.59it/s] 68%|██████▊   | 145/212 [00:38<00:17,  3.74it/s] 69%|██████▉   | 146/212 [00:38<00:17,  3.75it/s] 69%|██████▉   | 147/212 [00:38<00:17,  3.63it/s] 70%|██████▉   | 148/212 [00:39<00:17,  3.70it/s] 70%|███████   | 149/212 [00:39<00:17,  3.65it/s] 71%|███████   | 150/212 [00:39<00:16,  3.74it/s] 71%|███████   | 151/212 [00:39<00:16,  3.67it/s] 72%|███████▏  | 152/212 [00:40<00:16,  3.62it/s] 72%|███████▏  | 153/212 [00:40<00:16,  3.58it/s] 73%|███████▎  | 154/212 [00:40<00:15,  3.73it/s] 73%|███████▎  | 155/212 [00:40<00:15,  3.75it/s] 74%|███████▎  | 156/212 [00:41<00:15,  3.64it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.67it/s] 75%|███████▍  | 158/212 [00:41<00:14,  3.65it/s] 75%|███████▌  | 159/212 [00:42<00:14,  3.77it/s] 75%|███████▌  | 160/212 [00:42<00:14,  3.69it/s] 76%|███████▌  | 161/212 [00:42<00:14,  3.64it/s] 76%|███████▋  | 162/212 [00:42<00:13,  3.59it/s] 77%|███████▋  | 163/212 [00:43<00:13,  3.74it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.77it/s] 78%|███████▊  | 165/212 [00:43<00:12,  3.64it/s] 78%|███████▊  | 166/212 [00:43<00:12,  3.68it/s] 79%|███████▉  | 167/212 [00:44<00:12,  3.65it/s] 79%|███████▉  | 168/212 [00:44<00:11,  3.77it/s] 80%|███████▉  | 169/212 [00:44<00:11,  3.68it/s] 80%|████████  | 170/212 [00:45<00:11,  3.62it/s] 81%|████████  | 171/212 [00:45<00:11,  3.58it/s] 81%|████████  | 172/212 [00:45<00:10,  3.72it/s] 82%|████████▏ | 173/212 [00:45<00:10,  3.74it/s] 82%|████████▏ | 174/212 [00:46<00:10,  3.63it/s] 83%|████████▎ | 175/212 [00:46<00:10,  3.67it/s] 83%|████████▎ | 176/212 [00:46<00:09,  3.63it/s] 83%|████████▎ | 177/212 [00:46<00:09,  3.74it/s] 84%|████████▍ | 178/212 [00:47<00:09,  3.68it/s] 84%|████████▍ | 179/212 [00:47<00:09,  3.63it/s] 85%|████████▍ | 180/212 [00:47<00:08,  3.58it/s] 85%|████████▌ | 181/212 [00:48<00:08,  3.73it/s] 86%|████████▌ | 182/212 [00:48<00:07,  3.76it/s] 86%|████████▋ | 183/212 [00:48<00:07,  3.66it/s] 87%|████████▋ | 184/212 [00:48<00:07,  3.64it/s] 87%|████████▋ | 185/212 [00:49<00:07,  3.60it/s] 88%|████████▊ | 186/212 [00:49<00:06,  3.74it/s] 88%|████████▊ | 187/212 [00:49<00:06,  3.77it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.65it/s] 89%|████████▉ | 189/212 [00:50<00:06,  3.69it/s] 90%|████████▉ | 190/212 [00:50<00:06,  3.66it/s] 90%|█████████ | 191/212 [00:50<00:05,  3.76it/s] 91%|█████████ | 192/212 [00:51<00:05,  3.69it/s] 91%|█████████ | 193/212 [00:51<00:05,  3.63it/s] 92%|█████████▏| 194/212 [00:51<00:05,  3.58it/s] 92%|█████████▏| 195/212 [00:51<00:04,  3.73it/s] 92%|█████████▏| 196/212 [00:52<00:04,  3.75it/s] 93%|█████████▎| 197/212 [00:52<00:04,  3.63it/s] 93%|█████████▎| 198/212 [00:52<00:03,  3.65it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.60it/s] 94%|█████████▍| 200/212 [00:53<00:03,  3.74it/s] 95%|█████████▍| 201/212 [00:53<00:02,  3.76it/s] 95%|█████████▌| 202/212 [00:53<00:02,  3.63it/s] 96%|█████████▌| 203/212 [00:54<00:02,  3.69it/s] 96%|█████████▌| 204/212 [00:54<00:02,  3.64it/s] 97%|█████████▋| 205/212 [00:54<00:01,  3.73it/s] 97%|█████████▋| 206/212 [00:54<00:01,  3.66it/s] 98%|█████████▊| 207/212 [00:55<00:01,  3.61it/s] 98%|█████████▊| 208/212 [00:55<00:01,  3.57it/s] 99%|█████████▊| 209/212 [00:55<00:00,  3.72it/s] 99%|█████████▉| 210/212 [00:55<00:00,  3.75it/s]100%|█████████▉| 211/212 [00:56<00:00,  3.63it/s]100%|██████████| 212/212 [00:56<00:00,  3.64it/s]accuracy:  0.6698113207547169
100%|██████████| 212/212 [00:59<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 135
Training dataset size: 240, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 240, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.06s/it][2025-03-12 04:57:18,804] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[2025-03-12 04:57:20,028] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 04:57:20,188] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:33,  2.40s/it]  5%|▌         | 2/40 [00:04<01:28,  2.32s/it]  8%|▊         | 3/40 [00:06<01:24,  2.28s/it] 10%|█         | 4/40 [00:09<01:28,  2.45s/it] 12%|█▎        | 5/40 [00:12<01:25,  2.44s/it] 15%|█▌        | 6/40 [00:14<01:22,  2.43s/it] 18%|█▊        | 7/40 [00:16<01:20,  2.44s/it] 20%|██        | 8/40 [00:19<01:15,  2.35s/it] 22%|██▎       | 9/40 [00:22<01:19,  2.56s/it] 25%|██▌       | 10/40 [00:25<01:21,  2.71s/it]                                               {'loss': 0.6418, 'grad_norm': 9.37498664855957, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:25<01:21,  2.71s/it] 28%|██▊       | 11/40 [00:27<01:14,  2.55s/it] 30%|███       | 12/40 [00:29<01:11,  2.56s/it] 32%|███▎      | 13/40 [00:32<01:07,  2.50s/it] 35%|███▌      | 14/40 [00:34<01:03,  2.46s/it] 38%|███▊      | 15/40 [00:37<01:01,  2.46s/it] 40%|████      | 16/40 [00:39<00:55,  2.32s/it] 42%|████▎     | 17/40 [00:41<00:53,  2.30s/it] 45%|████▌     | 18/40 [00:43<00:50,  2.31s/it] 48%|████▊     | 19/40 [00:45<00:47,  2.24s/it] 50%|█████     | 20/40 [00:47<00:44,  2.24s/it]                                               {'loss': 0.4419, 'grad_norm': 5.372094631195068, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:47<00:44,  2.24s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:50<00:44,  2.32s/it] 55%|█████▌    | 22/40 [00:53<00:43,  2.39s/it] 57%|█████▊    | 23/40 [00:55<00:38,  2.29s/it] 60%|██████    | 24/40 [00:57<00:35,  2.19s/it] 62%|██████▎   | 25/40 [00:59<00:34,  2.27s/it] 65%|██████▌   | 26/40 [01:01<00:31,  2.27s/it] 68%|██████▊   | 27/40 [01:04<00:29,  2.30s/it] 70%|███████   | 28/40 [01:06<00:26,  2.24s/it] 72%|███████▎  | 29/40 [01:08<00:25,  2.30s/it] 75%|███████▌  | 30/40 [01:10<00:22,  2.27s/it]                                               {'loss': 0.5657, 'grad_norm': 1.1327853202819824, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:10<00:22,  2.27s/it] 78%|███████▊  | 31/40 [01:12<00:20,  2.22s/it] 80%|████████  | 32/40 [01:15<00:18,  2.35s/it] 82%|████████▎ | 33/40 [01:18<00:16,  2.43s/it] 85%|████████▌ | 34/40 [01:20<00:14,  2.46s/it] 88%|████████▊ | 35/40 [01:23<00:12,  2.45s/it] 90%|█████████ | 36/40 [01:25<00:10,  2.54s/it] 92%|█████████▎| 37/40 [01:28<00:07,  2.54s/it] 95%|█████████▌| 38/40 [01:31<00:05,  2.53s/it] 98%|█████████▊| 39/40 [01:33<00:02,  2.50s/it]100%|██████████| 40/40 [01:35<00:00,  2.45s/it]                                               {'loss': 0.4328, 'grad_norm': 1.533305287361145, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:35<00:00,  2.45s/it]                                               {'train_runtime': 96.4386, 'train_samples_per_second': 4.977, 'train_steps_per_second': 0.415, 'train_loss': 0.5205695390701294, 'epoch': 2.0}
100%|██████████| 40/40 [01:36<00:00,  2.45s/it]100%|██████████| 40/40 [01:36<00:00,  2.41s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:55,  2.93it/s]  1%|          | 2/163 [00:00<00:46,  3.45it/s]  2%|▏         | 3/163 [00:00<00:45,  3.52it/s]  2%|▏         | 4/163 [00:01<00:42,  3.74it/s]  3%|▎         | 5/163 [00:01<00:42,  3.72it/s]  4%|▎         | 6/163 [00:01<00:42,  3.71it/s]  4%|▍         | 7/163 [00:01<00:42,  3.66it/s]  5%|▍         | 8/163 [00:02<00:40,  3.81it/s]  6%|▌         | 9/163 [00:02<00:40,  3.83it/s]  6%|▌         | 10/163 [00:02<00:40,  3.78it/s]  7%|▋         | 11/163 [00:02<00:40,  3.78it/s]  7%|▋         | 12/163 [00:03<00:40,  3.72it/s]  8%|▊         | 13/163 [00:03<00:39,  3.84it/s]  9%|▊         | 14/163 [00:03<00:38,  3.86it/s]  9%|▉         | 15/163 [00:04<00:38,  3.82it/s] 10%|▉         | 16/163 [00:04<00:38,  3.79it/s] 10%|█         | 17/163 [00:04<00:39,  3.73it/s] 11%|█         | 18/163 [00:04<00:37,  3.86it/s] 12%|█▏        | 19/163 [00:05<00:37,  3.89it/s] 12%|█▏        | 20/163 [00:05<00:37,  3.81it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.80it/s] 13%|█▎        | 22/163 [00:05<00:37,  3.72it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.84it/s] 15%|█▍        | 24/163 [00:06<00:36,  3.86it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.78it/s] 16%|█▌        | 26/163 [00:06<00:36,  3.79it/s] 17%|█▋        | 27/163 [00:07<00:36,  3.72it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.84it/s] 18%|█▊        | 29/163 [00:07<00:34,  3.85it/s] 18%|█▊        | 30/163 [00:07<00:35,  3.78it/s] 19%|█▉        | 31/163 [00:08<00:34,  3.78it/s] 20%|█▉        | 32/163 [00:08<00:35,  3.71it/s] 20%|██        | 33/163 [00:08<00:33,  3.83it/s] 21%|██        | 34/163 [00:09<00:33,  3.85it/s] 21%|██▏       | 35/163 [00:09<00:33,  3.78it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.81it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.75it/s] 23%|██▎       | 38/163 [00:10<00:32,  3.87it/s] 24%|██▍       | 39/163 [00:10<00:32,  3.81it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.79it/s] 25%|██▌       | 41/163 [00:10<00:31,  3.82it/s] 26%|██▌       | 42/163 [00:11<00:32,  3.75it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.85it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.79it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.75it/s] 28%|██▊       | 46/163 [00:12<00:31,  3.69it/s] 29%|██▉       | 47/163 [00:12<00:30,  3.82it/s] 29%|██▉       | 48/163 [00:12<00:29,  3.84it/s] 30%|███       | 49/163 [00:12<00:30,  3.78it/s] 31%|███       | 50/163 [00:13<00:29,  3.78it/s] 31%|███▏      | 51/163 [00:13<00:30,  3.73it/s] 32%|███▏      | 52/163 [00:13<00:28,  3.85it/s] 33%|███▎      | 53/163 [00:14<00:28,  3.82it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.78it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.82it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.74it/s] 35%|███▍      | 57/163 [00:15<00:27,  3.83it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.79it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.76it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.70it/s] 37%|███▋      | 61/163 [00:16<00:26,  3.83it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.86it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.78it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.79it/s] 40%|███▉      | 65/163 [00:17<00:26,  3.73it/s] 40%|████      | 66/163 [00:17<00:25,  3.84it/s] 41%|████      | 67/163 [00:17<00:25,  3.78it/s] 42%|████▏     | 68/163 [00:17<00:25,  3.75it/s] 42%|████▏     | 69/163 [00:18<00:25,  3.69it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.82it/s] 44%|████▎     | 71/163 [00:18<00:23,  3.84it/s] 44%|████▍     | 72/163 [00:19<00:24,  3.77it/s] 45%|████▍     | 73/163 [00:19<00:23,  3.78it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.72it/s] 46%|████▌     | 75/163 [00:19<00:22,  3.84it/s] 47%|████▋     | 76/163 [00:20<00:23,  3.78it/s] 47%|████▋     | 77/163 [00:20<00:23,  3.73it/s] 48%|████▊     | 78/163 [00:20<00:23,  3.67it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.79it/s] 49%|████▉     | 80/163 [00:21<00:21,  3.82it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.76it/s] 50%|█████     | 82/163 [00:21<00:21,  3.76it/s] 51%|█████     | 83/163 [00:21<00:21,  3.71it/s] 52%|█████▏    | 84/163 [00:22<00:20,  3.82it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.82it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.76it/s] 53%|█████▎    | 87/163 [00:23<00:20,  3.79it/s] 54%|█████▍    | 88/163 [00:23<00:20,  3.73it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.81it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.77it/s] 56%|█████▌    | 91/163 [00:24<00:19,  3.74it/s] 56%|█████▋    | 92/163 [00:24<00:19,  3.67it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.80it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.81it/s] 58%|█████▊    | 95/163 [00:25<00:18,  3.75it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.79it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.73it/s] 60%|██████    | 98/163 [00:25<00:16,  3.84it/s] 61%|██████    | 99/163 [00:26<00:16,  3.79it/s] 61%|██████▏   | 100/163 [00:26<00:16,  3.75it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.68it/s] 63%|██████▎   | 102/163 [00:27<00:16,  3.80it/s] 63%|██████▎   | 103/163 [00:27<00:15,  3.83it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.76it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.74it/s] 65%|██████▌   | 106/163 [00:28<00:15,  3.70it/s] 66%|██████▌   | 107/163 [00:28<00:14,  3.82it/s] 66%|██████▋   | 108/163 [00:28<00:14,  3.83it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.77it/s] 67%|██████▋   | 110/163 [00:29<00:13,  3.79it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.72it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.82it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.78it/s] 70%|██████▉   | 114/163 [00:30<00:13,  3.73it/s] 71%|███████   | 115/163 [00:30<00:13,  3.67it/s] 71%|███████   | 116/163 [00:30<00:12,  3.79it/s] 72%|███████▏  | 117/163 [00:30<00:12,  3.82it/s] 72%|███████▏  | 118/163 [00:31<00:12,  3.75it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.75it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.70it/s] 74%|███████▍  | 121/163 [00:32<00:10,  3.82it/s] 75%|███████▍  | 122/163 [00:32<00:10,  3.84it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.77it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.79it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.72it/s] 77%|███████▋  | 126/163 [00:33<00:09,  3.81it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.77it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.73it/s] 79%|███████▉  | 129/163 [00:34<00:09,  3.67it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.79it/s] 80%|████████  | 131/163 [00:34<00:08,  3.83it/s] 81%|████████  | 132/163 [00:34<00:08,  3.77it/s] 82%|████████▏ | 133/163 [00:35<00:07,  3.76it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.69it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.81it/s] 83%|████████▎ | 136/163 [00:36<00:07,  3.84it/s] 84%|████████▍ | 137/163 [00:36<00:06,  3.76it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.76it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.69it/s] 86%|████████▌ | 140/163 [00:37<00:06,  3.81it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.82it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.75it/s] 88%|████████▊ | 143/163 [00:37<00:05,  3.77it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.70it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.82it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.77it/s] 90%|█████████ | 147/163 [00:38<00:04,  3.73it/s] 91%|█████████ | 148/163 [00:39<00:03,  3.76it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.72it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.80it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.76it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.73it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.67it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.79it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.83it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.76it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.75it/s] 97%|█████████▋| 158/163 [00:41<00:01,  3.68it/s] 98%|█████████▊| 159/163 [00:42<00:01,  3.80it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.81it/s] 99%|█████████▉| 161/163 [00:42<00:00,  3.74it/s] 99%|█████████▉| 162/163 [00:42<00:00,  3.76it/s]100%|██████████| 163/163 [00:43<00:00,  3.70it/s]accuracy:  0.8773006134969326
100%|██████████| 163/163 [00:45<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 198
Training dataset size: 240, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 240, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.11s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:00:09,353] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:00:09,858] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:00:10,530] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:56,  2.99s/it]  5%|▌         | 2/40 [00:05<01:49,  2.87s/it]  8%|▊         | 3/40 [00:08<01:48,  2.94s/it] 10%|█         | 4/40 [00:11<01:39,  2.77s/it] 12%|█▎        | 5/40 [00:13<01:30,  2.60s/it] 15%|█▌        | 6/40 [00:16<01:29,  2.64s/it] 18%|█▊        | 7/40 [00:18<01:22,  2.49s/it] 20%|██        | 8/40 [00:20<01:16,  2.40s/it] 22%|██▎       | 9/40 [00:23<01:13,  2.38s/it] 25%|██▌       | 10/40 [00:25<01:11,  2.38s/it]                                               {'loss': 0.6384, 'grad_norm': 7.0058088302612305, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:25<01:11,  2.38s/it] 28%|██▊       | 11/40 [00:27<01:09,  2.41s/it] 30%|███       | 12/40 [00:30<01:06,  2.38s/it] 32%|███▎      | 13/40 [00:32<01:07,  2.48s/it] 35%|███▌      | 14/40 [00:36<01:10,  2.69s/it] 38%|███▊      | 15/40 [00:38<01:05,  2.62s/it] 40%|████      | 16/40 [00:40<01:00,  2.51s/it] 42%|████▎     | 17/40 [00:42<00:54,  2.38s/it] 45%|████▌     | 18/40 [00:45<00:55,  2.52s/it] 48%|████▊     | 19/40 [00:48<00:54,  2.58s/it] 50%|█████     | 20/40 [00:50<00:49,  2.48s/it]                                               {'loss': 0.6808, 'grad_norm': 4.641908168792725, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:50<00:49,  2.48s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:53<00:48,  2.56s/it] 55%|█████▌    | 22/40 [00:56<00:49,  2.75s/it] 57%|█████▊    | 23/40 [00:58<00:44,  2.59s/it] 60%|██████    | 24/40 [01:01<00:40,  2.56s/it] 62%|██████▎   | 25/40 [01:03<00:36,  2.40s/it] 65%|██████▌   | 26/40 [01:05<00:34,  2.45s/it] 68%|██████▊   | 27/40 [01:08<00:33,  2.58s/it] 70%|███████   | 28/40 [01:11<00:30,  2.54s/it] 72%|███████▎  | 29/40 [01:13<00:27,  2.52s/it] 75%|███████▌  | 30/40 [01:16<00:25,  2.53s/it]                                               {'loss': 0.5399, 'grad_norm': 4.529574871063232, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:16<00:25,  2.53s/it] 78%|███████▊  | 31/40 [01:18<00:22,  2.46s/it] 80%|████████  | 32/40 [01:21<00:20,  2.56s/it] 82%|████████▎ | 33/40 [01:23<00:17,  2.52s/it] 85%|████████▌ | 34/40 [01:26<00:14,  2.43s/it] 88%|████████▊ | 35/40 [01:28<00:11,  2.34s/it] 90%|█████████ | 36/40 [01:30<00:09,  2.42s/it] 92%|█████████▎| 37/40 [01:33<00:07,  2.44s/it] 95%|█████████▌| 38/40 [01:35<00:04,  2.40s/it] 98%|█████████▊| 39/40 [01:38<00:02,  2.50s/it]100%|██████████| 40/40 [01:40<00:00,  2.53s/it]                                               {'loss': 0.4974, 'grad_norm': 4.544386386871338, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:40<00:00,  2.53s/it]                                               {'train_runtime': 101.5174, 'train_samples_per_second': 4.728, 'train_steps_per_second': 0.394, 'train_loss': 0.5891173005104064, 'epoch': 2.0}
100%|██████████| 40/40 [01:41<00:00,  2.53s/it]100%|██████████| 40/40 [01:41<00:00,  2.53s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:13,  3.16it/s]  1%|          | 2/232 [00:00<01:03,  3.60it/s]  1%|▏         | 3/232 [00:00<01:02,  3.68it/s]  2%|▏         | 4/232 [00:01<01:01,  3.68it/s]  2%|▏         | 5/232 [00:01<01:01,  3.70it/s]  3%|▎         | 6/232 [00:01<00:59,  3.77it/s]  3%|▎         | 7/232 [00:01<00:57,  3.89it/s]  3%|▎         | 8/232 [00:02<00:58,  3.81it/s]  4%|▍         | 9/232 [00:02<00:58,  3.81it/s]  4%|▍         | 10/232 [00:02<00:57,  3.83it/s]  5%|▍         | 11/232 [00:02<00:58,  3.77it/s]  5%|▌         | 12/232 [00:03<00:56,  3.89it/s]  6%|▌         | 13/232 [00:03<00:56,  3.90it/s]  6%|▌         | 14/232 [00:03<00:57,  3.81it/s]  6%|▋         | 15/232 [00:03<00:56,  3.82it/s]  7%|▋         | 16/232 [00:04<00:57,  3.79it/s]  7%|▋         | 17/232 [00:04<00:55,  3.84it/s]  8%|▊         | 18/232 [00:04<00:54,  3.91it/s]  8%|▊         | 19/232 [00:04<00:55,  3.86it/s]  9%|▊         | 20/232 [00:05<00:55,  3.81it/s]  9%|▉         | 21/232 [00:05<00:55,  3.79it/s]  9%|▉         | 22/232 [00:05<00:54,  3.83it/s] 10%|▉         | 23/232 [00:06<00:53,  3.92it/s] 10%|█         | 24/232 [00:06<00:53,  3.86it/s] 11%|█         | 25/232 [00:06<00:54,  3.81it/s] 11%|█         | 26/232 [00:06<00:53,  3.84it/s] 12%|█▏        | 27/232 [00:07<00:54,  3.77it/s] 12%|█▏        | 28/232 [00:07<00:52,  3.87it/s] 12%|█▎        | 29/232 [00:07<00:52,  3.88it/s] 13%|█▎        | 30/232 [00:07<00:52,  3.84it/s] 13%|█▎        | 31/232 [00:08<00:53,  3.79it/s] 14%|█▍        | 32/232 [00:08<00:52,  3.82it/s] 14%|█▍        | 33/232 [00:08<00:52,  3.80it/s] 15%|█▍        | 34/232 [00:08<00:50,  3.90it/s] 15%|█▌        | 35/232 [00:09<00:51,  3.84it/s] 16%|█▌        | 36/232 [00:09<00:51,  3.82it/s] 16%|█▌        | 37/232 [00:09<00:50,  3.84it/s] 16%|█▋        | 38/232 [00:09<00:51,  3.77it/s] 17%|█▋        | 39/232 [00:10<00:49,  3.88it/s] 17%|█▋        | 40/232 [00:10<00:49,  3.90it/s] 18%|█▊        | 41/232 [00:10<00:49,  3.86it/s] 18%|█▊        | 42/232 [00:11<00:49,  3.81it/s] 19%|█▊        | 43/232 [00:11<00:49,  3.79it/s] 19%|█▉        | 44/232 [00:11<00:49,  3.82it/s] 19%|█▉        | 45/232 [00:11<00:47,  3.91it/s] 20%|█▉        | 46/232 [00:12<00:48,  3.84it/s] 20%|██        | 47/232 [00:12<00:48,  3.81it/s] 21%|██        | 48/232 [00:12<00:47,  3.84it/s] 21%|██        | 49/232 [00:12<00:48,  3.77it/s] 22%|██▏       | 50/232 [00:13<00:46,  3.88it/s] 22%|██▏       | 51/232 [00:13<00:46,  3.89it/s] 22%|██▏       | 52/232 [00:13<00:46,  3.86it/s] 23%|██▎       | 53/232 [00:13<00:46,  3.81it/s] 23%|██▎       | 54/232 [00:14<00:47,  3.78it/s] 24%|██▎       | 55/232 [00:14<00:46,  3.83it/s] 24%|██▍       | 56/232 [00:14<00:45,  3.91it/s] 25%|██▍       | 57/232 [00:14<00:45,  3.85it/s] 25%|██▌       | 58/232 [00:15<00:45,  3.80it/s] 25%|██▌       | 59/232 [00:15<00:45,  3.77it/s] 26%|██▌       | 60/232 [00:15<00:44,  3.83it/s] 26%|██▋       | 61/232 [00:15<00:43,  3.91it/s] 27%|██▋       | 62/232 [00:16<00:43,  3.87it/s] 27%|██▋       | 63/232 [00:16<00:44,  3.81it/s] 28%|██▊       | 64/232 [00:16<00:45,  3.73it/s] 28%|██▊       | 65/232 [00:16<00:43,  3.83it/s] 28%|██▊       | 66/232 [00:17<00:42,  3.91it/s] 29%|██▉       | 67/232 [00:17<00:42,  3.85it/s] 29%|██▉       | 68/232 [00:17<00:43,  3.80it/s] 30%|██▉       | 69/232 [00:18<00:42,  3.79it/s] 30%|███       | 70/232 [00:18<00:42,  3.78it/s] 31%|███       | 71/232 [00:18<00:41,  3.88it/s] 31%|███       | 72/232 [00:18<00:41,  3.82it/s] 31%|███▏      | 73/232 [00:19<00:41,  3.82it/s] 32%|███▏      | 74/232 [00:19<00:41,  3.83it/s] 32%|███▏      | 75/232 [00:19<00:41,  3.76it/s] 33%|███▎      | 76/232 [00:19<00:40,  3.87it/s] 33%|███▎      | 77/232 [00:20<00:39,  3.88it/s] 34%|███▎      | 78/232 [00:20<00:40,  3.81it/s] 34%|███▍      | 79/232 [00:20<00:40,  3.78it/s] 34%|███▍      | 80/232 [00:20<00:40,  3.72it/s] 35%|███▍      | 81/232 [00:21<00:39,  3.83it/s] 35%|███▌      | 82/232 [00:21<00:38,  3.91it/s] 36%|███▌      | 83/232 [00:21<00:38,  3.85it/s] 36%|███▌      | 84/232 [00:21<00:39,  3.79it/s] 37%|███▋      | 85/232 [00:22<00:39,  3.71it/s] 37%|███▋      | 86/232 [00:22<00:38,  3.82it/s] 38%|███▊      | 87/232 [00:22<00:37,  3.88it/s] 38%|███▊      | 88/232 [00:23<00:37,  3.83it/s] 38%|███▊      | 89/232 [00:23<00:37,  3.79it/s] 39%|███▉      | 90/232 [00:23<00:38,  3.70it/s] 39%|███▉      | 91/232 [00:23<00:36,  3.83it/s] 40%|███▉      | 92/232 [00:24<00:35,  3.90it/s] 40%|████      | 93/232 [00:24<00:36,  3.85it/s] 41%|████      | 94/232 [00:24<00:36,  3.79it/s] 41%|████      | 95/232 [00:24<00:37,  3.70it/s] 41%|████▏     | 96/232 [00:25<00:35,  3.82it/s] 42%|████▏     | 97/232 [00:25<00:34,  3.90it/s] 42%|████▏     | 98/232 [00:25<00:34,  3.84it/s] 43%|████▎     | 99/232 [00:25<00:35,  3.78it/s] 43%|████▎     | 100/232 [00:26<00:35,  3.69it/s] 44%|████▎     | 101/232 [00:26<00:34,  3.81it/s] 44%|████▍     | 102/232 [00:26<00:33,  3.89it/s] 44%|████▍     | 103/232 [00:26<00:33,  3.83it/s] 45%|████▍     | 104/232 [00:27<00:33,  3.78it/s] 45%|████▌     | 105/232 [00:27<00:34,  3.69it/s] 46%|████▌     | 106/232 [00:27<00:32,  3.82it/s] 46%|████▌     | 107/232 [00:28<00:32,  3.86it/s] 47%|████▋     | 108/232 [00:28<00:32,  3.82it/s] 47%|████▋     | 109/232 [00:28<00:32,  3.78it/s] 47%|████▋     | 110/232 [00:28<00:33,  3.69it/s] 48%|████▊     | 111/232 [00:29<00:31,  3.82it/s] 48%|████▊     | 112/232 [00:29<00:30,  3.88it/s] 49%|████▊     | 113/232 [00:29<00:31,  3.83it/s] 49%|████▉     | 114/232 [00:29<00:31,  3.78it/s] 50%|████▉     | 115/232 [00:30<00:31,  3.69it/s] 50%|█████     | 116/232 [00:30<00:30,  3.82it/s] 50%|█████     | 117/232 [00:30<00:29,  3.89it/s] 51%|█████     | 118/232 [00:30<00:29,  3.84it/s] 51%|█████▏    | 119/232 [00:31<00:29,  3.78it/s] 52%|█████▏    | 120/232 [00:31<00:30,  3.69it/s] 52%|█████▏    | 121/232 [00:31<00:29,  3.82it/s] 53%|█████▎    | 122/232 [00:31<00:28,  3.87it/s] 53%|█████▎    | 123/232 [00:32<00:28,  3.83it/s] 53%|█████▎    | 124/232 [00:32<00:28,  3.78it/s] 54%|█████▍    | 125/232 [00:32<00:28,  3.69it/s] 54%|█████▍    | 126/232 [00:33<00:27,  3.82it/s] 55%|█████▍    | 127/232 [00:33<00:27,  3.88it/s] 55%|█████▌    | 128/232 [00:33<00:27,  3.83it/s] 56%|█████▌    | 129/232 [00:33<00:27,  3.78it/s] 56%|█████▌    | 130/232 [00:34<00:27,  3.69it/s] 56%|█████▋    | 131/232 [00:34<00:26,  3.81it/s] 57%|█████▋    | 132/232 [00:34<00:25,  3.88it/s] 57%|█████▋    | 133/232 [00:34<00:25,  3.83it/s] 58%|█████▊    | 134/232 [00:35<00:25,  3.77it/s] 58%|█████▊    | 135/232 [00:35<00:25,  3.80it/s] 59%|█████▊    | 136/232 [00:35<00:25,  3.74it/s] 59%|█████▉    | 137/232 [00:35<00:24,  3.85it/s] 59%|█████▉    | 138/232 [00:36<00:24,  3.83it/s] 60%|█████▉    | 139/232 [00:36<00:24,  3.78it/s] 60%|██████    | 140/232 [00:36<00:24,  3.80it/s] 61%|██████    | 141/232 [00:36<00:24,  3.73it/s] 61%|██████    | 142/232 [00:37<00:23,  3.84it/s] 62%|██████▏   | 143/232 [00:37<00:23,  3.84it/s] 62%|██████▏   | 144/232 [00:37<00:23,  3.79it/s] 62%|██████▎   | 145/232 [00:38<00:22,  3.79it/s] 63%|██████▎   | 146/232 [00:38<00:23,  3.72it/s] 63%|██████▎   | 147/232 [00:38<00:22,  3.83it/s] 64%|██████▍   | 148/232 [00:38<00:22,  3.81it/s] 64%|██████▍   | 149/232 [00:39<00:22,  3.76it/s] 65%|██████▍   | 150/232 [00:39<00:21,  3.78it/s] 65%|██████▌   | 151/232 [00:39<00:21,  3.72it/s] 66%|██████▌   | 152/232 [00:39<00:20,  3.84it/s] 66%|██████▌   | 153/232 [00:40<00:20,  3.83it/s] 66%|██████▋   | 154/232 [00:40<00:20,  3.78it/s] 67%|██████▋   | 155/232 [00:40<00:20,  3.79it/s] 67%|██████▋   | 156/232 [00:40<00:20,  3.72it/s] 68%|██████▊   | 157/232 [00:41<00:19,  3.84it/s] 68%|██████▊   | 158/232 [00:41<00:19,  3.85it/s] 69%|██████▊   | 159/232 [00:41<00:19,  3.79it/s] 69%|██████▉   | 160/232 [00:41<00:18,  3.79it/s] 69%|██████▉   | 161/232 [00:42<00:19,  3.72it/s] 70%|██████▉   | 162/232 [00:42<00:18,  3.83it/s] 70%|███████   | 163/232 [00:42<00:18,  3.81it/s] 71%|███████   | 164/232 [00:43<00:18,  3.76it/s] 71%|███████   | 165/232 [00:43<00:17,  3.78it/s] 72%|███████▏  | 166/232 [00:43<00:17,  3.72it/s] 72%|███████▏  | 167/232 [00:43<00:16,  3.83it/s] 72%|███████▏  | 168/232 [00:44<00:16,  3.82it/s] 73%|███████▎  | 169/232 [00:44<00:16,  3.77it/s] 73%|███████▎  | 170/232 [00:44<00:16,  3.79it/s] 74%|███████▎  | 171/232 [00:44<00:16,  3.72it/s] 74%|███████▍  | 172/232 [00:45<00:15,  3.83it/s] 75%|███████▍  | 173/232 [00:45<00:15,  3.82it/s] 75%|███████▌  | 174/232 [00:45<00:15,  3.77it/s] 75%|███████▌  | 175/232 [00:45<00:15,  3.79it/s] 76%|███████▌  | 176/232 [00:46<00:15,  3.72it/s] 76%|███████▋  | 177/232 [00:46<00:14,  3.84it/s] 77%|███████▋  | 178/232 [00:46<00:14,  3.81it/s] 77%|███████▋  | 179/232 [00:47<00:14,  3.77it/s] 78%|███████▊  | 180/232 [00:47<00:13,  3.79it/s] 78%|███████▊  | 181/232 [00:47<00:13,  3.72it/s] 78%|███████▊  | 182/232 [00:47<00:13,  3.83it/s] 79%|███████▉  | 183/232 [00:48<00:12,  3.80it/s] 79%|███████▉  | 184/232 [00:48<00:12,  3.73it/s] 80%|███████▉  | 185/232 [00:48<00:12,  3.71it/s] 80%|████████  | 186/232 [00:48<00:12,  3.82it/s] 81%|████████  | 187/232 [00:49<00:11,  3.89it/s] 81%|████████  | 188/232 [00:49<00:11,  3.95it/s] 81%|████████▏ | 189/232 [00:49<00:10,  4.00it/s] 82%|████████▏ | 190/232 [00:49<00:10,  4.04it/s] 82%|████████▏ | 191/232 [00:50<00:10,  4.07it/s] 83%|████████▎ | 192/232 [00:50<00:09,  4.07it/s] 83%|████████▎ | 193/232 [00:50<00:09,  4.08it/s] 84%|████████▎ | 194/232 [00:50<00:09,  4.09it/s] 84%|████████▍ | 195/232 [00:51<00:09,  4.10it/s] 84%|████████▍ | 196/232 [00:51<00:08,  4.11it/s] 85%|████████▍ | 197/232 [00:51<00:08,  4.11it/s] 85%|████████▌ | 198/232 [00:51<00:08,  4.11it/s] 86%|████████▌ | 199/232 [00:52<00:08,  4.10it/s] 86%|████████▌ | 200/232 [00:52<00:07,  4.10it/s] 87%|████████▋ | 201/232 [00:52<00:07,  4.11it/s] 87%|████████▋ | 202/232 [00:52<00:07,  4.12it/s] 88%|████████▊ | 203/232 [00:52<00:07,  4.12it/s] 88%|████████▊ | 204/232 [00:53<00:06,  4.12it/s] 88%|████████▊ | 205/232 [00:53<00:06,  4.11it/s] 89%|████████▉ | 206/232 [00:53<00:06,  4.10it/s] 89%|████████▉ | 207/232 [00:53<00:06,  4.11it/s] 90%|████████▉ | 208/232 [00:54<00:05,  4.12it/s] 90%|█████████ | 209/232 [00:54<00:05,  4.13it/s] 91%|█████████ | 210/232 [00:54<00:05,  4.13it/s] 91%|█████████ | 211/232 [00:54<00:05,  4.12it/s] 91%|█████████▏| 212/232 [00:55<00:04,  4.11it/s] 92%|█████████▏| 213/232 [00:55<00:04,  4.11it/s] 92%|█████████▏| 214/232 [00:55<00:04,  4.01it/s] 93%|█████████▎| 215/232 [00:55<00:04,  3.90it/s] 93%|█████████▎| 216/232 [00:56<00:04,  3.79it/s] 94%|█████████▎| 217/232 [00:56<00:03,  3.89it/s] 94%|█████████▍| 218/232 [00:56<00:03,  3.87it/s] 94%|█████████▍| 219/232 [00:57<00:03,  3.75it/s] 95%|█████████▍| 220/232 [00:57<00:03,  3.79it/s] 95%|█████████▌| 221/232 [00:57<00:02,  3.72it/s] 96%|█████████▌| 222/232 [00:57<00:02,  3.78it/s] 96%|█████████▌| 223/232 [00:58<00:02,  3.70it/s] 97%|█████████▋| 224/232 [00:58<00:02,  3.73it/s] 97%|█████████▋| 225/232 [00:58<00:01,  3.69it/s] 97%|█████████▋| 226/232 [00:58<00:01,  3.75it/s] 98%|█████████▊| 227/232 [00:59<00:01,  3.68it/s] 98%|█████████▊| 228/232 [00:59<00:01,  3.72it/s] 99%|█████████▊| 229/232 [00:59<00:00,  3.68it/s] 99%|█████████▉| 230/232 [00:59<00:00,  3.76it/s]100%|█████████▉| 231/232 [01:00<00:00,  3.68it/s]100%|██████████| 232/232 [01:00<00:00,  3.72it/s]accuracy:  0.8793103448275862
100%|██████████| 232/232 [01:04<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 164
Training dataset size: 240, validation dataset size: 164
Training dataset size: 240, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[2025-03-12 05:03:24,644] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:03:24,820] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 05:03:24,838] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:46,  2.72s/it]  5%|▌         | 2/40 [00:05<01:40,  2.65s/it]  8%|▊         | 3/40 [00:07<01:32,  2.51s/it] 10%|█         | 4/40 [00:10<01:28,  2.46s/it] 12%|█▎        | 5/40 [00:12<01:22,  2.37s/it] 15%|█▌        | 6/40 [00:15<01:25,  2.50s/it] 18%|█▊        | 7/40 [00:17<01:21,  2.46s/it] 20%|██        | 8/40 [00:19<01:17,  2.43s/it] 22%|██▎       | 9/40 [00:22<01:14,  2.40s/it] 25%|██▌       | 10/40 [00:24<01:12,  2.41s/it]                                               {'loss': 1.2199, 'grad_norm': 7.706661224365234, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:24<01:12,  2.41s/it] 28%|██▊       | 11/40 [00:26<01:07,  2.33s/it] 30%|███       | 12/40 [00:29<01:05,  2.35s/it] 32%|███▎      | 13/40 [00:31<01:04,  2.39s/it] 35%|███▌      | 14/40 [00:33<01:01,  2.37s/it] 38%|███▊      | 15/40 [00:36<01:00,  2.44s/it] 40%|████      | 16/40 [00:39<01:03,  2.64s/it] 42%|████▎     | 17/40 [00:42<01:01,  2.67s/it] 45%|████▌     | 18/40 [00:44<00:55,  2.53s/it] 48%|████▊     | 19/40 [00:46<00:52,  2.48s/it] 50%|█████     | 20/40 [00:49<00:48,  2.41s/it]                                               {'loss': 0.9957, 'grad_norm': 6.6780877113342285, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:49<00:48,  2.41s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:51<00:45,  2.39s/it] 55%|█████▌    | 22/40 [00:54<00:45,  2.51s/it] 57%|█████▊    | 23/40 [00:56<00:43,  2.56s/it] 60%|██████    | 24/40 [00:59<00:40,  2.53s/it] 62%|██████▎   | 25/40 [01:01<00:36,  2.46s/it] 65%|██████▌   | 26/40 [01:03<00:32,  2.35s/it] 68%|██████▊   | 27/40 [01:06<00:31,  2.40s/it] 70%|███████   | 28/40 [01:08<00:27,  2.29s/it] 72%|███████▎  | 29/40 [01:10<00:25,  2.28s/it] 75%|███████▌  | 30/40 [01:12<00:22,  2.28s/it]                                               {'loss': 0.7994, 'grad_norm': 8.660933494567871, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:12<00:22,  2.28s/it] 78%|███████▊  | 31/40 [01:15<00:20,  2.25s/it] 80%|████████  | 32/40 [01:17<00:17,  2.23s/it] 82%|████████▎ | 33/40 [01:19<00:15,  2.26s/it] 85%|████████▌ | 34/40 [01:21<00:13,  2.23s/it] 88%|████████▊ | 35/40 [01:24<00:11,  2.31s/it] 90%|█████████ | 36/40 [01:26<00:09,  2.40s/it] 92%|█████████▎| 37/40 [01:29<00:07,  2.42s/it] 95%|█████████▌| 38/40 [01:31<00:04,  2.49s/it] 98%|█████████▊| 39/40 [01:34<00:02,  2.51s/it]100%|██████████| 40/40 [01:36<00:00,  2.47s/it]                                               {'loss': 0.854, 'grad_norm': 7.088910102844238, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:36<00:00,  2.47s/it]                                               {'train_runtime': 97.5564, 'train_samples_per_second': 4.92, 'train_steps_per_second': 0.41, 'train_loss': 0.9672410488128662, 'epoch': 2.0}
100%|██████████| 40/40 [01:37<00:00,  2.47s/it]100%|██████████| 40/40 [01:37<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:24,  2.94it/s]  1%|          | 2/248 [00:00<01:14,  3.30it/s]  1%|          | 3/248 [00:00<01:07,  3.61it/s]  2%|▏         | 4/248 [00:01<01:07,  3.63it/s]  2%|▏         | 5/248 [00:01<01:06,  3.64it/s]  2%|▏         | 6/248 [00:01<01:06,  3.64it/s]  3%|▎         | 7/248 [00:01<01:03,  3.80it/s]  3%|▎         | 8/248 [00:02<01:03,  3.79it/s]  4%|▎         | 9/248 [00:02<01:03,  3.74it/s]  4%|▍         | 10/248 [00:02<01:02,  3.80it/s]  4%|▍         | 11/248 [00:02<01:03,  3.74it/s]  5%|▍         | 12/248 [00:03<01:02,  3.78it/s]  5%|▌         | 13/248 [00:03<01:03,  3.71it/s]  6%|▌         | 14/248 [00:03<01:01,  3.78it/s]  6%|▌         | 15/248 [00:04<01:02,  3.73it/s]  6%|▋         | 16/248 [00:04<01:01,  3.80it/s]  7%|▋         | 17/248 [00:04<01:01,  3.74it/s]  7%|▋         | 18/248 [00:04<01:00,  3.79it/s]  8%|▊         | 19/248 [00:05<01:01,  3.74it/s]  8%|▊         | 20/248 [00:05<00:59,  3.82it/s]  8%|▊         | 21/248 [00:05<01:00,  3.75it/s]  9%|▉         | 22/248 [00:05<01:00,  3.76it/s]  9%|▉         | 23/248 [00:06<01:00,  3.71it/s] 10%|▉         | 24/248 [00:06<00:58,  3.82it/s] 10%|█         | 25/248 [00:06<00:59,  3.77it/s] 10%|█         | 26/248 [00:06<00:59,  3.74it/s] 11%|█         | 27/248 [00:07<00:59,  3.69it/s] 11%|█▏        | 28/248 [00:07<00:57,  3.82it/s] 12%|█▏        | 29/248 [00:07<00:57,  3.82it/s] 12%|█▏        | 30/248 [00:08<00:58,  3.75it/s] 12%|█▎        | 31/248 [00:08<00:57,  3.81it/s] 13%|█▎        | 32/248 [00:08<00:57,  3.74it/s] 13%|█▎        | 33/248 [00:08<00:56,  3.78it/s] 14%|█▎        | 34/248 [00:09<00:57,  3.73it/s] 14%|█▍        | 35/248 [00:09<00:56,  3.78it/s] 15%|█▍        | 36/248 [00:09<00:56,  3.73it/s] 15%|█▍        | 37/248 [00:09<00:55,  3.80it/s] 15%|█▌        | 38/248 [00:10<00:56,  3.74it/s] 16%|█▌        | 39/248 [00:10<00:55,  3.78it/s] 16%|█▌        | 40/248 [00:10<00:55,  3.73it/s] 17%|█▋        | 41/248 [00:10<00:54,  3.81it/s] 17%|█▋        | 42/248 [00:11<00:55,  3.74it/s] 17%|█▋        | 43/248 [00:11<00:54,  3.77it/s] 18%|█▊        | 44/248 [00:11<00:54,  3.72it/s] 18%|█▊        | 45/248 [00:12<00:53,  3.79it/s] 19%|█▊        | 46/248 [00:12<00:54,  3.73it/s] 19%|█▉        | 47/248 [00:12<00:53,  3.77it/s] 19%|█▉        | 48/248 [00:12<00:53,  3.72it/s] 20%|█▉        | 49/248 [00:13<00:52,  3.78it/s] 20%|██        | 50/248 [00:13<00:53,  3.71it/s] 21%|██        | 51/248 [00:13<00:52,  3.76it/s] 21%|██        | 52/248 [00:13<00:52,  3.71it/s] 21%|██▏       | 53/248 [00:14<00:51,  3.79it/s] 22%|██▏       | 54/248 [00:14<00:52,  3.73it/s] 22%|██▏       | 55/248 [00:14<00:51,  3.77it/s] 23%|██▎       | 56/248 [00:14<00:51,  3.72it/s] 23%|██▎       | 57/248 [00:15<00:50,  3.79it/s] 23%|██▎       | 58/248 [00:15<00:50,  3.73it/s] 24%|██▍       | 59/248 [00:15<00:50,  3.77it/s] 24%|██▍       | 60/248 [00:16<00:50,  3.71it/s] 25%|██▍       | 61/248 [00:16<00:49,  3.80it/s] 25%|██▌       | 62/248 [00:16<00:49,  3.74it/s] 25%|██▌       | 63/248 [00:16<00:49,  3.77it/s] 26%|██▌       | 64/248 [00:17<00:49,  3.72it/s] 26%|██▌       | 65/248 [00:17<00:48,  3.80it/s] 27%|██▋       | 66/248 [00:17<00:48,  3.74it/s] 27%|██▋       | 67/248 [00:17<00:48,  3.74it/s] 27%|██▋       | 68/248 [00:18<00:48,  3.70it/s] 28%|██▊       | 69/248 [00:18<00:46,  3.82it/s] 28%|██▊       | 70/248 [00:18<00:47,  3.75it/s] 29%|██▊       | 71/248 [00:18<00:47,  3.72it/s] 29%|██▉       | 72/248 [00:19<00:48,  3.66it/s] 29%|██▉       | 73/248 [00:19<00:46,  3.79it/s] 30%|██▉       | 74/248 [00:19<00:46,  3.75it/s] 30%|███       | 75/248 [00:20<00:46,  3.73it/s] 31%|███       | 76/248 [00:20<00:45,  3.78it/s] 31%|███       | 77/248 [00:20<00:46,  3.72it/s] 31%|███▏      | 78/248 [00:20<00:45,  3.77it/s] 32%|███▏      | 79/248 [00:21<00:45,  3.71it/s] 32%|███▏      | 80/248 [00:21<00:44,  3.75it/s] 33%|███▎      | 81/248 [00:21<00:45,  3.70it/s] 33%|███▎      | 82/248 [00:21<00:44,  3.77it/s] 33%|███▎      | 83/248 [00:22<00:44,  3.72it/s] 34%|███▍      | 84/248 [00:22<00:43,  3.73it/s] 34%|███▍      | 85/248 [00:22<00:44,  3.68it/s] 35%|███▍      | 86/248 [00:22<00:42,  3.79it/s] 35%|███▌      | 87/248 [00:23<00:43,  3.74it/s] 35%|███▌      | 88/248 [00:23<00:43,  3.71it/s] 36%|███▌      | 89/248 [00:23<00:43,  3.66it/s] 36%|███▋      | 90/248 [00:24<00:41,  3.79it/s] 37%|███▋      | 91/248 [00:24<00:41,  3.75it/s] 37%|███▋      | 92/248 [00:24<00:41,  3.72it/s] 38%|███▊      | 93/248 [00:24<00:42,  3.67it/s] 38%|███▊      | 94/248 [00:25<00:40,  3.80it/s] 38%|███▊      | 95/248 [00:25<00:40,  3.76it/s] 39%|███▊      | 96/248 [00:25<00:40,  3.73it/s] 39%|███▉      | 97/248 [00:25<00:39,  3.78it/s] 40%|███▉      | 98/248 [00:26<00:40,  3.73it/s] 40%|███▉      | 99/248 [00:26<00:39,  3.77it/s] 40%|████      | 100/248 [00:26<00:39,  3.71it/s] 41%|████      | 101/248 [00:26<00:39,  3.73it/s] 41%|████      | 102/248 [00:27<00:38,  3.77it/s] 42%|████▏     | 103/248 [00:27<00:37,  3.87it/s] 42%|████▏     | 104/248 [00:27<00:36,  3.95it/s] 42%|████▏     | 105/248 [00:27<00:35,  4.01it/s] 43%|████▎     | 106/248 [00:28<00:35,  4.05it/s] 43%|████▎     | 107/248 [00:28<00:34,  4.07it/s] 44%|████▎     | 108/248 [00:28<00:34,  4.08it/s] 44%|████▍     | 109/248 [00:28<00:33,  4.09it/s] 44%|████▍     | 110/248 [00:29<00:33,  4.10it/s] 45%|████▍     | 111/248 [00:29<00:33,  4.11it/s] 45%|████▌     | 112/248 [00:29<00:33,  4.12it/s] 46%|████▌     | 113/248 [00:29<00:32,  4.13it/s] 46%|████▌     | 114/248 [00:30<00:32,  4.13it/s] 46%|████▋     | 115/248 [00:30<00:32,  4.14it/s] 47%|████▋     | 116/248 [00:30<00:31,  4.13it/s] 47%|████▋     | 117/248 [00:30<00:31,  4.12it/s] 48%|████▊     | 118/248 [00:31<00:31,  4.11it/s] 48%|████▊     | 119/248 [00:31<00:31,  4.12it/s] 48%|████▊     | 120/248 [00:31<00:31,  4.12it/s] 49%|████▉     | 121/248 [00:31<00:30,  4.13it/s] 49%|████▉     | 122/248 [00:32<00:30,  4.13it/s] 50%|████▉     | 123/248 [00:32<00:30,  4.13it/s] 50%|█████     | 124/248 [00:32<00:30,  4.13it/s] 50%|█████     | 125/248 [00:32<00:29,  4.12it/s] 51%|█████     | 126/248 [00:33<00:29,  4.11it/s] 51%|█████     | 127/248 [00:33<00:29,  4.12it/s] 52%|█████▏    | 128/248 [00:33<00:29,  4.13it/s] 52%|█████▏    | 129/248 [00:33<00:28,  4.13it/s] 52%|█████▏    | 130/248 [00:34<00:28,  4.13it/s] 53%|█████▎    | 131/248 [00:34<00:28,  4.13it/s] 53%|█████▎    | 132/248 [00:34<00:28,  4.12it/s] 54%|█████▎    | 133/248 [00:34<00:28,  4.09it/s] 54%|█████▍    | 134/248 [00:35<00:28,  3.97it/s] 54%|█████▍    | 135/248 [00:35<00:28,  3.92it/s] 55%|█████▍    | 136/248 [00:35<00:29,  3.83it/s] 55%|█████▌    | 137/248 [00:35<00:28,  3.91it/s] 56%|█████▌    | 138/248 [00:36<00:28,  3.84it/s] 56%|█████▌    | 139/248 [00:36<00:29,  3.75it/s] 56%|█████▋    | 140/248 [00:36<00:29,  3.72it/s] 57%|█████▋    | 141/248 [00:36<00:27,  3.83it/s] 57%|█████▋    | 142/248 [00:37<00:27,  3.91it/s] 58%|█████▊    | 143/248 [00:37<00:26,  3.97it/s] 58%|█████▊    | 144/248 [00:37<00:25,  4.02it/s] 58%|█████▊    | 145/248 [00:37<00:25,  4.04it/s] 59%|█████▉    | 146/248 [00:38<00:25,  4.06it/s] 59%|█████▉    | 147/248 [00:38<00:24,  4.07it/s] 60%|█████▉    | 148/248 [00:38<00:24,  4.09it/s] 60%|██████    | 149/248 [00:38<00:24,  4.10it/s] 60%|██████    | 150/248 [00:39<00:23,  4.10it/s] 61%|██████    | 151/248 [00:39<00:23,  4.10it/s] 61%|██████▏   | 152/248 [00:39<00:23,  4.10it/s] 62%|██████▏   | 153/248 [00:39<00:23,  4.10it/s] 62%|██████▏   | 154/248 [00:40<00:22,  4.11it/s] 62%|██████▎   | 155/248 [00:40<00:22,  4.12it/s] 63%|██████▎   | 156/248 [00:40<00:22,  4.12it/s] 63%|██████▎   | 157/248 [00:40<00:22,  4.12it/s] 64%|██████▎   | 158/248 [00:41<00:21,  4.11it/s] 64%|██████▍   | 159/248 [00:41<00:21,  4.10it/s] 65%|██████▍   | 160/248 [00:41<00:21,  4.11it/s] 65%|██████▍   | 161/248 [00:41<00:21,  4.11it/s] 65%|██████▌   | 162/248 [00:41<00:20,  4.12it/s] 66%|██████▌   | 163/248 [00:42<00:20,  4.11it/s] 66%|██████▌   | 164/248 [00:42<00:20,  4.10it/s] 67%|██████▋   | 165/248 [00:42<00:20,  4.11it/s] 67%|██████▋   | 166/248 [00:42<00:19,  4.11it/s] 67%|██████▋   | 167/248 [00:43<00:19,  4.12it/s] 68%|██████▊   | 168/248 [00:43<00:19,  4.12it/s] 68%|██████▊   | 169/248 [00:43<00:19,  4.12it/s] 69%|██████▊   | 170/248 [00:43<00:18,  4.12it/s] 69%|██████▉   | 171/248 [00:44<00:18,  4.12it/s] 69%|██████▉   | 172/248 [00:44<00:18,  4.11it/s] 70%|██████▉   | 173/248 [00:44<00:18,  4.12it/s] 70%|███████   | 174/248 [00:44<00:17,  4.12it/s] 71%|███████   | 175/248 [00:45<00:18,  3.99it/s] 71%|███████   | 176/248 [00:45<00:18,  3.89it/s] 71%|███████▏  | 177/248 [00:45<00:18,  3.83it/s] 72%|███████▏  | 178/248 [00:45<00:18,  3.88it/s] 72%|███████▏  | 179/248 [00:46<00:18,  3.79it/s] 73%|███████▎  | 180/248 [00:46<00:18,  3.74it/s] 73%|███████▎  | 181/248 [00:46<00:18,  3.69it/s] 73%|███████▎  | 182/248 [00:47<00:17,  3.79it/s] 74%|███████▍  | 183/248 [00:47<00:17,  3.73it/s] 74%|███████▍  | 184/248 [00:47<00:17,  3.70it/s] 75%|███████▍  | 185/248 [00:47<00:17,  3.66it/s] 75%|███████▌  | 186/248 [00:48<00:16,  3.77it/s] 75%|███████▌  | 187/248 [00:48<00:16,  3.68it/s] 76%|███████▌  | 188/248 [00:48<00:16,  3.70it/s] 76%|███████▌  | 189/248 [00:48<00:16,  3.66it/s] 77%|███████▋  | 190/248 [00:49<00:15,  3.75it/s] 77%|███████▋  | 191/248 [00:49<00:15,  3.66it/s] 77%|███████▋  | 192/248 [00:49<00:15,  3.67it/s] 78%|███████▊  | 193/248 [00:50<00:14,  3.68it/s] 78%|███████▊  | 194/248 [00:50<00:14,  3.73it/s] 79%|███████▊  | 195/248 [00:50<00:14,  3.66it/s] 79%|███████▉  | 196/248 [00:50<00:14,  3.67it/s] 79%|███████▉  | 197/248 [00:51<00:13,  3.69it/s] 80%|███████▉  | 198/248 [00:51<00:13,  3.74it/s] 80%|████████  | 199/248 [00:51<00:13,  3.67it/s] 81%|████████  | 200/248 [00:51<00:13,  3.68it/s] 81%|████████  | 201/248 [00:52<00:12,  3.70it/s] 81%|████████▏ | 202/248 [00:52<00:12,  3.75it/s] 82%|████████▏ | 203/248 [00:52<00:12,  3.67it/s] 82%|████████▏ | 204/248 [00:53<00:11,  3.67it/s] 83%|████████▎ | 205/248 [00:53<00:11,  3.70it/s] 83%|████████▎ | 206/248 [00:53<00:11,  3.74it/s] 83%|████████▎ | 207/248 [00:53<00:11,  3.66it/s] 84%|████████▍ | 208/248 [00:54<00:11,  3.60it/s] 84%|████████▍ | 209/248 [00:54<00:10,  3.74it/s] 85%|████████▍ | 210/248 [00:54<00:10,  3.70it/s] 85%|████████▌ | 211/248 [00:54<00:10,  3.67it/s] 85%|████████▌ | 212/248 [00:55<00:09,  3.64it/s] 86%|████████▌ | 213/248 [00:55<00:09,  3.77it/s] 86%|████████▋ | 214/248 [00:55<00:09,  3.70it/s] 87%|████████▋ | 215/248 [00:56<00:08,  3.67it/s] 87%|████████▋ | 216/248 [00:56<00:08,  3.64it/s] 88%|████████▊ | 217/248 [00:56<00:08,  3.76it/s] 88%|████████▊ | 218/248 [00:56<00:08,  3.70it/s] 88%|████████▊ | 219/248 [00:57<00:07,  3.66it/s] 89%|████████▊ | 220/248 [00:57<00:07,  3.63it/s] 89%|████████▉ | 221/248 [00:57<00:07,  3.72it/s] 90%|████████▉ | 222/248 [00:57<00:07,  3.65it/s] 90%|████████▉ | 223/248 [00:58<00:06,  3.71it/s] 90%|█████████ | 224/248 [00:58<00:06,  3.67it/s] 91%|█████████ | 225/248 [00:58<00:06,  3.68it/s] 91%|█████████ | 226/248 [00:59<00:06,  3.62it/s] 92%|█████████▏| 227/248 [00:59<00:05,  3.57it/s] 92%|█████████▏| 228/248 [00:59<00:05,  3.71it/s] 92%|█████████▏| 229/248 [00:59<00:05,  3.66it/s] 93%|█████████▎| 230/248 [01:00<00:04,  3.64it/s] 93%|█████████▎| 231/248 [01:00<00:04,  3.62it/s] 94%|█████████▎| 232/248 [01:00<00:04,  3.74it/s] 94%|█████████▍| 233/248 [01:00<00:04,  3.69it/s] 94%|█████████▍| 234/248 [01:01<00:03,  3.66it/s] 95%|█████████▍| 235/248 [01:01<00:03,  3.64it/s] 95%|█████████▌| 236/248 [01:01<00:03,  3.74it/s] 96%|█████████▌| 237/248 [01:02<00:03,  3.66it/s] 96%|█████████▌| 238/248 [01:02<00:02,  3.71it/s] 96%|█████████▋| 239/248 [01:02<00:02,  3.66it/s] 97%|█████████▋| 240/248 [01:02<00:02,  3.72it/s] 97%|█████████▋| 241/248 [01:03<00:01,  3.65it/s] 98%|█████████▊| 242/248 [01:03<00:01,  3.68it/s] 98%|█████████▊| 243/248 [01:03<00:01,  3.68it/s] 98%|█████████▊| 244/248 [01:03<00:01,  3.72it/s] 99%|█████████▉| 245/248 [01:04<00:00,  3.65it/s] 99%|█████████▉| 246/248 [01:04<00:00,  3.59it/s]100%|█████████▉| 247/248 [01:04<00:00,  3.74it/s]100%|██████████| 248/248 [01:04<00:00,  3.69it/s]accuracy:  0.6612903225806451
100%|██████████| 248/248 [01:08<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 236
Training dataset size: 240, validation dataset size: 236
Training dataset size: 240, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.02s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 05:06:39,233] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:06:39,648] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:06:39,892] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:40,  2.57s/it]  5%|▌         | 2/40 [00:05<01:37,  2.57s/it]  8%|▊         | 3/40 [00:07<01:29,  2.42s/it] 10%|█         | 4/40 [00:09<01:25,  2.36s/it] 12%|█▎        | 5/40 [00:11<01:21,  2.34s/it] 15%|█▌        | 6/40 [00:14<01:25,  2.51s/it] 18%|█▊        | 7/40 [00:16<01:14,  2.27s/it] 20%|██        | 8/40 [00:18<01:09,  2.17s/it] 22%|██▎       | 9/40 [00:20<01:08,  2.21s/it] 25%|██▌       | 10/40 [00:23<01:07,  2.24s/it]                                               {'loss': 1.0568, 'grad_norm': 12.060351371765137, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:23<01:07,  2.24s/it] 28%|██▊       | 11/40 [00:25<01:09,  2.39s/it] 30%|███       | 12/40 [00:27<01:04,  2.29s/it] 32%|███▎      | 13/40 [00:30<01:02,  2.33s/it] 35%|███▌      | 14/40 [00:32<01:01,  2.36s/it] 38%|███▊      | 15/40 [00:34<00:57,  2.30s/it] 40%|████      | 16/40 [00:37<00:57,  2.41s/it] 42%|████▎     | 17/40 [00:40<00:55,  2.43s/it] 45%|████▌     | 18/40 [00:42<00:53,  2.43s/it] 48%|████▊     | 19/40 [00:44<00:49,  2.33s/it] 50%|█████     | 20/40 [00:47<00:49,  2.50s/it]                                               {'loss': 0.8096, 'grad_norm': 11.861091613769531, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:47<00:49,  2.50s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:50<00:48,  2.55s/it] 55%|█████▌    | 22/40 [00:52<00:45,  2.51s/it] 57%|█████▊    | 23/40 [00:54<00:42,  2.47s/it] 60%|██████    | 24/40 [00:57<00:39,  2.44s/it] 62%|██████▎   | 25/40 [00:59<00:34,  2.30s/it] 65%|██████▌   | 26/40 [01:01<00:32,  2.29s/it] 68%|██████▊   | 27/40 [01:03<00:30,  2.33s/it] 70%|███████   | 28/40 [01:06<00:26,  2.24s/it] 72%|███████▎  | 29/40 [01:08<00:25,  2.29s/it] 75%|███████▌  | 30/40 [01:10<00:23,  2.34s/it]                                               {'loss': 0.7711, 'grad_norm': 4.740952014923096, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:10<00:23,  2.34s/it] 78%|███████▊  | 31/40 [01:13<00:22,  2.53s/it] 80%|████████  | 32/40 [01:15<00:19,  2.40s/it] 82%|████████▎ | 33/40 [01:18<00:17,  2.54s/it] 85%|████████▌ | 34/40 [01:21<00:16,  2.69s/it] 88%|████████▊ | 35/40 [01:23<00:11,  2.37s/it] 90%|█████████ | 36/40 [01:25<00:09,  2.28s/it] 92%|█████████▎| 37/40 [01:27<00:06,  2.24s/it] 95%|█████████▌| 38/40 [01:29<00:04,  2.19s/it] 98%|█████████▊| 39/40 [01:32<00:02,  2.26s/it]100%|██████████| 40/40 [01:34<00:00,  2.36s/it]                                               {'loss': 0.716, 'grad_norm': 6.99965238571167, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.36s/it]                                               {'train_runtime': 95.4226, 'train_samples_per_second': 5.03, 'train_steps_per_second': 0.419, 'train_loss': 0.8383935213088989, 'epoch': 2.0}
100%|██████████| 40/40 [01:35<00:00,  2.36s/it]100%|██████████| 40/40 [01:35<00:00,  2.38s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<02:12,  2.71it/s]  1%|          | 2/361 [00:00<01:45,  3.41it/s]  1%|          | 3/361 [00:00<01:36,  3.72it/s]  1%|          | 4/361 [00:01<01:35,  3.75it/s]  1%|▏         | 5/361 [00:01<01:35,  3.73it/s]  2%|▏         | 6/361 [00:01<01:33,  3.80it/s]  2%|▏         | 7/361 [00:01<01:31,  3.87it/s]  2%|▏         | 8/361 [00:02<01:32,  3.81it/s]  2%|▏         | 9/361 [00:02<01:31,  3.83it/s]  3%|▎         | 10/361 [00:02<01:33,  3.76it/s]  3%|▎         | 11/361 [00:02<01:31,  3.83it/s]  3%|▎         | 12/361 [00:03<01:32,  3.78it/s]  4%|▎         | 13/361 [00:03<01:31,  3.80it/s]  4%|▍         | 14/361 [00:03<01:32,  3.73it/s]  4%|▍         | 15/361 [00:04<01:30,  3.80it/s]  4%|▍         | 16/361 [00:04<01:31,  3.77it/s]  5%|▍         | 17/361 [00:04<01:30,  3.79it/s]  5%|▍         | 18/361 [00:04<01:32,  3.73it/s]  5%|▌         | 19/361 [00:05<01:30,  3.80it/s]  6%|▌         | 20/361 [00:05<01:30,  3.77it/s]  6%|▌         | 21/361 [00:05<01:29,  3.79it/s]  6%|▌         | 22/361 [00:05<01:31,  3.72it/s]  6%|▋         | 23/361 [00:06<01:28,  3.80it/s]  7%|▋         | 24/361 [00:06<01:29,  3.76it/s]  7%|▋         | 25/361 [00:06<01:28,  3.79it/s]  7%|▋         | 26/361 [00:06<01:29,  3.73it/s]  7%|▋         | 27/361 [00:07<01:27,  3.81it/s]  8%|▊         | 28/361 [00:07<01:28,  3.76it/s]  8%|▊         | 29/361 [00:07<01:28,  3.75it/s]  8%|▊         | 30/361 [00:08<01:29,  3.70it/s]  9%|▊         | 31/361 [00:08<01:26,  3.83it/s]  9%|▉         | 32/361 [00:08<01:26,  3.80it/s]  9%|▉         | 33/361 [00:08<01:27,  3.75it/s]  9%|▉         | 34/361 [00:09<01:28,  3.71it/s] 10%|▉         | 35/361 [00:09<01:25,  3.83it/s] 10%|▉         | 36/361 [00:09<01:25,  3.80it/s] 10%|█         | 37/361 [00:09<01:26,  3.76it/s] 11%|█         | 38/361 [00:10<01:26,  3.72it/s] 11%|█         | 39/361 [00:10<01:23,  3.84it/s] 11%|█         | 40/361 [00:10<01:24,  3.81it/s] 11%|█▏        | 41/361 [00:10<01:24,  3.77it/s] 12%|█▏        | 42/361 [00:11<01:26,  3.71it/s] 12%|█▏        | 43/361 [00:11<01:22,  3.83it/s] 12%|█▏        | 44/361 [00:11<01:23,  3.82it/s] 12%|█▏        | 45/361 [00:11<01:23,  3.79it/s] 13%|█▎        | 46/361 [00:12<01:22,  3.82it/s] 13%|█▎        | 47/361 [00:12<01:23,  3.74it/s] 13%|█▎        | 48/361 [00:12<01:22,  3.80it/s] 14%|█▎        | 49/361 [00:13<01:22,  3.76it/s] 14%|█▍        | 50/361 [00:13<01:22,  3.78it/s] 14%|█▍        | 51/361 [00:13<01:23,  3.72it/s] 14%|█▍        | 52/361 [00:13<01:20,  3.82it/s] 15%|█▍        | 53/361 [00:14<01:21,  3.78it/s] 15%|█▍        | 54/361 [00:14<01:21,  3.75it/s] 15%|█▌        | 55/361 [00:14<01:22,  3.71it/s] 16%|█▌        | 56/361 [00:14<01:19,  3.83it/s] 16%|█▌        | 57/361 [00:15<01:20,  3.80it/s] 16%|█▌        | 58/361 [00:15<01:20,  3.75it/s] 16%|█▋        | 59/361 [00:15<01:21,  3.71it/s] 17%|█▋        | 60/361 [00:15<01:18,  3.83it/s] 17%|█▋        | 61/361 [00:16<01:18,  3.81it/s] 17%|█▋        | 62/361 [00:16<01:19,  3.76it/s] 17%|█▋        | 63/361 [00:16<01:20,  3.69it/s] 18%|█▊        | 64/361 [00:16<01:17,  3.81it/s] 18%|█▊        | 65/361 [00:17<01:17,  3.81it/s] 18%|█▊        | 66/361 [00:17<01:18,  3.76it/s] 19%|█▊        | 67/361 [00:17<01:18,  3.76it/s] 19%|█▉        | 68/361 [00:18<01:17,  3.76it/s] 19%|█▉        | 69/361 [00:18<01:16,  3.80it/s] 19%|█▉        | 70/361 [00:18<01:17,  3.76it/s] 20%|█▉        | 71/361 [00:18<01:16,  3.80it/s] 20%|█▉        | 72/361 [00:19<01:17,  3.74it/s] 20%|██        | 73/361 [00:19<01:16,  3.79it/s] 20%|██        | 74/361 [00:19<01:16,  3.75it/s] 21%|██        | 75/361 [00:19<01:16,  3.74it/s] 21%|██        | 76/361 [00:20<01:16,  3.74it/s] 21%|██▏       | 77/361 [00:20<01:15,  3.78it/s] 22%|██▏       | 78/361 [00:20<01:15,  3.75it/s] 22%|██▏       | 79/361 [00:20<01:15,  3.73it/s] 22%|██▏       | 80/361 [00:21<01:15,  3.74it/s] 22%|██▏       | 81/361 [00:21<01:13,  3.80it/s] 23%|██▎       | 82/361 [00:21<01:14,  3.76it/s] 23%|██▎       | 83/361 [00:22<01:13,  3.79it/s] 23%|██▎       | 84/361 [00:22<01:14,  3.72it/s] 24%|██▎       | 85/361 [00:22<01:13,  3.78it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.74it/s] 24%|██▍       | 87/361 [00:23<01:12,  3.79it/s] 24%|██▍       | 88/361 [00:23<01:13,  3.73it/s] 25%|██▍       | 89/361 [00:23<01:11,  3.79it/s] 25%|██▍       | 90/361 [00:23<01:12,  3.75it/s] 25%|██▌       | 91/361 [00:24<01:11,  3.78it/s] 25%|██▌       | 92/361 [00:24<01:12,  3.71it/s] 26%|██▌       | 93/361 [00:24<01:10,  3.80it/s] 26%|██▌       | 94/361 [00:24<01:11,  3.74it/s] 26%|██▋       | 95/361 [00:25<01:11,  3.73it/s] 27%|██▋       | 96/361 [00:25<01:11,  3.69it/s] 27%|██▋       | 97/361 [00:25<01:09,  3.81it/s] 27%|██▋       | 98/361 [00:26<01:09,  3.77it/s] 27%|██▋       | 99/361 [00:26<01:10,  3.74it/s] 28%|██▊       | 100/361 [00:26<01:10,  3.68it/s] 28%|██▊       | 101/361 [00:26<01:08,  3.79it/s] 28%|██▊       | 102/361 [00:27<01:08,  3.77it/s] 29%|██▊       | 103/361 [00:27<01:08,  3.74it/s] 29%|██▉       | 104/361 [00:27<01:09,  3.69it/s] 29%|██▉       | 105/361 [00:27<01:07,  3.81it/s] 29%|██▉       | 106/361 [00:28<01:07,  3.79it/s] 30%|██▉       | 107/361 [00:28<01:07,  3.76it/s] 30%|██▉       | 108/361 [00:28<01:07,  3.73it/s] 30%|███       | 109/361 [00:28<01:07,  3.76it/s] 30%|███       | 110/361 [00:29<01:05,  3.80it/s] 31%|███       | 111/361 [00:29<01:06,  3.76it/s] 31%|███       | 112/361 [00:29<01:05,  3.79it/s] 31%|███▏      | 113/361 [00:30<01:06,  3.72it/s] 32%|███▏      | 114/361 [00:30<01:05,  3.79it/s] 32%|███▏      | 115/361 [00:30<01:05,  3.75it/s] 32%|███▏      | 116/361 [00:30<01:05,  3.74it/s] 32%|███▏      | 117/361 [00:31<01:06,  3.69it/s] 33%|███▎      | 118/361 [00:31<01:04,  3.79it/s] 33%|███▎      | 119/361 [00:31<01:04,  3.75it/s] 33%|███▎      | 120/361 [00:31<01:04,  3.72it/s] 34%|███▎      | 121/361 [00:32<01:05,  3.67it/s] 34%|███▍      | 122/361 [00:32<01:03,  3.79it/s] 34%|███▍      | 123/361 [00:32<01:03,  3.76it/s] 34%|███▍      | 124/361 [00:32<01:03,  3.73it/s] 35%|███▍      | 125/361 [00:33<01:04,  3.68it/s] 35%|███▍      | 126/361 [00:33<01:01,  3.80it/s] 35%|███▌      | 127/361 [00:33<01:01,  3.78it/s] 35%|███▌      | 128/361 [00:34<01:02,  3.73it/s] 36%|███▌      | 129/361 [00:34<01:03,  3.67it/s] 36%|███▌      | 130/361 [00:34<01:01,  3.78it/s] 36%|███▋      | 131/361 [00:34<01:01,  3.76it/s] 37%|███▋      | 132/361 [00:35<01:01,  3.73it/s] 37%|███▋      | 133/361 [00:35<01:02,  3.66it/s] 37%|███▋      | 134/361 [00:35<00:59,  3.79it/s] 37%|███▋      | 135/361 [00:35<00:59,  3.81it/s] 38%|███▊      | 136/361 [00:36<00:59,  3.77it/s] 38%|███▊      | 137/361 [00:36<01:00,  3.71it/s] 38%|███▊      | 138/361 [00:36<00:59,  3.75it/s] 39%|███▊      | 139/361 [00:36<00:58,  3.81it/s] 39%|███▉      | 140/361 [00:37<00:58,  3.77it/s] 39%|███▉      | 141/361 [00:37<00:57,  3.79it/s] 39%|███▉      | 142/361 [00:37<00:58,  3.74it/s] 40%|███▉      | 143/361 [00:38<00:57,  3.79it/s] 40%|███▉      | 144/361 [00:38<00:57,  3.75it/s] 40%|████      | 145/361 [00:38<00:56,  3.79it/s] 40%|████      | 146/361 [00:38<00:57,  3.73it/s] 41%|████      | 147/361 [00:39<00:56,  3.79it/s] 41%|████      | 148/361 [00:39<00:56,  3.75it/s] 41%|████▏     | 149/361 [00:39<00:56,  3.78it/s] 42%|████▏     | 150/361 [00:39<00:56,  3.73it/s] 42%|████▏     | 151/361 [00:40<00:55,  3.80it/s] 42%|████▏     | 152/361 [00:40<00:55,  3.76it/s] 42%|████▏     | 153/361 [00:40<00:55,  3.77it/s] 43%|████▎     | 154/361 [00:40<00:55,  3.71it/s] 43%|████▎     | 155/361 [00:41<00:54,  3.78it/s] 43%|████▎     | 156/361 [00:41<00:54,  3.74it/s] 43%|████▎     | 157/361 [00:41<00:54,  3.77it/s] 44%|████▍     | 158/361 [00:42<00:54,  3.71it/s] 44%|████▍     | 159/361 [00:42<00:53,  3.78it/s] 44%|████▍     | 160/361 [00:42<00:53,  3.74it/s] 45%|████▍     | 161/361 [00:42<00:53,  3.76it/s] 45%|████▍     | 162/361 [00:43<00:53,  3.72it/s] 45%|████▌     | 163/361 [00:43<00:52,  3.76it/s] 45%|████▌     | 164/361 [00:43<00:52,  3.73it/s] 46%|████▌     | 165/361 [00:43<00:53,  3.68it/s] 46%|████▌     | 166/361 [00:44<00:52,  3.73it/s] 46%|████▋     | 167/361 [00:44<00:51,  3.77it/s] 47%|████▋     | 168/361 [00:44<00:51,  3.73it/s] 47%|████▋     | 169/361 [00:44<00:52,  3.67it/s] 47%|████▋     | 170/361 [00:45<00:51,  3.74it/s] 47%|████▋     | 171/361 [00:45<00:50,  3.76it/s] 48%|████▊     | 172/361 [00:45<00:50,  3.73it/s] 48%|████▊     | 173/361 [00:46<00:51,  3.68it/s] 48%|████▊     | 174/361 [00:46<00:49,  3.75it/s] 48%|████▊     | 175/361 [00:46<00:49,  3.78it/s] 49%|████▉     | 176/361 [00:46<00:49,  3.75it/s] 49%|████▉     | 177/361 [00:47<00:49,  3.68it/s] 49%|████▉     | 178/361 [00:47<00:48,  3.75it/s] 50%|████▉     | 179/361 [00:47<00:48,  3.77it/s] 50%|████▉     | 180/361 [00:47<00:48,  3.73it/s] 50%|█████     | 181/361 [00:48<00:48,  3.68it/s] 50%|█████     | 182/361 [00:48<00:47,  3.76it/s] 51%|█████     | 183/361 [00:48<00:46,  3.79it/s] 51%|█████     | 184/361 [00:48<00:47,  3.75it/s] 51%|█████     | 185/361 [00:49<00:47,  3.69it/s] 52%|█████▏    | 186/361 [00:49<00:46,  3.75it/s] 52%|█████▏    | 187/361 [00:49<00:46,  3.77it/s] 52%|█████▏    | 188/361 [00:50<00:46,  3.73it/s] 52%|█████▏    | 189/361 [00:50<00:46,  3.68it/s] 53%|█████▎    | 190/361 [00:50<00:45,  3.76it/s] 53%|█████▎    | 191/361 [00:50<00:44,  3.78it/s] 53%|█████▎    | 192/361 [00:51<00:45,  3.75it/s] 53%|█████▎    | 193/361 [00:51<00:45,  3.70it/s] 54%|█████▎    | 194/361 [00:51<00:44,  3.74it/s] 54%|█████▍    | 195/361 [00:51<00:43,  3.78it/s] 54%|█████▍    | 196/361 [00:52<00:44,  3.74it/s] 55%|█████▍    | 197/361 [00:52<00:44,  3.72it/s] 55%|█████▍    | 198/361 [00:52<00:43,  3.74it/s] 55%|█████▌    | 199/361 [00:52<00:42,  3.80it/s] 55%|█████▌    | 200/361 [00:53<00:42,  3.75it/s] 56%|█████▌    | 201/361 [00:53<00:43,  3.72it/s] 56%|█████▌    | 202/361 [00:53<00:42,  3.73it/s] 56%|█████▌    | 203/361 [00:54<00:41,  3.77it/s] 57%|█████▋    | 204/361 [00:54<00:42,  3.74it/s] 57%|█████▋    | 205/361 [00:54<00:42,  3.68it/s] 57%|█████▋    | 206/361 [00:54<00:41,  3.74it/s] 57%|█████▋    | 207/361 [00:55<00:40,  3.77it/s] 58%|█████▊    | 208/361 [00:55<00:40,  3.74it/s] 58%|█████▊    | 209/361 [00:55<00:41,  3.68it/s] 58%|█████▊    | 210/361 [00:55<00:40,  3.74it/s] 58%|█████▊    | 211/361 [00:56<00:39,  3.76it/s] 59%|█████▊    | 212/361 [00:56<00:40,  3.72it/s] 59%|█████▉    | 213/361 [00:56<00:40,  3.66it/s] 59%|█████▉    | 214/361 [00:57<00:39,  3.73it/s] 60%|█████▉    | 215/361 [00:57<00:38,  3.74it/s] 60%|█████▉    | 216/361 [00:57<00:39,  3.70it/s] 60%|██████    | 217/361 [00:57<00:39,  3.65it/s] 60%|██████    | 218/361 [00:58<00:38,  3.72it/s] 61%|██████    | 219/361 [00:58<00:37,  3.75it/s] 61%|██████    | 220/361 [00:58<00:38,  3.71it/s] 61%|██████    | 221/361 [00:58<00:38,  3.65it/s] 61%|██████▏   | 222/361 [00:59<00:37,  3.72it/s] 62%|██████▏   | 223/361 [00:59<00:36,  3.75it/s] 62%|██████▏   | 224/361 [00:59<00:36,  3.71it/s] 62%|██████▏   | 225/361 [00:59<00:37,  3.66it/s] 63%|██████▎   | 226/361 [01:00<00:36,  3.73it/s] 63%|██████▎   | 227/361 [01:00<00:35,  3.75it/s] 63%|██████▎   | 228/361 [01:00<00:35,  3.72it/s] 63%|██████▎   | 229/361 [01:01<00:35,  3.67it/s] 64%|██████▎   | 230/361 [01:01<00:35,  3.74it/s] 64%|██████▍   | 231/361 [01:01<00:34,  3.77it/s] 64%|██████▍   | 232/361 [01:01<00:34,  3.73it/s] 65%|██████▍   | 233/361 [01:02<00:34,  3.67it/s] 65%|██████▍   | 234/361 [01:02<00:34,  3.73it/s] 65%|██████▌   | 235/361 [01:02<00:33,  3.76it/s] 65%|██████▌   | 236/361 [01:02<00:33,  3.72it/s] 66%|██████▌   | 237/361 [01:03<00:33,  3.67it/s] 66%|██████▌   | 238/361 [01:03<00:32,  3.73it/s] 66%|██████▌   | 239/361 [01:03<00:32,  3.75it/s] 66%|██████▋   | 240/361 [01:04<00:32,  3.71it/s] 67%|██████▋   | 241/361 [01:04<00:32,  3.66it/s] 67%|██████▋   | 242/361 [01:04<00:31,  3.72it/s] 67%|██████▋   | 243/361 [01:04<00:31,  3.75it/s] 68%|██████▊   | 244/361 [01:05<00:31,  3.71it/s] 68%|██████▊   | 245/361 [01:05<00:31,  3.65it/s] 68%|██████▊   | 246/361 [01:05<00:30,  3.72it/s] 68%|██████▊   | 247/361 [01:05<00:30,  3.75it/s] 69%|██████▊   | 248/361 [01:06<00:30,  3.72it/s] 69%|██████▉   | 249/361 [01:06<00:30,  3.66it/s] 69%|██████▉   | 250/361 [01:06<00:29,  3.73it/s] 70%|██████▉   | 251/361 [01:06<00:29,  3.77it/s] 70%|██████▉   | 252/361 [01:07<00:29,  3.73it/s] 70%|███████   | 253/361 [01:07<00:29,  3.68it/s] 70%|███████   | 254/361 [01:07<00:28,  3.74it/s] 71%|███████   | 255/361 [01:08<00:28,  3.77it/s] 71%|███████   | 256/361 [01:08<00:28,  3.73it/s] 71%|███████   | 257/361 [01:08<00:28,  3.67it/s] 71%|███████▏  | 258/361 [01:08<00:27,  3.74it/s] 72%|███████▏  | 259/361 [01:09<00:27,  3.76it/s] 72%|███████▏  | 260/361 [01:09<00:27,  3.72it/s] 72%|███████▏  | 261/361 [01:09<00:27,  3.68it/s] 73%|███████▎  | 262/361 [01:09<00:26,  3.74it/s] 73%|███████▎  | 263/361 [01:10<00:26,  3.77it/s] 73%|███████▎  | 264/361 [01:10<00:25,  3.74it/s] 73%|███████▎  | 265/361 [01:10<00:26,  3.68it/s] 74%|███████▎  | 266/361 [01:10<00:25,  3.75it/s] 74%|███████▍  | 267/361 [01:11<00:24,  3.77it/s] 74%|███████▍  | 268/361 [01:11<00:24,  3.73it/s] 75%|███████▍  | 269/361 [01:11<00:25,  3.67it/s] 75%|███████▍  | 270/361 [01:12<00:24,  3.74it/s] 75%|███████▌  | 271/361 [01:12<00:23,  3.76it/s] 75%|███████▌  | 272/361 [01:12<00:23,  3.73it/s] 76%|███████▌  | 273/361 [01:12<00:23,  3.67it/s] 76%|███████▌  | 274/361 [01:13<00:23,  3.74it/s] 76%|███████▌  | 275/361 [01:13<00:22,  3.77it/s] 76%|███████▋  | 276/361 [01:13<00:22,  3.73it/s] 77%|███████▋  | 277/361 [01:13<00:22,  3.67it/s] 77%|███████▋  | 278/361 [01:14<00:22,  3.74it/s] 77%|███████▋  | 279/361 [01:14<00:21,  3.76it/s] 78%|███████▊  | 280/361 [01:14<00:21,  3.72it/s] 78%|███████▊  | 281/361 [01:15<00:21,  3.67it/s] 78%|███████▊  | 282/361 [01:15<00:21,  3.73it/s] 78%|███████▊  | 283/361 [01:15<00:20,  3.77it/s] 79%|███████▊  | 284/361 [01:15<00:20,  3.72it/s] 79%|███████▉  | 285/361 [01:16<00:20,  3.66it/s] 79%|███████▉  | 286/361 [01:16<00:20,  3.73it/s] 80%|███████▉  | 287/361 [01:16<00:19,  3.75it/s] 80%|███████▉  | 288/361 [01:16<00:19,  3.72it/s] 80%|████████  | 289/361 [01:17<00:19,  3.66it/s] 80%|████████  | 290/361 [01:17<00:19,  3.73it/s] 81%|████████  | 291/361 [01:17<00:18,  3.75it/s] 81%|████████  | 292/361 [01:17<00:18,  3.70it/s] 81%|████████  | 293/361 [01:18<00:18,  3.66it/s] 81%|████████▏ | 294/361 [01:18<00:17,  3.73it/s] 82%|████████▏ | 295/361 [01:18<00:17,  3.75it/s] 82%|████████▏ | 296/361 [01:19<00:17,  3.72it/s] 82%|████████▏ | 297/361 [01:19<00:17,  3.67it/s] 83%|████████▎ | 298/361 [01:19<00:16,  3.74it/s] 83%|████████▎ | 299/361 [01:19<00:16,  3.77it/s] 83%|████████▎ | 300/361 [01:20<00:16,  3.74it/s] 83%|████████▎ | 301/361 [01:20<00:16,  3.67it/s] 84%|████████▎ | 302/361 [01:20<00:15,  3.73it/s] 84%|████████▍ | 303/361 [01:20<00:15,  3.76it/s] 84%|████████▍ | 304/361 [01:21<00:15,  3.72it/s] 84%|████████▍ | 305/361 [01:21<00:15,  3.66it/s] 85%|████████▍ | 306/361 [01:21<00:14,  3.72it/s] 85%|████████▌ | 307/361 [01:22<00:14,  3.75it/s] 85%|████████▌ | 308/361 [01:22<00:14,  3.71it/s] 86%|████████▌ | 309/361 [01:22<00:14,  3.65it/s] 86%|████████▌ | 310/361 [01:22<00:13,  3.72it/s] 86%|████████▌ | 311/361 [01:23<00:13,  3.76it/s] 86%|████████▋ | 312/361 [01:23<00:13,  3.72it/s] 87%|████████▋ | 313/361 [01:23<00:13,  3.67it/s] 87%|████████▋ | 314/361 [01:23<00:12,  3.74it/s] 87%|████████▋ | 315/361 [01:24<00:12,  3.76it/s] 88%|████████▊ | 316/361 [01:24<00:12,  3.73it/s] 88%|████████▊ | 317/361 [01:24<00:11,  3.67it/s] 88%|████████▊ | 318/361 [01:24<00:11,  3.74it/s] 88%|████████▊ | 319/361 [01:25<00:11,  3.78it/s] 89%|████████▊ | 320/361 [01:25<00:10,  3.74it/s] 89%|████████▉ | 321/361 [01:25<00:10,  3.68it/s] 89%|████████▉ | 322/361 [01:26<00:10,  3.74it/s] 89%|████████▉ | 323/361 [01:26<00:10,  3.77it/s] 90%|████████▉ | 324/361 [01:26<00:09,  3.73it/s] 90%|█████████ | 325/361 [01:26<00:09,  3.67it/s] 90%|█████████ | 326/361 [01:27<00:09,  3.73it/s] 91%|█████████ | 327/361 [01:27<00:09,  3.76it/s] 91%|█████████ | 328/361 [01:27<00:08,  3.73it/s] 91%|█████████ | 329/361 [01:27<00:08,  3.68it/s] 91%|█████████▏| 330/361 [01:28<00:08,  3.74it/s] 92%|█████████▏| 331/361 [01:28<00:07,  3.76it/s] 92%|█████████▏| 332/361 [01:28<00:07,  3.73it/s] 92%|█████████▏| 333/361 [01:29<00:07,  3.66it/s] 93%|█████████▎| 334/361 [01:29<00:07,  3.73it/s] 93%|█████████▎| 335/361 [01:29<00:06,  3.76it/s] 93%|█████████▎| 336/361 [01:29<00:06,  3.72it/s] 93%|█████████▎| 337/361 [01:30<00:06,  3.66it/s] 94%|█████████▎| 338/361 [01:30<00:06,  3.73it/s] 94%|█████████▍| 339/361 [01:30<00:05,  3.76it/s] 94%|█████████▍| 340/361 [01:30<00:05,  3.72it/s] 94%|█████████▍| 341/361 [01:31<00:05,  3.66it/s] 95%|█████████▍| 342/361 [01:31<00:05,  3.73it/s] 95%|█████████▌| 343/361 [01:31<00:04,  3.76it/s] 95%|█████████▌| 344/361 [01:31<00:04,  3.72it/s] 96%|█████████▌| 345/361 [01:32<00:04,  3.68it/s] 96%|█████████▌| 346/361 [01:32<00:04,  3.74it/s] 96%|█████████▌| 347/361 [01:32<00:03,  3.77it/s] 96%|█████████▋| 348/361 [01:33<00:03,  3.73it/s] 97%|█████████▋| 349/361 [01:33<00:03,  3.66it/s] 97%|█████████▋| 350/361 [01:33<00:02,  3.73it/s] 97%|█████████▋| 351/361 [01:33<00:02,  3.76it/s] 98%|█████████▊| 352/361 [01:34<00:02,  3.72it/s] 98%|█████████▊| 353/361 [01:34<00:02,  3.71it/s] 98%|█████████▊| 354/361 [01:34<00:01,  3.82it/s] 98%|█████████▊| 355/361 [01:34<00:01,  3.89it/s] 99%|█████████▊| 356/361 [01:35<00:01,  3.95it/s] 99%|█████████▉| 357/361 [01:35<00:01,  4.00it/s] 99%|█████████▉| 358/361 [01:35<00:00,  4.03it/s] 99%|█████████▉| 359/361 [01:35<00:00,  4.06it/s]100%|█████████▉| 360/361 [01:36<00:00,  4.06it/s]100%|██████████| 361/361 [01:36<00:00,  4.07it/s]accuracy:  0.6011080332409973
100%|██████████| 361/361 [01:41<00:00,  3.54it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 188
Training dataset size: 240, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 240, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:10:23,920] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 05:10:23,922] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:10:24,724] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:36,  2.47s/it]  5%|▌         | 2/40 [00:05<01:41,  2.68s/it]  8%|▊         | 3/40 [00:08<01:45,  2.85s/it] 10%|█         | 4/40 [00:10<01:34,  2.63s/it] 12%|█▎        | 5/40 [00:13<01:29,  2.57s/it] 15%|█▌        | 6/40 [00:15<01:30,  2.65s/it] 18%|█▊        | 7/40 [00:18<01:27,  2.64s/it] 20%|██        | 8/40 [00:21<01:24,  2.64s/it] 22%|██▎       | 9/40 [00:23<01:18,  2.53s/it] 25%|██▌       | 10/40 [00:26<01:16,  2.53s/it]                                               {'loss': 0.9693, 'grad_norm': 9.95165729522705, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:26<01:16,  2.53s/it] 28%|██▊       | 11/40 [00:28<01:16,  2.65s/it] 30%|███       | 12/40 [00:31<01:12,  2.58s/it] 32%|███▎      | 13/40 [00:34<01:12,  2.68s/it] 35%|███▌      | 14/40 [00:37<01:10,  2.73s/it] 38%|███▊      | 15/40 [00:39<01:06,  2.65s/it] 40%|████      | 16/40 [00:42<01:07,  2.80s/it] 42%|████▎     | 17/40 [00:45<01:01,  2.66s/it] 45%|████▌     | 18/40 [00:48<01:00,  2.76s/it] 48%|████▊     | 19/40 [00:50<00:57,  2.72s/it] 50%|█████     | 20/40 [00:52<00:51,  2.56s/it]                                               {'loss': 0.6104, 'grad_norm': 3.4858574867248535, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:52<00:51,  2.56s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:55<00:51,  2.69s/it] 55%|█████▌    | 22/40 [00:58<00:50,  2.83s/it] 57%|█████▊    | 23/40 [01:01<00:44,  2.65s/it] 60%|██████    | 24/40 [01:04<00:43,  2.70s/it] 62%|██████▎   | 25/40 [01:06<00:39,  2.65s/it] 65%|██████▌   | 26/40 [01:09<00:38,  2.76s/it] 68%|██████▊   | 27/40 [01:12<00:35,  2.74s/it] 70%|███████   | 28/40 [01:15<00:33,  2.77s/it] 72%|███████▎  | 29/40 [01:18<00:31,  2.84s/it] 75%|███████▌  | 30/40 [01:20<00:28,  2.81s/it]                                               {'loss': 0.324, 'grad_norm': 2.1108968257904053, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:20<00:28,  2.81s/it] 78%|███████▊  | 31/40 [01:23<00:24,  2.72s/it] 80%|████████  | 32/40 [01:26<00:21,  2.70s/it] 82%|████████▎ | 33/40 [01:28<00:18,  2.65s/it] 85%|████████▌ | 34/40 [01:31<00:15,  2.62s/it] 88%|████████▊ | 35/40 [01:33<00:12,  2.52s/it] 90%|█████████ | 36/40 [01:35<00:10,  2.54s/it] 92%|█████████▎| 37/40 [01:39<00:08,  2.71s/it] 95%|█████████▌| 38/40 [01:41<00:05,  2.71s/it] 98%|█████████▊| 39/40 [01:45<00:02,  2.87s/it]100%|██████████| 40/40 [01:47<00:00,  2.65s/it]                                               {'loss': 0.442, 'grad_norm': 5.7217841148376465, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:47<00:00,  2.65s/it]                                               {'train_runtime': 107.8233, 'train_samples_per_second': 4.452, 'train_steps_per_second': 0.371, 'train_loss': 0.5864243388175965, 'epoch': 2.0}
100%|██████████| 40/40 [01:47<00:00,  2.65s/it]100%|██████████| 40/40 [01:47<00:00,  2.69s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:12,  2.91it/s]  1%|          | 2/212 [00:00<01:04,  3.23it/s]  1%|▏         | 3/212 [00:00<00:58,  3.60it/s]  2%|▏         | 4/212 [00:01<00:57,  3.63it/s]  2%|▏         | 5/212 [00:01<00:56,  3.65it/s]  3%|▎         | 6/212 [00:01<00:56,  3.61it/s]  3%|▎         | 7/212 [00:01<00:54,  3.77it/s]  4%|▍         | 8/212 [00:02<00:53,  3.78it/s]  4%|▍         | 9/212 [00:02<00:54,  3.73it/s]  5%|▍         | 10/212 [00:02<00:53,  3.77it/s]  5%|▌         | 11/212 [00:03<00:53,  3.73it/s]  6%|▌         | 12/212 [00:03<00:53,  3.76it/s]  6%|▌         | 13/212 [00:03<00:53,  3.70it/s]  7%|▋         | 14/212 [00:03<00:52,  3.76it/s]  7%|▋         | 15/212 [00:04<00:53,  3.70it/s]  8%|▊         | 16/212 [00:04<00:52,  3.76it/s]  8%|▊         | 17/212 [00:04<00:52,  3.70it/s]  8%|▊         | 18/212 [00:04<00:51,  3.76it/s]  9%|▉         | 19/212 [00:05<00:51,  3.76it/s]  9%|▉         | 20/212 [00:05<00:49,  3.87it/s] 10%|▉         | 21/212 [00:05<00:48,  3.94it/s] 10%|█         | 22/212 [00:05<00:47,  4.00it/s] 11%|█         | 23/212 [00:06<00:46,  4.05it/s] 11%|█▏        | 24/212 [00:06<00:46,  4.08it/s] 12%|█▏        | 25/212 [00:06<00:45,  4.11it/s] 12%|█▏        | 26/212 [00:06<00:45,  4.12it/s] 13%|█▎        | 27/212 [00:07<00:44,  4.13it/s] 13%|█▎        | 28/212 [00:07<00:44,  4.14it/s] 14%|█▎        | 29/212 [00:07<00:44,  4.15it/s] 14%|█▍        | 30/212 [00:07<00:43,  4.15it/s] 15%|█▍        | 31/212 [00:08<00:43,  4.15it/s] 15%|█▌        | 32/212 [00:08<00:43,  4.15it/s] 16%|█▌        | 33/212 [00:08<00:43,  4.16it/s] 16%|█▌        | 34/212 [00:08<00:42,  4.16it/s] 17%|█▋        | 35/212 [00:08<00:42,  4.16it/s] 17%|█▋        | 36/212 [00:09<00:42,  4.17it/s] 17%|█▋        | 37/212 [00:09<00:42,  4.16it/s] 18%|█▊        | 38/212 [00:09<00:41,  4.16it/s] 18%|█▊        | 39/212 [00:09<00:41,  4.16it/s] 19%|█▉        | 40/212 [00:10<00:41,  4.17it/s] 19%|█▉        | 41/212 [00:10<00:40,  4.17it/s] 20%|█▉        | 42/212 [00:10<00:40,  4.17it/s] 20%|██        | 43/212 [00:10<00:40,  4.17it/s] 21%|██        | 44/212 [00:11<00:40,  4.16it/s] 21%|██        | 45/212 [00:11<00:40,  4.16it/s] 22%|██▏       | 46/212 [00:11<00:39,  4.16it/s] 22%|██▏       | 47/212 [00:11<00:40,  4.09it/s] 23%|██▎       | 48/212 [00:12<00:40,  4.00it/s] 23%|██▎       | 49/212 [00:12<00:41,  3.92it/s] 24%|██▎       | 50/212 [00:12<00:41,  3.91it/s] 24%|██▍       | 51/212 [00:12<00:40,  3.97it/s] 25%|██▍       | 52/212 [00:13<00:41,  3.85it/s] 25%|██▌       | 53/212 [00:13<00:42,  3.75it/s] 25%|██▌       | 54/212 [00:13<00:42,  3.67it/s] 26%|██▌       | 55/212 [00:14<00:41,  3.81it/s] 26%|██▋       | 56/212 [00:14<00:40,  3.83it/s] 27%|██▋       | 57/212 [00:14<00:41,  3.70it/s] 27%|██▋       | 58/212 [00:14<00:41,  3.75it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.76it/s] 28%|██▊       | 60/212 [00:15<00:39,  3.87it/s] 29%|██▉       | 61/212 [00:15<00:38,  3.94it/s] 29%|██▉       | 62/212 [00:15<00:37,  3.99it/s] 30%|██▉       | 63/212 [00:16<00:37,  4.02it/s] 30%|███       | 64/212 [00:16<00:36,  4.06it/s] 31%|███       | 65/212 [00:16<00:36,  4.08it/s] 31%|███       | 66/212 [00:16<00:35,  4.10it/s] 32%|███▏      | 67/212 [00:17<00:35,  4.12it/s] 32%|███▏      | 68/212 [00:17<00:34,  4.13it/s] 33%|███▎      | 69/212 [00:17<00:34,  4.14it/s] 33%|███▎      | 70/212 [00:17<00:34,  4.15it/s] 33%|███▎      | 71/212 [00:17<00:33,  4.15it/s] 34%|███▍      | 72/212 [00:18<00:33,  4.16it/s] 34%|███▍      | 73/212 [00:18<00:33,  4.16it/s] 35%|███▍      | 74/212 [00:18<00:33,  4.16it/s] 35%|███▌      | 75/212 [00:18<00:33,  4.11it/s] 36%|███▌      | 76/212 [00:19<00:34,  3.98it/s] 36%|███▋      | 77/212 [00:19<00:34,  3.90it/s] 37%|███▋      | 78/212 [00:19<00:34,  3.92it/s] 37%|███▋      | 79/212 [00:19<00:33,  3.98it/s] 38%|███▊      | 80/212 [00:20<00:34,  3.84it/s] 38%|███▊      | 81/212 [00:20<00:34,  3.78it/s] 39%|███▊      | 82/212 [00:20<00:34,  3.77it/s] 39%|███▉      | 83/212 [00:21<00:34,  3.69it/s] 40%|███▉      | 84/212 [00:21<00:33,  3.81it/s] 40%|████      | 85/212 [00:21<00:32,  3.90it/s] 41%|████      | 86/212 [00:21<00:33,  3.80it/s] 41%|████      | 87/212 [00:22<00:33,  3.74it/s] 42%|████▏     | 88/212 [00:22<00:33,  3.72it/s] 42%|████▏     | 89/212 [00:22<00:33,  3.65it/s] 42%|████▏     | 90/212 [00:22<00:32,  3.78it/s] 43%|████▎     | 91/212 [00:23<00:31,  3.85it/s] 43%|████▎     | 92/212 [00:23<00:31,  3.78it/s] 44%|████▍     | 93/212 [00:23<00:31,  3.72it/s] 44%|████▍     | 94/212 [00:24<00:31,  3.70it/s] 45%|████▍     | 95/212 [00:24<00:31,  3.75it/s] 45%|████▌     | 96/212 [00:24<00:30,  3.84it/s] 46%|████▌     | 97/212 [00:24<00:30,  3.76it/s] 46%|████▌     | 98/212 [00:25<00:30,  3.70it/s] 47%|████▋     | 99/212 [00:25<00:30,  3.73it/s] 47%|████▋     | 100/212 [00:25<00:30,  3.70it/s] 48%|████▊     | 101/212 [00:25<00:29,  3.81it/s] 48%|████▊     | 102/212 [00:26<00:29,  3.74it/s] 49%|████▊     | 103/212 [00:26<00:29,  3.69it/s] 49%|████▉     | 104/212 [00:26<00:28,  3.73it/s] 50%|████▉     | 105/212 [00:26<00:29,  3.68it/s] 50%|█████     | 106/212 [00:27<00:27,  3.81it/s] 50%|█████     | 107/212 [00:27<00:27,  3.81it/s] 51%|█████     | 108/212 [00:27<00:27,  3.76it/s] 51%|█████▏    | 109/212 [00:28<00:27,  3.71it/s] 52%|█████▏    | 110/212 [00:28<00:28,  3.63it/s] 52%|█████▏    | 111/212 [00:28<00:26,  3.77it/s] 53%|█████▎    | 112/212 [00:28<00:26,  3.84it/s] 53%|█████▎    | 113/212 [00:29<00:26,  3.76it/s] 54%|█████▍    | 114/212 [00:29<00:26,  3.69it/s] 54%|█████▍    | 115/212 [00:29<00:26,  3.72it/s] 55%|█████▍    | 116/212 [00:29<00:26,  3.69it/s] 55%|█████▌    | 117/212 [00:30<00:24,  3.81it/s] 56%|█████▌    | 118/212 [00:30<00:25,  3.74it/s] 56%|█████▌    | 119/212 [00:30<00:25,  3.69it/s] 57%|█████▋    | 120/212 [00:30<00:24,  3.73it/s] 57%|█████▋    | 121/212 [00:31<00:24,  3.68it/s] 58%|█████▊    | 122/212 [00:31<00:23,  3.80it/s] 58%|█████▊    | 123/212 [00:31<00:23,  3.72it/s] 58%|█████▊    | 124/212 [00:32<00:23,  3.69it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.70it/s] 59%|█████▉    | 126/212 [00:32<00:23,  3.63it/s] 60%|█████▉    | 127/212 [00:32<00:22,  3.77it/s] 60%|██████    | 128/212 [00:33<00:21,  3.83it/s] 61%|██████    | 129/212 [00:33<00:22,  3.75it/s] 61%|██████▏   | 130/212 [00:33<00:22,  3.69it/s] 62%|██████▏   | 131/212 [00:33<00:22,  3.64it/s] 62%|██████▏   | 132/212 [00:34<00:21,  3.73it/s] 63%|██████▎   | 133/212 [00:34<00:20,  3.82it/s] 63%|██████▎   | 134/212 [00:34<00:20,  3.74it/s] 64%|██████▎   | 135/212 [00:34<00:20,  3.69it/s] 64%|██████▍   | 136/212 [00:35<00:20,  3.73it/s] 65%|██████▍   | 137/212 [00:35<00:20,  3.68it/s] 65%|██████▌   | 138/212 [00:35<00:19,  3.80it/s] 66%|██████▌   | 139/212 [00:36<00:19,  3.74it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.68it/s] 67%|██████▋   | 141/212 [00:36<00:19,  3.71it/s] 67%|██████▋   | 142/212 [00:36<00:19,  3.63it/s] 67%|██████▋   | 143/212 [00:37<00:18,  3.76it/s] 68%|██████▊   | 144/212 [00:37<00:17,  3.83it/s] 68%|██████▊   | 145/212 [00:37<00:18,  3.70it/s] 69%|██████▉   | 146/212 [00:37<00:17,  3.74it/s] 69%|██████▉   | 147/212 [00:38<00:17,  3.74it/s] 70%|██████▉   | 148/212 [00:38<00:16,  3.83it/s] 70%|███████   | 149/212 [00:38<00:16,  3.91it/s] 71%|███████   | 150/212 [00:38<00:15,  3.97it/s] 71%|███████   | 151/212 [00:39<00:15,  4.01it/s] 72%|███████▏  | 152/212 [00:39<00:14,  4.03it/s] 72%|███████▏  | 153/212 [00:39<00:14,  4.04it/s] 73%|███████▎  | 154/212 [00:39<00:14,  4.06it/s] 73%|███████▎  | 155/212 [00:40<00:13,  4.07it/s] 74%|███████▎  | 156/212 [00:40<00:13,  4.07it/s] 74%|███████▍  | 157/212 [00:40<00:13,  4.07it/s] 75%|███████▍  | 158/212 [00:40<00:13,  4.08it/s] 75%|███████▌  | 159/212 [00:41<00:12,  4.08it/s] 75%|███████▌  | 160/212 [00:41<00:12,  4.07it/s] 76%|███████▌  | 161/212 [00:41<00:12,  4.08it/s] 76%|███████▋  | 162/212 [00:41<00:12,  4.09it/s] 77%|███████▋  | 163/212 [00:42<00:11,  4.09it/s] 77%|███████▋  | 164/212 [00:42<00:11,  4.09it/s] 78%|███████▊  | 165/212 [00:42<00:11,  4.08it/s] 78%|███████▊  | 166/212 [00:42<00:11,  4.09it/s] 79%|███████▉  | 167/212 [00:43<00:10,  4.09it/s] 79%|███████▉  | 168/212 [00:43<00:10,  4.10it/s] 80%|███████▉  | 169/212 [00:43<00:10,  4.09it/s] 80%|████████  | 170/212 [00:43<00:10,  4.09it/s] 81%|████████  | 171/212 [00:44<00:10,  4.09it/s] 81%|████████  | 172/212 [00:44<00:09,  4.09it/s] 82%|████████▏ | 173/212 [00:44<00:09,  4.08it/s] 82%|████████▏ | 174/212 [00:44<00:09,  4.08it/s] 83%|████████▎ | 175/212 [00:45<00:09,  4.09it/s] 83%|████████▎ | 176/212 [00:45<00:08,  4.09it/s] 83%|████████▎ | 177/212 [00:45<00:08,  4.09it/s] 84%|████████▍ | 178/212 [00:45<00:08,  4.08it/s] 84%|████████▍ | 179/212 [00:46<00:08,  3.96it/s] 85%|████████▍ | 180/212 [00:46<00:08,  3.88it/s] 85%|████████▌ | 181/212 [00:46<00:08,  3.79it/s] 86%|████████▌ | 182/212 [00:46<00:07,  3.87it/s] 86%|████████▋ | 183/212 [00:47<00:07,  3.73it/s] 87%|████████▋ | 184/212 [00:47<00:07,  3.76it/s] 87%|████████▋ | 185/212 [00:47<00:07,  3.75it/s] 88%|████████▊ | 186/212 [00:47<00:06,  3.85it/s] 88%|████████▊ | 187/212 [00:48<00:06,  3.92it/s] 89%|████████▊ | 188/212 [00:48<00:06,  3.96it/s] 89%|████████▉ | 189/212 [00:48<00:05,  4.00it/s] 90%|████████▉ | 190/212 [00:48<00:05,  4.03it/s] 90%|█████████ | 191/212 [00:49<00:05,  4.05it/s] 91%|█████████ | 192/212 [00:49<00:04,  4.06it/s] 91%|█████████ | 193/212 [00:49<00:04,  4.07it/s] 92%|█████████▏| 194/212 [00:49<00:04,  4.08it/s] 92%|█████████▏| 195/212 [00:50<00:04,  4.09it/s] 92%|█████████▏| 196/212 [00:50<00:03,  4.08it/s] 93%|█████████▎| 197/212 [00:50<00:03,  4.08it/s] 93%|█████████▎| 198/212 [00:50<00:03,  4.08it/s] 94%|█████████▍| 199/212 [00:51<00:03,  4.08it/s] 94%|█████████▍| 200/212 [00:51<00:02,  4.08it/s] 95%|█████████▍| 201/212 [00:51<00:02,  4.07it/s] 95%|█████████▌| 202/212 [00:51<00:02,  4.08it/s] 96%|█████████▌| 203/212 [00:52<00:02,  4.09it/s] 96%|█████████▌| 204/212 [00:52<00:01,  4.08it/s] 97%|█████████▋| 205/212 [00:52<00:01,  4.08it/s] 97%|█████████▋| 206/212 [00:52<00:01,  4.08it/s] 98%|█████████▊| 207/212 [00:53<00:01,  4.09it/s] 98%|█████████▊| 208/212 [00:53<00:00,  4.08it/s] 99%|█████████▊| 209/212 [00:53<00:00,  4.08it/s] 99%|█████████▉| 210/212 [00:53<00:00,  4.09it/s]100%|█████████▉| 211/212 [00:54<00:00,  4.09it/s]100%|██████████| 212/212 [00:54<00:00,  4.09it/s]accuracy:  0.8632075471698113
100%|██████████| 212/212 [00:57<00:00,  3.69it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 134
Training dataset size: 240, validation dataset size: 134
Training dataset size: 240, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
training start
[2025-03-12 05:13:37,137] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 05:13:37,261] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:13:37,421] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:03<02:11,  3.37s/it]  5%|▌         | 2/40 [00:05<01:48,  2.85s/it]  8%|▊         | 3/40 [00:08<01:40,  2.73s/it] 10%|█         | 4/40 [00:11<01:36,  2.67s/it] 12%|█▎        | 5/40 [00:13<01:34,  2.71s/it] 15%|█▌        | 6/40 [00:16<01:35,  2.82s/it] 18%|█▊        | 7/40 [00:19<01:29,  2.71s/it] 20%|██        | 8/40 [00:22<01:29,  2.79s/it] 22%|██▎       | 9/40 [00:25<01:28,  2.87s/it] 25%|██▌       | 10/40 [00:28<01:26,  2.88s/it]                                               {'loss': 0.9634, 'grad_norm': 5.883851051330566, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:28<01:26,  2.88s/it] 28%|██▊       | 11/40 [00:30<01:21,  2.82s/it] 30%|███       | 12/40 [00:33<01:19,  2.85s/it] 32%|███▎      | 13/40 [00:36<01:15,  2.78s/it] 35%|███▌      | 14/40 [00:39<01:12,  2.80s/it] 38%|███▊      | 15/40 [00:41<01:07,  2.71s/it] 40%|████      | 16/40 [00:44<01:03,  2.65s/it] 42%|████▎     | 17/40 [00:47<01:02,  2.74s/it] 45%|████▌     | 18/40 [00:49<00:58,  2.67s/it] 48%|████▊     | 19/40 [00:52<00:55,  2.66s/it] 50%|█████     | 20/40 [00:55<00:54,  2.70s/it]                                               {'loss': 0.6328, 'grad_norm': 4.017245292663574, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:55<00:54,  2.70s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:58<00:52,  2.77s/it] 55%|█████▌    | 22/40 [01:00<00:50,  2.80s/it] 57%|█████▊    | 23/40 [01:03<00:46,  2.75s/it] 60%|██████    | 24/40 [01:06<00:45,  2.85s/it] 62%|██████▎   | 25/40 [01:09<00:42,  2.81s/it] 65%|██████▌   | 26/40 [01:11<00:37,  2.71s/it] 68%|██████▊   | 27/40 [01:14<00:36,  2.82s/it] 70%|███████   | 28/40 [01:17<00:32,  2.72s/it] 72%|███████▎  | 29/40 [01:20<00:31,  2.86s/it] 75%|███████▌  | 30/40 [01:23<00:29,  2.90s/it]                                               {'loss': 0.4843, 'grad_norm': 8.37971019744873, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:23<00:29,  2.90s/it] 78%|███████▊  | 31/40 [01:25<00:24,  2.72s/it] 80%|████████  | 32/40 [01:28<00:21,  2.66s/it] 82%|████████▎ | 33/40 [01:31<00:18,  2.71s/it] 85%|████████▌ | 34/40 [01:33<00:16,  2.67s/it] 88%|████████▊ | 35/40 [01:37<00:14,  2.81s/it] 90%|█████████ | 36/40 [01:39<00:11,  2.84s/it] 92%|█████████▎| 37/40 [01:42<00:08,  2.70s/it] 95%|█████████▌| 38/40 [01:44<00:05,  2.64s/it] 98%|█████████▊| 39/40 [01:47<00:02,  2.62s/it]100%|██████████| 40/40 [01:50<00:00,  2.72s/it]                                               {'loss': 0.4746, 'grad_norm': 6.638595104217529, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:50<00:00,  2.72s/it]                                               {'train_runtime': 110.9742, 'train_samples_per_second': 4.325, 'train_steps_per_second': 0.36, 'train_loss': 0.6387986063957214, 'epoch': 2.0}
100%|██████████| 40/40 [01:50<00:00,  2.72s/it]100%|██████████| 40/40 [01:50<00:00,  2.77s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:58,  2.76it/s]  1%|          | 2/163 [00:00<00:50,  3.21it/s]  2%|▏         | 3/163 [00:00<00:45,  3.50it/s]  2%|▏         | 4/163 [00:01<00:44,  3.55it/s]  3%|▎         | 5/163 [00:01<00:43,  3.67it/s]  4%|▎         | 6/163 [00:01<00:42,  3.65it/s]  4%|▍         | 7/163 [00:01<00:41,  3.74it/s]  5%|▍         | 8/163 [00:02<00:41,  3.69it/s]  6%|▌         | 9/163 [00:02<00:41,  3.75it/s]  6%|▌         | 10/163 [00:02<00:41,  3.70it/s]  7%|▋         | 11/163 [00:03<00:40,  3.76it/s]  7%|▋         | 12/163 [00:03<00:40,  3.72it/s]  8%|▊         | 13/163 [00:03<00:39,  3.77it/s]  9%|▊         | 14/163 [00:03<00:40,  3.72it/s]  9%|▉         | 15/163 [00:04<00:39,  3.78it/s] 10%|▉         | 16/163 [00:04<00:39,  3.73it/s] 10%|█         | 17/163 [00:04<00:38,  3.79it/s] 11%|█         | 18/163 [00:04<00:38,  3.79it/s] 12%|█▏        | 19/163 [00:05<00:36,  3.89it/s] 12%|█▏        | 20/163 [00:05<00:36,  3.97it/s] 13%|█▎        | 21/163 [00:05<00:35,  4.03it/s] 13%|█▎        | 22/163 [00:05<00:34,  4.07it/s] 14%|█▍        | 23/163 [00:06<00:34,  4.10it/s] 15%|█▍        | 24/163 [00:06<00:33,  4.12it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.14it/s] 16%|█▌        | 26/163 [00:06<00:33,  4.14it/s] 17%|█▋        | 27/163 [00:07<00:32,  4.15it/s] 17%|█▋        | 28/163 [00:07<00:32,  4.14it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.14it/s] 18%|█▊        | 30/163 [00:07<00:32,  4.13it/s] 19%|█▉        | 31/163 [00:08<00:32,  4.12it/s] 20%|█▉        | 32/163 [00:08<00:32,  4.04it/s] 20%|██        | 33/163 [00:08<00:33,  3.93it/s] 21%|██        | 34/163 [00:08<00:33,  3.87it/s] 21%|██▏       | 35/163 [00:09<00:32,  3.94it/s] 22%|██▏       | 36/163 [00:09<00:32,  3.93it/s] 23%|██▎       | 37/163 [00:09<00:32,  3.88it/s] 23%|██▎       | 38/163 [00:09<00:32,  3.83it/s] 24%|██▍       | 39/163 [00:10<00:33,  3.75it/s] 25%|██▍       | 40/163 [00:10<00:31,  3.85it/s] 25%|██▌       | 41/163 [00:10<00:31,  3.88it/s] 26%|██▌       | 42/163 [00:10<00:31,  3.85it/s] 26%|██▋       | 43/163 [00:11<00:31,  3.81it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.74it/s] 28%|██▊       | 45/163 [00:11<00:30,  3.85it/s] 28%|██▊       | 46/163 [00:11<00:30,  3.89it/s] 29%|██▉       | 47/163 [00:12<00:30,  3.85it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.81it/s] 30%|███       | 49/163 [00:12<00:30,  3.74it/s] 31%|███       | 50/163 [00:12<00:29,  3.84it/s] 31%|███▏      | 51/163 [00:13<00:29,  3.85it/s] 32%|███▏      | 52/163 [00:13<00:29,  3.82it/s] 33%|███▎      | 53/163 [00:13<00:29,  3.79it/s] 33%|███▎      | 54/163 [00:14<00:29,  3.74it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.83it/s] 34%|███▍      | 56/163 [00:14<00:27,  3.89it/s] 35%|███▍      | 57/163 [00:14<00:27,  3.85it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.82it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.75it/s] 37%|███▋      | 60/163 [00:15<00:26,  3.83it/s] 37%|███▋      | 61/163 [00:15<00:26,  3.88it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.84it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.81it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.73it/s] 40%|███▉      | 65/163 [00:16<00:25,  3.84it/s] 40%|████      | 66/163 [00:17<00:24,  3.92it/s] 41%|████      | 67/163 [00:17<00:24,  3.86it/s] 42%|████▏     | 68/163 [00:17<00:24,  3.82it/s] 42%|████▏     | 69/163 [00:17<00:25,  3.74it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.85it/s] 44%|████▎     | 71/163 [00:18<00:23,  3.89it/s] 44%|████▍     | 72/163 [00:18<00:23,  3.84it/s] 45%|████▍     | 73/163 [00:19<00:23,  3.80it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.72it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.83it/s] 47%|████▋     | 76/163 [00:19<00:22,  3.88it/s] 47%|████▋     | 77/163 [00:20<00:22,  3.84it/s] 48%|████▊     | 78/163 [00:20<00:22,  3.81it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.73it/s] 49%|████▉     | 80/163 [00:20<00:21,  3.84it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.87it/s] 50%|█████     | 82/163 [00:21<00:21,  3.83it/s] 51%|█████     | 83/163 [00:21<00:21,  3.79it/s] 52%|█████▏    | 84/163 [00:21<00:21,  3.72it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.82it/s] 53%|█████▎    | 86/163 [00:22<00:19,  3.87it/s] 53%|█████▎    | 87/163 [00:22<00:19,  3.82it/s] 54%|█████▍    | 88/163 [00:22<00:19,  3.79it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.71it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.82it/s] 56%|█████▌    | 91/163 [00:23<00:18,  3.86it/s] 56%|█████▋    | 92/163 [00:23<00:18,  3.82it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.79it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.70it/s] 58%|█████▊    | 95/163 [00:24<00:17,  3.82it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.86it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.83it/s] 60%|██████    | 98/163 [00:25<00:17,  3.80it/s] 61%|██████    | 99/163 [00:25<00:17,  3.72it/s] 61%|██████▏   | 100/163 [00:26<00:16,  3.82it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.86it/s] 63%|██████▎   | 102/163 [00:26<00:15,  3.82it/s] 63%|██████▎   | 103/163 [00:26<00:15,  3.85it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.82it/s] 64%|██████▍   | 105/163 [00:27<00:14,  3.89it/s] 65%|██████▌   | 106/163 [00:27<00:14,  3.96it/s] 66%|██████▌   | 107/163 [00:27<00:13,  4.01it/s] 66%|██████▋   | 108/163 [00:28<00:13,  4.04it/s] 67%|██████▋   | 109/163 [00:28<00:13,  4.07it/s] 67%|██████▋   | 110/163 [00:28<00:12,  4.10it/s] 68%|██████▊   | 111/163 [00:28<00:12,  4.10it/s] 69%|██████▊   | 112/163 [00:29<00:12,  4.10it/s] 69%|██████▉   | 113/163 [00:29<00:12,  4.10it/s] 70%|██████▉   | 114/163 [00:29<00:11,  4.10it/s] 71%|███████   | 115/163 [00:29<00:11,  4.11it/s] 71%|███████   | 116/163 [00:30<00:11,  4.12it/s] 72%|███████▏  | 117/163 [00:30<00:11,  4.12it/s] 72%|███████▏  | 118/163 [00:30<00:10,  4.11it/s] 73%|███████▎  | 119/163 [00:30<00:10,  4.10it/s] 74%|███████▎  | 120/163 [00:31<00:10,  4.10it/s] 74%|███████▍  | 121/163 [00:31<00:10,  4.10it/s] 75%|███████▍  | 122/163 [00:31<00:09,  4.11it/s] 75%|███████▌  | 123/163 [00:31<00:09,  4.11it/s] 76%|███████▌  | 124/163 [00:32<00:09,  4.10it/s] 77%|███████▋  | 125/163 [00:32<00:09,  4.10it/s] 77%|███████▋  | 126/163 [00:32<00:09,  4.10it/s] 78%|███████▊  | 127/163 [00:32<00:08,  4.11it/s] 79%|███████▊  | 128/163 [00:32<00:08,  4.10it/s] 79%|███████▉  | 129/163 [00:33<00:08,  4.09it/s] 80%|███████▉  | 130/163 [00:33<00:08,  4.09it/s] 80%|████████  | 131/163 [00:33<00:07,  4.10it/s] 81%|████████  | 132/163 [00:33<00:07,  4.10it/s] 82%|████████▏ | 133/163 [00:34<00:07,  4.10it/s] 82%|████████▏ | 134/163 [00:34<00:07,  4.09it/s] 83%|████████▎ | 135/163 [00:34<00:06,  4.09it/s] 83%|████████▎ | 136/163 [00:34<00:06,  4.09it/s] 84%|████████▍ | 137/163 [00:35<00:06,  4.09it/s] 85%|████████▍ | 138/163 [00:35<00:06,  4.08it/s] 85%|████████▌ | 139/163 [00:35<00:05,  4.08it/s] 86%|████████▌ | 140/163 [00:35<00:05,  4.09it/s] 87%|████████▋ | 141/163 [00:36<00:05,  4.09it/s] 87%|████████▋ | 142/163 [00:36<00:05,  4.08it/s] 88%|████████▊ | 143/163 [00:36<00:04,  4.08it/s] 88%|████████▊ | 144/163 [00:36<00:04,  4.09it/s] 89%|████████▉ | 145/163 [00:37<00:04,  4.10it/s] 90%|████████▉ | 146/163 [00:37<00:04,  4.08it/s] 90%|█████████ | 147/163 [00:37<00:03,  4.08it/s] 91%|█████████ | 148/163 [00:37<00:03,  4.08it/s] 91%|█████████▏| 149/163 [00:38<00:03,  4.08it/s] 92%|█████████▏| 150/163 [00:38<00:03,  4.07it/s] 93%|█████████▎| 151/163 [00:38<00:02,  4.07it/s] 93%|█████████▎| 152/163 [00:38<00:02,  4.08it/s] 94%|█████████▍| 153/163 [00:39<00:02,  4.07it/s] 94%|█████████▍| 154/163 [00:39<00:02,  4.07it/s] 95%|█████████▌| 155/163 [00:39<00:01,  4.08it/s] 96%|█████████▌| 156/163 [00:39<00:01,  4.08it/s] 96%|█████████▋| 157/163 [00:40<00:01,  4.07it/s] 97%|█████████▋| 158/163 [00:40<00:01,  4.07it/s] 98%|█████████▊| 159/163 [00:40<00:00,  4.08it/s] 98%|█████████▊| 160/163 [00:40<00:00,  4.08it/s] 99%|█████████▉| 161/163 [00:41<00:00,  4.07it/s] 99%|█████████▉| 162/163 [00:41<00:00,  4.07it/s]100%|██████████| 163/163 [00:41<00:00,  4.08it/s]accuracy:  0.7730061349693251
100%|██████████| 163/163 [00:44<00:00,  3.68it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 171
Training dataset size: 240, validation dataset size: 171
Training dataset size: 240, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.29s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[2025-03-12 05:16:38,564] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 05:16:38,727] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 05:16:39,064] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:03<02:12,  3.40s/it]  5%|▌         | 2/40 [00:06<01:52,  2.97s/it]  8%|▊         | 3/40 [00:08<01:34,  2.57s/it] 10%|█         | 4/40 [00:10<01:33,  2.60s/it] 12%|█▎        | 5/40 [00:13<01:31,  2.63s/it] 15%|█▌        | 6/40 [00:16<01:33,  2.76s/it] 18%|█▊        | 7/40 [00:19<01:32,  2.81s/it] 20%|██        | 8/40 [00:21<01:26,  2.70s/it] 22%|██▎       | 9/40 [00:24<01:21,  2.63s/it] 25%|██▌       | 10/40 [00:27<01:19,  2.64s/it]                                               {'loss': 0.5049, 'grad_norm': 5.211737155914307, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:27<01:19,  2.64s/it] 28%|██▊       | 11/40 [00:29<01:18,  2.72s/it] 30%|███       | 12/40 [00:32<01:13,  2.64s/it] 32%|███▎      | 13/40 [00:34<01:09,  2.58s/it] 35%|███▌      | 14/40 [00:37<01:05,  2.53s/it] 38%|███▊      | 15/40 [00:39<01:03,  2.53s/it] 40%|████      | 16/40 [00:42<01:00,  2.51s/it] 42%|████▎     | 17/40 [00:44<00:57,  2.50s/it] 45%|████▌     | 18/40 [00:47<00:57,  2.63s/it] 48%|████▊     | 19/40 [00:49<00:52,  2.50s/it] 50%|█████     | 20/40 [00:52<00:51,  2.58s/it]                                               {'loss': 0.469, 'grad_norm': 7.16664457321167, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:52<00:51,  2.58s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:55<00:50,  2.67s/it] 55%|█████▌    | 22/40 [00:57<00:45,  2.54s/it] 57%|█████▊    | 23/40 [00:59<00:40,  2.39s/it] 60%|██████    | 24/40 [01:02<00:41,  2.62s/it] 62%|██████▎   | 25/40 [01:05<00:39,  2.64s/it] 65%|██████▌   | 26/40 [01:08<00:37,  2.66s/it] 68%|██████▊   | 27/40 [01:10<00:34,  2.64s/it] 70%|███████   | 28/40 [01:13<00:29,  2.49s/it] 72%|███████▎  | 29/40 [01:15<00:27,  2.48s/it] 75%|███████▌  | 30/40 [01:18<00:26,  2.65s/it]                                               {'loss': 0.3744, 'grad_norm': 1.28801429271698, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:18<00:26,  2.65s/it] 78%|███████▊  | 31/40 [01:21<00:23,  2.61s/it] 80%|████████  | 32/40 [01:23<00:20,  2.55s/it] 82%|████████▎ | 33/40 [01:26<00:17,  2.56s/it] 85%|████████▌ | 34/40 [01:28<00:15,  2.56s/it] 88%|████████▊ | 35/40 [01:31<00:12,  2.60s/it] 90%|█████████ | 36/40 [01:33<00:10,  2.57s/it] 92%|█████████▎| 37/40 [01:35<00:07,  2.46s/it] 95%|█████████▌| 38/40 [01:38<00:04,  2.40s/it] 98%|█████████▊| 39/40 [01:40<00:02,  2.39s/it]100%|██████████| 40/40 [01:43<00:00,  2.41s/it]                                               {'loss': 0.5181, 'grad_norm': 2.348182201385498, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:43<00:00,  2.41s/it]                                               {'train_runtime': 103.7467, 'train_samples_per_second': 4.627, 'train_steps_per_second': 0.386, 'train_loss': 0.4665739417076111, 'epoch': 2.0}
100%|██████████| 40/40 [01:43<00:00,  2.41s/it]100%|██████████| 40/40 [01:43<00:00,  2.59s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:09,  3.10it/s]  1%|          | 2/217 [00:00<01:03,  3.39it/s]  1%|▏         | 3/217 [00:00<00:58,  3.67it/s]  2%|▏         | 4/217 [00:01<00:57,  3.69it/s]  2%|▏         | 5/217 [00:01<00:57,  3.66it/s]  3%|▎         | 6/217 [00:01<00:55,  3.82it/s]  3%|▎         | 7/217 [00:01<00:53,  3.92it/s]  4%|▎         | 8/217 [00:02<00:52,  3.99it/s]  4%|▍         | 9/217 [00:02<00:51,  4.03it/s]  5%|▍         | 10/217 [00:02<00:50,  4.07it/s]  5%|▌         | 11/217 [00:02<00:50,  4.09it/s]  6%|▌         | 12/217 [00:03<00:49,  4.11it/s]  6%|▌         | 13/217 [00:03<00:49,  4.13it/s]  6%|▋         | 14/217 [00:03<00:49,  4.14it/s]  7%|▋         | 15/217 [00:03<00:48,  4.14it/s]  7%|▋         | 16/217 [00:04<00:48,  4.15it/s]  8%|▊         | 17/217 [00:04<00:48,  4.16it/s]  8%|▊         | 18/217 [00:04<00:47,  4.16it/s]  9%|▉         | 19/217 [00:04<00:47,  4.16it/s]  9%|▉         | 20/217 [00:04<00:47,  4.16it/s] 10%|▉         | 21/217 [00:05<00:47,  4.17it/s] 10%|█         | 22/217 [00:05<00:46,  4.17it/s] 11%|█         | 23/217 [00:05<00:46,  4.17it/s] 11%|█         | 24/217 [00:05<00:46,  4.17it/s] 12%|█▏        | 25/217 [00:06<00:46,  4.17it/s] 12%|█▏        | 26/217 [00:06<00:45,  4.17it/s] 12%|█▏        | 27/217 [00:06<00:46,  4.05it/s] 13%|█▎        | 28/217 [00:06<00:47,  3.96it/s] 13%|█▎        | 29/217 [00:07<00:48,  3.89it/s] 14%|█▍        | 30/217 [00:07<00:47,  3.97it/s] 14%|█▍        | 31/217 [00:07<00:46,  3.98it/s] 15%|█▍        | 32/217 [00:07<00:47,  3.88it/s] 15%|█▌        | 33/217 [00:08<00:48,  3.80it/s] 16%|█▌        | 34/217 [00:08<00:48,  3.79it/s] 16%|█▌        | 35/217 [00:08<00:48,  3.76it/s] 17%|█▋        | 36/217 [00:09<00:47,  3.85it/s] 17%|█▋        | 37/217 [00:09<00:47,  3.78it/s] 18%|█▊        | 38/217 [00:09<00:48,  3.72it/s] 18%|█▊        | 39/217 [00:09<00:47,  3.75it/s] 18%|█▊        | 40/217 [00:10<00:47,  3.69it/s] 19%|█▉        | 41/217 [00:10<00:46,  3.81it/s] 19%|█▉        | 42/217 [00:10<00:45,  3.81it/s] 20%|█▉        | 43/217 [00:10<00:46,  3.71it/s] 20%|██        | 44/217 [00:11<00:46,  3.73it/s] 21%|██        | 45/217 [00:11<00:47,  3.66it/s] 21%|██        | 46/217 [00:11<00:45,  3.80it/s] 22%|██▏       | 47/217 [00:11<00:43,  3.87it/s] 22%|██▏       | 48/217 [00:12<00:44,  3.80it/s] 23%|██▎       | 49/217 [00:12<00:44,  3.75it/s] 23%|██▎       | 50/217 [00:12<00:44,  3.77it/s] 24%|██▎       | 51/217 [00:13<00:44,  3.74it/s] 24%|██▍       | 52/217 [00:13<00:42,  3.85it/s] 24%|██▍       | 53/217 [00:13<00:43,  3.76it/s] 25%|██▍       | 54/217 [00:13<00:43,  3.72it/s] 25%|██▌       | 55/217 [00:14<00:43,  3.74it/s] 26%|██▌       | 56/217 [00:14<00:43,  3.67it/s] 26%|██▋       | 57/217 [00:14<00:42,  3.80it/s] 27%|██▋       | 58/217 [00:14<00:41,  3.84it/s] 27%|██▋       | 59/217 [00:15<00:41,  3.78it/s] 28%|██▊       | 60/217 [00:15<00:42,  3.73it/s] 28%|██▊       | 61/217 [00:15<00:41,  3.74it/s] 29%|██▊       | 62/217 [00:15<00:41,  3.73it/s] 29%|██▉       | 63/217 [00:16<00:40,  3.84it/s] 29%|██▉       | 64/217 [00:16<00:40,  3.75it/s] 30%|██▉       | 65/217 [00:16<00:40,  3.72it/s] 30%|███       | 66/217 [00:17<00:40,  3.74it/s] 31%|███       | 67/217 [00:17<00:40,  3.67it/s] 31%|███▏      | 68/217 [00:17<00:39,  3.81it/s] 32%|███▏      | 69/217 [00:17<00:38,  3.86it/s] 32%|███▏      | 70/217 [00:18<00:38,  3.79it/s] 33%|███▎      | 71/217 [00:18<00:39,  3.73it/s] 33%|███▎      | 72/217 [00:18<00:39,  3.71it/s] 34%|███▎      | 73/217 [00:18<00:38,  3.76it/s] 34%|███▍      | 74/217 [00:19<00:37,  3.84it/s] 35%|███▍      | 75/217 [00:19<00:37,  3.77it/s] 35%|███▌      | 76/217 [00:19<00:37,  3.73it/s] 35%|███▌      | 77/217 [00:19<00:37,  3.74it/s] 36%|███▌      | 78/217 [00:20<00:37,  3.72it/s] 36%|███▋      | 79/217 [00:20<00:36,  3.83it/s] 37%|███▋      | 80/217 [00:20<00:36,  3.75it/s] 37%|███▋      | 81/217 [00:21<00:36,  3.71it/s] 38%|███▊      | 82/217 [00:21<00:36,  3.74it/s] 38%|███▊      | 83/217 [00:21<00:36,  3.66it/s] 39%|███▊      | 84/217 [00:21<00:34,  3.80it/s] 39%|███▉      | 85/217 [00:22<00:34,  3.88it/s] 40%|███▉      | 86/217 [00:22<00:34,  3.80it/s] 40%|████      | 87/217 [00:22<00:34,  3.74it/s] 41%|████      | 88/217 [00:22<00:34,  3.73it/s] 41%|████      | 89/217 [00:23<00:34,  3.74it/s] 41%|████▏     | 90/217 [00:23<00:33,  3.85it/s] 42%|████▏     | 91/217 [00:23<00:33,  3.75it/s] 42%|████▏     | 92/217 [00:23<00:33,  3.71it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.73it/s] 43%|████▎     | 94/217 [00:24<00:33,  3.67it/s] 44%|████▍     | 95/217 [00:24<00:32,  3.80it/s] 44%|████▍     | 96/217 [00:25<00:31,  3.85it/s] 45%|████▍     | 97/217 [00:25<00:31,  3.78it/s] 45%|████▌     | 98/217 [00:25<00:31,  3.72it/s] 46%|████▌     | 99/217 [00:25<00:31,  3.71it/s] 46%|████▌     | 100/217 [00:26<00:31,  3.74it/s] 47%|████▋     | 101/217 [00:26<00:30,  3.84it/s] 47%|████▋     | 102/217 [00:26<00:30,  3.75it/s] 47%|████▋     | 103/217 [00:26<00:30,  3.71it/s] 48%|████▊     | 104/217 [00:27<00:30,  3.71it/s] 48%|████▊     | 105/217 [00:27<00:30,  3.64it/s] 49%|████▉     | 106/217 [00:27<00:29,  3.77it/s] 49%|████▉     | 107/217 [00:27<00:28,  3.81it/s] 50%|████▉     | 108/217 [00:28<00:29,  3.75it/s] 50%|█████     | 109/217 [00:28<00:29,  3.71it/s] 51%|█████     | 110/217 [00:28<00:29,  3.67it/s] 51%|█████     | 111/217 [00:29<00:28,  3.74it/s] 52%|█████▏    | 112/217 [00:29<00:27,  3.84it/s] 52%|█████▏    | 113/217 [00:29<00:27,  3.74it/s] 53%|█████▎    | 114/217 [00:29<00:27,  3.71it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.75it/s] 53%|█████▎    | 116/217 [00:30<00:26,  3.76it/s] 54%|█████▍    | 117/217 [00:30<00:25,  3.86it/s] 54%|█████▍    | 118/217 [00:30<00:25,  3.94it/s] 55%|█████▍    | 119/217 [00:31<00:24,  3.99it/s] 55%|█████▌    | 120/217 [00:31<00:24,  4.02it/s] 56%|█████▌    | 121/217 [00:31<00:23,  4.03it/s] 56%|█████▌    | 122/217 [00:31<00:23,  4.06it/s] 57%|█████▋    | 123/217 [00:32<00:23,  4.08it/s] 57%|█████▋    | 124/217 [00:32<00:22,  4.09it/s] 58%|█████▊    | 125/217 [00:32<00:22,  4.10it/s] 58%|█████▊    | 126/217 [00:32<00:22,  4.09it/s] 59%|█████▊    | 127/217 [00:33<00:21,  4.09it/s] 59%|█████▉    | 128/217 [00:33<00:21,  4.10it/s] 59%|█████▉    | 129/217 [00:33<00:21,  4.10it/s] 60%|█████▉    | 130/217 [00:33<00:21,  4.10it/s] 60%|██████    | 131/217 [00:34<00:21,  4.09it/s] 61%|██████    | 132/217 [00:34<00:20,  4.09it/s] 61%|██████▏   | 133/217 [00:34<00:20,  4.10it/s] 62%|██████▏   | 134/217 [00:34<00:20,  4.10it/s] 62%|██████▏   | 135/217 [00:35<00:20,  4.09it/s] 63%|██████▎   | 136/217 [00:35<00:19,  4.10it/s] 63%|██████▎   | 137/217 [00:35<00:19,  4.11it/s] 64%|██████▎   | 138/217 [00:35<00:19,  4.05it/s] 64%|██████▍   | 139/217 [00:36<00:19,  3.93it/s] 65%|██████▍   | 140/217 [00:36<00:19,  3.88it/s] 65%|██████▍   | 141/217 [00:36<00:19,  3.94it/s] 65%|██████▌   | 142/217 [00:36<00:19,  3.85it/s] 66%|██████▌   | 143/217 [00:37<00:19,  3.81it/s] 66%|██████▋   | 144/217 [00:37<00:19,  3.74it/s] 67%|██████▋   | 145/217 [00:37<00:18,  3.83it/s] 67%|██████▋   | 146/217 [00:37<00:18,  3.78it/s] 68%|██████▊   | 147/217 [00:38<00:18,  3.76it/s] 68%|██████▊   | 148/217 [00:38<00:18,  3.71it/s] 69%|██████▊   | 149/217 [00:38<00:17,  3.82it/s] 69%|██████▉   | 150/217 [00:38<00:17,  3.78it/s] 70%|██████▉   | 151/217 [00:39<00:17,  3.76it/s] 70%|███████   | 152/217 [00:39<00:17,  3.70it/s] 71%|███████   | 153/217 [00:39<00:16,  3.81it/s] 71%|███████   | 154/217 [00:39<00:16,  3.76it/s] 71%|███████▏  | 155/217 [00:40<00:16,  3.75it/s] 72%|███████▏  | 156/217 [00:40<00:16,  3.69it/s] 72%|███████▏  | 157/217 [00:40<00:15,  3.79it/s] 73%|███████▎  | 158/217 [00:41<00:15,  3.75it/s] 73%|███████▎  | 159/217 [00:41<00:15,  3.76it/s] 74%|███████▎  | 160/217 [00:41<00:15,  3.70it/s] 74%|███████▍  | 161/217 [00:41<00:14,  3.81it/s] 75%|███████▍  | 162/217 [00:42<00:14,  3.77it/s] 75%|███████▌  | 163/217 [00:42<00:14,  3.75it/s] 76%|███████▌  | 164/217 [00:42<00:14,  3.69it/s] 76%|███████▌  | 165/217 [00:42<00:13,  3.80it/s] 76%|███████▋  | 166/217 [00:43<00:13,  3.75it/s] 77%|███████▋  | 167/217 [00:43<00:13,  3.74it/s] 77%|███████▋  | 168/217 [00:43<00:13,  3.69it/s] 78%|███████▊  | 169/217 [00:43<00:12,  3.79it/s] 78%|███████▊  | 170/217 [00:44<00:12,  3.75it/s] 79%|███████▉  | 171/217 [00:44<00:12,  3.75it/s] 79%|███████▉  | 172/217 [00:44<00:12,  3.69it/s] 80%|███████▉  | 173/217 [00:45<00:11,  3.80it/s] 80%|████████  | 174/217 [00:45<00:11,  3.76it/s] 81%|████████  | 175/217 [00:45<00:11,  3.75it/s] 81%|████████  | 176/217 [00:45<00:11,  3.69it/s] 82%|████████▏ | 177/217 [00:46<00:10,  3.78it/s] 82%|████████▏ | 178/217 [00:46<00:10,  3.75it/s] 82%|████████▏ | 179/217 [00:46<00:10,  3.69it/s] 83%|████████▎ | 180/217 [00:46<00:09,  3.74it/s] 83%|████████▎ | 181/217 [00:47<00:09,  3.77it/s] 84%|████████▍ | 182/217 [00:47<00:09,  3.74it/s] 84%|████████▍ | 183/217 [00:47<00:09,  3.64it/s] 85%|████████▍ | 184/217 [00:48<00:08,  3.78it/s] 85%|████████▌ | 185/217 [00:48<00:08,  3.77it/s] 86%|████████▌ | 186/217 [00:48<00:08,  3.75it/s] 86%|████████▌ | 187/217 [00:48<00:08,  3.67it/s] 87%|████████▋ | 188/217 [00:49<00:07,  3.79it/s] 87%|████████▋ | 189/217 [00:49<00:07,  3.77it/s] 88%|████████▊ | 190/217 [00:49<00:07,  3.73it/s] 88%|████████▊ | 191/217 [00:49<00:07,  3.68it/s] 88%|████████▊ | 192/217 [00:50<00:06,  3.80it/s] 89%|████████▉ | 193/217 [00:50<00:06,  3.75it/s] 89%|████████▉ | 194/217 [00:50<00:06,  3.73it/s] 90%|████████▉ | 195/217 [00:50<00:05,  3.69it/s] 90%|█████████ | 196/217 [00:51<00:05,  3.78it/s] 91%|█████████ | 197/217 [00:51<00:05,  3.75it/s] 91%|█████████ | 198/217 [00:51<00:05,  3.70it/s] 92%|█████████▏| 199/217 [00:52<00:04,  3.75it/s] 92%|█████████▏| 200/217 [00:52<00:04,  3.77it/s] 93%|█████████▎| 201/217 [00:52<00:04,  3.74it/s] 93%|█████████▎| 202/217 [00:52<00:04,  3.64it/s] 94%|█████████▎| 203/217 [00:53<00:03,  3.77it/s] 94%|█████████▍| 204/217 [00:53<00:03,  3.77it/s] 94%|█████████▍| 205/217 [00:53<00:03,  3.75it/s] 95%|█████████▍| 206/217 [00:53<00:03,  3.66it/s] 95%|█████████▌| 207/217 [00:54<00:02,  3.78it/s] 96%|█████████▌| 208/217 [00:54<00:02,  3.78it/s] 96%|█████████▋| 209/217 [00:54<00:02,  3.75it/s] 97%|█████████▋| 210/217 [00:54<00:01,  3.66it/s] 97%|█████████▋| 211/217 [00:55<00:01,  3.79it/s] 98%|█████████▊| 212/217 [00:55<00:01,  3.78it/s] 98%|█████████▊| 213/217 [00:55<00:01,  3.74it/s] 99%|█████████▊| 214/217 [00:56<00:00,  3.67it/s] 99%|█████████▉| 215/217 [00:56<00:00,  3.79it/s]100%|█████████▉| 216/217 [00:56<00:00,  3.77it/s]100%|██████████| 217/217 [00:56<00:00,  3.74it/s]accuracy:  0.8387096774193549
100%|██████████| 217/217 [01:00<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 103
Training dataset size: 240, validation dataset size: 103
Training dataset size: 240, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:19:49,241] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:19:49,282] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 05:19:49,308] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:24,  2.17s/it]  5%|▌         | 2/40 [00:04<01:29,  2.36s/it]  8%|▊         | 3/40 [00:07<01:32,  2.49s/it] 10%|█         | 4/40 [00:09<01:20,  2.23s/it] 12%|█▎        | 5/40 [00:11<01:16,  2.20s/it] 15%|█▌        | 6/40 [00:13<01:17,  2.28s/it] 18%|█▊        | 7/40 [00:16<01:15,  2.29s/it] 20%|██        | 8/40 [00:18<01:17,  2.42s/it] 22%|██▎       | 9/40 [00:21<01:21,  2.63s/it] 25%|██▌       | 10/40 [00:24<01:21,  2.70s/it]                                               {'loss': 0.6305, 'grad_norm': 11.071048736572266, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:24<01:21,  2.70s/it] 28%|██▊       | 11/40 [00:26<01:08,  2.35s/it] 30%|███       | 12/40 [00:28<01:07,  2.41s/it] 32%|███▎      | 13/40 [00:30<01:02,  2.32s/it] 35%|███▌      | 14/40 [00:32<00:58,  2.25s/it] 38%|███▊      | 15/40 [00:35<00:54,  2.19s/it] 40%|████      | 16/40 [00:38<00:58,  2.46s/it] 42%|████▎     | 17/40 [00:41<00:59,  2.59s/it] 45%|████▌     | 18/40 [00:43<00:55,  2.54s/it] 48%|████▊     | 19/40 [00:45<00:52,  2.51s/it] 50%|█████     | 20/40 [00:48<00:48,  2.42s/it]                                               {'loss': 1.079, 'grad_norm': 12.865976333618164, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:48<00:48,  2.42s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:50<00:46,  2.47s/it] 55%|█████▌    | 22/40 [00:53<00:44,  2.49s/it] 57%|█████▊    | 23/40 [00:56<00:43,  2.59s/it] 60%|██████    | 24/40 [00:58<00:42,  2.64s/it] 62%|██████▎   | 25/40 [01:01<00:38,  2.56s/it] 65%|██████▌   | 26/40 [01:03<00:36,  2.59s/it] 68%|██████▊   | 27/40 [01:05<00:30,  2.33s/it] 70%|███████   | 28/40 [01:07<00:27,  2.30s/it] 72%|███████▎  | 29/40 [01:10<00:25,  2.31s/it] 75%|███████▌  | 30/40 [01:12<00:24,  2.47s/it]                                               {'loss': 0.5803, 'grad_norm': 8.099043846130371, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:12<00:24,  2.47s/it] 78%|███████▊  | 31/40 [01:14<00:19,  2.21s/it] 80%|████████  | 32/40 [01:16<00:17,  2.16s/it] 82%|████████▎ | 33/40 [01:17<00:13,  1.88s/it] 85%|████████▌ | 34/40 [01:20<00:12,  2.06s/it] 88%|████████▊ | 35/40 [01:22<00:10,  2.08s/it] 90%|█████████ | 36/40 [01:24<00:07,  1.94s/it] 92%|█████████▎| 37/40 [01:26<00:06,  2.10s/it] 95%|█████████▌| 38/40 [01:28<00:04,  2.17s/it] 98%|█████████▊| 39/40 [01:31<00:02,  2.21s/it]100%|██████████| 40/40 [01:33<00:00,  2.39s/it]                                               {'loss': 0.5328, 'grad_norm': 10.47492504119873, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:33<00:00,  2.39s/it]                                               {'train_runtime': 94.5795, 'train_samples_per_second': 5.075, 'train_steps_per_second': 0.423, 'train_loss': 0.7056469202041626, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.39s/it]100%|██████████| 40/40 [01:34<00:00,  2.36s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:48,  3.07it/s]  1%|▏         | 2/150 [00:00<00:40,  3.66it/s]  2%|▏         | 3/150 [00:00<00:37,  3.88it/s]  3%|▎         | 4/150 [00:01<00:36,  3.99it/s]  3%|▎         | 5/150 [00:01<00:35,  4.06it/s]  4%|▍         | 6/150 [00:01<00:35,  4.10it/s]  5%|▍         | 7/150 [00:01<00:35,  4.07it/s]  5%|▌         | 8/150 [00:02<00:35,  3.96it/s]  6%|▌         | 9/150 [00:02<00:36,  3.83it/s]  7%|▋         | 10/150 [00:02<00:35,  3.94it/s]  7%|▋         | 11/150 [00:02<00:35,  3.94it/s]  8%|▊         | 12/150 [00:03<00:36,  3.83it/s]  9%|▊         | 13/150 [00:03<00:35,  3.85it/s]  9%|▉         | 14/150 [00:03<00:35,  3.78it/s] 10%|█         | 15/150 [00:03<00:35,  3.84it/s] 11%|█         | 16/150 [00:04<00:35,  3.77it/s] 11%|█▏        | 17/150 [00:04<00:35,  3.77it/s] 12%|█▏        | 18/150 [00:04<00:35,  3.74it/s] 13%|█▎        | 19/150 [00:04<00:33,  3.86it/s] 13%|█▎        | 20/150 [00:05<00:34,  3.80it/s] 14%|█▍        | 21/150 [00:05<00:34,  3.75it/s] 15%|█▍        | 22/150 [00:05<00:34,  3.68it/s] 15%|█▌        | 23/150 [00:05<00:33,  3.81it/s] 16%|█▌        | 24/150 [00:06<00:32,  3.84it/s] 17%|█▋        | 25/150 [00:06<00:33,  3.77it/s] 17%|█▋        | 26/150 [00:06<00:32,  3.81it/s] 18%|█▊        | 27/150 [00:07<00:32,  3.75it/s] 19%|█▊        | 28/150 [00:07<00:31,  3.83it/s] 19%|█▉        | 29/150 [00:07<00:32,  3.75it/s] 20%|██        | 30/150 [00:07<00:32,  3.75it/s] 21%|██        | 31/150 [00:08<00:31,  3.82it/s] 21%|██▏       | 32/150 [00:08<00:30,  3.91it/s] 22%|██▏       | 33/150 [00:08<00:29,  3.99it/s] 23%|██▎       | 34/150 [00:08<00:28,  4.05it/s] 23%|██▎       | 35/150 [00:09<00:28,  4.09it/s] 24%|██▍       | 36/150 [00:09<00:27,  4.11it/s] 25%|██▍       | 37/150 [00:09<00:27,  4.14it/s] 25%|██▌       | 38/150 [00:09<00:26,  4.15it/s] 26%|██▌       | 39/150 [00:10<00:26,  4.15it/s] 27%|██▋       | 40/150 [00:10<00:26,  4.16it/s] 27%|██▋       | 41/150 [00:10<00:26,  4.16it/s] 28%|██▊       | 42/150 [00:10<00:25,  4.16it/s] 29%|██▊       | 43/150 [00:10<00:25,  4.17it/s] 29%|██▉       | 44/150 [00:11<00:25,  4.08it/s] 30%|███       | 45/150 [00:11<00:26,  3.97it/s] 31%|███       | 46/150 [00:11<00:26,  3.85it/s] 31%|███▏      | 47/150 [00:12<00:26,  3.94it/s] 32%|███▏      | 48/150 [00:12<00:25,  4.01it/s] 33%|███▎      | 49/150 [00:12<00:25,  3.93it/s] 33%|███▎      | 50/150 [00:12<00:25,  3.87it/s] 34%|███▍      | 51/150 [00:13<00:25,  3.88it/s] 35%|███▍      | 52/150 [00:13<00:25,  3.79it/s] 35%|███▌      | 53/150 [00:13<00:24,  3.89it/s] 36%|███▌      | 54/150 [00:13<00:25,  3.83it/s] 37%|███▋      | 55/150 [00:14<00:24,  3.81it/s] 37%|███▋      | 56/150 [00:14<00:24,  3.84it/s] 38%|███▊      | 57/150 [00:14<00:24,  3.78it/s] 39%|███▊      | 58/150 [00:14<00:23,  3.88it/s] 39%|███▉      | 59/150 [00:15<00:23,  3.82it/s] 40%|████      | 60/150 [00:15<00:23,  3.81it/s] 41%|████      | 61/150 [00:15<00:23,  3.82it/s] 41%|████▏     | 62/150 [00:15<00:23,  3.74it/s] 42%|████▏     | 63/150 [00:16<00:22,  3.86it/s] 43%|████▎     | 64/150 [00:16<00:22,  3.80it/s] 43%|████▎     | 65/150 [00:16<00:22,  3.80it/s] 44%|████▍     | 66/150 [00:16<00:22,  3.82it/s] 45%|████▍     | 67/150 [00:17<00:22,  3.75it/s] 45%|████▌     | 68/150 [00:17<00:21,  3.86it/s] 46%|████▌     | 69/150 [00:17<00:21,  3.84it/s] 47%|████▋     | 70/150 [00:18<00:21,  3.79it/s] 47%|████▋     | 71/150 [00:18<00:20,  3.82it/s] 48%|████▊     | 72/150 [00:18<00:20,  3.75it/s] 49%|████▊     | 73/150 [00:18<00:19,  3.87it/s] 49%|████▉     | 74/150 [00:19<00:19,  3.85it/s] 50%|█████     | 75/150 [00:19<00:19,  3.80it/s] 51%|█████     | 76/150 [00:19<00:19,  3.82it/s] 51%|█████▏    | 77/150 [00:19<00:19,  3.74it/s] 52%|█████▏    | 78/150 [00:20<00:18,  3.85it/s] 53%|█████▎    | 79/150 [00:20<00:18,  3.79it/s] 53%|█████▎    | 80/150 [00:20<00:18,  3.78it/s] 54%|█████▍    | 81/150 [00:20<00:18,  3.80it/s] 55%|█████▍    | 82/150 [00:21<00:18,  3.73it/s] 55%|█████▌    | 83/150 [00:21<00:17,  3.84it/s] 56%|█████▌    | 84/150 [00:21<00:17,  3.81it/s] 57%|█████▋    | 85/150 [00:21<00:17,  3.78it/s] 57%|█████▋    | 86/150 [00:22<00:16,  3.80it/s] 58%|█████▊    | 87/150 [00:22<00:16,  3.73it/s] 59%|█████▊    | 88/150 [00:22<00:16,  3.84it/s] 59%|█████▉    | 89/150 [00:23<00:15,  3.83it/s] 60%|██████    | 90/150 [00:23<00:15,  3.79it/s] 61%|██████    | 91/150 [00:23<00:15,  3.81it/s] 61%|██████▏   | 92/150 [00:23<00:15,  3.73it/s] 62%|██████▏   | 93/150 [00:24<00:14,  3.84it/s] 63%|██████▎   | 94/150 [00:24<00:14,  3.82it/s] 63%|██████▎   | 95/150 [00:24<00:14,  3.78it/s] 64%|██████▍   | 96/150 [00:24<00:14,  3.81it/s] 65%|██████▍   | 97/150 [00:25<00:14,  3.74it/s] 65%|██████▌   | 98/150 [00:25<00:13,  3.85it/s] 66%|██████▌   | 99/150 [00:25<00:13,  3.79it/s] 67%|██████▋   | 100/150 [00:25<00:13,  3.78it/s] 67%|██████▋   | 101/150 [00:26<00:12,  3.81it/s] 68%|██████▊   | 102/150 [00:26<00:12,  3.74it/s] 69%|██████▊   | 103/150 [00:26<00:12,  3.86it/s] 69%|██████▉   | 104/150 [00:26<00:11,  3.85it/s] 70%|███████   | 105/150 [00:27<00:11,  3.80it/s] 71%|███████   | 106/150 [00:27<00:11,  3.81it/s] 71%|███████▏  | 107/150 [00:27<00:11,  3.74it/s] 72%|███████▏  | 108/150 [00:28<00:10,  3.85it/s] 73%|███████▎  | 109/150 [00:28<00:10,  3.79it/s] 73%|███████▎  | 110/150 [00:28<00:10,  3.78it/s] 74%|███████▍  | 111/150 [00:28<00:10,  3.80it/s] 75%|███████▍  | 112/150 [00:29<00:10,  3.74it/s] 75%|███████▌  | 113/150 [00:29<00:09,  3.85it/s] 76%|███████▌  | 114/150 [00:29<00:09,  3.82it/s] 77%|███████▋  | 115/150 [00:29<00:09,  3.77it/s] 77%|███████▋  | 116/150 [00:30<00:08,  3.79it/s] 78%|███████▊  | 117/150 [00:30<00:08,  3.73it/s] 79%|███████▊  | 118/150 [00:30<00:08,  3.84it/s] 79%|███████▉  | 119/150 [00:30<00:08,  3.79it/s] 80%|████████  | 120/150 [00:31<00:08,  3.74it/s] 81%|████████  | 121/150 [00:31<00:07,  3.76it/s] 81%|████████▏ | 122/150 [00:31<00:07,  3.74it/s] 82%|████████▏ | 123/150 [00:31<00:07,  3.85it/s] 83%|████████▎ | 124/150 [00:32<00:06,  3.79it/s] 83%|████████▎ | 125/150 [00:32<00:06,  3.75it/s] 84%|████████▍ | 126/150 [00:32<00:06,  3.72it/s] 85%|████████▍ | 127/150 [00:33<00:05,  3.84it/s] 85%|████████▌ | 128/150 [00:33<00:05,  3.93it/s] 86%|████████▌ | 129/150 [00:33<00:05,  3.99it/s] 87%|████████▋ | 130/150 [00:33<00:04,  4.04it/s] 87%|████████▋ | 131/150 [00:34<00:04,  4.07it/s] 88%|████████▊ | 132/150 [00:34<00:04,  4.08it/s] 89%|████████▊ | 133/150 [00:34<00:04,  4.08it/s] 89%|████████▉ | 134/150 [00:34<00:03,  4.09it/s] 90%|█████████ | 135/150 [00:34<00:03,  4.11it/s] 91%|█████████ | 136/150 [00:35<00:03,  4.12it/s] 91%|█████████▏| 137/150 [00:35<00:03,  4.13it/s] 92%|█████████▏| 138/150 [00:35<00:02,  4.13it/s] 93%|█████████▎| 139/150 [00:35<00:02,  4.13it/s] 93%|█████████▎| 140/150 [00:36<00:02,  4.13it/s] 94%|█████████▍| 141/150 [00:36<00:02,  4.13it/s] 95%|█████████▍| 142/150 [00:36<00:01,  4.11it/s] 95%|█████████▌| 143/150 [00:36<00:01,  4.12it/s] 96%|█████████▌| 144/150 [00:37<00:01,  4.12it/s] 97%|█████████▋| 145/150 [00:37<00:01,  4.13it/s] 97%|█████████▋| 146/150 [00:37<00:00,  4.13it/s] 98%|█████████▊| 147/150 [00:37<00:00,  4.14it/s] 99%|█████████▊| 148/150 [00:38<00:00,  4.13it/s] 99%|█████████▉| 149/150 [00:38<00:00,  4.13it/s]100%|██████████| 150/150 [00:38<00:00,  4.13it/s]accuracy:  0.64
100%|██████████| 150/150 [00:40<00:00,  3.66it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 145
Training dataset size: 240, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 240, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.25s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 05:22:34,677] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 05:22:34,803] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 05:22:35,189] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:54,  2.94s/it]  5%|▌         | 2/40 [00:05<01:39,  2.63s/it]  8%|▊         | 3/40 [00:07<01:29,  2.43s/it] 10%|█         | 4/40 [00:09<01:20,  2.23s/it] 12%|█▎        | 5/40 [00:11<01:18,  2.25s/it] 15%|█▌        | 6/40 [00:14<01:19,  2.35s/it] 18%|█▊        | 7/40 [00:16<01:14,  2.27s/it] 20%|██        | 8/40 [00:18<01:09,  2.16s/it] 22%|██▎       | 9/40 [00:20<01:07,  2.18s/it] 25%|██▌       | 10/40 [00:22<01:06,  2.22s/it]                                               {'loss': 1.4136, 'grad_norm': 12.08414363861084, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:22<01:06,  2.22s/it] 28%|██▊       | 11/40 [00:25<01:04,  2.22s/it] 30%|███       | 12/40 [00:27<01:00,  2.17s/it] 32%|███▎      | 13/40 [00:29<01:00,  2.26s/it] 35%|███▌      | 14/40 [00:31<00:58,  2.26s/it] 38%|███▊      | 15/40 [00:34<00:56,  2.25s/it] 40%|████      | 16/40 [00:36<00:55,  2.31s/it] 42%|████▎     | 17/40 [00:38<00:49,  2.16s/it] 45%|████▌     | 18/40 [00:40<00:48,  2.19s/it] 48%|████▊     | 19/40 [00:43<00:51,  2.46s/it] 50%|█████     | 20/40 [00:45<00:47,  2.36s/it]                                               {'loss': 1.1286, 'grad_norm': 7.251787185668945, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:45<00:47,  2.36s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:48<00:49,  2.58s/it] 55%|█████▌    | 22/40 [00:51<00:47,  2.63s/it] 57%|█████▊    | 23/40 [00:53<00:41,  2.46s/it] 60%|██████    | 24/40 [00:55<00:38,  2.39s/it] 62%|██████▎   | 25/40 [00:58<00:35,  2.38s/it] 65%|██████▌   | 26/40 [01:00<00:32,  2.35s/it] 68%|██████▊   | 27/40 [01:03<00:31,  2.42s/it] 70%|███████   | 28/40 [01:05<00:29,  2.43s/it] 72%|███████▎  | 29/40 [01:07<00:26,  2.38s/it] 75%|███████▌  | 30/40 [01:10<00:24,  2.43s/it]                                               {'loss': 1.1299, 'grad_norm': 12.915733337402344, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:10<00:24,  2.43s/it] 78%|███████▊  | 31/40 [01:12<00:21,  2.40s/it] 80%|████████  | 32/40 [01:15<00:19,  2.46s/it] 82%|████████▎ | 33/40 [01:17<00:16,  2.40s/it] 85%|████████▌ | 34/40 [01:20<00:14,  2.40s/it] 88%|████████▊ | 35/40 [01:22<00:12,  2.40s/it] 90%|█████████ | 36/40 [01:24<00:09,  2.36s/it] 92%|█████████▎| 37/40 [01:26<00:06,  2.30s/it] 95%|█████████▌| 38/40 [01:29<00:04,  2.30s/it] 98%|█████████▊| 39/40 [01:31<00:02,  2.45s/it]100%|██████████| 40/40 [01:34<00:00,  2.39s/it]                                               {'loss': 0.9304, 'grad_norm': 7.116194248199463, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.39s/it]                                               {'train_runtime': 94.9139, 'train_samples_per_second': 5.057, 'train_steps_per_second': 0.421, 'train_loss': 1.1506349325180054, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.39s/it]100%|██████████| 40/40 [01:34<00:00,  2.37s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:03,  3.13it/s]  1%|          | 2/200 [00:00<00:56,  3.52it/s]  2%|▏         | 3/200 [00:00<00:54,  3.64it/s]  2%|▏         | 4/200 [00:01<00:52,  3.75it/s]  2%|▎         | 5/200 [00:01<00:52,  3.69it/s]  3%|▎         | 6/200 [00:01<00:50,  3.84it/s]  4%|▎         | 7/200 [00:01<00:50,  3.80it/s]  4%|▍         | 8/200 [00:02<00:50,  3.78it/s]  4%|▍         | 9/200 [00:02<00:51,  3.73it/s]  5%|▌         | 10/200 [00:02<00:49,  3.86it/s]  6%|▌         | 11/200 [00:02<00:49,  3.86it/s]  6%|▌         | 12/200 [00:03<00:49,  3.81it/s]  6%|▋         | 13/200 [00:03<00:49,  3.81it/s]  7%|▋         | 14/200 [00:03<00:48,  3.80it/s]  8%|▊         | 15/200 [00:03<00:47,  3.87it/s]  8%|▊         | 16/200 [00:04<00:48,  3.82it/s]  8%|▊         | 17/200 [00:04<00:47,  3.85it/s]  9%|▉         | 18/200 [00:04<00:48,  3.76it/s] 10%|▉         | 19/200 [00:05<00:46,  3.86it/s] 10%|█         | 20/200 [00:05<00:47,  3.81it/s] 10%|█         | 21/200 [00:05<00:47,  3.79it/s] 11%|█         | 22/200 [00:05<00:47,  3.73it/s] 12%|█▏        | 23/200 [00:06<00:46,  3.84it/s] 12%|█▏        | 24/200 [00:06<00:45,  3.85it/s] 12%|█▎        | 25/200 [00:06<00:46,  3.80it/s] 13%|█▎        | 26/200 [00:06<00:45,  3.80it/s] 14%|█▎        | 27/200 [00:07<00:45,  3.79it/s] 14%|█▍        | 28/200 [00:07<00:44,  3.85it/s] 14%|█▍        | 29/200 [00:07<00:44,  3.81it/s] 15%|█▌        | 30/200 [00:07<00:44,  3.82it/s] 16%|█▌        | 31/200 [00:08<00:45,  3.75it/s] 16%|█▌        | 32/200 [00:08<00:43,  3.86it/s] 16%|█▋        | 33/200 [00:08<00:43,  3.84it/s] 17%|█▋        | 34/200 [00:08<00:43,  3.81it/s] 18%|█▊        | 35/200 [00:09<00:43,  3.78it/s] 18%|█▊        | 36/200 [00:09<00:42,  3.82it/s] 18%|█▊        | 37/200 [00:09<00:42,  3.87it/s] 19%|█▉        | 38/200 [00:10<00:42,  3.80it/s] 20%|█▉        | 39/200 [00:10<00:42,  3.76it/s] 20%|██        | 40/200 [00:10<00:41,  3.88it/s] 20%|██        | 41/200 [00:10<00:40,  3.95it/s] 21%|██        | 42/200 [00:11<00:39,  4.01it/s] 22%|██▏       | 43/200 [00:11<00:38,  4.06it/s] 22%|██▏       | 44/200 [00:11<00:38,  4.09it/s] 22%|██▎       | 45/200 [00:11<00:37,  4.11it/s] 23%|██▎       | 46/200 [00:11<00:37,  4.13it/s] 24%|██▎       | 47/200 [00:12<00:36,  4.14it/s] 24%|██▍       | 48/200 [00:12<00:36,  4.15it/s] 24%|██▍       | 49/200 [00:12<00:36,  4.15it/s] 25%|██▌       | 50/200 [00:12<00:36,  4.16it/s] 26%|██▌       | 51/200 [00:13<00:35,  4.15it/s] 26%|██▌       | 52/200 [00:13<00:35,  4.15it/s] 26%|██▋       | 53/200 [00:13<00:35,  4.13it/s] 27%|██▋       | 54/200 [00:13<00:35,  4.13it/s] 28%|██▊       | 55/200 [00:14<00:35,  4.13it/s] 28%|██▊       | 56/200 [00:14<00:34,  4.13it/s] 28%|██▊       | 57/200 [00:14<00:34,  4.14it/s] 29%|██▉       | 58/200 [00:14<00:34,  4.14it/s] 30%|██▉       | 59/200 [00:15<00:33,  4.15it/s] 30%|███       | 60/200 [00:15<00:33,  4.15it/s] 30%|███       | 61/200 [00:15<00:33,  4.15it/s] 31%|███       | 62/200 [00:15<00:34,  4.00it/s] 32%|███▏      | 63/200 [00:16<00:34,  3.92it/s] 32%|███▏      | 64/200 [00:16<00:34,  3.90it/s] 32%|███▎      | 65/200 [00:16<00:34,  3.97it/s] 33%|███▎      | 66/200 [00:16<00:34,  3.92it/s] 34%|███▎      | 67/200 [00:17<00:34,  3.81it/s] 34%|███▍      | 68/200 [00:17<00:35,  3.72it/s] 34%|███▍      | 69/200 [00:17<00:34,  3.82it/s] 35%|███▌      | 70/200 [00:17<00:33,  3.84it/s] 36%|███▌      | 71/200 [00:18<00:34,  3.75it/s] 36%|███▌      | 72/200 [00:18<00:33,  3.78it/s] 36%|███▋      | 73/200 [00:18<00:34,  3.71it/s] 37%|███▋      | 74/200 [00:19<00:33,  3.79it/s] 38%|███▊      | 75/200 [00:19<00:33,  3.72it/s] 38%|███▊      | 76/200 [00:19<00:33,  3.73it/s] 38%|███▊      | 77/200 [00:19<00:33,  3.70it/s] 39%|███▉      | 78/200 [00:20<00:31,  3.82it/s] 40%|███▉      | 79/200 [00:20<00:31,  3.80it/s] 40%|████      | 80/200 [00:20<00:32,  3.72it/s] 40%|████      | 81/200 [00:20<00:32,  3.63it/s] 41%|████      | 82/200 [00:21<00:31,  3.78it/s] 42%|████▏     | 83/200 [00:21<00:30,  3.81it/s] 42%|████▏     | 84/200 [00:21<00:31,  3.74it/s] 42%|████▎     | 85/200 [00:21<00:30,  3.78it/s] 43%|████▎     | 86/200 [00:22<00:30,  3.72it/s] 44%|████▎     | 87/200 [00:22<00:29,  3.83it/s] 44%|████▍     | 88/200 [00:22<00:29,  3.77it/s] 44%|████▍     | 89/200 [00:23<00:29,  3.72it/s] 45%|████▌     | 90/200 [00:23<00:30,  3.66it/s] 46%|████▌     | 91/200 [00:23<00:28,  3.79it/s] 46%|████▌     | 92/200 [00:23<00:28,  3.82it/s] 46%|████▋     | 93/200 [00:24<00:28,  3.73it/s] 47%|████▋     | 94/200 [00:24<00:28,  3.71it/s] 48%|████▊     | 95/200 [00:24<00:28,  3.75it/s] 48%|████▊     | 96/200 [00:24<00:27,  3.82it/s] 48%|████▊     | 97/200 [00:25<00:27,  3.75it/s] 49%|████▉     | 98/200 [00:25<00:27,  3.73it/s] 50%|████▉     | 99/200 [00:25<00:27,  3.69it/s] 50%|█████     | 100/200 [00:25<00:26,  3.80it/s] 50%|█████     | 101/200 [00:26<00:26,  3.79it/s] 51%|█████     | 102/200 [00:26<00:26,  3.71it/s] 52%|█████▏    | 103/200 [00:26<00:26,  3.63it/s] 52%|█████▏    | 104/200 [00:27<00:25,  3.76it/s] 52%|█████▎    | 105/200 [00:27<00:25,  3.80it/s] 53%|█████▎    | 106/200 [00:27<00:25,  3.71it/s] 54%|█████▎    | 107/200 [00:27<00:24,  3.73it/s] 54%|█████▍    | 108/200 [00:28<00:24,  3.70it/s] 55%|█████▍    | 109/200 [00:28<00:23,  3.81it/s] 55%|█████▌    | 110/200 [00:28<00:24,  3.75it/s] 56%|█████▌    | 111/200 [00:28<00:23,  3.71it/s] 56%|█████▌    | 112/200 [00:29<00:23,  3.67it/s] 56%|█████▋    | 113/200 [00:29<00:22,  3.79it/s] 57%|█████▋    | 114/200 [00:29<00:22,  3.80it/s] 57%|█████▊    | 115/200 [00:29<00:22,  3.72it/s] 58%|█████▊    | 116/200 [00:30<00:22,  3.70it/s] 58%|█████▊    | 117/200 [00:30<00:22,  3.74it/s] 59%|█████▉    | 118/200 [00:30<00:21,  3.82it/s] 60%|█████▉    | 119/200 [00:31<00:21,  3.73it/s] 60%|██████    | 120/200 [00:31<00:21,  3.74it/s] 60%|██████    | 121/200 [00:31<00:21,  3.70it/s] 61%|██████    | 122/200 [00:31<00:20,  3.81it/s] 62%|██████▏   | 123/200 [00:32<00:20,  3.81it/s] 62%|██████▏   | 124/200 [00:32<00:20,  3.73it/s] 62%|██████▎   | 125/200 [00:32<00:20,  3.70it/s] 63%|██████▎   | 126/200 [00:32<00:19,  3.75it/s] 64%|██████▎   | 127/200 [00:33<00:19,  3.81it/s] 64%|██████▍   | 128/200 [00:33<00:19,  3.73it/s] 64%|██████▍   | 129/200 [00:33<00:19,  3.73it/s] 65%|██████▌   | 130/200 [00:33<00:19,  3.68it/s] 66%|██████▌   | 131/200 [00:34<00:18,  3.81it/s] 66%|██████▌   | 132/200 [00:34<00:17,  3.81it/s] 66%|██████▋   | 133/200 [00:34<00:17,  3.73it/s] 67%|██████▋   | 134/200 [00:35<00:17,  3.70it/s] 68%|██████▊   | 135/200 [00:35<00:17,  3.75it/s] 68%|██████▊   | 136/200 [00:35<00:16,  3.81it/s] 68%|██████▊   | 137/200 [00:35<00:16,  3.71it/s] 69%|██████▉   | 138/200 [00:36<00:16,  3.74it/s] 70%|██████▉   | 139/200 [00:36<00:16,  3.68it/s] 70%|███████   | 140/200 [00:36<00:15,  3.78it/s] 70%|███████   | 141/200 [00:36<00:15,  3.72it/s] 71%|███████   | 142/200 [00:37<00:15,  3.69it/s] 72%|███████▏  | 143/200 [00:37<00:15,  3.66it/s] 72%|███████▏  | 144/200 [00:37<00:14,  3.79it/s] 72%|███████▎  | 145/200 [00:37<00:14,  3.78it/s] 73%|███████▎  | 146/200 [00:38<00:14,  3.69it/s] 74%|███████▎  | 147/200 [00:38<00:14,  3.61it/s] 74%|███████▍  | 148/200 [00:38<00:13,  3.74it/s] 74%|███████▍  | 149/200 [00:39<00:13,  3.78it/s] 75%|███████▌  | 150/200 [00:39<00:13,  3.69it/s] 76%|███████▌  | 151/200 [00:39<00:13,  3.71it/s] 76%|███████▌  | 152/200 [00:39<00:13,  3.68it/s] 76%|███████▋  | 153/200 [00:40<00:12,  3.77it/s] 77%|███████▋  | 154/200 [00:40<00:12,  3.69it/s] 78%|███████▊  | 155/200 [00:40<00:12,  3.71it/s] 78%|███████▊  | 156/200 [00:40<00:11,  3.67it/s] 78%|███████▊  | 157/200 [00:41<00:11,  3.79it/s] 79%|███████▉  | 158/200 [00:41<00:11,  3.79it/s] 80%|███████▉  | 159/200 [00:41<00:11,  3.71it/s] 80%|████████  | 160/200 [00:42<00:11,  3.62it/s] 80%|████████  | 161/200 [00:42<00:10,  3.76it/s] 81%|████████  | 162/200 [00:42<00:10,  3.80it/s] 82%|████████▏ | 163/200 [00:42<00:09,  3.71it/s] 82%|████████▏ | 164/200 [00:43<00:09,  3.70it/s] 82%|████████▎ | 165/200 [00:43<00:09,  3.69it/s] 83%|████████▎ | 166/200 [00:43<00:09,  3.76it/s] 84%|████████▎ | 167/200 [00:43<00:08,  3.68it/s] 84%|████████▍ | 168/200 [00:44<00:08,  3.69it/s] 84%|████████▍ | 169/200 [00:44<00:08,  3.66it/s] 85%|████████▌ | 170/200 [00:44<00:08,  3.75it/s] 86%|████████▌ | 171/200 [00:45<00:07,  3.66it/s] 86%|████████▌ | 172/200 [00:45<00:07,  3.66it/s] 86%|████████▋ | 173/200 [00:45<00:07,  3.67it/s] 87%|████████▋ | 174/200 [00:45<00:06,  3.75it/s] 88%|████████▊ | 175/200 [00:46<00:06,  3.67it/s] 88%|████████▊ | 176/200 [00:46<00:06,  3.72it/s] 88%|████████▊ | 177/200 [00:46<00:06,  3.66it/s] 89%|████████▉ | 178/200 [00:46<00:05,  3.76it/s] 90%|████████▉ | 179/200 [00:47<00:05,  3.66it/s] 90%|█████████ | 180/200 [00:47<00:05,  3.72it/s] 90%|█████████ | 181/200 [00:47<00:05,  3.72it/s] 91%|█████████ | 182/200 [00:47<00:04,  3.82it/s] 92%|█████████▏| 183/200 [00:48<00:04,  3.90it/s] 92%|█████████▏| 184/200 [00:48<00:04,  3.95it/s] 92%|█████████▎| 185/200 [00:48<00:03,  3.98it/s] 93%|█████████▎| 186/200 [00:48<00:03,  4.01it/s] 94%|█████████▎| 187/200 [00:49<00:03,  4.03it/s] 94%|█████████▍| 188/200 [00:49<00:02,  4.04it/s] 94%|█████████▍| 189/200 [00:49<00:02,  4.05it/s] 95%|█████████▌| 190/200 [00:49<00:02,  4.06it/s] 96%|█████████▌| 191/200 [00:50<00:02,  4.07it/s] 96%|█████████▌| 192/200 [00:50<00:01,  4.06it/s] 96%|█████████▋| 193/200 [00:50<00:01,  4.06it/s] 97%|█████████▋| 194/200 [00:50<00:01,  4.07it/s] 98%|█████████▊| 195/200 [00:51<00:01,  4.07it/s] 98%|█████████▊| 196/200 [00:51<00:00,  4.06it/s] 98%|█████████▊| 197/200 [00:51<00:00,  4.07it/s] 99%|█████████▉| 198/200 [00:51<00:00,  4.07it/s]100%|█████████▉| 199/200 [00:52<00:00,  4.07it/s]100%|██████████| 200/200 [00:52<00:00,  4.07it/s]accuracy:  0.57
100%|██████████| 200/200 [00:55<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 191
Training dataset size: 240, validation dataset size: 191
Training dataset size: 240, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.99s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2025-03-12 05:25:35,613] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:25:36,445] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:25:36,887] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:51,  2.85s/it]  5%|▌         | 2/40 [00:05<01:43,  2.73s/it]  8%|▊         | 3/40 [00:08<01:41,  2.76s/it] 10%|█         | 4/40 [00:10<01:38,  2.73s/it] 12%|█▎        | 5/40 [00:13<01:29,  2.57s/it] 15%|█▌        | 6/40 [00:16<01:31,  2.71s/it] 18%|█▊        | 7/40 [00:18<01:23,  2.54s/it] 20%|██        | 8/40 [00:20<01:20,  2.50s/it] 22%|██▎       | 9/40 [00:22<01:13,  2.38s/it] 25%|██▌       | 10/40 [00:25<01:11,  2.40s/it]                                               {'loss': 1.8816, 'grad_norm': 15.560799598693848, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:25<01:11,  2.40s/it] 28%|██▊       | 11/40 [00:27<01:08,  2.38s/it] 30%|███       | 12/40 [00:30<01:09,  2.47s/it] 32%|███▎      | 13/40 [00:33<01:09,  2.57s/it] 35%|███▌      | 14/40 [00:35<01:08,  2.62s/it] 38%|███▊      | 15/40 [00:38<01:06,  2.66s/it] 40%|████      | 16/40 [00:40<01:01,  2.55s/it] 42%|████▎     | 17/40 [00:43<00:57,  2.51s/it] 45%|████▌     | 18/40 [00:46<01:01,  2.79s/it] 48%|████▊     | 19/40 [00:49<00:56,  2.68s/it] 50%|█████     | 20/40 [00:51<00:52,  2.64s/it]                                               {'loss': 1.2368, 'grad_norm': 17.990169525146484, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:51<00:52,  2.64s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:54<00:51,  2.72s/it] 55%|█████▌    | 22/40 [00:57<00:47,  2.66s/it] 57%|█████▊    | 23/40 [00:59<00:44,  2.63s/it] 60%|██████    | 24/40 [01:02<00:41,  2.59s/it] 62%|██████▎   | 25/40 [01:05<00:39,  2.63s/it] 65%|██████▌   | 26/40 [01:07<00:36,  2.61s/it] 68%|██████▊   | 27/40 [01:10<00:34,  2.67s/it] 70%|███████   | 28/40 [01:12<00:31,  2.62s/it] 72%|███████▎  | 29/40 [01:15<00:27,  2.53s/it] 75%|███████▌  | 30/40 [01:17<00:24,  2.50s/it]                                               {'loss': 0.8762, 'grad_norm': 7.1611762046813965, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:17<00:24,  2.50s/it] 78%|███████▊  | 31/40 [01:20<00:22,  2.51s/it] 80%|████████  | 32/40 [01:22<00:18,  2.35s/it] 82%|████████▎ | 33/40 [01:24<00:16,  2.35s/it] 85%|████████▌ | 34/40 [01:27<00:15,  2.53s/it] 88%|████████▊ | 35/40 [01:30<00:12,  2.53s/it] 90%|█████████ | 36/40 [01:32<00:09,  2.48s/it] 92%|█████████▎| 37/40 [01:35<00:07,  2.58s/it] 95%|█████████▌| 38/40 [01:37<00:05,  2.60s/it] 98%|█████████▊| 39/40 [01:40<00:02,  2.52s/it]100%|██████████| 40/40 [01:42<00:00,  2.43s/it]                                               {'loss': 0.9409, 'grad_norm': 11.916876792907715, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:42<00:00,  2.43s/it]                                               {'train_runtime': 103.0472, 'train_samples_per_second': 4.658, 'train_steps_per_second': 0.388, 'train_loss': 1.2338684797286987, 'epoch': 2.0}
100%|██████████| 40/40 [01:42<00:00,  2.43s/it]100%|██████████| 40/40 [01:42<00:00,  2.57s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:26,  3.23it/s]  1%|          | 2/279 [00:00<01:16,  3.60it/s]  1%|          | 3/279 [00:00<01:16,  3.60it/s]  1%|▏         | 4/279 [00:01<01:16,  3.60it/s]  2%|▏         | 5/279 [00:01<01:15,  3.65it/s]  2%|▏         | 6/279 [00:01<01:13,  3.72it/s]  3%|▎         | 7/279 [00:01<01:10,  3.85it/s]  3%|▎         | 8/279 [00:02<01:10,  3.85it/s]  3%|▎         | 9/279 [00:02<01:11,  3.76it/s]  4%|▎         | 10/279 [00:02<01:12,  3.73it/s]  4%|▍         | 11/279 [00:02<01:13,  3.65it/s]  4%|▍         | 12/279 [00:03<01:10,  3.80it/s]  5%|▍         | 13/279 [00:03<01:08,  3.90it/s]  5%|▌         | 14/279 [00:03<01:09,  3.79it/s]  5%|▌         | 15/279 [00:04<01:10,  3.74it/s]  6%|▌         | 16/279 [00:04<01:10,  3.75it/s]  6%|▌         | 17/279 [00:04<01:10,  3.74it/s]  6%|▋         | 18/279 [00:04<01:08,  3.80it/s]  7%|▋         | 19/279 [00:05<01:06,  3.90it/s]  7%|▋         | 20/279 [00:05<01:06,  3.89it/s]  8%|▊         | 21/279 [00:05<01:07,  3.81it/s]  8%|▊         | 22/279 [00:05<01:08,  3.75it/s]  8%|▊         | 23/279 [00:06<01:09,  3.67it/s]  9%|▊         | 24/279 [00:06<01:06,  3.81it/s]  9%|▉         | 25/279 [00:06<01:05,  3.91it/s]  9%|▉         | 26/279 [00:06<01:06,  3.81it/s] 10%|▉         | 27/279 [00:07<01:07,  3.75it/s] 10%|█         | 28/279 [00:07<01:06,  3.76it/s] 10%|█         | 29/279 [00:07<01:07,  3.68it/s] 11%|█         | 30/279 [00:07<01:05,  3.82it/s] 11%|█         | 31/279 [00:08<01:03,  3.91it/s] 11%|█▏        | 32/279 [00:08<01:04,  3.82it/s] 12%|█▏        | 33/279 [00:08<01:05,  3.75it/s] 12%|█▏        | 34/279 [00:09<01:05,  3.76it/s] 13%|█▎        | 35/279 [00:09<01:05,  3.71it/s] 13%|█▎        | 36/279 [00:09<01:03,  3.83it/s] 13%|█▎        | 37/279 [00:09<01:02,  3.85it/s] 14%|█▎        | 38/279 [00:10<01:03,  3.78it/s] 14%|█▍        | 39/279 [00:10<01:04,  3.72it/s] 14%|█▍        | 40/279 [00:10<01:04,  3.68it/s] 15%|█▍        | 41/279 [00:10<01:03,  3.77it/s] 15%|█▌        | 42/279 [00:11<01:01,  3.88it/s] 15%|█▌        | 43/279 [00:11<01:01,  3.86it/s] 16%|█▌        | 44/279 [00:11<01:02,  3.73it/s] 16%|█▌        | 45/279 [00:11<01:02,  3.76it/s] 16%|█▋        | 46/279 [00:12<01:03,  3.69it/s] 17%|█▋        | 47/279 [00:12<01:00,  3.81it/s] 17%|█▋        | 48/279 [00:12<00:59,  3.90it/s] 18%|█▊        | 49/279 [00:12<01:00,  3.80it/s] 18%|█▊        | 50/279 [00:13<01:01,  3.73it/s] 18%|█▊        | 51/279 [00:13<01:00,  3.74it/s] 19%|█▊        | 52/279 [00:13<01:01,  3.66it/s] 19%|█▉        | 53/279 [00:14<00:59,  3.79it/s] 19%|█▉        | 54/279 [00:14<00:58,  3.88it/s] 20%|█▉        | 55/279 [00:14<00:59,  3.77it/s] 20%|██        | 56/279 [00:14<00:59,  3.73it/s] 20%|██        | 57/279 [00:15<00:59,  3.73it/s] 21%|██        | 58/279 [00:15<01:00,  3.66it/s] 21%|██        | 59/279 [00:15<00:58,  3.79it/s] 22%|██▏       | 60/279 [00:15<00:56,  3.86it/s] 22%|██▏       | 61/279 [00:16<00:57,  3.78it/s] 22%|██▏       | 62/279 [00:16<00:58,  3.72it/s] 23%|██▎       | 63/279 [00:16<00:57,  3.76it/s] 23%|██▎       | 64/279 [00:17<00:58,  3.70it/s] 23%|██▎       | 65/279 [00:17<00:56,  3.81it/s] 24%|██▎       | 66/279 [00:17<00:55,  3.82it/s] 24%|██▍       | 67/279 [00:17<00:57,  3.70it/s] 24%|██▍       | 68/279 [00:18<00:56,  3.74it/s] 25%|██▍       | 69/279 [00:18<00:55,  3.76it/s] 25%|██▌       | 70/279 [00:18<00:54,  3.86it/s] 25%|██▌       | 71/279 [00:18<00:52,  3.93it/s] 26%|██▌       | 72/279 [00:19<00:51,  3.99it/s] 26%|██▌       | 73/279 [00:19<00:51,  4.04it/s] 27%|██▋       | 74/279 [00:19<00:50,  4.07it/s] 27%|██▋       | 75/279 [00:19<00:49,  4.09it/s] 27%|██▋       | 76/279 [00:20<00:49,  4.10it/s] 28%|██▊       | 77/279 [00:20<00:49,  4.10it/s] 28%|██▊       | 78/279 [00:20<00:49,  4.10it/s] 28%|██▊       | 79/279 [00:20<00:48,  4.10it/s] 29%|██▊       | 80/279 [00:20<00:48,  4.11it/s] 29%|██▉       | 81/279 [00:21<00:48,  4.12it/s] 29%|██▉       | 82/279 [00:21<00:47,  4.12it/s] 30%|██▉       | 83/279 [00:21<00:47,  4.11it/s] 30%|███       | 84/279 [00:21<00:47,  4.11it/s] 30%|███       | 85/279 [00:22<00:47,  4.10it/s] 31%|███       | 86/279 [00:22<00:46,  4.12it/s] 31%|███       | 87/279 [00:22<00:46,  4.13it/s] 32%|███▏      | 88/279 [00:22<00:46,  4.13it/s] 32%|███▏      | 89/279 [00:23<00:45,  4.14it/s] 32%|███▏      | 90/279 [00:23<00:45,  4.13it/s] 33%|███▎      | 91/279 [00:23<00:45,  4.13it/s] 33%|███▎      | 92/279 [00:23<00:45,  4.12it/s] 33%|███▎      | 93/279 [00:24<00:45,  4.11it/s] 34%|███▎      | 94/279 [00:24<00:44,  4.12it/s] 34%|███▍      | 95/279 [00:24<00:44,  4.13it/s] 34%|███▍      | 96/279 [00:24<00:45,  4.06it/s] 35%|███▍      | 97/279 [00:25<00:46,  3.93it/s] 35%|███▌      | 98/279 [00:25<00:47,  3.83it/s] 35%|███▌      | 99/279 [00:25<00:46,  3.90it/s] 36%|███▌      | 100/279 [00:25<00:46,  3.88it/s] 36%|███▌      | 101/279 [00:26<00:47,  3.77it/s] 37%|███▋      | 102/279 [00:26<00:47,  3.70it/s] 37%|███▋      | 103/279 [00:26<00:48,  3.64it/s] 37%|███▋      | 104/279 [00:27<00:46,  3.78it/s] 38%|███▊      | 105/279 [00:27<00:45,  3.78it/s] 38%|███▊      | 106/279 [00:27<00:47,  3.66it/s] 38%|███▊      | 107/279 [00:27<00:46,  3.69it/s] 39%|███▊      | 108/279 [00:28<00:46,  3.68it/s] 39%|███▉      | 109/279 [00:28<00:45,  3.77it/s] 39%|███▉      | 110/279 [00:28<00:45,  3.69it/s] 40%|███▉      | 111/279 [00:28<00:46,  3.65it/s] 40%|████      | 112/279 [00:29<00:46,  3.60it/s] 41%|████      | 113/279 [00:29<00:44,  3.74it/s] 41%|████      | 114/279 [00:29<00:44,  3.75it/s] 41%|████      | 115/279 [00:30<00:45,  3.63it/s] 42%|████▏     | 116/279 [00:30<00:44,  3.68it/s] 42%|████▏     | 117/279 [00:30<00:44,  3.67it/s] 42%|████▏     | 118/279 [00:30<00:42,  3.75it/s] 43%|████▎     | 119/279 [00:31<00:43,  3.67it/s] 43%|████▎     | 120/279 [00:31<00:43,  3.63it/s] 43%|████▎     | 121/279 [00:31<00:44,  3.59it/s] 44%|████▎     | 122/279 [00:31<00:42,  3.73it/s] 44%|████▍     | 123/279 [00:32<00:41,  3.76it/s] 44%|████▍     | 124/279 [00:32<00:42,  3.64it/s] 45%|████▍     | 125/279 [00:32<00:41,  3.69it/s] 45%|████▌     | 126/279 [00:33<00:41,  3.65it/s] 46%|████▌     | 127/279 [00:33<00:40,  3.75it/s] 46%|████▌     | 128/279 [00:33<00:41,  3.68it/s] 46%|████▌     | 129/279 [00:33<00:41,  3.63it/s] 47%|████▋     | 130/279 [00:34<00:41,  3.59it/s] 47%|████▋     | 131/279 [00:34<00:39,  3.73it/s] 47%|████▋     | 132/279 [00:34<00:39,  3.77it/s] 48%|████▊     | 133/279 [00:34<00:40,  3.65it/s] 48%|████▊     | 134/279 [00:35<00:39,  3.70it/s] 48%|████▊     | 135/279 [00:35<00:39,  3.66it/s] 49%|████▊     | 136/279 [00:35<00:38,  3.74it/s] 49%|████▉     | 137/279 [00:35<00:38,  3.67it/s] 49%|████▉     | 138/279 [00:36<00:38,  3.63it/s] 50%|████▉     | 139/279 [00:36<00:39,  3.59it/s] 50%|█████     | 140/279 [00:36<00:37,  3.72it/s] 51%|█████     | 141/279 [00:37<00:36,  3.74it/s] 51%|█████     | 142/279 [00:37<00:37,  3.63it/s] 51%|█████▏    | 143/279 [00:37<00:37,  3.67it/s] 52%|█████▏    | 144/279 [00:37<00:37,  3.63it/s] 52%|█████▏    | 145/279 [00:38<00:35,  3.74it/s] 52%|█████▏    | 146/279 [00:38<00:36,  3.67it/s] 53%|█████▎    | 147/279 [00:38<00:36,  3.61it/s] 53%|█████▎    | 148/279 [00:39<00:36,  3.57it/s] 53%|█████▎    | 149/279 [00:39<00:35,  3.71it/s] 54%|█████▍    | 150/279 [00:39<00:34,  3.73it/s] 54%|█████▍    | 151/279 [00:39<00:35,  3.62it/s] 54%|█████▍    | 152/279 [00:40<00:34,  3.67it/s] 55%|█████▍    | 153/279 [00:40<00:34,  3.64it/s] 55%|█████▌    | 154/279 [00:40<00:33,  3.72it/s] 56%|█████▌    | 155/279 [00:40<00:33,  3.65it/s] 56%|█████▌    | 156/279 [00:41<00:34,  3.61it/s] 56%|█████▋    | 157/279 [00:41<00:34,  3.57it/s] 57%|█████▋    | 158/279 [00:41<00:32,  3.71it/s] 57%|█████▋    | 159/279 [00:41<00:32,  3.74it/s] 57%|█████▋    | 160/279 [00:42<00:32,  3.62it/s] 58%|█████▊    | 161/279 [00:42<00:32,  3.68it/s] 58%|█████▊    | 162/279 [00:42<00:32,  3.63it/s] 58%|█████▊    | 163/279 [00:43<00:31,  3.72it/s] 59%|█████▉    | 164/279 [00:43<00:31,  3.65it/s] 59%|█████▉    | 165/279 [00:43<00:31,  3.61it/s] 59%|█████▉    | 166/279 [00:43<00:31,  3.57it/s] 60%|█████▉    | 167/279 [00:44<00:30,  3.71it/s] 60%|██████    | 168/279 [00:44<00:29,  3.73it/s] 61%|██████    | 169/279 [00:44<00:30,  3.62it/s] 61%|██████    | 170/279 [00:45<00:29,  3.66it/s] 61%|██████▏   | 171/279 [00:45<00:29,  3.63it/s] 62%|██████▏   | 172/279 [00:45<00:28,  3.72it/s] 62%|██████▏   | 173/279 [00:45<00:29,  3.65it/s] 62%|██████▏   | 174/279 [00:46<00:29,  3.61it/s] 63%|██████▎   | 175/279 [00:46<00:29,  3.57it/s] 63%|██████▎   | 176/279 [00:46<00:27,  3.70it/s] 63%|██████▎   | 177/279 [00:46<00:27,  3.73it/s] 64%|██████▍   | 178/279 [00:47<00:27,  3.62it/s] 64%|██████▍   | 179/279 [00:47<00:27,  3.64it/s] 65%|██████▍   | 180/279 [00:47<00:27,  3.66it/s] 65%|██████▍   | 181/279 [00:48<00:26,  3.74it/s] 65%|██████▌   | 182/279 [00:48<00:26,  3.66it/s] 66%|██████▌   | 183/279 [00:48<00:26,  3.62it/s] 66%|██████▌   | 184/279 [00:48<00:26,  3.57it/s] 66%|██████▋   | 185/279 [00:49<00:25,  3.71it/s] 67%|██████▋   | 186/279 [00:49<00:24,  3.73it/s] 67%|██████▋   | 187/279 [00:49<00:25,  3.62it/s] 67%|██████▋   | 188/279 [00:49<00:24,  3.67it/s] 68%|██████▊   | 189/279 [00:50<00:24,  3.64it/s] 68%|██████▊   | 190/279 [00:50<00:23,  3.72it/s] 68%|██████▊   | 191/279 [00:50<00:24,  3.65it/s] 69%|██████▉   | 192/279 [00:51<00:24,  3.61it/s] 69%|██████▉   | 193/279 [00:51<00:24,  3.57it/s] 70%|██████▉   | 194/279 [00:51<00:22,  3.71it/s] 70%|██████▉   | 195/279 [00:51<00:22,  3.73it/s] 70%|███████   | 196/279 [00:52<00:22,  3.61it/s] 71%|███████   | 197/279 [00:52<00:22,  3.68it/s] 71%|███████   | 198/279 [00:52<00:22,  3.64it/s] 71%|███████▏  | 199/279 [00:52<00:21,  3.73it/s] 72%|███████▏  | 200/279 [00:53<00:21,  3.66it/s] 72%|███████▏  | 201/279 [00:53<00:21,  3.61it/s] 72%|███████▏  | 202/279 [00:53<00:21,  3.57it/s] 73%|███████▎  | 203/279 [00:54<00:20,  3.71it/s] 73%|███████▎  | 204/279 [00:54<00:20,  3.73it/s] 73%|███████▎  | 205/279 [00:54<00:20,  3.62it/s] 74%|███████▍  | 206/279 [00:54<00:19,  3.68it/s] 74%|███████▍  | 207/279 [00:55<00:19,  3.63it/s] 75%|███████▍  | 208/279 [00:55<00:18,  3.74it/s] 75%|███████▍  | 209/279 [00:55<00:19,  3.60it/s] 75%|███████▌  | 210/279 [00:55<00:19,  3.62it/s] 76%|███████▌  | 211/279 [00:56<00:18,  3.70it/s] 76%|███████▌  | 212/279 [00:56<00:17,  3.80it/s] 76%|███████▋  | 213/279 [00:56<00:17,  3.88it/s] 77%|███████▋  | 214/279 [00:56<00:16,  3.94it/s] 77%|███████▋  | 215/279 [00:57<00:16,  3.98it/s] 77%|███████▋  | 216/279 [00:57<00:15,  4.00it/s] 78%|███████▊  | 217/279 [00:57<00:15,  4.02it/s] 78%|███████▊  | 218/279 [00:57<00:15,  4.04it/s] 78%|███████▊  | 219/279 [00:58<00:14,  4.04it/s] 79%|███████▉  | 220/279 [00:58<00:14,  4.05it/s] 79%|███████▉  | 221/279 [00:58<00:14,  4.06it/s] 80%|███████▉  | 222/279 [00:58<00:14,  4.06it/s] 80%|███████▉  | 223/279 [00:59<00:13,  4.06it/s] 80%|████████  | 224/279 [00:59<00:13,  4.07it/s] 81%|████████  | 225/279 [00:59<00:13,  4.08it/s] 81%|████████  | 226/279 [00:59<00:13,  4.07it/s] 81%|████████▏ | 227/279 [01:00<00:12,  4.07it/s] 82%|████████▏ | 228/279 [01:00<00:12,  4.08it/s] 82%|████████▏ | 229/279 [01:00<00:12,  4.08it/s] 82%|████████▏ | 230/279 [01:00<00:12,  4.08it/s] 83%|████████▎ | 231/279 [01:01<00:11,  4.07it/s] 83%|████████▎ | 232/279 [01:01<00:11,  4.08it/s] 84%|████████▎ | 233/279 [01:01<00:11,  4.02it/s] 84%|████████▍ | 234/279 [01:01<00:11,  3.93it/s] 84%|████████▍ | 235/279 [01:02<00:11,  3.84it/s] 85%|████████▍ | 236/279 [01:02<00:11,  3.84it/s] 85%|████████▍ | 237/279 [01:02<00:10,  3.88it/s] 85%|████████▌ | 238/279 [01:02<00:10,  3.78it/s] 86%|████████▌ | 239/279 [01:03<00:10,  3.73it/s] 86%|████████▌ | 240/279 [01:03<00:10,  3.66it/s] 86%|████████▋ | 241/279 [01:03<00:10,  3.78it/s] 87%|████████▋ | 242/279 [01:04<00:09,  3.78it/s] 87%|████████▋ | 243/279 [01:04<00:09,  3.69it/s] 87%|████████▋ | 244/279 [01:04<00:09,  3.70it/s] 88%|████████▊ | 245/279 [01:04<00:09,  3.70it/s] 88%|████████▊ | 246/279 [01:05<00:08,  3.75it/s] 89%|████████▊ | 247/279 [01:05<00:08,  3.67it/s] 89%|████████▉ | 248/279 [01:05<00:08,  3.70it/s] 89%|████████▉ | 249/279 [01:05<00:08,  3.65it/s] 90%|████████▉ | 250/279 [01:06<00:07,  3.75it/s] 90%|████████▉ | 251/279 [01:06<00:07,  3.70it/s] 90%|█████████ | 252/279 [01:06<00:07,  3.67it/s] 91%|█████████ | 253/279 [01:07<00:07,  3.63it/s] 91%|█████████ | 254/279 [01:07<00:06,  3.75it/s] 91%|█████████▏| 255/279 [01:07<00:06,  3.70it/s] 92%|█████████▏| 256/279 [01:07<00:06,  3.67it/s] 92%|█████████▏| 257/279 [01:08<00:06,  3.60it/s] 92%|█████████▏| 258/279 [01:08<00:05,  3.72it/s] 93%|█████████▎| 259/279 [01:08<00:05,  3.75it/s] 93%|█████████▎| 260/279 [01:08<00:05,  3.66it/s] 94%|█████████▎| 261/279 [01:09<00:05,  3.59it/s] 94%|█████████▍| 262/279 [01:09<00:04,  3.73it/s] 94%|█████████▍| 263/279 [01:09<00:04,  3.77it/s] 95%|█████████▍| 264/279 [01:10<00:04,  3.69it/s] 95%|█████████▍| 265/279 [01:10<00:03,  3.73it/s] 95%|█████████▌| 266/279 [01:10<00:03,  3.67it/s] 96%|█████████▌| 267/279 [01:10<00:03,  3.75it/s] 96%|█████████▌| 268/279 [01:11<00:03,  3.67it/s] 96%|█████████▋| 269/279 [01:11<00:02,  3.68it/s] 97%|█████████▋| 270/279 [01:11<00:02,  3.64it/s] 97%|█████████▋| 271/279 [01:11<00:02,  3.76it/s] 97%|█████████▋| 272/279 [01:12<00:01,  3.73it/s] 98%|█████████▊| 273/279 [01:12<00:01,  3.67it/s] 98%|█████████▊| 274/279 [01:12<00:01,  3.59it/s] 99%|█████████▊| 275/279 [01:12<00:01,  3.73it/s] 99%|█████████▉| 276/279 [01:13<00:00,  3.76it/s] 99%|█████████▉| 277/279 [01:13<00:00,  3.67it/s]100%|█████████▉| 278/279 [01:13<00:00,  3.73it/s]100%|██████████| 279/279 [01:14<00:00,  3.67it/s]accuracy:  0.5878136200716846
100%|██████████| 279/279 [01:18<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 241
Training dataset size: 240, validation dataset size: 241
Training dataset size: 240, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 05:29:04,624] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:29:04,784] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:29:05,020] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:21,  2.09s/it]  5%|▌         | 2/40 [00:04<01:18,  2.08s/it]  8%|▊         | 3/40 [00:07<01:29,  2.43s/it] 10%|█         | 4/40 [00:09<01:22,  2.30s/it] 12%|█▎        | 5/40 [00:11<01:24,  2.43s/it] 15%|█▌        | 6/40 [00:13<01:15,  2.22s/it] 18%|█▊        | 7/40 [00:16<01:18,  2.36s/it] 20%|██        | 8/40 [00:19<01:23,  2.60s/it] 22%|██▎       | 9/40 [00:21<01:16,  2.45s/it] 25%|██▌       | 10/40 [00:23<01:10,  2.36s/it]                                               {'loss': 0.3707, 'grad_norm': 5.277037620544434, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:23<01:10,  2.36s/it] 28%|██▊       | 11/40 [00:26<01:11,  2.46s/it] 30%|███       | 12/40 [00:28<01:08,  2.45s/it] 32%|███▎      | 13/40 [00:31<01:04,  2.40s/it] 35%|███▌      | 14/40 [00:33<01:00,  2.33s/it] 38%|███▊      | 15/40 [00:35<00:57,  2.29s/it] 40%|████      | 16/40 [00:37<00:53,  2.22s/it] 42%|████▎     | 17/40 [00:39<00:46,  2.03s/it] 45%|████▌     | 18/40 [00:41<00:44,  2.01s/it] 48%|████▊     | 19/40 [00:43<00:44,  2.11s/it] 50%|█████     | 20/40 [00:45<00:43,  2.16s/it]                                               {'loss': 0.4497, 'grad_norm': 7.659440517425537, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:45<00:43,  2.16s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:48<00:43,  2.29s/it] 55%|█████▌    | 22/40 [00:50<00:43,  2.39s/it] 57%|█████▊    | 23/40 [00:53<00:39,  2.32s/it] 60%|██████    | 24/40 [00:55<00:36,  2.30s/it] 62%|██████▎   | 25/40 [00:57<00:32,  2.18s/it] 65%|██████▌   | 26/40 [00:59<00:32,  2.31s/it] 68%|██████▊   | 27/40 [01:02<00:29,  2.29s/it] 70%|███████   | 28/40 [01:04<00:27,  2.28s/it] 72%|███████▎  | 29/40 [01:06<00:24,  2.27s/it] 75%|███████▌  | 30/40 [01:09<00:24,  2.40s/it]                                               {'loss': 0.3459, 'grad_norm': 5.615311145782471, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:09<00:24,  2.40s/it] 78%|███████▊  | 31/40 [01:12<00:23,  2.61s/it] 80%|████████  | 32/40 [01:14<00:19,  2.45s/it] 82%|████████▎ | 33/40 [01:17<00:17,  2.51s/it] 85%|████████▌ | 34/40 [01:19<00:15,  2.52s/it] 88%|████████▊ | 35/40 [01:22<00:12,  2.51s/it] 90%|█████████ | 36/40 [01:24<00:09,  2.39s/it] 92%|█████████▎| 37/40 [01:26<00:07,  2.44s/it] 95%|█████████▌| 38/40 [01:28<00:04,  2.23s/it] 98%|█████████▊| 39/40 [01:30<00:02,  2.23s/it]100%|██████████| 40/40 [01:33<00:00,  2.31s/it]                                               {'loss': 0.2847, 'grad_norm': 2.6269302368164062, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:33<00:00,  2.31s/it]                                               {'train_runtime': 93.8235, 'train_samples_per_second': 5.116, 'train_steps_per_second': 0.426, 'train_loss': 0.3627627491950989, 'epoch': 2.0}
100%|██████████| 40/40 [01:33<00:00,  2.31s/it]100%|██████████| 40/40 [01:33<00:00,  2.34s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:43,  2.63it/s]  1%|          | 2/272 [00:00<01:23,  3.22it/s]  1%|          | 3/272 [00:00<01:19,  3.39it/s]  1%|▏         | 4/272 [00:01<01:13,  3.65it/s]  2%|▏         | 5/272 [00:01<01:12,  3.70it/s]  2%|▏         | 6/272 [00:01<01:11,  3.70it/s]  3%|▎         | 7/272 [00:01<01:12,  3.65it/s]  3%|▎         | 8/272 [00:02<01:09,  3.80it/s]  3%|▎         | 9/272 [00:02<01:08,  3.82it/s]  4%|▎         | 10/272 [00:02<01:08,  3.80it/s]  4%|▍         | 11/272 [00:03<01:09,  3.73it/s]  4%|▍         | 12/272 [00:03<01:08,  3.81it/s]  5%|▍         | 13/272 [00:03<01:07,  3.86it/s]  5%|▌         | 14/272 [00:03<01:07,  3.82it/s]  6%|▌         | 15/272 [00:04<01:08,  3.76it/s]  6%|▌         | 16/272 [00:04<01:07,  3.81it/s]  6%|▋         | 17/272 [00:04<01:06,  3.84it/s]  7%|▋         | 18/272 [00:04<01:06,  3.81it/s]  7%|▋         | 19/272 [00:05<01:07,  3.76it/s]  7%|▋         | 20/272 [00:05<01:06,  3.81it/s]  8%|▊         | 21/272 [00:05<01:04,  3.86it/s]  8%|▊         | 22/272 [00:05<01:05,  3.82it/s]  8%|▊         | 23/272 [00:06<01:06,  3.76it/s]  9%|▉         | 24/272 [00:06<01:05,  3.80it/s]  9%|▉         | 25/272 [00:06<01:04,  3.86it/s] 10%|▉         | 26/272 [00:06<01:04,  3.82it/s] 10%|▉         | 27/272 [00:07<01:04,  3.82it/s] 10%|█         | 28/272 [00:07<01:05,  3.74it/s] 11%|█         | 29/272 [00:07<01:03,  3.83it/s] 11%|█         | 30/272 [00:07<01:03,  3.80it/s] 11%|█▏        | 31/272 [00:08<01:03,  3.79it/s] 12%|█▏        | 32/272 [00:08<01:04,  3.74it/s] 12%|█▏        | 33/272 [00:08<01:01,  3.86it/s] 12%|█▎        | 34/272 [00:09<01:02,  3.83it/s] 13%|█▎        | 35/272 [00:09<01:02,  3.80it/s] 13%|█▎        | 36/272 [00:09<01:02,  3.75it/s] 14%|█▎        | 37/272 [00:09<01:00,  3.87it/s] 14%|█▍        | 38/272 [00:10<01:00,  3.85it/s] 14%|█▍        | 39/272 [00:10<01:01,  3.82it/s] 15%|█▍        | 40/272 [00:10<01:02,  3.72it/s] 15%|█▌        | 41/272 [00:10<01:00,  3.84it/s] 15%|█▌        | 42/272 [00:11<00:59,  3.86it/s] 16%|█▌        | 43/272 [00:11<00:59,  3.83it/s] 16%|█▌        | 44/272 [00:11<01:00,  3.77it/s] 17%|█▋        | 45/272 [00:11<00:59,  3.81it/s] 17%|█▋        | 46/272 [00:12<00:58,  3.87it/s] 17%|█▋        | 47/272 [00:12<00:58,  3.81it/s] 18%|█▊        | 48/272 [00:12<00:59,  3.76it/s] 18%|█▊        | 49/272 [00:12<00:58,  3.81it/s] 18%|█▊        | 50/272 [00:13<00:57,  3.87it/s] 19%|█▉        | 51/272 [00:13<00:57,  3.82it/s] 19%|█▉        | 52/272 [00:13<00:58,  3.76it/s] 19%|█▉        | 53/272 [00:14<00:57,  3.80it/s] 20%|█▉        | 54/272 [00:14<00:56,  3.86it/s] 20%|██        | 55/272 [00:14<00:56,  3.82it/s] 21%|██        | 56/272 [00:14<00:57,  3.76it/s] 21%|██        | 57/272 [00:15<00:56,  3.80it/s] 21%|██▏       | 58/272 [00:15<00:55,  3.86it/s] 22%|██▏       | 59/272 [00:15<00:55,  3.82it/s] 22%|██▏       | 60/272 [00:15<00:56,  3.78it/s] 22%|██▏       | 61/272 [00:16<00:55,  3.79it/s] 23%|██▎       | 62/272 [00:16<00:54,  3.84it/s] 23%|██▎       | 63/272 [00:16<00:54,  3.80it/s] 24%|██▎       | 64/272 [00:16<00:55,  3.75it/s] 24%|██▍       | 65/272 [00:17<00:54,  3.79it/s] 24%|██▍       | 66/272 [00:17<00:53,  3.86it/s] 25%|██▍       | 67/272 [00:17<00:53,  3.81it/s] 25%|██▌       | 68/272 [00:17<00:54,  3.76it/s] 25%|██▌       | 69/272 [00:18<00:53,  3.79it/s] 26%|██▌       | 70/272 [00:18<00:52,  3.84it/s] 26%|██▌       | 71/272 [00:18<00:52,  3.80it/s] 26%|██▋       | 72/272 [00:19<00:53,  3.75it/s] 27%|██▋       | 73/272 [00:19<00:52,  3.79it/s] 27%|██▋       | 74/272 [00:19<00:51,  3.83it/s] 28%|██▊       | 75/272 [00:19<00:51,  3.80it/s] 28%|██▊       | 76/272 [00:20<00:52,  3.74it/s] 28%|██▊       | 77/272 [00:20<00:51,  3.79it/s] 29%|██▊       | 78/272 [00:20<00:50,  3.83it/s] 29%|██▉       | 79/272 [00:20<00:50,  3.80it/s] 29%|██▉       | 80/272 [00:21<00:51,  3.75it/s] 30%|██▉       | 81/272 [00:21<00:50,  3.80it/s] 30%|███       | 82/272 [00:21<00:49,  3.84it/s] 31%|███       | 83/272 [00:21<00:49,  3.81it/s] 31%|███       | 84/272 [00:22<00:50,  3.75it/s] 31%|███▏      | 85/272 [00:22<00:49,  3.79it/s] 32%|███▏      | 86/272 [00:22<00:48,  3.84it/s] 32%|███▏      | 87/272 [00:22<00:48,  3.80it/s] 32%|███▏      | 88/272 [00:23<00:49,  3.74it/s] 33%|███▎      | 89/272 [00:23<00:48,  3.79it/s] 33%|███▎      | 90/272 [00:23<00:47,  3.82it/s] 33%|███▎      | 91/272 [00:24<00:47,  3.80it/s] 34%|███▍      | 92/272 [00:24<00:48,  3.74it/s] 34%|███▍      | 93/272 [00:24<00:47,  3.80it/s] 35%|███▍      | 94/272 [00:24<00:46,  3.86it/s] 35%|███▍      | 95/272 [00:25<00:46,  3.82it/s] 35%|███▌      | 96/272 [00:25<00:46,  3.75it/s] 36%|███▌      | 97/272 [00:25<00:46,  3.80it/s] 36%|███▌      | 98/272 [00:25<00:45,  3.85it/s] 36%|███▋      | 99/272 [00:26<00:45,  3.81it/s] 37%|███▋      | 100/272 [00:26<00:45,  3.74it/s] 37%|███▋      | 101/272 [00:26<00:45,  3.79it/s] 38%|███▊      | 102/272 [00:26<00:44,  3.84it/s] 38%|███▊      | 103/272 [00:27<00:44,  3.80it/s] 38%|███▊      | 104/272 [00:27<00:44,  3.74it/s] 39%|███▊      | 105/272 [00:27<00:44,  3.79it/s] 39%|███▉      | 106/272 [00:27<00:43,  3.84it/s] 39%|███▉      | 107/272 [00:28<00:43,  3.80it/s] 40%|███▉      | 108/272 [00:28<00:43,  3.74it/s] 40%|████      | 109/272 [00:28<00:42,  3.79it/s] 40%|████      | 110/272 [00:29<00:42,  3.85it/s] 41%|████      | 111/272 [00:29<00:42,  3.81it/s] 41%|████      | 112/272 [00:29<00:42,  3.75it/s] 42%|████▏     | 113/272 [00:29<00:41,  3.79it/s] 42%|████▏     | 114/272 [00:30<00:41,  3.83it/s] 42%|████▏     | 115/272 [00:30<00:41,  3.79it/s] 43%|████▎     | 116/272 [00:30<00:41,  3.74it/s] 43%|████▎     | 117/272 [00:30<00:40,  3.79it/s] 43%|████▎     | 118/272 [00:31<00:40,  3.82it/s] 44%|████▍     | 119/272 [00:31<00:40,  3.79it/s] 44%|████▍     | 120/272 [00:31<00:40,  3.72it/s] 44%|████▍     | 121/272 [00:31<00:39,  3.78it/s] 45%|████▍     | 122/272 [00:32<00:39,  3.82it/s] 45%|████▌     | 123/272 [00:32<00:39,  3.79it/s] 46%|████▌     | 124/272 [00:32<00:39,  3.74it/s] 46%|████▌     | 125/272 [00:33<00:38,  3.78it/s] 46%|████▋     | 126/272 [00:33<00:38,  3.83it/s] 47%|████▋     | 127/272 [00:33<00:38,  3.80it/s] 47%|████▋     | 128/272 [00:33<00:38,  3.73it/s] 47%|████▋     | 129/272 [00:34<00:37,  3.79it/s] 48%|████▊     | 130/272 [00:34<00:36,  3.84it/s] 48%|████▊     | 131/272 [00:34<00:37,  3.80it/s] 49%|████▊     | 132/272 [00:34<00:37,  3.75it/s] 49%|████▉     | 133/272 [00:35<00:36,  3.79it/s] 49%|████▉     | 134/272 [00:35<00:35,  3.84it/s] 50%|████▉     | 135/272 [00:35<00:36,  3.80it/s] 50%|█████     | 136/272 [00:35<00:36,  3.73it/s] 50%|█████     | 137/272 [00:36<00:35,  3.78it/s] 51%|█████     | 138/272 [00:36<00:35,  3.82it/s] 51%|█████     | 139/272 [00:36<00:35,  3.79it/s] 51%|█████▏    | 140/272 [00:36<00:35,  3.73it/s] 52%|█████▏    | 141/272 [00:37<00:34,  3.78it/s] 52%|█████▏    | 142/272 [00:37<00:34,  3.82it/s] 53%|█████▎    | 143/272 [00:37<00:34,  3.78it/s] 53%|█████▎    | 144/272 [00:38<00:34,  3.72it/s] 53%|█████▎    | 145/272 [00:38<00:33,  3.77it/s] 54%|█████▎    | 146/272 [00:38<00:32,  3.83it/s] 54%|█████▍    | 147/272 [00:38<00:33,  3.78it/s] 54%|█████▍    | 148/272 [00:39<00:33,  3.72it/s] 55%|█████▍    | 149/272 [00:39<00:32,  3.78it/s] 55%|█████▌    | 150/272 [00:39<00:31,  3.82it/s] 56%|█████▌    | 151/272 [00:39<00:32,  3.78it/s] 56%|█████▌    | 152/272 [00:40<00:32,  3.72it/s] 56%|█████▋    | 153/272 [00:40<00:31,  3.78it/s] 57%|█████▋    | 154/272 [00:40<00:30,  3.83it/s] 57%|█████▋    | 155/272 [00:40<00:30,  3.78it/s] 57%|█████▋    | 156/272 [00:41<00:31,  3.72it/s] 58%|█████▊    | 157/272 [00:41<00:30,  3.77it/s] 58%|█████▊    | 158/272 [00:41<00:29,  3.80it/s] 58%|█████▊    | 159/272 [00:42<00:30,  3.77it/s] 59%|█████▉    | 160/272 [00:42<00:30,  3.71it/s] 59%|█████▉    | 161/272 [00:42<00:29,  3.77it/s] 60%|█████▉    | 162/272 [00:42<00:28,  3.83it/s] 60%|█████▉    | 163/272 [00:43<00:28,  3.78it/s] 60%|██████    | 164/272 [00:43<00:29,  3.72it/s] 61%|██████    | 165/272 [00:43<00:28,  3.78it/s] 61%|██████    | 166/272 [00:43<00:27,  3.82it/s] 61%|██████▏   | 167/272 [00:44<00:27,  3.78it/s] 62%|██████▏   | 168/272 [00:44<00:28,  3.71it/s] 62%|██████▏   | 169/272 [00:44<00:27,  3.78it/s] 62%|██████▎   | 170/272 [00:44<00:26,  3.83it/s] 63%|██████▎   | 171/272 [00:45<00:26,  3.78it/s] 63%|██████▎   | 172/272 [00:45<00:26,  3.72it/s] 64%|██████▎   | 173/272 [00:45<00:26,  3.78it/s] 64%|██████▍   | 174/272 [00:45<00:25,  3.83it/s] 64%|██████▍   | 175/272 [00:46<00:25,  3.78it/s] 65%|██████▍   | 176/272 [00:46<00:25,  3.70it/s] 65%|██████▌   | 177/272 [00:46<00:25,  3.77it/s] 65%|██████▌   | 178/272 [00:47<00:24,  3.82it/s] 66%|██████▌   | 179/272 [00:47<00:24,  3.77it/s] 66%|██████▌   | 180/272 [00:47<00:24,  3.70it/s] 67%|██████▋   | 181/272 [00:47<00:24,  3.77it/s] 67%|██████▋   | 182/272 [00:48<00:23,  3.82it/s] 67%|██████▋   | 183/272 [00:48<00:23,  3.78it/s] 68%|██████▊   | 184/272 [00:48<00:23,  3.71it/s] 68%|██████▊   | 185/272 [00:48<00:23,  3.77it/s] 68%|██████▊   | 186/272 [00:49<00:22,  3.80it/s] 69%|██████▉   | 187/272 [00:49<00:22,  3.76it/s] 69%|██████▉   | 188/272 [00:49<00:22,  3.74it/s] 69%|██████▉   | 189/272 [00:49<00:21,  3.84it/s] 70%|██████▉   | 190/272 [00:50<00:20,  3.91it/s] 70%|███████   | 191/272 [00:50<00:20,  3.97it/s] 71%|███████   | 192/272 [00:50<00:19,  4.01it/s] 71%|███████   | 193/272 [00:50<00:19,  4.04it/s] 71%|███████▏  | 194/272 [00:51<00:19,  4.05it/s] 72%|███████▏  | 195/272 [00:51<00:18,  4.06it/s] 72%|███████▏  | 196/272 [00:51<00:18,  4.08it/s] 72%|███████▏  | 197/272 [00:51<00:18,  4.09it/s] 73%|███████▎  | 198/272 [00:52<00:18,  4.09it/s] 73%|███████▎  | 199/272 [00:52<00:17,  4.09it/s] 74%|███████▎  | 200/272 [00:52<00:17,  4.09it/s] 74%|███████▍  | 201/272 [00:52<00:17,  4.09it/s] 74%|███████▍  | 202/272 [00:53<00:17,  4.10it/s] 75%|███████▍  | 203/272 [00:53<00:16,  4.10it/s] 75%|███████▌  | 204/272 [00:53<00:16,  4.10it/s] 75%|███████▌  | 205/272 [00:53<00:16,  4.09it/s] 76%|███████▌  | 206/272 [00:54<00:16,  4.10it/s] 76%|███████▌  | 207/272 [00:54<00:15,  4.10it/s] 76%|███████▋  | 208/272 [00:54<00:15,  4.09it/s] 77%|███████▋  | 209/272 [00:54<00:15,  4.09it/s] 77%|███████▋  | 210/272 [00:55<00:15,  4.09it/s] 78%|███████▊  | 211/272 [00:55<00:14,  4.10it/s] 78%|███████▊  | 212/272 [00:55<00:14,  4.11it/s] 78%|███████▊  | 213/272 [00:55<00:14,  4.10it/s] 79%|███████▊  | 214/272 [00:56<00:14,  4.08it/s] 79%|███████▉  | 215/272 [00:56<00:14,  3.94it/s] 79%|███████▉  | 216/272 [00:56<00:14,  3.83it/s] 80%|███████▉  | 217/272 [00:56<00:14,  3.90it/s] 80%|████████  | 218/272 [00:57<00:14,  3.84it/s] 81%|████████  | 219/272 [00:57<00:14,  3.77it/s] 81%|████████  | 220/272 [00:57<00:14,  3.70it/s] 81%|████████▏ | 221/272 [00:57<00:13,  3.80it/s] 82%|████████▏ | 222/272 [00:58<00:13,  3.77it/s] 82%|████████▏ | 223/272 [00:58<00:13,  3.73it/s] 82%|████████▏ | 224/272 [00:58<00:13,  3.68it/s] 83%|████████▎ | 225/272 [00:58<00:12,  3.80it/s] 83%|████████▎ | 226/272 [00:59<00:12,  3.76it/s] 83%|████████▎ | 227/272 [00:59<00:12,  3.71it/s] 84%|████████▍ | 228/272 [00:59<00:11,  3.67it/s] 84%|████████▍ | 229/272 [01:00<00:11,  3.78it/s] 85%|████████▍ | 230/272 [01:00<00:11,  3.73it/s] 85%|████████▍ | 231/272 [01:00<00:11,  3.70it/s] 85%|████████▌ | 232/272 [01:00<00:10,  3.66it/s] 86%|████████▌ | 233/272 [01:01<00:10,  3.78it/s] 86%|████████▌ | 234/272 [01:01<00:10,  3.74it/s] 86%|████████▋ | 235/272 [01:01<00:09,  3.71it/s] 87%|████████▋ | 236/272 [01:01<00:09,  3.67it/s] 87%|████████▋ | 237/272 [01:02<00:09,  3.79it/s] 88%|████████▊ | 238/272 [01:02<00:09,  3.75it/s] 88%|████████▊ | 239/272 [01:02<00:08,  3.71it/s] 88%|████████▊ | 240/272 [01:03<00:08,  3.67it/s] 89%|████████▊ | 241/272 [01:03<00:08,  3.78it/s] 89%|████████▉ | 242/272 [01:03<00:08,  3.75it/s] 89%|████████▉ | 243/272 [01:03<00:07,  3.70it/s] 90%|████████▉ | 244/272 [01:04<00:07,  3.67it/s] 90%|█████████ | 245/272 [01:04<00:07,  3.78it/s] 90%|█████████ | 246/272 [01:04<00:06,  3.73it/s] 91%|█████████ | 247/272 [01:04<00:06,  3.69it/s] 91%|█████████ | 248/272 [01:05<00:06,  3.66it/s] 92%|█████████▏| 249/272 [01:05<00:06,  3.78it/s] 92%|█████████▏| 250/272 [01:05<00:05,  3.75it/s] 92%|█████████▏| 251/272 [01:05<00:05,  3.72it/s] 93%|█████████▎| 252/272 [01:06<00:05,  3.67it/s] 93%|█████████▎| 253/272 [01:06<00:05,  3.79it/s] 93%|█████████▎| 254/272 [01:06<00:04,  3.75it/s] 94%|█████████▍| 255/272 [01:07<00:04,  3.71it/s] 94%|█████████▍| 256/272 [01:07<00:04,  3.67it/s] 94%|█████████▍| 257/272 [01:07<00:03,  3.78it/s] 95%|█████████▍| 258/272 [01:07<00:03,  3.74it/s] 95%|█████████▌| 259/272 [01:08<00:03,  3.70it/s] 96%|█████████▌| 260/272 [01:08<00:03,  3.67it/s] 96%|█████████▌| 261/272 [01:08<00:02,  3.78it/s] 96%|█████████▋| 262/272 [01:08<00:02,  3.73it/s] 97%|█████████▋| 263/272 [01:09<00:02,  3.70it/s] 97%|█████████▋| 264/272 [01:09<00:02,  3.66it/s] 97%|█████████▋| 265/272 [01:09<00:01,  3.78it/s] 98%|█████████▊| 266/272 [01:10<00:01,  3.74it/s] 98%|█████████▊| 267/272 [01:10<00:01,  3.71it/s] 99%|█████████▊| 268/272 [01:10<00:01,  3.66it/s] 99%|█████████▉| 269/272 [01:10<00:00,  3.78it/s] 99%|█████████▉| 270/272 [01:11<00:00,  3.76it/s]100%|█████████▉| 271/272 [01:11<00:00,  3.71it/s]100%|██████████| 272/272 [01:11<00:00,  3.67it/s]accuracy:  0.9007352941176471
100%|██████████| 272/272 [01:15<00:00,  3.59it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 149
Training dataset size: 240, validation dataset size: 149
Training dataset size: 240, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:32:20,809] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.50s/it][2025-03-12 05:32:20,892] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:32:22,613] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:42,  2.62s/it]  5%|▌         | 2/40 [00:04<01:25,  2.25s/it]  8%|▊         | 3/40 [00:06<01:22,  2.22s/it] 10%|█         | 4/40 [00:08<01:19,  2.20s/it] 12%|█▎        | 5/40 [00:11<01:15,  2.15s/it] 15%|█▌        | 6/40 [00:13<01:12,  2.14s/it] 18%|█▊        | 7/40 [00:16<01:18,  2.39s/it] 20%|██        | 8/40 [00:18<01:16,  2.40s/it] 22%|██▎       | 9/40 [00:20<01:14,  2.40s/it] 25%|██▌       | 10/40 [00:23<01:16,  2.54s/it]                                               {'loss': 0.5351, 'grad_norm': 2.9198505878448486, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:23<01:16,  2.54s/it] 28%|██▊       | 11/40 [00:25<01:09,  2.39s/it] 30%|███       | 12/40 [00:28<01:12,  2.57s/it] 32%|███▎      | 13/40 [00:30<01:04,  2.39s/it] 35%|███▌      | 14/40 [00:32<00:59,  2.28s/it] 38%|███▊      | 15/40 [00:35<00:58,  2.33s/it] 40%|████      | 16/40 [00:37<00:56,  2.37s/it] 42%|████▎     | 17/40 [00:39<00:53,  2.34s/it] 45%|████▌     | 18/40 [00:41<00:45,  2.09s/it] 48%|████▊     | 19/40 [00:43<00:43,  2.09s/it] 50%|█████     | 20/40 [00:45<00:42,  2.10s/it]                                               {'loss': 0.5776, 'grad_norm': 5.018629550933838, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:45<00:42,  2.10s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:48<00:45,  2.38s/it] 55%|█████▌    | 22/40 [00:50<00:41,  2.31s/it] 57%|█████▊    | 23/40 [00:53<00:43,  2.56s/it] 60%|██████    | 24/40 [00:55<00:37,  2.33s/it] 62%|██████▎   | 25/40 [00:58<00:36,  2.40s/it] 65%|██████▌   | 26/40 [01:01<00:34,  2.50s/it] 68%|██████▊   | 27/40 [01:03<00:31,  2.43s/it] 70%|███████   | 28/40 [01:05<00:29,  2.44s/it] 72%|███████▎  | 29/40 [01:07<00:26,  2.37s/it] 75%|███████▌  | 30/40 [01:10<00:23,  2.39s/it]                                               {'loss': 0.6971, 'grad_norm': 10.062565803527832, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:10<00:23,  2.39s/it] 78%|███████▊  | 31/40 [01:12<00:21,  2.43s/it] 80%|████████  | 32/40 [01:15<00:18,  2.31s/it] 82%|████████▎ | 33/40 [01:17<00:15,  2.23s/it] 85%|████████▌ | 34/40 [01:19<00:13,  2.25s/it] 88%|████████▊ | 35/40 [01:21<00:11,  2.28s/it] 90%|█████████ | 36/40 [01:24<00:09,  2.39s/it] 92%|█████████▎| 37/40 [01:26<00:07,  2.41s/it] 95%|█████████▌| 38/40 [01:29<00:04,  2.41s/it] 98%|█████████▊| 39/40 [01:32<00:02,  2.54s/it]100%|██████████| 40/40 [01:33<00:00,  2.27s/it]                                               {'loss': 0.4971, 'grad_norm': 10.649423599243164, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:33<00:00,  2.27s/it]                                               {'train_runtime': 94.3141, 'train_samples_per_second': 5.089, 'train_steps_per_second': 0.424, 'train_loss': 0.5767249703407288, 'epoch': 2.0}
100%|██████████| 40/40 [01:34<00:00,  2.27s/it]100%|██████████| 40/40 [01:34<00:00,  2.35s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<00:54,  3.21it/s]  1%|          | 2/175 [00:00<00:49,  3.53it/s]  2%|▏         | 3/175 [00:00<00:47,  3.62it/s]  2%|▏         | 4/175 [00:01<00:47,  3.60it/s]  3%|▎         | 5/175 [00:01<00:44,  3.78it/s]  3%|▎         | 6/175 [00:01<00:43,  3.85it/s]  4%|▍         | 7/175 [00:01<00:44,  3.80it/s]  5%|▍         | 8/175 [00:02<00:44,  3.79it/s]  5%|▌         | 9/175 [00:02<00:44,  3.74it/s]  6%|▌         | 10/175 [00:02<00:42,  3.86it/s]  6%|▋         | 11/175 [00:02<00:42,  3.87it/s]  7%|▋         | 12/175 [00:03<00:42,  3.82it/s]  7%|▋         | 13/175 [00:03<00:41,  3.86it/s]  8%|▊         | 14/175 [00:03<00:42,  3.77it/s]  9%|▊         | 15/175 [00:03<00:41,  3.86it/s]  9%|▉         | 16/175 [00:04<00:41,  3.81it/s] 10%|▉         | 17/175 [00:04<00:41,  3.79it/s] 10%|█         | 18/175 [00:04<00:42,  3.73it/s] 11%|█         | 19/175 [00:05<00:40,  3.85it/s] 11%|█▏        | 20/175 [00:05<00:40,  3.85it/s] 12%|█▏        | 21/175 [00:05<00:40,  3.81it/s] 13%|█▎        | 22/175 [00:05<00:39,  3.85it/s] 13%|█▎        | 23/175 [00:06<00:40,  3.77it/s] 14%|█▎        | 24/175 [00:06<00:39,  3.85it/s] 14%|█▍        | 25/175 [00:06<00:39,  3.79it/s] 15%|█▍        | 26/175 [00:06<00:39,  3.78it/s] 15%|█▌        | 27/175 [00:07<00:39,  3.74it/s] 16%|█▌        | 28/175 [00:07<00:38,  3.85it/s] 17%|█▋        | 29/175 [00:07<00:38,  3.83it/s] 17%|█▋        | 30/175 [00:07<00:38,  3.81it/s] 18%|█▊        | 31/175 [00:08<00:37,  3.84it/s] 18%|█▊        | 32/175 [00:08<00:37,  3.77it/s] 19%|█▉        | 33/175 [00:08<00:36,  3.86it/s] 19%|█▉        | 34/175 [00:08<00:36,  3.82it/s] 20%|██        | 35/175 [00:09<00:37,  3.78it/s] 21%|██        | 36/175 [00:09<00:37,  3.73it/s] 21%|██        | 37/175 [00:09<00:35,  3.85it/s] 22%|██▏       | 38/175 [00:10<00:35,  3.85it/s] 22%|██▏       | 39/175 [00:10<00:35,  3.80it/s] 23%|██▎       | 40/175 [00:10<00:35,  3.84it/s] 23%|██▎       | 41/175 [00:10<00:35,  3.76it/s] 24%|██▍       | 42/175 [00:11<00:34,  3.87it/s] 25%|██▍       | 43/175 [00:11<00:34,  3.82it/s] 25%|██▌       | 44/175 [00:11<00:34,  3.78it/s] 26%|██▌       | 45/175 [00:11<00:34,  3.73it/s] 26%|██▋       | 46/175 [00:12<00:33,  3.85it/s] 27%|██▋       | 47/175 [00:12<00:33,  3.87it/s] 27%|██▋       | 48/175 [00:12<00:33,  3.81it/s] 28%|██▊       | 49/175 [00:12<00:32,  3.85it/s] 29%|██▊       | 50/175 [00:13<00:33,  3.77it/s] 29%|██▉       | 51/175 [00:13<00:31,  3.88it/s] 30%|██▉       | 52/175 [00:13<00:32,  3.82it/s] 30%|███       | 53/175 [00:13<00:32,  3.78it/s] 31%|███       | 54/175 [00:14<00:32,  3.72it/s] 31%|███▏      | 55/175 [00:14<00:31,  3.85it/s] 32%|███▏      | 56/175 [00:14<00:30,  3.85it/s] 33%|███▎      | 57/175 [00:15<00:31,  3.80it/s] 33%|███▎      | 58/175 [00:15<00:30,  3.83it/s] 34%|███▎      | 59/175 [00:15<00:30,  3.76it/s] 34%|███▍      | 60/175 [00:15<00:29,  3.84it/s] 35%|███▍      | 61/175 [00:16<00:30,  3.77it/s] 35%|███▌      | 62/175 [00:16<00:29,  3.77it/s] 36%|███▌      | 63/175 [00:16<00:30,  3.72it/s] 37%|███▋      | 64/175 [00:16<00:28,  3.84it/s] 37%|███▋      | 65/175 [00:17<00:28,  3.85it/s] 38%|███▊      | 66/175 [00:17<00:28,  3.80it/s] 38%|███▊      | 67/175 [00:17<00:28,  3.84it/s] 39%|███▉      | 68/175 [00:17<00:28,  3.76it/s] 39%|███▉      | 69/175 [00:18<00:27,  3.86it/s] 40%|████      | 70/175 [00:18<00:27,  3.81it/s] 41%|████      | 71/175 [00:18<00:27,  3.76it/s] 41%|████      | 72/175 [00:18<00:27,  3.72it/s] 42%|████▏     | 73/175 [00:19<00:26,  3.84it/s] 42%|████▏     | 74/175 [00:19<00:26,  3.85it/s] 43%|████▎     | 75/175 [00:19<00:26,  3.80it/s] 43%|████▎     | 76/175 [00:20<00:26,  3.79it/s] 44%|████▍     | 77/175 [00:20<00:25,  3.78it/s] 45%|████▍     | 78/175 [00:20<00:25,  3.86it/s] 45%|████▌     | 79/175 [00:20<00:25,  3.80it/s] 46%|████▌     | 80/175 [00:21<00:24,  3.82it/s] 46%|████▋     | 81/175 [00:21<00:25,  3.75it/s] 47%|████▋     | 82/175 [00:21<00:24,  3.85it/s] 47%|████▋     | 83/175 [00:21<00:24,  3.80it/s] 48%|████▊     | 84/175 [00:22<00:24,  3.76it/s] 49%|████▊     | 85/175 [00:22<00:24,  3.71it/s] 49%|████▉     | 86/175 [00:22<00:23,  3.83it/s] 50%|████▉     | 87/175 [00:22<00:23,  3.81it/s] 50%|█████     | 88/175 [00:23<00:22,  3.78it/s] 51%|█████     | 89/175 [00:23<00:22,  3.78it/s] 51%|█████▏    | 90/175 [00:23<00:22,  3.76it/s] 52%|█████▏    | 91/175 [00:23<00:21,  3.83it/s] 53%|█████▎    | 92/175 [00:24<00:21,  3.78it/s] 53%|█████▎    | 93/175 [00:24<00:21,  3.76it/s] 54%|█████▎    | 94/175 [00:24<00:21,  3.71it/s] 54%|█████▍    | 95/175 [00:25<00:20,  3.83it/s] 55%|█████▍    | 96/175 [00:25<00:20,  3.83it/s] 55%|█████▌    | 97/175 [00:25<00:20,  3.79it/s] 56%|█████▌    | 98/175 [00:25<00:20,  3.82it/s] 57%|█████▋    | 99/175 [00:26<00:20,  3.74it/s] 57%|█████▋    | 100/175 [00:26<00:19,  3.82it/s] 58%|█████▊    | 101/175 [00:26<00:19,  3.77it/s] 58%|█████▊    | 102/175 [00:26<00:19,  3.75it/s] 59%|█████▉    | 103/175 [00:27<00:19,  3.71it/s] 59%|█████▉    | 104/175 [00:27<00:18,  3.83it/s] 60%|██████    | 105/175 [00:27<00:18,  3.83it/s] 61%|██████    | 106/175 [00:27<00:18,  3.78it/s] 61%|██████    | 107/175 [00:28<00:17,  3.80it/s] 62%|██████▏   | 108/175 [00:28<00:17,  3.75it/s] 62%|██████▏   | 109/175 [00:28<00:17,  3.83it/s] 63%|██████▎   | 110/175 [00:28<00:17,  3.77it/s] 63%|██████▎   | 111/175 [00:29<00:17,  3.76it/s] 64%|██████▍   | 112/175 [00:29<00:16,  3.71it/s] 65%|██████▍   | 113/175 [00:29<00:16,  3.83it/s] 65%|██████▌   | 114/175 [00:30<00:15,  3.84it/s] 66%|██████▌   | 115/175 [00:30<00:15,  3.79it/s] 66%|██████▋   | 116/175 [00:30<00:15,  3.82it/s] 67%|██████▋   | 117/175 [00:30<00:15,  3.74it/s] 67%|██████▋   | 118/175 [00:31<00:14,  3.83it/s] 68%|██████▊   | 119/175 [00:31<00:14,  3.79it/s] 69%|██████▊   | 120/175 [00:31<00:14,  3.75it/s] 69%|██████▉   | 121/175 [00:31<00:14,  3.71it/s] 70%|██████▉   | 122/175 [00:32<00:13,  3.83it/s] 70%|███████   | 123/175 [00:32<00:13,  3.84it/s] 71%|███████   | 124/175 [00:32<00:13,  3.79it/s] 71%|███████▏  | 125/175 [00:32<00:13,  3.78it/s] 72%|███████▏  | 126/175 [00:33<00:13,  3.76it/s] 73%|███████▎  | 127/175 [00:33<00:12,  3.83it/s] 73%|███████▎  | 128/175 [00:33<00:12,  3.79it/s] 74%|███████▎  | 129/175 [00:34<00:12,  3.76it/s] 74%|███████▍  | 130/175 [00:34<00:12,  3.71it/s] 75%|███████▍  | 131/175 [00:34<00:11,  3.82it/s] 75%|███████▌  | 132/175 [00:34<00:11,  3.83it/s] 76%|███████▌  | 133/175 [00:35<00:11,  3.78it/s] 77%|███████▋  | 134/175 [00:35<00:10,  3.76it/s] 77%|███████▋  | 135/175 [00:35<00:10,  3.76it/s] 78%|███████▊  | 136/175 [00:35<00:10,  3.81it/s] 78%|███████▊  | 137/175 [00:36<00:10,  3.77it/s] 79%|███████▉  | 138/175 [00:36<00:09,  3.79it/s] 79%|███████▉  | 139/175 [00:36<00:09,  3.72it/s] 80%|████████  | 140/175 [00:36<00:09,  3.80it/s] 81%|████████  | 141/175 [00:37<00:09,  3.75it/s] 81%|████████  | 142/175 [00:37<00:08,  3.74it/s] 82%|████████▏ | 143/175 [00:37<00:08,  3.68it/s] 82%|████████▏ | 144/175 [00:37<00:08,  3.80it/s] 83%|████████▎ | 145/175 [00:38<00:07,  3.78it/s] 83%|████████▎ | 146/175 [00:38<00:07,  3.76it/s] 84%|████████▍ | 147/175 [00:38<00:07,  3.74it/s] 85%|████████▍ | 148/175 [00:39<00:07,  3.74it/s] 85%|████████▌ | 149/175 [00:39<00:06,  3.81it/s] 86%|████████▌ | 150/175 [00:39<00:06,  3.75it/s] 86%|████████▋ | 151/175 [00:39<00:06,  3.73it/s] 87%|████████▋ | 152/175 [00:40<00:06,  3.69it/s] 87%|████████▋ | 153/175 [00:40<00:05,  3.80it/s] 88%|████████▊ | 154/175 [00:40<00:05,  3.78it/s] 89%|████████▊ | 155/175 [00:40<00:05,  3.76it/s] 89%|████████▉ | 156/175 [00:41<00:05,  3.75it/s] 90%|████████▉ | 157/175 [00:41<00:04,  3.74it/s] 90%|█████████ | 158/175 [00:41<00:04,  3.81it/s] 91%|█████████ | 159/175 [00:41<00:04,  3.75it/s] 91%|█████████▏| 160/175 [00:42<00:04,  3.73it/s] 92%|█████████▏| 161/175 [00:42<00:03,  3.69it/s] 93%|█████████▎| 162/175 [00:42<00:03,  3.81it/s] 93%|█████████▎| 163/175 [00:43<00:03,  3.79it/s] 94%|█████████▎| 164/175 [00:43<00:02,  3.77it/s] 94%|█████████▍| 165/175 [00:43<00:02,  3.75it/s] 95%|█████████▍| 166/175 [00:43<00:02,  3.74it/s] 95%|█████████▌| 167/175 [00:44<00:02,  3.83it/s] 96%|█████████▌| 168/175 [00:44<00:01,  3.78it/s] 97%|█████████▋| 169/175 [00:44<00:01,  3.74it/s] 97%|█████████▋| 170/175 [00:44<00:01,  3.69it/s] 98%|█████████▊| 171/175 [00:45<00:01,  3.80it/s] 98%|█████████▊| 172/175 [00:45<00:00,  3.81it/s] 99%|█████████▉| 173/175 [00:45<00:00,  3.76it/s] 99%|█████████▉| 174/175 [00:45<00:00,  3.77it/s]100%|██████████| 175/175 [00:46<00:00,  3.73it/s]accuracy:  0.8228571428571428
100%|██████████| 175/175 [00:48<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 240, validation dataset size: 119
Training dataset size: 240, validation dataset size: 119
Training dataset size: 240, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.39s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 05:35:12,274] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 05:35:12,353] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:35:12,639] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/40 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▎         | 1/40 [00:02<01:36,  2.47s/it]  5%|▌         | 2/40 [00:04<01:33,  2.45s/it]  8%|▊         | 3/40 [00:07<01:33,  2.52s/it] 10%|█         | 4/40 [00:09<01:28,  2.45s/it] 12%|█▎        | 5/40 [00:12<01:25,  2.43s/it] 15%|█▌        | 6/40 [00:15<01:27,  2.59s/it] 18%|█▊        | 7/40 [00:17<01:27,  2.66s/it] 20%|██        | 8/40 [00:20<01:23,  2.62s/it] 22%|██▎       | 9/40 [00:22<01:20,  2.58s/it] 25%|██▌       | 10/40 [00:25<01:18,  2.62s/it]                                               {'loss': 1.2302, 'grad_norm': 8.77003288269043, 'learning_rate': 8.94570254698197e-06, 'epoch': 0.5}
 25%|██▌       | 10/40 [00:25<01:18,  2.62s/it] 28%|██▊       | 11/40 [00:28<01:13,  2.54s/it] 30%|███       | 12/40 [00:30<01:11,  2.54s/it] 32%|███▎      | 13/40 [00:33<01:09,  2.59s/it] 35%|███▌      | 14/40 [00:35<01:07,  2.60s/it] 38%|███▊      | 15/40 [00:38<01:05,  2.63s/it] 40%|████      | 16/40 [00:41<01:04,  2.69s/it] 42%|████▎     | 17/40 [00:44<01:01,  2.69s/it] 45%|████▌     | 18/40 [00:46<00:55,  2.54s/it] 48%|████▊     | 19/40 [00:48<00:51,  2.47s/it] 50%|█████     | 20/40 [00:51<00:50,  2.54s/it]                                               {'loss': 1.0081, 'grad_norm': 4.157856464385986, 'learning_rate': 5.412896727361663e-06, 'epoch': 1.0}
 50%|█████     | 20/40 [00:51<00:50,  2.54s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▎    | 21/40 [00:53<00:47,  2.53s/it] 55%|█████▌    | 22/40 [00:56<00:46,  2.57s/it] 57%|█████▊    | 23/40 [00:59<00:44,  2.60s/it] 60%|██████    | 24/40 [01:01<00:40,  2.50s/it] 62%|██████▎   | 25/40 [01:03<00:35,  2.37s/it] 65%|██████▌   | 26/40 [01:05<00:33,  2.39s/it] 68%|██████▊   | 27/40 [01:08<00:32,  2.49s/it] 70%|███████   | 28/40 [01:11<00:30,  2.56s/it] 72%|███████▎  | 29/40 [01:13<00:26,  2.44s/it] 75%|███████▌  | 30/40 [01:15<00:24,  2.40s/it]                                               {'loss': 0.8509, 'grad_norm': 5.569456577301025, 'learning_rate': 1.6135921418712959e-06, 'epoch': 1.5}
 75%|███████▌  | 30/40 [01:15<00:24,  2.40s/it] 78%|███████▊  | 31/40 [01:18<00:21,  2.43s/it] 80%|████████  | 32/40 [01:20<00:19,  2.45s/it] 82%|████████▎ | 33/40 [01:23<00:18,  2.62s/it] 85%|████████▌ | 34/40 [01:26<00:15,  2.66s/it] 88%|████████▊ | 35/40 [01:29<00:13,  2.60s/it] 90%|█████████ | 36/40 [01:31<00:10,  2.56s/it] 92%|█████████▎| 37/40 [01:34<00:07,  2.64s/it] 95%|█████████▌| 38/40 [01:37<00:05,  2.69s/it] 98%|█████████▊| 39/40 [01:39<00:02,  2.65s/it]100%|██████████| 40/40 [01:42<00:00,  2.64s/it]                                               {'loss': 0.9021, 'grad_norm': 4.420116424560547, 'learning_rate': 0.0, 'epoch': 2.0}
100%|██████████| 40/40 [01:42<00:00,  2.64s/it]                                               {'train_runtime': 103.0115, 'train_samples_per_second': 4.66, 'train_steps_per_second': 0.388, 'train_loss': 0.9978359460830688, 'epoch': 2.0}
100%|██████████| 40/40 [01:42<00:00,  2.64s/it]100%|██████████| 40/40 [01:42<00:00,  2.57s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:52,  2.89it/s]  1%|▏         | 2/153 [00:00<00:42,  3.54it/s]  2%|▏         | 3/153 [00:00<00:40,  3.66it/s]  3%|▎         | 4/153 [00:01<00:40,  3.72it/s]  3%|▎         | 5/153 [00:01<00:40,  3.68it/s]  4%|▍         | 6/153 [00:01<00:38,  3.81it/s]  5%|▍         | 7/153 [00:01<00:37,  3.87it/s]  5%|▌         | 8/153 [00:02<00:37,  3.83it/s]  6%|▌         | 9/153 [00:02<00:37,  3.86it/s]  7%|▋         | 10/153 [00:02<00:37,  3.81it/s]  7%|▋         | 11/153 [00:02<00:36,  3.88it/s]  8%|▊         | 12/153 [00:03<00:36,  3.83it/s]  8%|▊         | 13/153 [00:03<00:36,  3.86it/s]  9%|▉         | 14/153 [00:03<00:36,  3.79it/s] 10%|▉         | 15/153 [00:03<00:35,  3.88it/s] 10%|█         | 16/153 [00:04<00:35,  3.83it/s] 11%|█         | 17/153 [00:04<00:35,  3.83it/s] 12%|█▏        | 18/153 [00:04<00:35,  3.77it/s] 12%|█▏        | 19/153 [00:05<00:34,  3.88it/s] 13%|█▎        | 20/153 [00:05<00:34,  3.86it/s] 14%|█▎        | 21/153 [00:05<00:34,  3.81it/s] 14%|█▍        | 22/153 [00:05<00:34,  3.75it/s] 15%|█▌        | 23/153 [00:06<00:33,  3.87it/s] 16%|█▌        | 24/153 [00:06<00:33,  3.88it/s] 16%|█▋        | 25/153 [00:06<00:33,  3.85it/s] 17%|█▋        | 26/153 [00:06<00:33,  3.81it/s] 18%|█▊        | 27/153 [00:07<00:32,  3.83it/s] 18%|█▊        | 28/153 [00:07<00:32,  3.88it/s] 19%|█▉        | 29/153 [00:07<00:32,  3.84it/s] 20%|█▉        | 30/153 [00:07<00:31,  3.87it/s] 20%|██        | 31/153 [00:08<00:32,  3.78it/s] 21%|██        | 32/153 [00:08<00:31,  3.84it/s] 22%|██▏       | 33/153 [00:08<00:31,  3.81it/s] 22%|██▏       | 34/153 [00:08<00:30,  3.85it/s] 23%|██▎       | 35/153 [00:09<00:31,  3.78it/s] 24%|██▎       | 36/153 [00:09<00:30,  3.87it/s] 24%|██▍       | 37/153 [00:09<00:30,  3.83it/s] 25%|██▍       | 38/153 [00:09<00:29,  3.86it/s] 25%|██▌       | 39/153 [00:10<00:30,  3.79it/s] 26%|██▌       | 40/153 [00:10<00:29,  3.89it/s] 27%|██▋       | 41/153 [00:10<00:29,  3.84it/s] 27%|██▋       | 42/153 [00:11<00:29,  3.81it/s] 28%|██▊       | 43/153 [00:11<00:29,  3.76it/s] 29%|██▉       | 44/153 [00:11<00:28,  3.87it/s] 29%|██▉       | 45/153 [00:11<00:27,  3.86it/s] 30%|███       | 46/153 [00:12<00:28,  3.82it/s] 31%|███       | 47/153 [00:12<00:28,  3.72it/s] 31%|███▏      | 48/153 [00:12<00:27,  3.85it/s] 32%|███▏      | 49/153 [00:12<00:26,  3.86it/s] 33%|███▎      | 50/153 [00:13<00:26,  3.82it/s] 33%|███▎      | 51/153 [00:13<00:27,  3.77it/s] 34%|███▍      | 52/153 [00:13<00:26,  3.82it/s] 35%|███▍      | 53/153 [00:13<00:25,  3.87it/s] 35%|███▌      | 54/153 [00:14<00:25,  3.83it/s] 36%|███▌      | 55/153 [00:14<00:25,  3.83it/s] 37%|███▋      | 56/153 [00:14<00:25,  3.78it/s] 37%|███▋      | 57/153 [00:14<00:24,  3.85it/s] 38%|███▊      | 58/153 [00:15<00:24,  3.82it/s] 39%|███▊      | 59/153 [00:15<00:24,  3.85it/s] 39%|███▉      | 60/153 [00:15<00:24,  3.78it/s] 40%|███▉      | 61/153 [00:15<00:23,  3.86it/s] 41%|████      | 62/153 [00:16<00:23,  3.83it/s] 41%|████      | 63/153 [00:16<00:23,  3.82it/s] 42%|████▏     | 64/153 [00:16<00:23,  3.75it/s] 42%|████▏     | 65/153 [00:17<00:22,  3.84it/s] 43%|████▎     | 66/153 [00:17<00:22,  3.80it/s] 44%|████▍     | 67/153 [00:17<00:22,  3.78it/s] 44%|████▍     | 68/153 [00:17<00:22,  3.73it/s] 45%|████▌     | 69/153 [00:18<00:21,  3.84it/s] 46%|████▌     | 70/153 [00:18<00:21,  3.82it/s] 46%|████▋     | 71/153 [00:18<00:21,  3.78it/s] 47%|████▋     | 72/153 [00:18<00:21,  3.73it/s] 48%|████▊     | 73/153 [00:19<00:20,  3.84it/s] 48%|████▊     | 74/153 [00:19<00:20,  3.82it/s] 49%|████▉     | 75/153 [00:19<00:20,  3.80it/s] 50%|████▉     | 76/153 [00:19<00:20,  3.70it/s] 50%|█████     | 77/153 [00:20<00:19,  3.82it/s] 51%|█████     | 78/153 [00:20<00:19,  3.84it/s] 52%|█████▏    | 79/153 [00:20<00:19,  3.80it/s] 52%|█████▏    | 80/153 [00:21<00:19,  3.74it/s] 53%|█████▎    | 81/153 [00:21<00:18,  3.79it/s] 54%|█████▎    | 82/153 [00:21<00:18,  3.83it/s] 54%|█████▍    | 83/153 [00:21<00:18,  3.80it/s] 55%|█████▍    | 84/153 [00:22<00:18,  3.75it/s] 56%|█████▌    | 85/153 [00:22<00:17,  3.81it/s] 56%|█████▌    | 86/153 [00:22<00:17,  3.85it/s] 57%|█████▋    | 87/153 [00:22<00:17,  3.81it/s] 58%|█████▊    | 88/153 [00:23<00:17,  3.75it/s] 58%|█████▊    | 89/153 [00:23<00:16,  3.80it/s] 59%|█████▉    | 90/153 [00:23<00:16,  3.83it/s] 59%|█████▉    | 91/153 [00:23<00:16,  3.80it/s] 60%|██████    | 92/153 [00:24<00:16,  3.74it/s] 61%|██████    | 93/153 [00:24<00:15,  3.79it/s] 61%|██████▏   | 94/153 [00:24<00:15,  3.82it/s] 62%|██████▏   | 95/153 [00:24<00:15,  3.79it/s] 63%|██████▎   | 96/153 [00:25<00:15,  3.73it/s] 63%|██████▎   | 97/153 [00:25<00:14,  3.79it/s] 64%|██████▍   | 98/153 [00:25<00:14,  3.82it/s] 65%|██████▍   | 99/153 [00:26<00:14,  3.79it/s] 65%|██████▌   | 100/153 [00:26<00:14,  3.73it/s] 66%|██████▌   | 101/153 [00:26<00:13,  3.78it/s] 67%|██████▋   | 102/153 [00:26<00:13,  3.81it/s] 67%|██████▋   | 103/153 [00:27<00:13,  3.78it/s] 68%|██████▊   | 104/153 [00:27<00:13,  3.73it/s] 69%|██████▊   | 105/153 [00:27<00:12,  3.78it/s] 69%|██████▉   | 106/153 [00:27<00:12,  3.83it/s] 70%|██████▉   | 107/153 [00:28<00:12,  3.79it/s] 71%|███████   | 108/153 [00:28<00:12,  3.74it/s] 71%|███████   | 109/153 [00:28<00:11,  3.79it/s] 72%|███████▏  | 110/153 [00:28<00:11,  3.83it/s] 73%|███████▎  | 111/153 [00:29<00:11,  3.79it/s] 73%|███████▎  | 112/153 [00:29<00:11,  3.72it/s] 74%|███████▍  | 113/153 [00:29<00:10,  3.79it/s] 75%|███████▍  | 114/153 [00:29<00:10,  3.83it/s] 75%|███████▌  | 115/153 [00:30<00:10,  3.79it/s] 76%|███████▌  | 116/153 [00:30<00:09,  3.72it/s] 76%|███████▋  | 117/153 [00:30<00:09,  3.78it/s] 77%|███████▋  | 118/153 [00:31<00:09,  3.84it/s] 78%|███████▊  | 119/153 [00:31<00:08,  3.79it/s] 78%|███████▊  | 120/153 [00:31<00:08,  3.73it/s] 79%|███████▉  | 121/153 [00:31<00:08,  3.79it/s] 80%|███████▉  | 122/153 [00:32<00:08,  3.84it/s] 80%|████████  | 123/153 [00:32<00:07,  3.80it/s] 81%|████████  | 124/153 [00:32<00:07,  3.73it/s] 82%|████████▏ | 125/153 [00:32<00:07,  3.79it/s] 82%|████████▏ | 126/153 [00:33<00:07,  3.83it/s] 83%|████████▎ | 127/153 [00:33<00:06,  3.79it/s] 84%|████████▎ | 128/153 [00:33<00:06,  3.72it/s] 84%|████████▍ | 129/153 [00:33<00:06,  3.77it/s] 85%|████████▍ | 130/153 [00:34<00:06,  3.81it/s] 86%|████████▌ | 131/153 [00:34<00:05,  3.77it/s] 86%|████████▋ | 132/153 [00:34<00:05,  3.71it/s] 87%|████████▋ | 133/153 [00:35<00:05,  3.78it/s] 88%|████████▊ | 134/153 [00:35<00:04,  3.83it/s] 88%|████████▊ | 135/153 [00:35<00:04,  3.78it/s] 89%|████████▉ | 136/153 [00:35<00:04,  3.72it/s] 90%|████████▉ | 137/153 [00:36<00:04,  3.78it/s] 90%|█████████ | 138/153 [00:36<00:03,  3.82it/s] 91%|█████████ | 139/153 [00:36<00:03,  3.78it/s] 92%|█████████▏| 140/153 [00:36<00:03,  3.73it/s] 92%|█████████▏| 141/153 [00:37<00:03,  3.78it/s] 93%|█████████▎| 142/153 [00:37<00:02,  3.83it/s] 93%|█████████▎| 143/153 [00:37<00:02,  3.78it/s] 94%|█████████▍| 144/153 [00:37<00:02,  3.73it/s] 95%|█████████▍| 145/153 [00:38<00:02,  3.77it/s] 95%|█████████▌| 146/153 [00:38<00:01,  3.81it/s] 96%|█████████▌| 147/153 [00:38<00:01,  3.77it/s] 97%|█████████▋| 148/153 [00:38<00:01,  3.71it/s] 97%|█████████▋| 149/153 [00:39<00:01,  3.78it/s] 98%|█████████▊| 150/153 [00:39<00:00,  3.81it/s] 99%|█████████▊| 151/153 [00:39<00:00,  3.77it/s] 99%|█████████▉| 152/153 [00:40<00:00,  3.72it/s]100%|██████████| 153/153 [00:40<00:00,  3.77it/s]accuracy:  0.673202614379085
100%|██████████| 153/153 [00:42<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 152
Training dataset size: 288, validation dataset size: 152
Training dataset size: 288, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:38:07,059] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 05:38:07,074] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 05:38:07,153] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:03<02:24,  3.07s/it]  4%|▍         | 2/48 [00:05<01:58,  2.57s/it]  6%|▋         | 3/48 [00:07<01:55,  2.57s/it]  8%|▊         | 4/48 [00:10<01:48,  2.47s/it] 10%|█         | 5/48 [00:12<01:42,  2.39s/it] 12%|█▎        | 6/48 [00:14<01:31,  2.17s/it] 15%|█▍        | 7/48 [00:16<01:31,  2.22s/it] 17%|█▋        | 8/48 [00:18<01:26,  2.15s/it] 19%|█▉        | 9/48 [00:20<01:25,  2.19s/it] 21%|██        | 10/48 [00:23<01:26,  2.27s/it]                                               {'loss': 1.2076, 'grad_norm': 11.21467113494873, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:23<01:26,  2.27s/it] 23%|██▎       | 11/48 [00:25<01:18,  2.13s/it] 25%|██▌       | 12/48 [00:27<01:19,  2.20s/it] 27%|██▋       | 13/48 [00:30<01:23,  2.38s/it] 29%|██▉       | 14/48 [00:32<01:18,  2.30s/it] 31%|███▏      | 15/48 [00:34<01:11,  2.17s/it] 33%|███▎      | 16/48 [00:36<01:14,  2.33s/it] 35%|███▌      | 17/48 [00:39<01:12,  2.35s/it] 38%|███▊      | 18/48 [00:41<01:08,  2.30s/it] 40%|███▉      | 19/48 [00:43<01:07,  2.32s/it] 42%|████▏     | 20/48 [00:46<01:06,  2.39s/it]                                               {'loss': 0.8198, 'grad_norm': 5.721517086029053, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:46<01:06,  2.39s/it] 44%|████▍     | 21/48 [00:48<01:00,  2.25s/it] 46%|████▌     | 22/48 [00:49<00:53,  2.08s/it] 48%|████▊     | 23/48 [00:52<00:53,  2.15s/it] 50%|█████     | 24/48 [00:54<00:52,  2.17s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [00:57<00:54,  2.39s/it] 54%|█████▍    | 26/48 [00:59<00:53,  2.42s/it] 56%|█████▋    | 27/48 [01:02<00:53,  2.56s/it] 58%|█████▊    | 28/48 [01:04<00:48,  2.42s/it] 60%|██████    | 29/48 [01:07<00:46,  2.43s/it] 62%|██████▎   | 30/48 [01:09<00:44,  2.49s/it]                                               {'loss': 0.7705, 'grad_norm': 4.063690185546875, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:09<00:44,  2.49s/it] 65%|██████▍   | 31/48 [01:12<00:43,  2.58s/it] 67%|██████▋   | 32/48 [01:14<00:39,  2.45s/it] 69%|██████▉   | 33/48 [01:17<00:35,  2.39s/it] 71%|███████   | 34/48 [01:19<00:32,  2.33s/it] 73%|███████▎  | 35/48 [01:21<00:29,  2.26s/it] 75%|███████▌  | 36/48 [01:23<00:26,  2.17s/it] 77%|███████▋  | 37/48 [01:25<00:23,  2.15s/it] 79%|███████▉  | 38/48 [01:27<00:21,  2.16s/it] 81%|████████▏ | 39/48 [01:30<00:20,  2.25s/it] 83%|████████▎ | 40/48 [01:32<00:18,  2.27s/it]                                               {'loss': 0.6126, 'grad_norm': 4.778172016143799, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:32<00:18,  2.27s/it] 85%|████████▌ | 41/48 [01:34<00:15,  2.28s/it] 88%|████████▊ | 42/48 [01:37<00:13,  2.28s/it] 90%|████████▉ | 43/48 [01:39<00:11,  2.29s/it] 92%|█████████▏| 44/48 [01:41<00:09,  2.39s/it] 94%|█████████▍| 45/48 [01:43<00:06,  2.20s/it] 96%|█████████▌| 46/48 [01:46<00:04,  2.34s/it] 98%|█████████▊| 47/48 [01:48<00:02,  2.31s/it]100%|██████████| 48/48 [01:51<00:00,  2.50s/it]                                               {'train_runtime': 112.236, 'train_samples_per_second': 5.132, 'train_steps_per_second': 0.428, 'train_loss': 0.8524886866410574, 'epoch': 2.0}
100%|██████████| 48/48 [01:52<00:00,  2.50s/it]100%|██████████| 48/48 [01:52<00:00,  2.33s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:11,  2.97it/s]  1%|          | 2/212 [00:00<01:01,  3.39it/s]  1%|▏         | 3/212 [00:00<00:58,  3.59it/s]  2%|▏         | 4/212 [00:01<00:57,  3.59it/s]  2%|▏         | 5/212 [00:01<00:54,  3.77it/s]  3%|▎         | 6/212 [00:01<00:54,  3.81it/s]  3%|▎         | 7/212 [00:01<00:54,  3.78it/s]  4%|▍         | 8/212 [00:02<00:53,  3.79it/s]  4%|▍         | 9/212 [00:02<00:54,  3.74it/s]  5%|▍         | 10/212 [00:02<00:52,  3.86it/s]  5%|▌         | 11/212 [00:02<00:51,  3.88it/s]  6%|▌         | 12/212 [00:03<00:52,  3.81it/s]  6%|▌         | 13/212 [00:03<00:52,  3.80it/s]  7%|▋         | 14/212 [00:03<00:53,  3.72it/s]  7%|▋         | 15/212 [00:03<00:51,  3.84it/s]  8%|▊         | 16/212 [00:04<00:50,  3.89it/s]  8%|▊         | 17/212 [00:04<00:50,  3.84it/s]  8%|▊         | 18/212 [00:04<00:51,  3.80it/s]  9%|▉         | 19/212 [00:05<00:51,  3.72it/s]  9%|▉         | 20/212 [00:05<00:49,  3.86it/s] 10%|▉         | 21/212 [00:05<00:48,  3.94it/s] 10%|█         | 22/212 [00:05<00:49,  3.87it/s] 11%|█         | 23/212 [00:06<00:49,  3.81it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.73it/s] 12%|█▏        | 25/212 [00:06<00:48,  3.86it/s] 12%|█▏        | 26/212 [00:06<00:47,  3.94it/s] 13%|█▎        | 27/212 [00:07<00:47,  3.87it/s] 13%|█▎        | 28/212 [00:07<00:48,  3.81it/s] 14%|█▎        | 29/212 [00:07<00:49,  3.73it/s] 14%|█▍        | 30/212 [00:07<00:47,  3.85it/s] 15%|█▍        | 31/212 [00:08<00:46,  3.93it/s] 15%|█▌        | 32/212 [00:08<00:46,  3.86it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.80it/s] 16%|█▌        | 34/212 [00:08<00:47,  3.72it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.84it/s] 17%|█▋        | 36/212 [00:09<00:45,  3.90it/s] 17%|█▋        | 37/212 [00:09<00:45,  3.84it/s] 18%|█▊        | 38/212 [00:10<00:45,  3.80it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.72it/s] 19%|█▉        | 40/212 [00:10<00:44,  3.85it/s] 19%|█▉        | 41/212 [00:10<00:43,  3.92it/s] 20%|█▉        | 42/212 [00:11<00:44,  3.86it/s] 20%|██        | 43/212 [00:11<00:44,  3.80it/s] 21%|██        | 44/212 [00:11<00:45,  3.71it/s] 21%|██        | 45/212 [00:11<00:43,  3.83it/s] 22%|██▏       | 46/212 [00:12<00:42,  3.90it/s] 22%|██▏       | 47/212 [00:12<00:43,  3.83it/s] 23%|██▎       | 48/212 [00:12<00:43,  3.78it/s] 23%|██▎       | 49/212 [00:12<00:43,  3.71it/s] 24%|██▎       | 50/212 [00:13<00:42,  3.83it/s] 24%|██▍       | 51/212 [00:13<00:41,  3.89it/s] 25%|██▍       | 52/212 [00:13<00:41,  3.83it/s] 25%|██▌       | 53/212 [00:13<00:41,  3.79it/s] 25%|██▌       | 54/212 [00:14<00:42,  3.71it/s] 26%|██▌       | 55/212 [00:14<00:40,  3.83it/s] 26%|██▋       | 56/212 [00:14<00:39,  3.91it/s] 27%|██▋       | 57/212 [00:14<00:40,  3.84it/s] 27%|██▋       | 58/212 [00:15<00:40,  3.79it/s] 28%|██▊       | 59/212 [00:15<00:41,  3.70it/s] 28%|██▊       | 60/212 [00:15<00:39,  3.82it/s] 29%|██▉       | 61/212 [00:16<00:39,  3.85it/s] 29%|██▉       | 62/212 [00:16<00:39,  3.81it/s] 30%|██▉       | 63/212 [00:16<00:39,  3.77it/s] 30%|███       | 64/212 [00:16<00:40,  3.70it/s] 31%|███       | 65/212 [00:17<00:38,  3.81it/s] 31%|███       | 66/212 [00:17<00:37,  3.86it/s] 32%|███▏      | 67/212 [00:17<00:37,  3.82it/s] 32%|███▏      | 68/212 [00:17<00:38,  3.78it/s] 33%|███▎      | 69/212 [00:18<00:38,  3.70it/s] 33%|███▎      | 70/212 [00:18<00:37,  3.82it/s] 33%|███▎      | 71/212 [00:18<00:36,  3.87it/s] 34%|███▍      | 72/212 [00:18<00:36,  3.82it/s] 34%|███▍      | 73/212 [00:19<00:36,  3.78it/s] 35%|███▍      | 74/212 [00:19<00:37,  3.71it/s] 35%|███▌      | 75/212 [00:19<00:35,  3.84it/s] 36%|███▌      | 76/212 [00:19<00:34,  3.93it/s] 36%|███▋      | 77/212 [00:20<00:33,  3.99it/s] 37%|███▋      | 78/212 [00:20<00:34,  3.89it/s] 37%|███▋      | 79/212 [00:20<00:34,  3.85it/s] 38%|███▊      | 80/212 [00:21<00:34,  3.78it/s] 38%|███▊      | 81/212 [00:21<00:33,  3.89it/s] 39%|███▊      | 82/212 [00:21<00:32,  3.96it/s] 39%|███▉      | 83/212 [00:21<00:32,  4.02it/s] 40%|███▉      | 84/212 [00:21<00:31,  4.06it/s] 40%|████      | 85/212 [00:22<00:31,  4.08it/s] 41%|████      | 86/212 [00:22<00:30,  4.11it/s] 41%|████      | 87/212 [00:22<00:30,  4.12it/s] 42%|████▏     | 88/212 [00:22<00:30,  4.13it/s] 42%|████▏     | 89/212 [00:23<00:29,  4.13it/s] 42%|████▏     | 90/212 [00:23<00:29,  4.13it/s] 43%|████▎     | 91/212 [00:23<00:29,  4.12it/s] 43%|████▎     | 92/212 [00:23<00:29,  4.12it/s] 44%|████▍     | 93/212 [00:24<00:28,  4.13it/s] 44%|████▍     | 94/212 [00:24<00:28,  4.14it/s] 45%|████▍     | 95/212 [00:24<00:28,  4.14it/s] 45%|████▌     | 96/212 [00:24<00:27,  4.15it/s] 46%|████▌     | 97/212 [00:25<00:27,  4.15it/s] 46%|████▌     | 98/212 [00:25<00:27,  4.16it/s] 47%|████▋     | 99/212 [00:25<00:27,  4.16it/s] 47%|████▋     | 100/212 [00:25<00:26,  4.16it/s] 48%|████▊     | 101/212 [00:26<00:26,  4.16it/s] 48%|████▊     | 102/212 [00:26<00:26,  4.16it/s] 49%|████▊     | 103/212 [00:26<00:26,  4.16it/s] 49%|████▉     | 104/212 [00:26<00:25,  4.16it/s] 50%|████▉     | 105/212 [00:27<00:25,  4.16it/s] 50%|█████     | 106/212 [00:27<00:25,  4.16it/s] 50%|█████     | 107/212 [00:27<00:25,  4.16it/s] 51%|█████     | 108/212 [00:27<00:24,  4.17it/s] 51%|█████▏    | 109/212 [00:28<00:24,  4.16it/s] 52%|█████▏    | 110/212 [00:28<00:24,  4.17it/s] 52%|█████▏    | 111/212 [00:28<00:24,  4.17it/s] 53%|█████▎    | 112/212 [00:28<00:24,  4.17it/s] 53%|█████▎    | 113/212 [00:28<00:23,  4.16it/s] 54%|█████▍    | 114/212 [00:29<00:23,  4.16it/s] 54%|█████▍    | 115/212 [00:29<00:23,  4.16it/s] 55%|█████▍    | 116/212 [00:29<00:23,  4.15it/s] 55%|█████▌    | 117/212 [00:29<00:23,  4.12it/s] 56%|█████▌    | 118/212 [00:30<00:23,  3.98it/s] 56%|█████▌    | 119/212 [00:30<00:23,  3.90it/s] 57%|█████▋    | 120/212 [00:30<00:23,  3.87it/s] 57%|█████▋    | 121/212 [00:30<00:23,  3.90it/s] 58%|█████▊    | 122/212 [00:31<00:23,  3.78it/s] 58%|█████▊    | 123/212 [00:31<00:23,  3.80it/s] 58%|█████▊    | 124/212 [00:31<00:23,  3.73it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.78it/s] 59%|█████▉    | 126/212 [00:32<00:23,  3.71it/s] 60%|█████▉    | 127/212 [00:32<00:22,  3.76it/s] 60%|██████    | 128/212 [00:32<00:22,  3.71it/s] 61%|██████    | 129/212 [00:33<00:22,  3.76it/s] 61%|██████▏   | 130/212 [00:33<00:22,  3.70it/s] 62%|██████▏   | 131/212 [00:33<00:21,  3.74it/s] 62%|██████▏   | 132/212 [00:33<00:21,  3.71it/s] 63%|██████▎   | 133/212 [00:34<00:20,  3.76it/s] 63%|██████▎   | 134/212 [00:34<00:21,  3.70it/s] 64%|██████▎   | 135/212 [00:34<00:20,  3.73it/s] 64%|██████▍   | 136/212 [00:35<00:20,  3.69it/s] 65%|██████▍   | 137/212 [00:35<00:19,  3.75it/s] 65%|██████▌   | 138/212 [00:35<00:20,  3.68it/s] 66%|██████▌   | 139/212 [00:35<00:19,  3.73it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.69it/s] 67%|██████▋   | 141/212 [00:36<00:18,  3.74it/s] 67%|██████▋   | 142/212 [00:36<00:19,  3.68it/s] 67%|██████▋   | 143/212 [00:36<00:18,  3.72it/s] 68%|██████▊   | 144/212 [00:37<00:18,  3.67it/s] 68%|██████▊   | 145/212 [00:37<00:17,  3.76it/s] 69%|██████▉   | 146/212 [00:37<00:17,  3.70it/s] 69%|██████▉   | 147/212 [00:37<00:17,  3.75it/s] 70%|██████▉   | 148/212 [00:38<00:17,  3.69it/s] 70%|███████   | 149/212 [00:38<00:16,  3.76it/s] 71%|███████   | 150/212 [00:38<00:16,  3.71it/s] 71%|███████   | 151/212 [00:39<00:16,  3.75it/s] 72%|███████▏  | 152/212 [00:39<00:16,  3.70it/s] 72%|███████▏  | 153/212 [00:39<00:15,  3.76it/s] 73%|███████▎  | 154/212 [00:39<00:15,  3.69it/s] 73%|███████▎  | 155/212 [00:40<00:15,  3.73it/s] 74%|███████▎  | 156/212 [00:40<00:15,  3.68it/s] 74%|███████▍  | 157/212 [00:40<00:14,  3.75it/s] 75%|███████▍  | 158/212 [00:40<00:14,  3.68it/s] 75%|███████▌  | 159/212 [00:41<00:14,  3.73it/s] 75%|███████▌  | 160/212 [00:41<00:14,  3.68it/s] 76%|███████▌  | 161/212 [00:41<00:13,  3.75it/s] 76%|███████▋  | 162/212 [00:42<00:13,  3.68it/s] 77%|███████▋  | 163/212 [00:42<00:13,  3.72it/s] 77%|███████▋  | 164/212 [00:42<00:13,  3.68it/s] 78%|███████▊  | 165/212 [00:42<00:12,  3.74it/s] 78%|███████▊  | 166/212 [00:43<00:12,  3.68it/s] 79%|███████▉  | 167/212 [00:43<00:12,  3.72it/s] 79%|███████▉  | 168/212 [00:43<00:11,  3.67it/s] 80%|███████▉  | 169/212 [00:43<00:11,  3.73it/s] 80%|████████  | 170/212 [00:44<00:11,  3.67it/s] 81%|████████  | 171/212 [00:44<00:11,  3.72it/s] 81%|████████  | 172/212 [00:44<00:10,  3.67it/s] 82%|████████▏ | 173/212 [00:44<00:10,  3.75it/s] 82%|████████▏ | 174/212 [00:45<00:10,  3.68it/s] 83%|████████▎ | 175/212 [00:45<00:09,  3.72it/s] 83%|████████▎ | 176/212 [00:45<00:09,  3.68it/s] 83%|████████▎ | 177/212 [00:46<00:09,  3.75it/s] 84%|████████▍ | 178/212 [00:46<00:09,  3.68it/s] 84%|████████▍ | 179/212 [00:46<00:08,  3.73it/s] 85%|████████▍ | 180/212 [00:46<00:08,  3.67it/s] 85%|████████▌ | 181/212 [00:47<00:08,  3.74it/s] 86%|████████▌ | 182/212 [00:47<00:08,  3.68it/s] 86%|████████▋ | 183/212 [00:47<00:07,  3.72it/s] 87%|████████▋ | 184/212 [00:47<00:07,  3.67it/s] 87%|████████▋ | 185/212 [00:48<00:07,  3.75it/s] 88%|████████▊ | 186/212 [00:48<00:07,  3.68it/s] 88%|████████▊ | 187/212 [00:48<00:06,  3.72it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.67it/s] 89%|████████▉ | 189/212 [00:49<00:06,  3.74it/s] 90%|████████▉ | 190/212 [00:49<00:05,  3.68it/s] 90%|█████████ | 191/212 [00:49<00:05,  3.72it/s] 91%|█████████ | 192/212 [00:50<00:05,  3.67it/s] 91%|█████████ | 193/212 [00:50<00:05,  3.75it/s] 92%|█████████▏| 194/212 [00:50<00:04,  3.68it/s] 92%|█████████▏| 195/212 [00:50<00:04,  3.72it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.67it/s] 93%|█████████▎| 197/212 [00:51<00:04,  3.73it/s] 93%|█████████▎| 198/212 [00:51<00:03,  3.68it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.72it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.67it/s] 95%|█████████▍| 201/212 [00:52<00:02,  3.75it/s] 95%|█████████▌| 202/212 [00:52<00:02,  3.68it/s] 96%|█████████▌| 203/212 [00:53<00:02,  3.72it/s] 96%|█████████▌| 204/212 [00:53<00:02,  3.67it/s] 97%|█████████▋| 205/212 [00:53<00:01,  3.74it/s] 97%|█████████▋| 206/212 [00:53<00:01,  3.67it/s] 98%|█████████▊| 207/212 [00:54<00:01,  3.72it/s] 98%|█████████▊| 208/212 [00:54<00:01,  3.67it/s] 99%|█████████▊| 209/212 [00:54<00:00,  3.74it/s] 99%|█████████▉| 210/212 [00:54<00:00,  3.68it/s]100%|█████████▉| 211/212 [00:55<00:00,  3.72it/s]100%|██████████| 212/212 [00:55<00:00,  3.68it/s]accuracy:  0.6650943396226415
100%|██████████| 212/212 [00:58<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 135
Training dataset size: 288, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:41:24,776] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 05:41:24,839] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:41:25,136] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<01:52,  2.40s/it]  4%|▍         | 2/48 [00:05<02:00,  2.63s/it]  6%|▋         | 3/48 [00:07<01:55,  2.57s/it]  8%|▊         | 4/48 [00:09<01:47,  2.44s/it] 10%|█         | 5/48 [00:12<01:49,  2.55s/it] 12%|█▎        | 6/48 [00:15<01:46,  2.54s/it] 15%|█▍        | 7/48 [00:17<01:41,  2.47s/it] 17%|█▋        | 8/48 [00:20<01:41,  2.55s/it] 19%|█▉        | 9/48 [00:22<01:39,  2.54s/it] 21%|██        | 10/48 [00:25<01:34,  2.48s/it]                                               {'loss': 0.5123, 'grad_norm': 9.680031776428223, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:25<01:34,  2.48s/it] 23%|██▎       | 11/48 [00:27<01:34,  2.55s/it] 25%|██▌       | 12/48 [00:30<01:32,  2.56s/it] 27%|██▋       | 13/48 [00:32<01:27,  2.51s/it] 29%|██▉       | 14/48 [00:35<01:24,  2.49s/it] 31%|███▏      | 15/48 [00:37<01:19,  2.39s/it] 33%|███▎      | 16/48 [00:39<01:16,  2.41s/it] 35%|███▌      | 17/48 [00:42<01:14,  2.41s/it] 38%|███▊      | 18/48 [00:45<01:15,  2.52s/it] 40%|███▉      | 19/48 [00:47<01:14,  2.55s/it] 42%|████▏     | 20/48 [00:50<01:10,  2.53s/it]                                               {'loss': 0.4706, 'grad_norm': 0.6506470441818237, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:50<01:10,  2.53s/it] 44%|████▍     | 21/48 [00:52<01:06,  2.48s/it] 46%|████▌     | 22/48 [00:55<01:04,  2.49s/it] 48%|████▊     | 23/48 [00:57<00:58,  2.35s/it] 50%|█████     | 24/48 [00:59<00:55,  2.31s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:01<00:54,  2.35s/it] 54%|█████▍    | 26/48 [01:04<00:53,  2.42s/it] 56%|█████▋    | 27/48 [01:06<00:51,  2.44s/it] 58%|█████▊    | 28/48 [01:09<00:49,  2.47s/it] 60%|██████    | 29/48 [01:11<00:42,  2.25s/it] 62%|██████▎   | 30/48 [01:13<00:39,  2.22s/it]                                               {'loss': 0.4254, 'grad_norm': 0.25240546464920044, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:13<00:39,  2.22s/it] 65%|██████▍   | 31/48 [01:15<00:37,  2.21s/it] 67%|██████▋   | 32/48 [01:17<00:34,  2.16s/it] 69%|██████▉   | 33/48 [01:19<00:32,  2.18s/it] 71%|███████   | 34/48 [01:22<00:32,  2.32s/it] 73%|███████▎  | 35/48 [01:24<00:30,  2.32s/it] 75%|███████▌  | 36/48 [01:27<00:28,  2.41s/it] 77%|███████▋  | 37/48 [01:29<00:24,  2.22s/it] 79%|███████▉  | 38/48 [01:31<00:22,  2.20s/it] 81%|████████▏ | 39/48 [01:33<00:19,  2.22s/it] 83%|████████▎ | 40/48 [01:35<00:17,  2.24s/it]                                               {'loss': 0.4063, 'grad_norm': 7.1700921058654785, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:35<00:17,  2.24s/it] 85%|████████▌ | 41/48 [01:37<00:14,  2.14s/it] 88%|████████▊ | 42/48 [01:39<00:13,  2.19s/it] 90%|████████▉ | 43/48 [01:41<00:10,  2.10s/it] 92%|█████████▏| 44/48 [01:44<00:08,  2.24s/it] 94%|█████████▍| 45/48 [01:47<00:07,  2.44s/it] 96%|█████████▌| 46/48 [01:49<00:04,  2.42s/it] 98%|█████████▊| 47/48 [01:52<00:02,  2.40s/it]100%|██████████| 48/48 [01:54<00:00,  2.33s/it]                                               {'train_runtime': 115.0425, 'train_samples_per_second': 5.007, 'train_steps_per_second': 0.417, 'train_loss': 0.48929067452748615, 'epoch': 2.0}
100%|██████████| 48/48 [01:54<00:00,  2.33s/it]100%|██████████| 48/48 [01:54<00:00,  2.39s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:55,  2.92it/s]  1%|          | 2/163 [00:00<00:48,  3.33it/s]  2%|▏         | 3/163 [00:00<00:46,  3.47it/s]  2%|▏         | 4/163 [00:01<00:42,  3.71it/s]  3%|▎         | 5/163 [00:01<00:42,  3.69it/s]  4%|▎         | 6/163 [00:01<00:42,  3.69it/s]  4%|▍         | 7/163 [00:01<00:42,  3.67it/s]  5%|▍         | 8/163 [00:02<00:40,  3.81it/s]  6%|▌         | 9/163 [00:02<00:40,  3.77it/s]  6%|▌         | 10/163 [00:02<00:40,  3.74it/s]  7%|▋         | 11/163 [00:03<00:41,  3.71it/s]  7%|▋         | 12/163 [00:03<00:39,  3.80it/s]  8%|▊         | 13/163 [00:03<00:39,  3.75it/s]  9%|▊         | 14/163 [00:03<00:39,  3.81it/s]  9%|▉         | 15/163 [00:04<00:39,  3.74it/s] 10%|▉         | 16/163 [00:04<00:38,  3.79it/s] 10%|█         | 17/163 [00:04<00:38,  3.75it/s] 11%|█         | 18/163 [00:04<00:38,  3.81it/s] 12%|█▏        | 19/163 [00:05<00:38,  3.73it/s] 12%|█▏        | 20/163 [00:05<00:37,  3.78it/s] 13%|█▎        | 21/163 [00:05<00:38,  3.74it/s] 13%|█▎        | 22/163 [00:05<00:37,  3.79it/s] 14%|█▍        | 23/163 [00:06<00:37,  3.73it/s] 15%|█▍        | 24/163 [00:06<00:37,  3.75it/s] 15%|█▌        | 25/163 [00:06<00:37,  3.72it/s] 16%|█▌        | 26/163 [00:06<00:36,  3.78it/s] 17%|█▋        | 27/163 [00:07<00:36,  3.73it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.78it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.74it/s] 18%|█▊        | 30/163 [00:08<00:35,  3.79it/s] 19%|█▉        | 31/163 [00:08<00:35,  3.72it/s] 20%|█▉        | 32/163 [00:08<00:34,  3.76it/s] 20%|██        | 33/163 [00:08<00:34,  3.72it/s] 21%|██        | 34/163 [00:09<00:33,  3.80it/s] 21%|██▏       | 35/163 [00:09<00:34,  3.73it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.78it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.73it/s] 23%|██▎       | 38/163 [00:10<00:32,  3.79it/s] 24%|██▍       | 39/163 [00:10<00:33,  3.74it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.79it/s] 25%|██▌       | 41/163 [00:10<00:32,  3.74it/s] 26%|██▌       | 42/163 [00:11<00:31,  3.80it/s] 26%|██▋       | 43/163 [00:11<00:32,  3.73it/s] 27%|██▋       | 44/163 [00:11<00:31,  3.78it/s] 28%|██▊       | 45/163 [00:12<00:31,  3.74it/s] 28%|██▊       | 46/163 [00:12<00:30,  3.79it/s] 29%|██▉       | 47/163 [00:12<00:31,  3.73it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.77it/s] 30%|███       | 49/163 [00:13<00:30,  3.73it/s] 31%|███       | 50/163 [00:13<00:29,  3.79it/s] 31%|███▏      | 51/163 [00:13<00:30,  3.72it/s] 32%|███▏      | 52/163 [00:13<00:29,  3.77it/s] 33%|███▎      | 53/163 [00:14<00:29,  3.72it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.78it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.73it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.79it/s] 35%|███▍      | 57/163 [00:15<00:28,  3.73it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.78it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.72it/s] 37%|███▋      | 60/163 [00:16<00:27,  3.75it/s] 37%|███▋      | 61/163 [00:16<00:27,  3.72it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.79it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.72it/s] 39%|███▉      | 64/163 [00:17<00:26,  3.72it/s] 40%|███▉      | 65/163 [00:17<00:26,  3.70it/s] 40%|████      | 66/163 [00:17<00:26,  3.67it/s] 41%|████      | 67/163 [00:17<00:25,  3.80it/s] 42%|████▏     | 68/163 [00:18<00:25,  3.75it/s] 42%|████▏     | 69/163 [00:18<00:25,  3.71it/s] 43%|████▎     | 70/163 [00:18<00:25,  3.67it/s] 44%|████▎     | 71/163 [00:18<00:24,  3.78it/s] 44%|████▍     | 72/163 [00:19<00:24,  3.74it/s] 45%|████▍     | 73/163 [00:19<00:24,  3.72it/s] 45%|████▌     | 74/163 [00:19<00:24,  3.68it/s] 46%|████▌     | 75/163 [00:20<00:23,  3.81it/s] 47%|████▋     | 76/163 [00:20<00:23,  3.76it/s] 47%|████▋     | 77/163 [00:20<00:23,  3.72it/s] 48%|████▊     | 78/163 [00:20<00:23,  3.68it/s] 48%|████▊     | 79/163 [00:21<00:22,  3.79it/s] 49%|████▉     | 80/163 [00:21<00:22,  3.75it/s] 50%|████▉     | 81/163 [00:21<00:22,  3.72it/s] 50%|█████     | 82/163 [00:21<00:21,  3.69it/s] 51%|█████     | 83/163 [00:22<00:20,  3.81it/s] 52%|█████▏    | 84/163 [00:22<00:20,  3.76it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.72it/s] 53%|█████▎    | 86/163 [00:23<00:20,  3.68it/s] 53%|█████▎    | 87/163 [00:23<00:20,  3.80it/s] 54%|█████▍    | 88/163 [00:23<00:20,  3.75it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.73it/s] 55%|█████▌    | 90/163 [00:24<00:19,  3.69it/s] 56%|█████▌    | 91/163 [00:24<00:18,  3.81it/s] 56%|█████▋    | 92/163 [00:24<00:18,  3.75it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.71it/s] 58%|█████▊    | 94/163 [00:25<00:18,  3.67it/s] 58%|█████▊    | 95/163 [00:25<00:18,  3.75it/s] 59%|█████▉    | 96/163 [00:25<00:18,  3.71it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.77it/s] 60%|██████    | 98/163 [00:26<00:17,  3.70it/s] 61%|██████    | 99/163 [00:26<00:17,  3.75it/s] 61%|██████▏   | 100/163 [00:26<00:17,  3.71it/s] 62%|██████▏   | 101/163 [00:27<00:16,  3.72it/s] 63%|██████▎   | 102/163 [00:27<00:16,  3.71it/s] 63%|██████▎   | 103/163 [00:27<00:16,  3.74it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.70it/s] 64%|██████▍   | 105/163 [00:28<00:15,  3.76it/s] 65%|██████▌   | 106/163 [00:28<00:15,  3.70it/s] 66%|██████▌   | 107/163 [00:28<00:14,  3.75it/s] 66%|██████▋   | 108/163 [00:28<00:14,  3.72it/s] 67%|██████▋   | 109/163 [00:29<00:14,  3.73it/s] 67%|██████▋   | 110/163 [00:29<00:14,  3.72it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.75it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.70it/s] 69%|██████▉   | 113/163 [00:30<00:13,  3.72it/s] 70%|██████▉   | 114/163 [00:30<00:13,  3.71it/s] 71%|███████   | 115/163 [00:30<00:12,  3.74it/s] 71%|███████   | 116/163 [00:31<00:12,  3.70it/s] 72%|███████▏  | 117/163 [00:31<00:12,  3.71it/s] 72%|███████▏  | 118/163 [00:31<00:12,  3.70it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.71it/s] 74%|███████▎  | 120/163 [00:32<00:11,  3.69it/s] 74%|███████▍  | 121/163 [00:32<00:11,  3.72it/s] 75%|███████▍  | 122/163 [00:32<00:11,  3.70it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.74it/s] 76%|███████▌  | 124/163 [00:33<00:10,  3.69it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.69it/s] 77%|███████▋  | 126/163 [00:33<00:09,  3.71it/s] 78%|███████▊  | 127/163 [00:34<00:09,  3.69it/s] 79%|███████▊  | 128/163 [00:34<00:09,  3.68it/s] 79%|███████▉  | 129/163 [00:34<00:09,  3.65it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.78it/s] 80%|████████  | 131/163 [00:35<00:08,  3.73it/s] 81%|████████  | 132/163 [00:35<00:08,  3.70it/s] 82%|████████▏ | 133/163 [00:35<00:08,  3.65it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.77it/s] 83%|████████▎ | 135/163 [00:36<00:07,  3.72it/s] 83%|████████▎ | 136/163 [00:36<00:07,  3.70it/s] 84%|████████▍ | 137/163 [00:36<00:07,  3.66it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.77it/s] 85%|████████▌ | 139/163 [00:37<00:06,  3.72it/s] 86%|████████▌ | 140/163 [00:37<00:06,  3.70it/s] 87%|████████▋ | 141/163 [00:37<00:06,  3.66it/s] 87%|████████▋ | 142/163 [00:38<00:05,  3.75it/s] 88%|████████▊ | 143/163 [00:38<00:05,  3.70it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.67it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.72it/s] 90%|████████▉ | 146/163 [00:39<00:04,  3.75it/s] 90%|█████████ | 147/163 [00:39<00:04,  3.70it/s] 91%|█████████ | 148/163 [00:39<00:04,  3.63it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.73it/s] 92%|█████████▏| 150/163 [00:40<00:03,  3.73it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.71it/s] 93%|█████████▎| 152/163 [00:40<00:03,  3.64it/s] 94%|█████████▍| 153/163 [00:41<00:02,  3.77it/s] 94%|█████████▍| 154/163 [00:41<00:02,  3.73it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.70it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.67it/s] 96%|█████████▋| 157/163 [00:42<00:01,  3.79it/s] 97%|█████████▋| 158/163 [00:42<00:01,  3.73it/s] 98%|█████████▊| 159/163 [00:42<00:01,  3.71it/s] 98%|█████████▊| 160/163 [00:42<00:00,  3.66it/s] 99%|█████████▉| 161/163 [00:43<00:00,  3.73it/s] 99%|█████████▉| 162/163 [00:43<00:00,  3.68it/s]100%|██████████| 163/163 [00:43<00:00,  3.73it/s]accuracy:  0.8773006134969326
100%|██████████| 163/163 [00:46<00:00,  3.52it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 198
Training dataset size: 288, validation dataset size: 198
Training dataset size: 288, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:44:33,783] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:44:34,021] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 05:44:34,288] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<02:18,  2.94s/it]  4%|▍         | 2/48 [00:05<02:04,  2.71s/it]  6%|▋         | 3/48 [00:07<01:52,  2.50s/it]  8%|▊         | 4/48 [00:10<01:48,  2.46s/it] 10%|█         | 5/48 [00:12<01:44,  2.42s/it] 12%|█▎        | 6/48 [00:15<01:43,  2.46s/it] 15%|█▍        | 7/48 [00:17<01:41,  2.47s/it] 17%|█▋        | 8/48 [00:20<01:41,  2.55s/it] 19%|█▉        | 9/48 [00:22<01:39,  2.54s/it] 21%|██        | 10/48 [00:25<01:37,  2.56s/it]                                               {'loss': 0.6725, 'grad_norm': 5.1657938957214355, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:25<01:37,  2.56s/it] 23%|██▎       | 11/48 [00:28<01:39,  2.69s/it] 25%|██▌       | 12/48 [00:30<01:34,  2.63s/it] 27%|██▋       | 13/48 [00:33<01:32,  2.64s/it] 29%|██▉       | 14/48 [00:35<01:27,  2.57s/it] 31%|███▏      | 15/48 [00:38<01:24,  2.56s/it] 33%|███▎      | 16/48 [00:40<01:19,  2.49s/it] 35%|███▌      | 17/48 [00:43<01:17,  2.51s/it] 38%|███▊      | 18/48 [00:45<01:15,  2.53s/it] 40%|███▉      | 19/48 [00:48<01:13,  2.52s/it] 42%|████▏     | 20/48 [00:50<01:10,  2.51s/it]                                               {'loss': 0.6258, 'grad_norm': 0.2297743409872055, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:50<01:10,  2.51s/it] 44%|████▍     | 21/48 [00:53<01:08,  2.52s/it] 46%|████▌     | 22/48 [00:56<01:07,  2.61s/it] 48%|████▊     | 23/48 [00:58<01:04,  2.58s/it] 50%|█████     | 24/48 [01:00<00:59,  2.47s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:04<01:00,  2.65s/it] 54%|█████▍    | 26/48 [01:06<00:55,  2.53s/it] 56%|█████▋    | 27/48 [01:08<00:53,  2.57s/it] 58%|█████▊    | 28/48 [01:11<00:48,  2.45s/it] 60%|██████    | 29/48 [01:14<00:49,  2.59s/it] 62%|██████▎   | 30/48 [01:16<00:45,  2.51s/it]                                               {'loss': 0.4988, 'grad_norm': 8.481624603271484, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:16<00:45,  2.51s/it] 65%|██████▍   | 31/48 [01:18<00:41,  2.42s/it] 67%|██████▋   | 32/48 [01:20<00:38,  2.41s/it] 69%|██████▉   | 33/48 [01:23<00:37,  2.48s/it] 71%|███████   | 34/48 [01:25<00:33,  2.42s/it] 73%|███████▎  | 35/48 [01:28<00:31,  2.40s/it] 75%|███████▌  | 36/48 [01:30<00:29,  2.47s/it] 77%|███████▋  | 37/48 [01:33<00:27,  2.52s/it] 79%|███████▉  | 38/48 [01:36<00:25,  2.56s/it] 81%|████████▏ | 39/48 [01:38<00:22,  2.47s/it] 83%|████████▎ | 40/48 [01:40<00:19,  2.43s/it]                                               {'loss': 0.4211, 'grad_norm': 0.8493133187294006, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:40<00:19,  2.43s/it] 85%|████████▌ | 41/48 [01:43<00:16,  2.40s/it] 88%|████████▊ | 42/48 [01:45<00:14,  2.48s/it] 90%|████████▉ | 43/48 [01:48<00:12,  2.59s/it] 92%|█████████▏| 44/48 [01:50<00:10,  2.53s/it] 94%|█████████▍| 45/48 [01:53<00:07,  2.46s/it] 96%|█████████▌| 46/48 [01:56<00:05,  2.58s/it] 98%|█████████▊| 47/48 [01:58<00:02,  2.51s/it]100%|██████████| 48/48 [02:01<00:00,  2.62s/it]                                               {'train_runtime': 121.9893, 'train_samples_per_second': 4.722, 'train_steps_per_second': 0.393, 'train_loss': 0.5481436550617218, 'epoch': 2.0}
100%|██████████| 48/48 [02:01<00:00,  2.62s/it]100%|██████████| 48/48 [02:01<00:00,  2.54s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:19,  2.91it/s]  1%|          | 2/232 [00:00<01:07,  3.42it/s]  1%|▏         | 3/232 [00:00<01:05,  3.50it/s]  2%|▏         | 4/232 [00:01<01:02,  3.66it/s]  2%|▏         | 5/232 [00:01<01:02,  3.65it/s]  3%|▎         | 6/232 [00:01<00:59,  3.77it/s]  3%|▎         | 7/232 [00:01<01:00,  3.74it/s]  3%|▎         | 8/232 [00:02<01:00,  3.73it/s]  4%|▍         | 9/232 [00:02<01:00,  3.68it/s]  4%|▍         | 10/232 [00:02<00:58,  3.82it/s]  5%|▍         | 11/232 [00:02<00:58,  3.78it/s]  5%|▌         | 12/232 [00:03<00:58,  3.76it/s]  6%|▌         | 13/232 [00:03<00:57,  3.81it/s]  6%|▌         | 14/232 [00:03<00:57,  3.77it/s]  6%|▋         | 15/232 [00:04<00:56,  3.81it/s]  7%|▋         | 16/232 [00:04<00:57,  3.76it/s]  7%|▋         | 17/232 [00:04<00:56,  3.81it/s]  8%|▊         | 18/232 [00:04<00:57,  3.75it/s]  8%|▊         | 19/232 [00:05<00:55,  3.82it/s]  9%|▊         | 20/232 [00:05<00:56,  3.76it/s]  9%|▉         | 21/232 [00:05<00:55,  3.80it/s]  9%|▉         | 22/232 [00:05<00:56,  3.74it/s] 10%|▉         | 23/232 [00:06<00:54,  3.81it/s] 10%|█         | 24/232 [00:06<00:55,  3.76it/s] 11%|█         | 25/232 [00:06<00:54,  3.80it/s] 11%|█         | 26/232 [00:06<00:55,  3.74it/s] 12%|█▏        | 27/232 [00:07<00:53,  3.81it/s] 12%|█▏        | 28/232 [00:07<00:54,  3.76it/s] 12%|█▎        | 29/232 [00:07<00:53,  3.79it/s] 13%|█▎        | 30/232 [00:08<00:54,  3.73it/s] 13%|█▎        | 31/232 [00:08<00:52,  3.80it/s] 14%|█▍        | 32/232 [00:08<00:53,  3.74it/s] 14%|█▍        | 33/232 [00:08<00:52,  3.79it/s] 15%|█▍        | 34/232 [00:09<00:52,  3.74it/s] 15%|█▌        | 35/232 [00:09<00:51,  3.81it/s] 16%|█▌        | 36/232 [00:09<00:52,  3.75it/s] 16%|█▌        | 37/232 [00:09<00:51,  3.78it/s] 16%|█▋        | 38/232 [00:10<00:52,  3.73it/s] 17%|█▋        | 39/232 [00:10<00:50,  3.81it/s] 17%|█▋        | 40/232 [00:10<00:51,  3.74it/s] 18%|█▊        | 41/232 [00:10<00:50,  3.79it/s] 18%|█▊        | 42/232 [00:11<00:50,  3.73it/s] 19%|█▊        | 43/232 [00:11<00:49,  3.82it/s] 19%|█▉        | 44/232 [00:11<00:50,  3.75it/s] 19%|█▉        | 45/232 [00:11<00:49,  3.80it/s] 20%|█▉        | 46/232 [00:12<00:49,  3.74it/s] 20%|██        | 47/232 [00:12<00:48,  3.82it/s] 21%|██        | 48/232 [00:12<00:48,  3.77it/s] 21%|██        | 49/232 [00:13<00:48,  3.76it/s] 22%|██▏       | 50/232 [00:13<00:49,  3.69it/s] 22%|██▏       | 51/232 [00:13<00:47,  3.82it/s] 22%|██▏       | 52/232 [00:13<00:47,  3.77it/s] 23%|██▎       | 53/232 [00:14<00:47,  3.73it/s] 23%|██▎       | 54/232 [00:14<00:48,  3.68it/s] 24%|██▎       | 55/232 [00:14<00:46,  3.81it/s] 24%|██▍       | 56/232 [00:14<00:46,  3.81it/s] 25%|██▍       | 57/232 [00:15<00:46,  3.75it/s] 25%|██▌       | 58/232 [00:15<00:46,  3.77it/s] 25%|██▌       | 59/232 [00:15<00:46,  3.75it/s] 26%|██▌       | 60/232 [00:15<00:45,  3.79it/s] 26%|██▋       | 61/232 [00:16<00:45,  3.72it/s] 27%|██▋       | 62/232 [00:16<00:45,  3.76it/s] 27%|██▋       | 63/232 [00:16<00:45,  3.71it/s] 28%|██▊       | 64/232 [00:17<00:44,  3.76it/s] 28%|██▊       | 65/232 [00:17<00:44,  3.72it/s] 28%|██▊       | 66/232 [00:17<00:44,  3.76it/s] 29%|██▉       | 67/232 [00:17<00:44,  3.72it/s] 29%|██▉       | 68/232 [00:18<00:43,  3.79it/s] 30%|██▉       | 69/232 [00:18<00:43,  3.73it/s] 30%|███       | 70/232 [00:18<00:42,  3.78it/s] 31%|███       | 71/232 [00:18<00:43,  3.72it/s] 31%|███       | 72/232 [00:19<00:42,  3.79it/s] 31%|███▏      | 73/232 [00:19<00:42,  3.73it/s] 32%|███▏      | 74/232 [00:19<00:42,  3.76it/s] 32%|███▏      | 75/232 [00:19<00:42,  3.71it/s] 33%|███▎      | 76/232 [00:20<00:41,  3.80it/s] 33%|███▎      | 77/232 [00:20<00:41,  3.74it/s] 34%|███▎      | 78/232 [00:20<00:41,  3.73it/s] 34%|███▍      | 79/232 [00:21<00:41,  3.67it/s] 34%|███▍      | 80/232 [00:21<00:40,  3.80it/s] 35%|███▍      | 81/232 [00:21<00:40,  3.74it/s] 35%|███▌      | 82/232 [00:21<00:40,  3.70it/s] 36%|███▌      | 83/232 [00:22<00:40,  3.66it/s] 36%|███▌      | 84/232 [00:22<00:39,  3.78it/s] 37%|███▋      | 85/232 [00:22<00:39,  3.74it/s] 37%|███▋      | 86/232 [00:22<00:39,  3.71it/s] 38%|███▊      | 87/232 [00:23<00:39,  3.66it/s] 38%|███▊      | 88/232 [00:23<00:37,  3.80it/s] 38%|███▊      | 89/232 [00:23<00:37,  3.80it/s] 39%|███▉      | 90/232 [00:24<00:38,  3.73it/s] 39%|███▉      | 91/232 [00:24<00:37,  3.79it/s] 40%|███▉      | 92/232 [00:24<00:37,  3.74it/s] 40%|████      | 93/232 [00:24<00:36,  3.78it/s] 41%|████      | 94/232 [00:25<00:37,  3.73it/s] 41%|████      | 95/232 [00:25<00:36,  3.76it/s] 41%|████▏     | 96/232 [00:25<00:36,  3.70it/s] 42%|████▏     | 97/232 [00:25<00:35,  3.77it/s] 42%|████▏     | 98/232 [00:26<00:36,  3.72it/s] 43%|████▎     | 99/232 [00:26<00:35,  3.76it/s] 43%|████▎     | 100/232 [00:26<00:35,  3.71it/s] 44%|████▎     | 101/232 [00:26<00:34,  3.77it/s] 44%|████▍     | 102/232 [00:27<00:34,  3.72it/s] 44%|████▍     | 103/232 [00:27<00:34,  3.76it/s] 45%|████▍     | 104/232 [00:27<00:34,  3.70it/s] 45%|████▌     | 105/232 [00:28<00:33,  3.77it/s] 46%|████▌     | 106/232 [00:28<00:33,  3.72it/s] 46%|████▌     | 107/232 [00:28<00:33,  3.76it/s] 47%|████▋     | 108/232 [00:28<00:33,  3.71it/s] 47%|████▋     | 109/232 [00:29<00:32,  3.77it/s] 47%|████▋     | 110/232 [00:29<00:32,  3.71it/s] 48%|████▊     | 111/232 [00:29<00:32,  3.75it/s] 48%|████▊     | 112/232 [00:29<00:32,  3.69it/s] 49%|████▊     | 113/232 [00:30<00:31,  3.76it/s] 49%|████▉     | 114/232 [00:30<00:31,  3.70it/s] 50%|████▉     | 115/232 [00:30<00:31,  3.74it/s] 50%|█████     | 116/232 [00:30<00:31,  3.68it/s] 50%|█████     | 117/232 [00:31<00:30,  3.74it/s] 51%|█████     | 118/232 [00:31<00:30,  3.69it/s] 51%|█████▏    | 119/232 [00:31<00:30,  3.74it/s] 52%|█████▏    | 120/232 [00:32<00:30,  3.68it/s] 52%|█████▏    | 121/232 [00:32<00:29,  3.75it/s] 53%|█████▎    | 122/232 [00:32<00:29,  3.69it/s] 53%|█████▎    | 123/232 [00:32<00:29,  3.73it/s] 53%|█████▎    | 124/232 [00:33<00:29,  3.68it/s] 54%|█████▍    | 125/232 [00:33<00:28,  3.74it/s] 54%|█████▍    | 126/232 [00:33<00:28,  3.68it/s] 55%|█████▍    | 127/232 [00:33<00:28,  3.73it/s] 55%|█████▌    | 128/232 [00:34<00:28,  3.68it/s] 56%|█████▌    | 129/232 [00:34<00:27,  3.74it/s] 56%|█████▌    | 130/232 [00:34<00:27,  3.68it/s] 56%|█████▋    | 131/232 [00:35<00:27,  3.73it/s] 57%|█████▋    | 132/232 [00:35<00:27,  3.68it/s] 57%|█████▋    | 133/232 [00:35<00:26,  3.74it/s] 58%|█████▊    | 134/232 [00:35<00:26,  3.69it/s] 58%|█████▊    | 135/232 [00:36<00:25,  3.74it/s] 59%|█████▊    | 136/232 [00:36<00:26,  3.69it/s] 59%|█████▉    | 137/232 [00:36<00:25,  3.75it/s] 59%|█████▉    | 138/232 [00:36<00:25,  3.70it/s] 60%|█████▉    | 139/232 [00:37<00:24,  3.74it/s] 60%|██████    | 140/232 [00:37<00:24,  3.69it/s] 61%|██████    | 141/232 [00:37<00:24,  3.77it/s] 61%|██████    | 142/232 [00:37<00:24,  3.70it/s] 62%|██████▏   | 143/232 [00:38<00:23,  3.74it/s] 62%|██████▏   | 144/232 [00:38<00:23,  3.69it/s] 62%|██████▎   | 145/232 [00:38<00:23,  3.74it/s] 63%|██████▎   | 146/232 [00:39<00:23,  3.68it/s] 63%|██████▎   | 147/232 [00:39<00:22,  3.73it/s] 64%|██████▍   | 148/232 [00:39<00:22,  3.68it/s] 64%|██████▍   | 149/232 [00:39<00:22,  3.74it/s] 65%|██████▍   | 150/232 [00:40<00:22,  3.68it/s] 65%|██████▌   | 151/232 [00:40<00:21,  3.72it/s] 66%|██████▌   | 152/232 [00:40<00:21,  3.68it/s] 66%|██████▌   | 153/232 [00:40<00:21,  3.74it/s] 66%|██████▋   | 154/232 [00:41<00:21,  3.68it/s] 67%|██████▋   | 155/232 [00:41<00:20,  3.72it/s] 67%|██████▋   | 156/232 [00:41<00:20,  3.68it/s] 68%|██████▊   | 157/232 [00:42<00:20,  3.74it/s] 68%|██████▊   | 158/232 [00:42<00:20,  3.68it/s] 69%|██████▊   | 159/232 [00:42<00:19,  3.72it/s] 69%|██████▉   | 160/232 [00:42<00:19,  3.67it/s] 69%|██████▉   | 161/232 [00:43<00:19,  3.73it/s] 70%|██████▉   | 162/232 [00:43<00:19,  3.68it/s] 70%|███████   | 163/232 [00:43<00:18,  3.72it/s] 71%|███████   | 164/232 [00:43<00:18,  3.68it/s] 71%|███████   | 165/232 [00:44<00:17,  3.73it/s] 72%|███████▏  | 166/232 [00:44<00:17,  3.68it/s] 72%|███████▏  | 167/232 [00:44<00:17,  3.72it/s] 72%|███████▏  | 168/232 [00:44<00:17,  3.68it/s] 73%|███████▎  | 169/232 [00:45<00:16,  3.73it/s] 73%|███████▎  | 170/232 [00:45<00:16,  3.68it/s] 74%|███████▎  | 171/232 [00:45<00:16,  3.72it/s] 74%|███████▍  | 172/232 [00:46<00:16,  3.67it/s] 75%|███████▍  | 173/232 [00:46<00:15,  3.74it/s] 75%|███████▌  | 174/232 [00:46<00:15,  3.68it/s] 75%|███████▌  | 175/232 [00:46<00:15,  3.73it/s] 76%|███████▌  | 176/232 [00:47<00:15,  3.68it/s] 76%|███████▋  | 177/232 [00:47<00:14,  3.75it/s] 77%|███████▋  | 178/232 [00:47<00:14,  3.68it/s] 77%|███████▋  | 179/232 [00:47<00:14,  3.72it/s] 78%|███████▊  | 180/232 [00:48<00:14,  3.67it/s] 78%|███████▊  | 181/232 [00:48<00:13,  3.74it/s] 78%|███████▊  | 182/232 [00:48<00:13,  3.68it/s] 79%|███████▉  | 183/232 [00:49<00:13,  3.72it/s] 79%|███████▉  | 184/232 [00:49<00:13,  3.67it/s] 80%|███████▉  | 185/232 [00:49<00:12,  3.73it/s] 80%|████████  | 186/232 [00:49<00:12,  3.68it/s] 81%|████████  | 187/232 [00:50<00:12,  3.72it/s] 81%|████████  | 188/232 [00:50<00:11,  3.67it/s] 81%|████████▏ | 189/232 [00:50<00:11,  3.73it/s] 82%|████████▏ | 190/232 [00:50<00:11,  3.67it/s] 82%|████████▏ | 191/232 [00:51<00:11,  3.72it/s] 83%|████████▎ | 192/232 [00:51<00:10,  3.68it/s] 83%|████████▎ | 193/232 [00:51<00:10,  3.73it/s] 84%|████████▎ | 194/232 [00:52<00:10,  3.67it/s] 84%|████████▍ | 195/232 [00:52<00:09,  3.73it/s] 84%|████████▍ | 196/232 [00:52<00:09,  3.68it/s] 85%|████████▍ | 197/232 [00:52<00:09,  3.73it/s] 85%|████████▌ | 198/232 [00:53<00:09,  3.67it/s] 86%|████████▌ | 199/232 [00:53<00:08,  3.68it/s] 86%|████████▌ | 200/232 [00:53<00:08,  3.70it/s] 87%|████████▋ | 201/232 [00:53<00:08,  3.73it/s] 87%|████████▋ | 202/232 [00:54<00:08,  3.67it/s] 88%|████████▊ | 203/232 [00:54<00:07,  3.66it/s] 88%|████████▊ | 204/232 [00:54<00:07,  3.71it/s] 88%|████████▊ | 205/232 [00:54<00:07,  3.75it/s] 89%|████████▉ | 206/232 [00:55<00:07,  3.69it/s] 89%|████████▉ | 207/232 [00:55<00:06,  3.68it/s] 90%|████████▉ | 208/232 [00:55<00:06,  3.71it/s] 90%|█████████ | 209/232 [00:56<00:06,  3.75it/s] 91%|█████████ | 210/232 [00:56<00:05,  3.69it/s] 91%|█████████ | 211/232 [00:56<00:05,  3.69it/s] 91%|█████████▏| 212/232 [00:56<00:05,  3.71it/s] 92%|█████████▏| 213/232 [00:57<00:05,  3.75it/s] 92%|█████████▏| 214/232 [00:57<00:04,  3.69it/s] 93%|█████████▎| 215/232 [00:57<00:04,  3.68it/s] 93%|█████████▎| 216/232 [00:57<00:04,  3.70it/s] 94%|█████████▎| 217/232 [00:58<00:04,  3.74it/s] 94%|█████████▍| 218/232 [00:58<00:03,  3.68it/s] 94%|█████████▍| 219/232 [00:58<00:03,  3.73it/s] 95%|█████████▍| 220/232 [00:59<00:03,  3.69it/s] 95%|█████████▌| 221/232 [00:59<00:02,  3.74it/s] 96%|█████████▌| 222/232 [00:59<00:02,  3.68it/s] 96%|█████████▌| 223/232 [00:59<00:02,  3.73it/s] 97%|█████████▋| 224/232 [01:00<00:02,  3.69it/s] 97%|█████████▋| 225/232 [01:00<00:01,  3.73it/s] 97%|█████████▋| 226/232 [01:00<00:01,  3.68it/s] 98%|█████████▊| 227/232 [01:00<00:01,  3.70it/s] 98%|█████████▊| 228/232 [01:01<00:01,  3.69it/s] 99%|█████████▊| 229/232 [01:01<00:00,  3.74it/s] 99%|█████████▉| 230/232 [01:01<00:00,  3.68it/s]100%|█████████▉| 231/232 [01:01<00:00,  3.73it/s]100%|██████████| 232/232 [01:02<00:00,  3.68it/s]accuracy:  0.875
100%|██████████| 232/232 [01:05<00:00,  3.53it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
training start
[2025-03-12 05:48:11,076] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:48:11,176] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2025-03-12 05:48:11,184] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<02:16,  2.90s/it]  4%|▍         | 2/48 [00:05<01:55,  2.51s/it]  6%|▋         | 3/48 [00:07<01:49,  2.44s/it]  8%|▊         | 4/48 [00:10<01:55,  2.63s/it] 10%|█         | 5/48 [00:12<01:47,  2.50s/it] 12%|█▎        | 6/48 [00:14<01:39,  2.36s/it] 15%|█▍        | 7/48 [00:17<01:35,  2.34s/it] 17%|█▋        | 8/48 [00:19<01:35,  2.38s/it] 19%|█▉        | 9/48 [00:21<01:33,  2.38s/it] 21%|██        | 10/48 [00:24<01:34,  2.49s/it]                                               {'loss': 1.2392, 'grad_norm': 7.152047634124756, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:24<01:34,  2.49s/it] 23%|██▎       | 11/48 [00:27<01:31,  2.48s/it] 25%|██▌       | 12/48 [00:29<01:26,  2.40s/it] 27%|██▋       | 13/48 [00:31<01:24,  2.42s/it] 29%|██▉       | 14/48 [00:33<01:19,  2.33s/it] 31%|███▏      | 15/48 [00:36<01:20,  2.43s/it] 33%|███▎      | 16/48 [00:38<01:11,  2.25s/it] 35%|███▌      | 17/48 [00:40<01:07,  2.16s/it] 38%|███▊      | 18/48 [00:42<01:06,  2.22s/it] 40%|███▉      | 19/48 [00:45<01:05,  2.25s/it] 42%|████▏     | 20/48 [00:47<01:02,  2.21s/it]                                               {'loss': 1.0882, 'grad_norm': 7.2324957847595215, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:47<01:02,  2.21s/it] 44%|████▍     | 21/48 [00:49<00:58,  2.18s/it] 46%|████▌     | 22/48 [00:51<01:00,  2.33s/it] 48%|████▊     | 23/48 [00:54<01:00,  2.42s/it] 50%|█████     | 24/48 [00:57<01:01,  2.58s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:00<00:59,  2.61s/it] 54%|█████▍    | 26/48 [01:02<00:53,  2.42s/it] 56%|█████▋    | 27/48 [01:04<00:49,  2.38s/it] 58%|█████▊    | 28/48 [01:06<00:47,  2.36s/it] 60%|██████    | 29/48 [01:09<00:44,  2.33s/it] 62%|██████▎   | 30/48 [01:11<00:41,  2.30s/it]                                               {'loss': 0.8619, 'grad_norm': 5.49153470993042, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:11<00:41,  2.30s/it] 65%|██████▍   | 31/48 [01:13<00:40,  2.36s/it] 67%|██████▋   | 32/48 [01:16<00:38,  2.41s/it] 69%|██████▉   | 33/48 [01:18<00:35,  2.37s/it] 71%|███████   | 34/48 [01:21<00:35,  2.52s/it] 73%|███████▎  | 35/48 [01:23<00:32,  2.49s/it] 75%|███████▌  | 36/48 [01:25<00:28,  2.36s/it] 77%|███████▋  | 37/48 [01:28<00:25,  2.35s/it] 79%|███████▉  | 38/48 [01:30<00:23,  2.37s/it] 81%|████████▏ | 39/48 [01:33<00:23,  2.59s/it] 83%|████████▎ | 40/48 [01:35<00:19,  2.47s/it]                                               {'loss': 0.8369, 'grad_norm': 6.2144455909729, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:35<00:19,  2.47s/it] 85%|████████▌ | 41/48 [01:38<00:17,  2.44s/it] 88%|████████▊ | 42/48 [01:40<00:14,  2.41s/it] 90%|████████▉ | 43/48 [01:42<00:11,  2.38s/it] 92%|█████████▏| 44/48 [01:45<00:09,  2.40s/it] 94%|█████████▍| 45/48 [01:48<00:07,  2.53s/it] 96%|█████████▌| 46/48 [01:50<00:04,  2.40s/it] 98%|█████████▊| 47/48 [01:52<00:02,  2.34s/it]100%|██████████| 48/48 [01:55<00:00,  2.38s/it]                                               {'train_runtime': 115.6851, 'train_samples_per_second': 4.979, 'train_steps_per_second': 0.415, 'train_loss': 0.9846496482690176, 'epoch': 2.0}
100%|██████████| 48/48 [01:55<00:00,  2.38s/it]100%|██████████| 48/48 [01:55<00:00,  2.41s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:35,  2.59it/s]  1%|          | 2/248 [00:00<01:13,  3.33it/s]  1%|          | 3/248 [00:00<01:10,  3.49it/s]  2%|▏         | 4/248 [00:01<01:07,  3.59it/s]  2%|▏         | 5/248 [00:01<01:05,  3.69it/s]  2%|▏         | 6/248 [00:01<01:06,  3.66it/s]  3%|▎         | 7/248 [00:01<01:03,  3.79it/s]  3%|▎         | 8/248 [00:02<01:04,  3.73it/s]  4%|▎         | 9/248 [00:02<01:03,  3.75it/s]  4%|▍         | 10/248 [00:02<01:03,  3.75it/s]  4%|▍         | 11/248 [00:02<01:01,  3.87it/s]  5%|▍         | 12/248 [00:03<00:59,  3.95it/s]  5%|▌         | 13/248 [00:03<00:58,  4.01it/s]  6%|▌         | 14/248 [00:03<00:57,  4.04it/s]  6%|▌         | 15/248 [00:03<00:57,  4.07it/s]  6%|▋         | 16/248 [00:04<00:56,  4.09it/s]  7%|▋         | 17/248 [00:04<00:56,  4.10it/s]  7%|▋         | 18/248 [00:04<00:55,  4.11it/s]  8%|▊         | 19/248 [00:04<00:55,  4.12it/s]  8%|▊         | 20/248 [00:05<00:55,  4.13it/s]  8%|▊         | 21/248 [00:05<00:54,  4.14it/s]  9%|▉         | 22/248 [00:05<00:54,  4.15it/s]  9%|▉         | 23/248 [00:05<00:54,  4.16it/s] 10%|▉         | 24/248 [00:06<00:53,  4.16it/s] 10%|█         | 25/248 [00:06<00:53,  4.16it/s] 10%|█         | 26/248 [00:06<00:53,  4.16it/s] 11%|█         | 27/248 [00:06<00:53,  4.16it/s] 11%|█▏        | 28/248 [00:07<00:52,  4.16it/s] 12%|█▏        | 29/248 [00:07<00:52,  4.16it/s] 12%|█▏        | 30/248 [00:07<00:52,  4.16it/s] 12%|█▎        | 31/248 [00:07<00:52,  4.17it/s] 13%|█▎        | 32/248 [00:08<00:51,  4.16it/s] 13%|█▎        | 33/248 [00:08<00:51,  4.16it/s] 14%|█▎        | 34/248 [00:08<00:51,  4.16it/s] 14%|█▍        | 35/248 [00:08<00:51,  4.17it/s] 15%|█▍        | 36/248 [00:09<00:50,  4.16it/s] 15%|█▍        | 37/248 [00:09<00:50,  4.16it/s] 15%|█▌        | 38/248 [00:09<00:50,  4.17it/s] 16%|█▌        | 39/248 [00:09<00:50,  4.17it/s] 16%|█▌        | 40/248 [00:09<00:49,  4.17it/s] 17%|█▋        | 41/248 [00:10<00:50,  4.07it/s] 17%|█▋        | 42/248 [00:10<00:52,  3.94it/s] 17%|█▋        | 43/248 [00:10<00:52,  3.92it/s] 18%|█▊        | 44/248 [00:11<00:52,  3.90it/s] 18%|█▊        | 45/248 [00:11<00:53,  3.82it/s] 19%|█▊        | 46/248 [00:11<00:53,  3.76it/s] 19%|█▉        | 47/248 [00:11<00:52,  3.81it/s] 19%|█▉        | 48/248 [00:12<00:52,  3.81it/s] 20%|█▉        | 49/248 [00:12<00:53,  3.75it/s] 20%|██        | 50/248 [00:12<00:53,  3.70it/s] 21%|██        | 51/248 [00:12<00:52,  3.77it/s] 21%|██        | 52/248 [00:13<00:51,  3.79it/s] 21%|██▏       | 53/248 [00:13<00:52,  3.74it/s] 22%|██▏       | 54/248 [00:13<00:52,  3.71it/s] 22%|██▏       | 55/248 [00:13<00:51,  3.77it/s] 23%|██▎       | 56/248 [00:14<00:50,  3.79it/s] 23%|██▎       | 57/248 [00:14<00:51,  3.74it/s] 23%|██▎       | 58/248 [00:14<00:51,  3.69it/s] 24%|██▍       | 59/248 [00:15<00:50,  3.77it/s] 24%|██▍       | 60/248 [00:15<00:49,  3.78it/s] 25%|██▍       | 61/248 [00:15<00:49,  3.74it/s] 25%|██▌       | 62/248 [00:15<00:50,  3.69it/s] 25%|██▌       | 63/248 [00:16<00:49,  3.76it/s] 26%|██▌       | 64/248 [00:16<00:48,  3.78it/s] 26%|██▌       | 65/248 [00:16<00:48,  3.74it/s] 27%|██▋       | 66/248 [00:16<00:49,  3.70it/s] 27%|██▋       | 67/248 [00:17<00:48,  3.77it/s] 27%|██▋       | 68/248 [00:17<00:47,  3.78it/s] 28%|██▊       | 69/248 [00:17<00:47,  3.74it/s] 28%|██▊       | 70/248 [00:17<00:48,  3.69it/s] 29%|██▊       | 71/248 [00:18<00:47,  3.76it/s] 29%|██▉       | 72/248 [00:18<00:46,  3.78it/s] 29%|██▉       | 73/248 [00:18<00:46,  3.73it/s] 30%|██▉       | 74/248 [00:19<00:47,  3.69it/s] 30%|███       | 75/248 [00:19<00:45,  3.76it/s] 31%|███       | 76/248 [00:19<00:45,  3.78it/s] 31%|███       | 77/248 [00:19<00:45,  3.73it/s] 31%|███▏      | 78/248 [00:20<00:45,  3.75it/s] 32%|███▏      | 79/248 [00:20<00:43,  3.85it/s] 32%|███▏      | 80/248 [00:20<00:42,  3.94it/s] 33%|███▎      | 81/248 [00:20<00:41,  4.00it/s] 33%|███▎      | 82/248 [00:21<00:41,  4.05it/s] 33%|███▎      | 83/248 [00:21<00:40,  4.08it/s] 34%|███▍      | 84/248 [00:21<00:40,  4.09it/s] 34%|███▍      | 85/248 [00:21<00:39,  4.10it/s] 35%|███▍      | 86/248 [00:22<00:39,  4.10it/s] 35%|███▌      | 87/248 [00:22<00:39,  4.10it/s] 35%|███▌      | 88/248 [00:22<00:38,  4.11it/s] 36%|███▌      | 89/248 [00:22<00:38,  4.12it/s] 36%|███▋      | 90/248 [00:23<00:38,  4.13it/s] 37%|███▋      | 91/248 [00:23<00:38,  4.13it/s] 37%|███▋      | 92/248 [00:23<00:37,  4.13it/s] 38%|███▊      | 93/248 [00:23<00:37,  4.12it/s] 38%|███▊      | 94/248 [00:23<00:37,  4.11it/s] 38%|███▊      | 95/248 [00:24<00:37,  4.11it/s] 39%|███▊      | 96/248 [00:24<00:36,  4.12it/s] 39%|███▉      | 97/248 [00:24<00:36,  4.12it/s] 40%|███▉      | 98/248 [00:24<00:36,  4.13it/s] 40%|███▉      | 99/248 [00:25<00:36,  4.13it/s] 40%|████      | 100/248 [00:25<00:35,  4.13it/s] 41%|████      | 101/248 [00:25<00:35,  4.13it/s] 41%|████      | 102/248 [00:25<00:35,  4.11it/s] 42%|████▏     | 103/248 [00:26<00:35,  4.11it/s] 42%|████▏     | 104/248 [00:26<00:34,  4.12it/s] 42%|████▏     | 105/248 [00:26<00:34,  4.13it/s] 43%|████▎     | 106/248 [00:26<00:34,  4.13it/s] 43%|████▎     | 107/248 [00:27<00:34,  4.14it/s] 44%|████▎     | 108/248 [00:27<00:33,  4.14it/s] 44%|████▍     | 109/248 [00:27<00:33,  4.13it/s] 44%|████▍     | 110/248 [00:27<00:33,  4.12it/s] 45%|████▍     | 111/248 [00:28<00:33,  4.12it/s] 45%|████▌     | 112/248 [00:28<00:33,  4.12it/s] 46%|████▌     | 113/248 [00:28<00:33,  4.05it/s] 46%|████▌     | 114/248 [00:28<00:34,  3.93it/s] 46%|████▋     | 115/248 [00:29<00:34,  3.87it/s] 47%|████▋     | 116/248 [00:29<00:34,  3.88it/s] 47%|████▋     | 117/248 [00:29<00:33,  3.87it/s] 48%|████▊     | 118/248 [00:29<00:34,  3.76it/s] 48%|████▊     | 119/248 [00:30<00:34,  3.75it/s] 48%|████▊     | 120/248 [00:30<00:34,  3.73it/s] 49%|████▉     | 121/248 [00:30<00:33,  3.77it/s] 49%|████▉     | 122/248 [00:31<00:34,  3.70it/s] 50%|████▉     | 123/248 [00:31<00:33,  3.74it/s] 50%|█████     | 124/248 [00:31<00:33,  3.68it/s] 50%|█████     | 125/248 [00:31<00:32,  3.75it/s] 51%|█████     | 126/248 [00:32<00:33,  3.69it/s] 51%|█████     | 127/248 [00:32<00:32,  3.73it/s] 52%|█████▏    | 128/248 [00:32<00:32,  3.68it/s] 52%|█████▏    | 129/248 [00:32<00:31,  3.74it/s] 52%|█████▏    | 130/248 [00:33<00:32,  3.68it/s] 53%|█████▎    | 131/248 [00:33<00:31,  3.72it/s] 53%|█████▎    | 132/248 [00:33<00:31,  3.68it/s] 54%|█████▎    | 133/248 [00:33<00:30,  3.74it/s] 54%|█████▍    | 134/248 [00:34<00:30,  3.68it/s] 54%|█████▍    | 135/248 [00:34<00:30,  3.72it/s] 55%|█████▍    | 136/248 [00:34<00:30,  3.68it/s] 55%|█████▌    | 137/248 [00:35<00:29,  3.74it/s] 56%|█████▌    | 138/248 [00:35<00:29,  3.68it/s] 56%|█████▌    | 139/248 [00:35<00:29,  3.72it/s] 56%|█████▋    | 140/248 [00:35<00:29,  3.67it/s] 57%|█████▋    | 141/248 [00:36<00:28,  3.74it/s] 57%|█████▋    | 142/248 [00:36<00:28,  3.68it/s] 58%|█████▊    | 143/248 [00:36<00:28,  3.72it/s] 58%|█████▊    | 144/248 [00:36<00:28,  3.67it/s] 58%|█████▊    | 145/248 [00:37<00:27,  3.74it/s] 59%|█████▉    | 146/248 [00:37<00:27,  3.68it/s] 59%|█████▉    | 147/248 [00:37<00:27,  3.72it/s] 60%|█████▉    | 148/248 [00:38<00:27,  3.67it/s] 60%|██████    | 149/248 [00:38<00:26,  3.73it/s] 60%|██████    | 150/248 [00:38<00:26,  3.67it/s] 61%|██████    | 151/248 [00:38<00:26,  3.72it/s] 61%|██████▏   | 152/248 [00:39<00:26,  3.67it/s] 62%|██████▏   | 153/248 [00:39<00:25,  3.74it/s] 62%|██████▏   | 154/248 [00:39<00:25,  3.68it/s] 62%|██████▎   | 155/248 [00:39<00:24,  3.73it/s] 63%|██████▎   | 156/248 [00:40<00:25,  3.67it/s] 63%|██████▎   | 157/248 [00:40<00:24,  3.73it/s] 64%|██████▎   | 158/248 [00:40<00:24,  3.68it/s] 64%|██████▍   | 159/248 [00:40<00:23,  3.72it/s] 65%|██████▍   | 160/248 [00:41<00:23,  3.67it/s] 65%|██████▍   | 161/248 [00:41<00:23,  3.75it/s] 65%|██████▌   | 162/248 [00:41<00:23,  3.69it/s] 66%|██████▌   | 163/248 [00:42<00:22,  3.73it/s] 66%|██████▌   | 164/248 [00:42<00:22,  3.68it/s] 67%|██████▋   | 165/248 [00:42<00:22,  3.74it/s] 67%|██████▋   | 166/248 [00:42<00:22,  3.68it/s] 67%|██████▋   | 167/248 [00:43<00:21,  3.73it/s] 68%|██████▊   | 168/248 [00:43<00:21,  3.68it/s] 68%|██████▊   | 169/248 [00:43<00:21,  3.75it/s] 69%|██████▊   | 170/248 [00:43<00:21,  3.68it/s] 69%|██████▉   | 171/248 [00:44<00:20,  3.72it/s] 69%|██████▉   | 172/248 [00:44<00:20,  3.67it/s] 70%|██████▉   | 173/248 [00:44<00:20,  3.74it/s] 70%|███████   | 174/248 [00:45<00:20,  3.68it/s] 71%|███████   | 175/248 [00:45<00:19,  3.72it/s] 71%|███████   | 176/248 [00:45<00:19,  3.67it/s] 71%|███████▏  | 177/248 [00:45<00:19,  3.73it/s] 72%|███████▏  | 178/248 [00:46<00:19,  3.68it/s] 72%|███████▏  | 179/248 [00:46<00:18,  3.72it/s] 73%|███████▎  | 180/248 [00:46<00:18,  3.67it/s] 73%|███████▎  | 181/248 [00:46<00:17,  3.74it/s] 73%|███████▎  | 182/248 [00:47<00:17,  3.68it/s] 74%|███████▍  | 183/248 [00:47<00:17,  3.73it/s] 74%|███████▍  | 184/248 [00:47<00:17,  3.68it/s] 75%|███████▍  | 185/248 [00:48<00:16,  3.74it/s] 75%|███████▌  | 186/248 [00:48<00:16,  3.68it/s] 75%|███████▌  | 187/248 [00:48<00:16,  3.73it/s] 76%|███████▌  | 188/248 [00:48<00:16,  3.68it/s] 76%|███████▌  | 189/248 [00:49<00:15,  3.74it/s] 77%|███████▋  | 190/248 [00:49<00:15,  3.68it/s] 77%|███████▋  | 191/248 [00:49<00:15,  3.72it/s] 77%|███████▋  | 192/248 [00:49<00:15,  3.68it/s] 78%|███████▊  | 193/248 [00:50<00:14,  3.73it/s] 78%|███████▊  | 194/248 [00:50<00:14,  3.67it/s] 79%|███████▊  | 195/248 [00:50<00:14,  3.72it/s] 79%|███████▉  | 196/248 [00:50<00:14,  3.67it/s] 79%|███████▉  | 197/248 [00:51<00:13,  3.73it/s] 80%|███████▉  | 198/248 [00:51<00:13,  3.68it/s] 80%|████████  | 199/248 [00:51<00:13,  3.73it/s] 81%|████████  | 200/248 [00:52<00:13,  3.67it/s] 81%|████████  | 201/248 [00:52<00:12,  3.73it/s] 81%|████████▏ | 202/248 [00:52<00:12,  3.67it/s] 82%|████████▏ | 203/248 [00:52<00:12,  3.73it/s] 82%|████████▏ | 204/248 [00:53<00:11,  3.67it/s] 83%|████████▎ | 205/248 [00:53<00:11,  3.73it/s] 83%|████████▎ | 206/248 [00:53<00:11,  3.66it/s] 83%|████████▎ | 207/248 [00:53<00:11,  3.71it/s] 84%|████████▍ | 208/248 [00:54<00:10,  3.65it/s] 84%|████████▍ | 209/248 [00:54<00:10,  3.70it/s] 85%|████████▍ | 210/248 [00:54<00:10,  3.65it/s] 85%|████████▌ | 211/248 [00:55<00:09,  3.70it/s] 85%|████████▌ | 212/248 [00:55<00:09,  3.66it/s] 86%|████████▌ | 213/248 [00:55<00:09,  3.72it/s] 86%|████████▋ | 214/248 [00:55<00:09,  3.66it/s] 87%|████████▋ | 215/248 [00:56<00:08,  3.71it/s] 87%|████████▋ | 216/248 [00:56<00:08,  3.65it/s] 88%|████████▊ | 217/248 [00:56<00:08,  3.70it/s] 88%|████████▊ | 218/248 [00:56<00:08,  3.64it/s] 88%|████████▊ | 219/248 [00:57<00:07,  3.69it/s] 89%|████████▊ | 220/248 [00:57<00:07,  3.65it/s] 89%|████████▉ | 221/248 [00:57<00:07,  3.71it/s] 90%|████████▉ | 222/248 [00:58<00:07,  3.66it/s] 90%|████████▉ | 223/248 [00:58<00:06,  3.71it/s] 90%|█████████ | 224/248 [00:58<00:06,  3.67it/s] 91%|█████████ | 225/248 [00:58<00:06,  3.71it/s] 91%|█████████ | 226/248 [00:59<00:06,  3.66it/s] 92%|█████████▏| 227/248 [00:59<00:05,  3.70it/s] 92%|█████████▏| 228/248 [00:59<00:05,  3.66it/s] 92%|█████████▏| 229/248 [00:59<00:05,  3.70it/s] 93%|█████████▎| 230/248 [01:00<00:04,  3.64it/s] 93%|█████████▎| 231/248 [01:00<00:04,  3.64it/s] 94%|█████████▎| 232/248 [01:00<00:04,  3.68it/s] 94%|█████████▍| 233/248 [01:01<00:04,  3.72it/s] 94%|█████████▍| 234/248 [01:01<00:03,  3.66it/s] 95%|█████████▍| 235/248 [01:01<00:03,  3.67it/s] 95%|█████████▌| 236/248 [01:01<00:03,  3.68it/s] 96%|█████████▌| 237/248 [01:02<00:02,  3.72it/s] 96%|█████████▌| 238/248 [01:02<00:02,  3.65it/s] 96%|█████████▋| 239/248 [01:02<00:02,  3.68it/s] 97%|█████████▋| 240/248 [01:02<00:02,  3.66it/s] 97%|█████████▋| 241/248 [01:03<00:01,  3.71it/s] 98%|█████████▊| 242/248 [01:03<00:01,  3.65it/s] 98%|█████████▊| 243/248 [01:03<00:01,  3.71it/s] 98%|█████████▊| 244/248 [01:04<00:01,  3.66it/s] 99%|█████████▉| 245/248 [01:04<00:00,  3.70it/s] 99%|█████████▉| 246/248 [01:04<00:00,  3.64it/s]100%|█████████▉| 247/248 [01:04<00:00,  3.68it/s]100%|██████████| 248/248 [01:05<00:00,  3.65it/s]accuracy:  0.6370967741935484
100%|██████████| 248/248 [01:08<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 236
Training dataset size: 288, validation dataset size: 236
Training dataset size: 288, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 05:51:42,105] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 05:51:42,232] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 05:51:42,628] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<01:44,  2.23s/it]  4%|▍         | 2/48 [00:04<01:55,  2.50s/it]  6%|▋         | 3/48 [00:07<01:53,  2.52s/it]  8%|▊         | 4/48 [00:09<01:43,  2.35s/it] 10%|█         | 5/48 [00:11<01:34,  2.20s/it] 12%|█▎        | 6/48 [00:14<01:42,  2.45s/it] 15%|█▍        | 7/48 [00:16<01:36,  2.35s/it] 17%|█▋        | 8/48 [00:18<01:33,  2.34s/it] 19%|█▉        | 9/48 [00:21<01:29,  2.29s/it] 21%|██        | 10/48 [00:23<01:29,  2.37s/it]                                               {'loss': 1.0015, 'grad_norm': 18.14934730529785, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:23<01:29,  2.37s/it] 23%|██▎       | 11/48 [00:25<01:26,  2.35s/it] 25%|██▌       | 12/48 [00:28<01:23,  2.32s/it] 27%|██▋       | 13/48 [00:31<01:27,  2.49s/it] 29%|██▉       | 14/48 [00:33<01:22,  2.41s/it] 31%|███▏      | 15/48 [00:35<01:16,  2.31s/it] 33%|███▎      | 16/48 [00:37<01:13,  2.29s/it] 35%|███▌      | 17/48 [00:40<01:15,  2.43s/it] 38%|███▊      | 18/48 [00:43<01:17,  2.60s/it] 40%|███▉      | 19/48 [00:45<01:15,  2.61s/it] 42%|████▏     | 20/48 [00:48<01:10,  2.50s/it]                                               {'loss': 0.8785, 'grad_norm': 3.8096179962158203, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:48<01:10,  2.50s/it] 44%|████▍     | 21/48 [00:50<01:06,  2.48s/it] 46%|████▌     | 22/48 [00:52<01:02,  2.42s/it] 48%|████▊     | 23/48 [00:55<00:58,  2.36s/it] 50%|█████     | 24/48 [00:57<00:57,  2.40s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:00<01:00,  2.62s/it] 54%|█████▍    | 26/48 [01:03<00:56,  2.56s/it] 56%|█████▋    | 27/48 [01:05<00:50,  2.40s/it] 58%|█████▊    | 28/48 [01:07<00:47,  2.35s/it] 60%|██████    | 29/48 [01:09<00:42,  2.24s/it] 62%|██████▎   | 30/48 [01:11<00:39,  2.17s/it]                                               {'loss': 0.6808, 'grad_norm': 7.206377029418945, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:11<00:39,  2.17s/it] 65%|██████▍   | 31/48 [01:14<00:39,  2.30s/it] 67%|██████▋   | 32/48 [01:15<00:35,  2.19s/it] 69%|██████▉   | 33/48 [01:18<00:34,  2.28s/it] 71%|███████   | 34/48 [01:21<00:33,  2.38s/it] 73%|███████▎  | 35/48 [01:24<00:34,  2.67s/it] 75%|███████▌  | 36/48 [01:27<00:31,  2.65s/it] 77%|███████▋  | 37/48 [01:29<00:27,  2.52s/it] 79%|███████▉  | 38/48 [01:31<00:23,  2.35s/it] 81%|████████▏ | 39/48 [01:33<00:21,  2.34s/it] 83%|████████▎ | 40/48 [01:36<00:19,  2.50s/it]                                               {'loss': 0.7828, 'grad_norm': 10.682783126831055, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:36<00:19,  2.50s/it] 85%|████████▌ | 41/48 [01:38<00:17,  2.53s/it] 88%|████████▊ | 42/48 [01:41<00:14,  2.41s/it] 90%|████████▉ | 43/48 [01:43<00:11,  2.39s/it] 92%|█████████▏| 44/48 [01:45<00:09,  2.42s/it] 94%|█████████▍| 45/48 [01:47<00:06,  2.19s/it] 96%|█████████▌| 46/48 [01:49<00:04,  2.24s/it] 98%|█████████▊| 47/48 [01:52<00:02,  2.28s/it]100%|██████████| 48/48 [01:54<00:00,  2.22s/it]                                               {'train_runtime': 115.0409, 'train_samples_per_second': 5.007, 'train_steps_per_second': 0.417, 'train_loss': 0.7842802306016287, 'epoch': 2.0}
100%|██████████| 48/48 [01:54<00:00,  2.22s/it]100%|██████████| 48/48 [01:54<00:00,  2.39s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<02:02,  2.93it/s]  1%|          | 2/361 [00:00<01:47,  3.34it/s]  1%|          | 3/361 [00:00<01:40,  3.55it/s]  1%|          | 4/361 [00:01<01:39,  3.58it/s]  1%|▏         | 5/361 [00:01<01:34,  3.78it/s]  2%|▏         | 6/361 [00:01<01:34,  3.75it/s]  2%|▏         | 7/361 [00:01<01:34,  3.73it/s]  2%|▏         | 8/361 [00:02<01:35,  3.70it/s]  2%|▏         | 9/361 [00:02<01:31,  3.83it/s]  3%|▎         | 10/361 [00:02<01:31,  3.84it/s]  3%|▎         | 11/361 [00:02<01:32,  3.80it/s]  3%|▎         | 12/361 [00:03<01:33,  3.74it/s]  4%|▎         | 13/361 [00:03<01:31,  3.81it/s]  4%|▍         | 14/361 [00:03<01:30,  3.85it/s]  4%|▍         | 15/361 [00:04<01:30,  3.81it/s]  4%|▍         | 16/361 [00:04<01:31,  3.77it/s]  5%|▍         | 17/361 [00:04<01:30,  3.79it/s]  5%|▍         | 18/361 [00:04<01:29,  3.83it/s]  5%|▌         | 19/361 [00:05<01:30,  3.79it/s]  6%|▌         | 20/361 [00:05<01:31,  3.74it/s]  6%|▌         | 21/361 [00:05<01:29,  3.78it/s]  6%|▌         | 22/361 [00:05<01:28,  3.84it/s]  6%|▋         | 23/361 [00:06<01:29,  3.80it/s]  7%|▋         | 24/361 [00:06<01:29,  3.78it/s]  7%|▋         | 25/361 [00:06<01:30,  3.73it/s]  7%|▋         | 26/361 [00:06<01:27,  3.85it/s]  7%|▋         | 27/361 [00:07<01:27,  3.82it/s]  8%|▊         | 28/361 [00:07<01:27,  3.78it/s]  8%|▊         | 29/361 [00:07<01:29,  3.71it/s]  8%|▊         | 30/361 [00:07<01:26,  3.82it/s]  9%|▊         | 31/361 [00:08<01:26,  3.82it/s]  9%|▉         | 32/361 [00:08<01:26,  3.78it/s]  9%|▉         | 33/361 [00:08<01:26,  3.79it/s]  9%|▉         | 34/361 [00:09<01:26,  3.77it/s] 10%|▉         | 35/361 [00:09<01:24,  3.86it/s] 10%|▉         | 36/361 [00:09<01:25,  3.82it/s] 10%|█         | 37/361 [00:09<01:25,  3.78it/s] 11%|█         | 38/361 [00:10<01:26,  3.73it/s] 11%|█         | 39/361 [00:10<01:23,  3.85it/s] 11%|█         | 40/361 [00:10<01:23,  3.85it/s] 11%|█▏        | 41/361 [00:10<01:24,  3.81it/s] 12%|█▏        | 42/361 [00:11<01:23,  3.84it/s] 12%|█▏        | 43/361 [00:11<01:24,  3.77it/s] 12%|█▏        | 44/361 [00:11<01:21,  3.87it/s] 12%|█▏        | 45/361 [00:11<01:22,  3.82it/s] 13%|█▎        | 46/361 [00:12<01:23,  3.77it/s] 13%|█▎        | 47/361 [00:12<01:24,  3.71it/s] 13%|█▎        | 48/361 [00:12<01:21,  3.82it/s] 14%|█▎        | 49/361 [00:12<01:21,  3.83it/s] 14%|█▍        | 50/361 [00:13<01:22,  3.79it/s] 14%|█▍        | 51/361 [00:13<01:20,  3.83it/s] 14%|█▍        | 52/361 [00:13<01:22,  3.75it/s] 15%|█▍        | 53/361 [00:14<01:19,  3.85it/s] 15%|█▍        | 54/361 [00:14<01:20,  3.80it/s] 15%|█▌        | 55/361 [00:14<01:21,  3.76it/s] 16%|█▌        | 56/361 [00:14<01:22,  3.71it/s] 16%|█▌        | 57/361 [00:15<01:19,  3.84it/s] 16%|█▌        | 58/361 [00:15<01:18,  3.86it/s] 16%|█▋        | 59/361 [00:15<01:19,  3.81it/s] 17%|█▋        | 60/361 [00:15<01:18,  3.84it/s] 17%|█▋        | 61/361 [00:16<01:19,  3.75it/s] 17%|█▋        | 62/361 [00:16<01:18,  3.83it/s] 17%|█▋        | 63/361 [00:16<01:19,  3.77it/s] 18%|█▊        | 64/361 [00:16<01:18,  3.76it/s] 18%|█▊        | 65/361 [00:17<01:19,  3.71it/s] 18%|█▊        | 66/361 [00:17<01:17,  3.82it/s] 19%|█▊        | 67/361 [00:17<01:17,  3.80it/s] 19%|█▉        | 68/361 [00:17<01:17,  3.78it/s] 19%|█▉        | 69/361 [00:18<01:17,  3.78it/s] 19%|█▉        | 70/361 [00:18<01:17,  3.74it/s] 20%|█▉        | 71/361 [00:18<01:15,  3.82it/s] 20%|█▉        | 72/361 [00:19<01:16,  3.78it/s] 20%|██        | 73/361 [00:19<01:17,  3.74it/s] 20%|██        | 74/361 [00:19<01:17,  3.70it/s] 21%|██        | 75/361 [00:19<01:14,  3.81it/s] 21%|██        | 76/361 [00:20<01:14,  3.80it/s] 21%|██▏       | 77/361 [00:20<01:15,  3.76it/s] 22%|██▏       | 78/361 [00:20<01:14,  3.81it/s] 22%|██▏       | 79/361 [00:20<01:15,  3.74it/s] 22%|██▏       | 80/361 [00:21<01:13,  3.81it/s] 22%|██▏       | 81/361 [00:21<01:14,  3.77it/s] 23%|██▎       | 82/361 [00:21<01:13,  3.79it/s] 23%|██▎       | 83/361 [00:21<01:14,  3.73it/s] 23%|██▎       | 84/361 [00:22<01:12,  3.85it/s] 24%|██▎       | 85/361 [00:22<01:12,  3.81it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.76it/s] 24%|██▍       | 87/361 [00:23<01:14,  3.70it/s] 24%|██▍       | 88/361 [00:23<01:11,  3.82it/s] 25%|██▍       | 89/361 [00:23<01:11,  3.83it/s] 25%|██▍       | 90/361 [00:23<01:11,  3.79it/s] 25%|██▌       | 91/361 [00:24<01:10,  3.82it/s] 25%|██▌       | 92/361 [00:24<01:12,  3.73it/s] 26%|██▌       | 93/361 [00:24<01:10,  3.81it/s] 26%|██▌       | 94/361 [00:24<01:10,  3.78it/s] 26%|██▋       | 95/361 [00:25<01:11,  3.73it/s] 27%|██▋       | 96/361 [00:25<01:11,  3.68it/s] 27%|██▋       | 97/361 [00:25<01:09,  3.80it/s] 27%|██▋       | 98/361 [00:25<01:08,  3.82it/s] 27%|██▋       | 99/361 [00:26<01:09,  3.77it/s] 28%|██▊       | 100/361 [00:26<01:08,  3.81it/s] 28%|██▊       | 101/361 [00:26<01:09,  3.75it/s] 28%|██▊       | 102/361 [00:26<01:07,  3.82it/s] 29%|██▊       | 103/361 [00:27<01:08,  3.77it/s] 29%|██▉       | 104/361 [00:27<01:08,  3.76it/s] 29%|██▉       | 105/361 [00:27<01:09,  3.70it/s] 29%|██▉       | 106/361 [00:28<01:06,  3.81it/s] 30%|██▉       | 107/361 [00:28<01:07,  3.79it/s] 30%|██▉       | 108/361 [00:28<01:07,  3.77it/s] 30%|███       | 109/361 [00:28<01:06,  3.81it/s] 30%|███       | 110/361 [00:29<01:06,  3.75it/s] 31%|███       | 111/361 [00:29<01:05,  3.82it/s] 31%|███       | 112/361 [00:29<01:06,  3.75it/s] 31%|███▏      | 113/361 [00:29<01:06,  3.75it/s] 32%|███▏      | 114/361 [00:30<01:06,  3.70it/s] 32%|███▏      | 115/361 [00:30<01:04,  3.82it/s] 32%|███▏      | 116/361 [00:30<01:04,  3.82it/s] 32%|███▏      | 117/361 [00:30<01:04,  3.78it/s] 33%|███▎      | 118/361 [00:31<01:03,  3.80it/s] 33%|███▎      | 119/361 [00:31<01:04,  3.74it/s] 33%|███▎      | 120/361 [00:31<01:03,  3.81it/s] 34%|███▎      | 121/361 [00:32<01:03,  3.77it/s] 34%|███▍      | 122/361 [00:32<01:03,  3.74it/s] 34%|███▍      | 123/361 [00:32<01:04,  3.69it/s] 34%|███▍      | 124/361 [00:32<01:02,  3.81it/s] 35%|███▍      | 125/361 [00:33<01:02,  3.81it/s] 35%|███▍      | 126/361 [00:33<01:02,  3.76it/s] 35%|███▌      | 127/361 [00:33<01:01,  3.81it/s] 35%|███▌      | 128/361 [00:33<01:02,  3.74it/s] 36%|███▌      | 129/361 [00:34<01:00,  3.82it/s] 36%|███▌      | 130/361 [00:34<01:01,  3.77it/s] 36%|███▋      | 131/361 [00:34<01:01,  3.75it/s] 37%|███▋      | 132/361 [00:34<01:02,  3.69it/s] 37%|███▋      | 133/361 [00:35<00:59,  3.80it/s] 37%|███▋      | 134/361 [00:35<00:59,  3.79it/s] 37%|███▋      | 135/361 [00:35<01:00,  3.76it/s] 38%|███▊      | 136/361 [00:36<01:00,  3.71it/s] 38%|███▊      | 137/361 [00:36<00:59,  3.75it/s] 38%|███▊      | 138/361 [00:36<00:58,  3.81it/s] 39%|███▊      | 139/361 [00:36<00:58,  3.77it/s] 39%|███▉      | 140/361 [00:37<00:58,  3.78it/s] 39%|███▉      | 141/361 [00:37<00:59,  3.72it/s] 39%|███▉      | 142/361 [00:37<00:57,  3.83it/s] 40%|███▉      | 143/361 [00:37<00:57,  3.78it/s] 40%|███▉      | 144/361 [00:38<00:58,  3.74it/s] 40%|████      | 145/361 [00:38<00:58,  3.69it/s] 40%|████      | 146/361 [00:38<00:56,  3.80it/s] 41%|████      | 147/361 [00:38<00:56,  3.81it/s] 41%|████      | 148/361 [00:39<00:56,  3.77it/s] 41%|████▏     | 149/361 [00:39<00:55,  3.79it/s] 42%|████▏     | 150/361 [00:39<00:56,  3.75it/s] 42%|████▏     | 151/361 [00:39<00:54,  3.82it/s] 42%|████▏     | 152/361 [00:40<00:55,  3.78it/s] 42%|████▏     | 153/361 [00:40<00:55,  3.77it/s] 43%|████▎     | 154/361 [00:40<00:55,  3.71it/s] 43%|████▎     | 155/361 [00:41<00:53,  3.82it/s] 43%|████▎     | 156/361 [00:41<00:54,  3.79it/s] 43%|████▎     | 157/361 [00:41<00:54,  3.75it/s] 44%|████▍     | 158/361 [00:41<00:55,  3.68it/s] 44%|████▍     | 159/361 [00:42<00:53,  3.80it/s] 44%|████▍     | 160/361 [00:42<00:52,  3.82it/s] 45%|████▍     | 161/361 [00:42<00:53,  3.77it/s] 45%|████▍     | 162/361 [00:42<00:52,  3.78it/s] 45%|████▌     | 163/361 [00:43<00:53,  3.73it/s] 45%|████▌     | 164/361 [00:43<00:52,  3.79it/s] 46%|████▌     | 165/361 [00:43<00:52,  3.75it/s] 46%|████▌     | 166/361 [00:43<00:51,  3.76it/s] 46%|████▋     | 167/361 [00:44<00:52,  3.70it/s] 47%|████▋     | 168/361 [00:44<00:50,  3.82it/s] 47%|████▋     | 169/361 [00:44<00:50,  3.78it/s] 47%|████▋     | 170/361 [00:45<00:51,  3.73it/s] 47%|████▋     | 171/361 [00:45<00:51,  3.67it/s] 48%|████▊     | 172/361 [00:45<00:49,  3.79it/s] 48%|████▊     | 173/361 [00:45<00:49,  3.79it/s] 48%|████▊     | 174/361 [00:46<00:49,  3.76it/s] 48%|████▊     | 175/361 [00:46<00:49,  3.79it/s] 49%|████▉     | 176/361 [00:46<00:49,  3.71it/s] 49%|████▉     | 177/361 [00:46<00:48,  3.80it/s] 49%|████▉     | 178/361 [00:47<00:48,  3.75it/s] 50%|████▉     | 179/361 [00:47<00:48,  3.73it/s] 50%|████▉     | 180/361 [00:47<00:49,  3.68it/s] 50%|█████     | 181/361 [00:47<00:47,  3.79it/s] 50%|█████     | 182/361 [00:48<00:46,  3.82it/s] 51%|█████     | 183/361 [00:48<00:47,  3.77it/s] 51%|█████     | 184/361 [00:48<00:46,  3.80it/s] 51%|█████     | 185/361 [00:49<00:47,  3.72it/s] 52%|█████▏    | 186/361 [00:49<00:45,  3.81it/s] 52%|█████▏    | 187/361 [00:49<00:46,  3.75it/s] 52%|█████▏    | 188/361 [00:49<00:46,  3.73it/s] 52%|█████▏    | 189/361 [00:50<00:46,  3.69it/s] 53%|█████▎    | 190/361 [00:50<00:44,  3.80it/s] 53%|█████▎    | 191/361 [00:50<00:44,  3.79it/s] 53%|█████▎    | 192/361 [00:50<00:45,  3.74it/s] 53%|█████▎    | 193/361 [00:51<00:44,  3.77it/s] 54%|█████▎    | 194/361 [00:51<00:44,  3.73it/s] 54%|█████▍    | 195/361 [00:51<00:43,  3.80it/s] 54%|█████▍    | 196/361 [00:51<00:44,  3.74it/s] 55%|█████▍    | 197/361 [00:52<00:43,  3.73it/s] 55%|█████▍    | 198/361 [00:52<00:44,  3.68it/s] 55%|█████▌    | 199/361 [00:52<00:42,  3.80it/s] 55%|█████▌    | 200/361 [00:53<00:42,  3.82it/s] 56%|█████▌    | 201/361 [00:53<00:42,  3.76it/s] 56%|█████▌    | 202/361 [00:53<00:41,  3.80it/s] 56%|█████▌    | 203/361 [00:53<00:42,  3.71it/s] 57%|█████▋    | 204/361 [00:54<00:41,  3.79it/s] 57%|█████▋    | 205/361 [00:54<00:41,  3.73it/s] 57%|█████▋    | 206/361 [00:54<00:41,  3.74it/s] 57%|█████▋    | 207/361 [00:54<00:40,  3.77it/s] 58%|█████▊    | 208/361 [00:55<00:39,  3.86it/s] 58%|█████▊    | 209/361 [00:55<00:38,  3.92it/s] 58%|█████▊    | 210/361 [00:55<00:38,  3.97it/s] 58%|█████▊    | 211/361 [00:55<00:37,  3.99it/s] 59%|█████▊    | 212/361 [00:56<00:37,  4.02it/s] 59%|█████▉    | 213/361 [00:56<00:36,  4.04it/s] 59%|█████▉    | 214/361 [00:56<00:36,  4.05it/s] 60%|█████▉    | 215/361 [00:56<00:36,  4.05it/s] 60%|█████▉    | 216/361 [00:57<00:35,  4.06it/s] 60%|██████    | 217/361 [00:57<00:35,  4.08it/s] 60%|██████    | 218/361 [00:57<00:35,  4.07it/s] 61%|██████    | 219/361 [00:57<00:34,  4.07it/s] 61%|██████    | 220/361 [00:58<00:34,  4.08it/s] 61%|██████    | 221/361 [00:58<00:34,  4.08it/s] 61%|██████▏   | 222/361 [00:58<00:34,  4.07it/s] 62%|██████▏   | 223/361 [00:58<00:33,  4.07it/s] 62%|██████▏   | 224/361 [00:59<00:33,  4.08it/s] 62%|██████▏   | 225/361 [00:59<00:33,  4.08it/s] 63%|██████▎   | 226/361 [00:59<00:33,  4.07it/s] 63%|██████▎   | 227/361 [00:59<00:32,  4.08it/s] 63%|██████▎   | 228/361 [01:00<00:32,  4.08it/s] 63%|██████▎   | 229/361 [01:00<00:32,  4.08it/s] 64%|██████▎   | 230/361 [01:00<00:32,  4.07it/s] 64%|██████▍   | 231/361 [01:00<00:31,  4.08it/s] 64%|██████▍   | 232/361 [01:01<00:31,  4.08it/s] 65%|██████▍   | 233/361 [01:01<00:31,  4.08it/s] 65%|██████▍   | 234/361 [01:01<00:31,  4.07it/s] 65%|██████▌   | 235/361 [01:01<00:30,  4.08it/s] 65%|██████▌   | 236/361 [01:01<00:30,  4.09it/s] 66%|██████▌   | 237/361 [01:02<00:30,  4.08it/s] 66%|██████▌   | 238/361 [01:02<00:30,  4.08it/s] 66%|██████▌   | 239/361 [01:02<00:29,  4.09it/s] 66%|██████▋   | 240/361 [01:02<00:29,  4.09it/s] 67%|██████▋   | 241/361 [01:03<00:29,  4.08it/s] 67%|██████▋   | 242/361 [01:03<00:29,  4.08it/s] 67%|██████▋   | 243/361 [01:03<00:29,  3.96it/s] 68%|██████▊   | 244/361 [01:04<00:30,  3.86it/s] 68%|██████▊   | 245/361 [01:04<00:30,  3.80it/s] 68%|██████▊   | 246/361 [01:04<00:29,  3.88it/s] 68%|██████▊   | 247/361 [01:04<00:30,  3.77it/s] 69%|██████▊   | 248/361 [01:05<00:30,  3.71it/s] 69%|██████▉   | 249/361 [01:05<00:30,  3.67it/s] 69%|██████▉   | 250/361 [01:05<00:29,  3.77it/s] 70%|██████▉   | 251/361 [01:05<00:29,  3.68it/s] 70%|██████▉   | 252/361 [01:06<00:29,  3.68it/s] 70%|███████   | 253/361 [01:06<00:29,  3.64it/s] 70%|███████   | 254/361 [01:06<00:28,  3.74it/s] 71%|███████   | 255/361 [01:06<00:28,  3.67it/s] 71%|███████   | 256/361 [01:07<00:28,  3.67it/s] 71%|███████   | 257/361 [01:07<00:28,  3.64it/s] 71%|███████▏  | 258/361 [01:07<00:27,  3.74it/s] 72%|███████▏  | 259/361 [01:08<00:27,  3.65it/s] 72%|███████▏  | 260/361 [01:08<00:27,  3.70it/s] 72%|███████▏  | 261/361 [01:08<00:27,  3.65it/s] 73%|███████▎  | 262/361 [01:08<00:26,  3.72it/s] 73%|███████▎  | 263/361 [01:09<00:26,  3.64it/s] 73%|███████▎  | 264/361 [01:09<00:26,  3.71it/s] 73%|███████▎  | 265/361 [01:09<00:26,  3.66it/s] 74%|███████▎  | 266/361 [01:09<00:25,  3.70it/s] 74%|███████▍  | 267/361 [01:10<00:25,  3.64it/s] 74%|███████▍  | 268/361 [01:10<00:25,  3.63it/s] 75%|███████▍  | 269/361 [01:10<00:24,  3.74it/s] 75%|███████▍  | 270/361 [01:11<00:23,  3.84it/s] 75%|███████▌  | 271/361 [01:11<00:23,  3.90it/s] 75%|███████▌  | 272/361 [01:11<00:22,  3.95it/s] 76%|███████▌  | 273/361 [01:11<00:22,  3.99it/s] 76%|███████▌  | 274/361 [01:12<00:21,  4.02it/s] 76%|███████▌  | 275/361 [01:12<00:21,  4.02it/s] 76%|███████▋  | 276/361 [01:12<00:21,  4.04it/s] 77%|███████▋  | 277/361 [01:12<00:20,  4.05it/s] 77%|███████▋  | 278/361 [01:12<00:20,  4.05it/s] 77%|███████▋  | 279/361 [01:13<00:20,  4.05it/s] 78%|███████▊  | 280/361 [01:13<00:19,  4.06it/s] 78%|███████▊  | 281/361 [01:13<00:19,  4.06it/s] 78%|███████▊  | 282/361 [01:13<00:19,  4.06it/s] 78%|███████▊  | 283/361 [01:14<00:19,  4.06it/s] 79%|███████▊  | 284/361 [01:14<00:18,  4.07it/s] 79%|███████▉  | 285/361 [01:14<00:18,  4.06it/s] 79%|███████▉  | 286/361 [01:14<00:18,  4.06it/s] 80%|███████▉  | 287/361 [01:15<00:18,  4.07it/s] 80%|███████▉  | 288/361 [01:15<00:17,  4.07it/s] 80%|████████  | 289/361 [01:15<00:17,  4.07it/s] 80%|████████  | 290/361 [01:15<00:17,  4.07it/s] 81%|████████  | 291/361 [01:16<00:17,  4.08it/s] 81%|████████  | 292/361 [01:16<00:16,  4.06it/s] 81%|████████  | 293/361 [01:16<00:16,  4.06it/s] 81%|████████▏ | 294/361 [01:16<00:16,  4.07it/s] 82%|████████▏ | 295/361 [01:17<00:16,  4.06it/s] 82%|████████▏ | 296/361 [01:17<00:16,  4.06it/s] 82%|████████▏ | 297/361 [01:17<00:15,  4.07it/s] 83%|████████▎ | 298/361 [01:17<00:15,  4.06it/s] 83%|████████▎ | 299/361 [01:18<00:15,  4.06it/s] 83%|████████▎ | 300/361 [01:18<00:14,  4.07it/s] 83%|████████▎ | 301/361 [01:18<00:14,  4.06it/s] 84%|████████▎ | 302/361 [01:18<00:14,  4.06it/s] 84%|████████▍ | 303/361 [01:19<00:14,  4.07it/s] 84%|████████▍ | 304/361 [01:19<00:13,  4.08it/s] 84%|████████▍ | 305/361 [01:19<00:13,  4.07it/s] 85%|████████▍ | 306/361 [01:19<00:13,  4.07it/s] 85%|████████▌ | 307/361 [01:20<00:13,  4.07it/s] 85%|████████▌ | 308/361 [01:20<00:13,  4.07it/s] 86%|████████▌ | 309/361 [01:20<00:12,  4.06it/s] 86%|████████▌ | 310/361 [01:20<00:12,  4.07it/s] 86%|████████▌ | 311/361 [01:21<00:12,  4.07it/s] 86%|████████▋ | 312/361 [01:21<00:12,  4.01it/s] 87%|████████▋ | 313/361 [01:21<00:12,  3.91it/s] 87%|████████▋ | 314/361 [01:21<00:12,  3.84it/s] 87%|████████▋ | 315/361 [01:22<00:11,  3.85it/s] 88%|████████▊ | 316/361 [01:22<00:11,  3.84it/s] 88%|████████▊ | 317/361 [01:22<00:11,  3.73it/s] 88%|████████▊ | 318/361 [01:22<00:11,  3.77it/s] 88%|████████▊ | 319/361 [01:23<00:11,  3.69it/s] 89%|████████▊ | 320/361 [01:23<00:10,  3.74it/s] 89%|████████▉ | 321/361 [01:23<00:10,  3.69it/s] 89%|████████▉ | 322/361 [01:24<00:10,  3.73it/s] 89%|████████▉ | 323/361 [01:24<00:10,  3.67it/s] 90%|████████▉ | 324/361 [01:24<00:09,  3.74it/s] 90%|█████████ | 325/361 [01:24<00:09,  3.68it/s] 90%|█████████ | 326/361 [01:25<00:09,  3.72it/s] 91%|█████████ | 327/361 [01:25<00:09,  3.67it/s] 91%|█████████ | 328/361 [01:25<00:08,  3.75it/s] 91%|█████████ | 329/361 [01:25<00:08,  3.69it/s] 91%|█████████▏| 330/361 [01:26<00:08,  3.68it/s] 92%|█████████▏| 331/361 [01:26<00:08,  3.63it/s] 92%|█████████▏| 332/361 [01:26<00:07,  3.75it/s] 92%|█████████▏| 333/361 [01:27<00:07,  3.71it/s] 93%|█████████▎| 334/361 [01:27<00:07,  3.68it/s] 93%|█████████▎| 335/361 [01:27<00:07,  3.62it/s] 93%|█████████▎| 336/361 [01:27<00:06,  3.75it/s] 93%|█████████▎| 337/361 [01:28<00:06,  3.76it/s] 94%|█████████▎| 338/361 [01:28<00:06,  3.68it/s] 94%|█████████▍| 339/361 [01:28<00:05,  3.73it/s] 94%|█████████▍| 340/361 [01:28<00:05,  3.66it/s] 94%|█████████▍| 341/361 [01:29<00:05,  3.73it/s] 95%|█████████▍| 342/361 [01:29<00:05,  3.67it/s] 95%|█████████▌| 343/361 [01:29<00:04,  3.70it/s] 95%|█████████▌| 344/361 [01:30<00:04,  3.65it/s] 96%|█████████▌| 345/361 [01:30<00:04,  3.74it/s] 96%|█████████▌| 346/361 [01:30<00:04,  3.70it/s] 96%|█████████▌| 347/361 [01:30<00:03,  3.67it/s] 96%|█████████▋| 348/361 [01:31<00:03,  3.63it/s] 97%|█████████▋| 349/361 [01:31<00:03,  3.74it/s] 97%|█████████▋| 350/361 [01:31<00:02,  3.70it/s] 97%|█████████▋| 351/361 [01:31<00:02,  3.68it/s] 98%|█████████▊| 352/361 [01:32<00:02,  3.62it/s] 98%|█████████▊| 353/361 [01:32<00:02,  3.75it/s] 98%|█████████▊| 354/361 [01:32<00:01,  3.72it/s] 98%|█████████▊| 355/361 [01:32<00:01,  3.68it/s] 99%|█████████▊| 356/361 [01:33<00:01,  3.74it/s] 99%|█████████▉| 357/361 [01:33<00:01,  3.67it/s] 99%|█████████▉| 358/361 [01:33<00:00,  3.69it/s] 99%|█████████▉| 359/361 [01:34<00:00,  3.64it/s]100%|█████████▉| 360/361 [01:34<00:00,  3.69it/s]100%|██████████| 361/361 [01:34<00:00,  3.65it/s]accuracy:  0.628808864265928
100%|██████████| 361/361 [01:40<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 188
Training dataset size: 288, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[2025-03-12 05:55:44,392] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 05:55:44,552] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:55:45,572] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:03<02:23,  3.05s/it]  4%|▍         | 2/48 [00:05<02:11,  2.85s/it]  6%|▋         | 3/48 [00:08<01:58,  2.63s/it]  8%|▊         | 4/48 [00:10<01:57,  2.67s/it] 10%|█         | 5/48 [00:13<01:51,  2.60s/it] 12%|█▎        | 6/48 [00:16<01:52,  2.67s/it] 15%|█▍        | 7/48 [00:18<01:51,  2.71s/it] 17%|█▋        | 8/48 [00:21<01:45,  2.64s/it] 19%|█▉        | 9/48 [00:24<01:48,  2.79s/it] 21%|██        | 10/48 [00:27<01:43,  2.73s/it]                                               {'loss': 0.676, 'grad_norm': 3.797100067138672, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:27<01:43,  2.73s/it] 23%|██▎       | 11/48 [00:29<01:41,  2.75s/it] 25%|██▌       | 12/48 [00:32<01:40,  2.79s/it] 27%|██▋       | 13/48 [00:34<01:30,  2.60s/it] 29%|██▉       | 14/48 [00:37<01:31,  2.69s/it] 31%|███▏      | 15/48 [00:40<01:27,  2.64s/it] 33%|███▎      | 16/48 [00:43<01:30,  2.83s/it] 35%|███▌      | 17/48 [00:45<01:20,  2.60s/it] 38%|███▊      | 18/48 [00:48<01:23,  2.78s/it] 40%|███▉      | 19/48 [00:51<01:22,  2.84s/it] 42%|████▏     | 20/48 [00:55<01:22,  2.95s/it]                                               {'loss': 0.4611, 'grad_norm': 1.4134403467178345, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:55<01:22,  2.95s/it] 44%|████▍     | 21/48 [00:58<01:22,  3.04s/it] 46%|████▌     | 22/48 [01:01<01:21,  3.15s/it] 48%|████▊     | 23/48 [01:04<01:18,  3.15s/it] 50%|█████     | 24/48 [01:07<01:11,  2.96s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:10<01:07,  2.92s/it] 54%|█████▍    | 26/48 [01:13<01:03,  2.89s/it] 56%|█████▋    | 27/48 [01:15<00:57,  2.74s/it] 58%|█████▊    | 28/48 [01:17<00:51,  2.58s/it] 60%|██████    | 29/48 [01:20<00:49,  2.61s/it] 62%|██████▎   | 30/48 [01:23<00:47,  2.66s/it]                                               {'loss': 0.4467, 'grad_norm': 1.28999662399292, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:23<00:47,  2.66s/it] 65%|██████▍   | 31/48 [01:25<00:45,  2.70s/it] 67%|██████▋   | 32/48 [01:28<00:43,  2.74s/it] 69%|██████▉   | 33/48 [01:31<00:42,  2.82s/it] 71%|███████   | 34/48 [01:34<00:39,  2.81s/it] 73%|███████▎  | 35/48 [01:36<00:34,  2.63s/it] 75%|███████▌  | 36/48 [01:39<00:33,  2.77s/it] 77%|███████▋  | 37/48 [01:42<00:30,  2.78s/it] 79%|███████▉  | 38/48 [01:45<00:26,  2.68s/it] 81%|████████▏ | 39/48 [01:47<00:24,  2.69s/it] 83%|████████▎ | 40/48 [01:50<00:21,  2.68s/it]                                               {'loss': 0.5101, 'grad_norm': 5.513221263885498, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:50<00:21,  2.68s/it] 85%|████████▌ | 41/48 [01:53<00:19,  2.73s/it] 88%|████████▊ | 42/48 [01:56<00:16,  2.72s/it] 90%|████████▉ | 43/48 [01:59<00:14,  2.90s/it] 92%|█████████▏| 44/48 [02:01<00:11,  2.81s/it] 94%|█████████▍| 45/48 [02:05<00:09,  3.00s/it] 96%|█████████▌| 46/48 [02:07<00:05,  2.81s/it] 98%|█████████▊| 47/48 [02:10<00:02,  2.71s/it]100%|██████████| 48/48 [02:12<00:00,  2.60s/it]                                               {'train_runtime': 133.2395, 'train_samples_per_second': 4.323, 'train_steps_per_second': 0.36, 'train_loss': 0.5007987171411514, 'epoch': 2.0}
100%|██████████| 48/48 [02:13<00:00,  2.60s/it]100%|██████████| 48/48 [02:13<00:00,  2.77s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:10,  2.99it/s]  1%|          | 2/212 [00:00<01:02,  3.38it/s]  1%|▏         | 3/212 [00:00<00:57,  3.61it/s]  2%|▏         | 4/212 [00:01<00:57,  3.62it/s]  2%|▏         | 5/212 [00:01<00:54,  3.81it/s]  3%|▎         | 6/212 [00:01<00:54,  3.78it/s]  3%|▎         | 7/212 [00:01<00:54,  3.76it/s]  4%|▍         | 8/212 [00:02<00:53,  3.81it/s]  4%|▍         | 9/212 [00:02<00:54,  3.74it/s]  5%|▍         | 10/212 [00:02<00:52,  3.87it/s]  5%|▌         | 11/212 [00:02<00:52,  3.85it/s]  6%|▌         | 12/212 [00:03<00:52,  3.79it/s]  6%|▌         | 13/212 [00:03<00:52,  3.82it/s]  7%|▋         | 14/212 [00:03<00:52,  3.76it/s]  7%|▋         | 15/212 [00:03<00:50,  3.87it/s]  8%|▊         | 16/212 [00:04<00:49,  3.95it/s]  8%|▊         | 17/212 [00:04<00:49,  3.93it/s]  8%|▊         | 18/212 [00:04<00:50,  3.84it/s]  9%|▉         | 19/212 [00:05<00:50,  3.83it/s]  9%|▉         | 20/212 [00:05<00:51,  3.75it/s] 10%|▉         | 21/212 [00:05<00:49,  3.88it/s] 10%|█         | 22/212 [00:05<00:48,  3.90it/s] 11%|█         | 23/212 [00:06<00:49,  3.85it/s] 11%|█▏        | 24/212 [00:06<00:49,  3.81it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.79it/s] 12%|█▏        | 26/212 [00:06<00:48,  3.85it/s] 13%|█▎        | 27/212 [00:07<00:47,  3.88it/s] 13%|█▎        | 28/212 [00:07<00:47,  3.84it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.79it/s] 14%|█▍        | 30/212 [00:07<00:48,  3.75it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.83it/s] 15%|█▌        | 32/212 [00:08<00:46,  3.88it/s] 16%|█▌        | 33/212 [00:08<00:46,  3.83it/s] 16%|█▌        | 34/212 [00:08<00:46,  3.80it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.77it/s] 17%|█▋        | 36/212 [00:09<00:45,  3.83it/s] 17%|█▋        | 37/212 [00:09<00:45,  3.89it/s] 18%|█▊        | 38/212 [00:09<00:45,  3.84it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.78it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.74it/s] 19%|█▉        | 41/212 [00:10<00:44,  3.83it/s] 20%|█▉        | 42/212 [00:11<00:43,  3.93it/s] 20%|██        | 43/212 [00:11<00:42,  3.99it/s] 21%|██        | 44/212 [00:11<00:42,  3.93it/s] 21%|██        | 45/212 [00:11<00:43,  3.84it/s] 22%|██▏       | 46/212 [00:12<00:43,  3.86it/s] 22%|██▏       | 47/212 [00:12<00:43,  3.79it/s] 23%|██▎       | 48/212 [00:12<00:42,  3.90it/s] 23%|██▎       | 49/212 [00:12<00:42,  3.87it/s] 24%|██▎       | 50/212 [00:13<00:42,  3.80it/s] 24%|██▍       | 51/212 [00:13<00:42,  3.83it/s] 25%|██▍       | 52/212 [00:13<00:42,  3.77it/s] 25%|██▌       | 53/212 [00:13<00:40,  3.88it/s] 25%|██▌       | 54/212 [00:14<00:41,  3.83it/s] 26%|██▌       | 55/212 [00:14<00:41,  3.79it/s] 26%|██▋       | 56/212 [00:14<00:40,  3.83it/s] 27%|██▋       | 57/212 [00:14<00:41,  3.76it/s] 27%|██▋       | 58/212 [00:15<00:39,  3.87it/s] 28%|██▊       | 59/212 [00:15<00:39,  3.85it/s] 28%|██▊       | 60/212 [00:15<00:40,  3.78it/s] 29%|██▉       | 61/212 [00:15<00:39,  3.81it/s] 29%|██▉       | 62/212 [00:16<00:40,  3.74it/s] 30%|██▉       | 63/212 [00:16<00:38,  3.85it/s] 30%|███       | 64/212 [00:16<00:38,  3.85it/s] 31%|███       | 65/212 [00:17<00:38,  3.78it/s] 31%|███       | 66/212 [00:17<00:38,  3.81it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.75it/s] 32%|███▏      | 68/212 [00:17<00:37,  3.86it/s] 33%|███▎      | 69/212 [00:18<00:37,  3.84it/s] 33%|███▎      | 70/212 [00:18<00:37,  3.77it/s] 33%|███▎      | 71/212 [00:18<00:37,  3.81it/s] 34%|███▍      | 72/212 [00:18<00:37,  3.75it/s] 34%|███▍      | 73/212 [00:19<00:35,  3.87it/s] 35%|███▍      | 74/212 [00:19<00:35,  3.85it/s] 35%|███▌      | 75/212 [00:19<00:36,  3.78it/s] 36%|███▌      | 76/212 [00:19<00:35,  3.81it/s] 36%|███▋      | 77/212 [00:20<00:36,  3.74it/s] 37%|███▋      | 78/212 [00:20<00:34,  3.85it/s] 37%|███▋      | 79/212 [00:20<00:34,  3.84it/s] 38%|███▊      | 80/212 [00:20<00:34,  3.78it/s] 38%|███▊      | 81/212 [00:21<00:34,  3.80it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.74it/s] 39%|███▉      | 83/212 [00:21<00:33,  3.84it/s] 40%|███▉      | 84/212 [00:22<00:33,  3.81it/s] 40%|████      | 85/212 [00:22<00:33,  3.77it/s] 41%|████      | 86/212 [00:22<00:33,  3.80it/s] 41%|████      | 87/212 [00:22<00:33,  3.73it/s] 42%|████▏     | 88/212 [00:23<00:32,  3.84it/s] 42%|████▏     | 89/212 [00:23<00:32,  3.78it/s] 42%|████▏     | 90/212 [00:23<00:32,  3.76it/s] 43%|████▎     | 91/212 [00:23<00:31,  3.80it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.74it/s] 44%|████▍     | 93/212 [00:24<00:30,  3.85it/s] 44%|████▍     | 94/212 [00:24<00:30,  3.82it/s] 45%|████▍     | 95/212 [00:24<00:31,  3.76it/s] 45%|████▌     | 96/212 [00:25<00:30,  3.80it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.73it/s] 46%|████▌     | 98/212 [00:25<00:29,  3.84it/s] 47%|████▋     | 99/212 [00:25<00:29,  3.78it/s] 47%|████▋     | 100/212 [00:26<00:29,  3.75it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.79it/s] 48%|████▊     | 102/212 [00:26<00:29,  3.72it/s] 49%|████▊     | 103/212 [00:27<00:28,  3.83it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.78it/s] 50%|████▉     | 105/212 [00:27<00:28,  3.74it/s] 50%|█████     | 106/212 [00:27<00:28,  3.67it/s] 50%|█████     | 107/212 [00:28<00:27,  3.79it/s] 51%|█████     | 108/212 [00:28<00:27,  3.82it/s] 51%|█████▏    | 109/212 [00:28<00:27,  3.75it/s] 52%|█████▏    | 110/212 [00:28<00:27,  3.76it/s] 52%|█████▏    | 111/212 [00:29<00:27,  3.70it/s] 53%|█████▎    | 112/212 [00:29<00:26,  3.82it/s] 53%|█████▎    | 113/212 [00:29<00:25,  3.84it/s] 54%|█████▍    | 114/212 [00:29<00:25,  3.77it/s] 54%|█████▍    | 115/212 [00:30<00:25,  3.78it/s] 55%|█████▍    | 116/212 [00:30<00:25,  3.72it/s] 55%|█████▌    | 117/212 [00:30<00:24,  3.83it/s] 56%|█████▌    | 118/212 [00:31<00:24,  3.78it/s] 56%|█████▌    | 119/212 [00:31<00:24,  3.76it/s] 57%|█████▋    | 120/212 [00:31<00:24,  3.80it/s] 57%|█████▋    | 121/212 [00:31<00:24,  3.73it/s] 58%|█████▊    | 122/212 [00:32<00:23,  3.83it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.78it/s] 58%|█████▊    | 124/212 [00:32<00:23,  3.74it/s] 59%|█████▉    | 125/212 [00:32<00:23,  3.67it/s] 59%|█████▉    | 126/212 [00:33<00:22,  3.80it/s] 60%|█████▉    | 127/212 [00:33<00:22,  3.83it/s] 60%|██████    | 128/212 [00:33<00:22,  3.76it/s] 61%|██████    | 129/212 [00:33<00:22,  3.76it/s] 61%|██████▏   | 130/212 [00:34<00:22,  3.69it/s] 62%|██████▏   | 131/212 [00:34<00:21,  3.81it/s] 62%|██████▏   | 132/212 [00:34<00:20,  3.83it/s] 63%|██████▎   | 133/212 [00:35<00:21,  3.76it/s] 63%|██████▎   | 134/212 [00:35<00:20,  3.75it/s] 64%|██████▎   | 135/212 [00:35<00:20,  3.69it/s] 64%|██████▍   | 136/212 [00:35<00:19,  3.81it/s] 65%|██████▍   | 137/212 [00:36<00:19,  3.83it/s] 65%|██████▌   | 138/212 [00:36<00:19,  3.76it/s] 66%|██████▌   | 139/212 [00:36<00:19,  3.80it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.73it/s] 67%|██████▋   | 141/212 [00:37<00:18,  3.85it/s] 67%|██████▋   | 142/212 [00:37<00:18,  3.79it/s] 67%|██████▋   | 143/212 [00:37<00:18,  3.74it/s] 68%|██████▊   | 144/212 [00:37<00:18,  3.67it/s] 68%|██████▊   | 145/212 [00:38<00:17,  3.79it/s] 69%|██████▉   | 146/212 [00:38<00:17,  3.82it/s] 69%|██████▉   | 147/212 [00:38<00:17,  3.77it/s] 70%|██████▉   | 148/212 [00:39<00:17,  3.75it/s] 70%|███████   | 149/212 [00:39<00:17,  3.68it/s] 71%|███████   | 150/212 [00:39<00:16,  3.81it/s] 71%|███████   | 151/212 [00:39<00:15,  3.83it/s] 72%|███████▏  | 152/212 [00:40<00:15,  3.76it/s] 72%|███████▏  | 153/212 [00:40<00:15,  3.76it/s] 73%|███████▎  | 154/212 [00:40<00:15,  3.71it/s] 73%|███████▎  | 155/212 [00:40<00:14,  3.82it/s] 74%|███████▎  | 156/212 [00:41<00:14,  3.82it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.75it/s] 75%|███████▍  | 158/212 [00:41<00:14,  3.79it/s] 75%|███████▌  | 159/212 [00:41<00:14,  3.72it/s] 75%|███████▌  | 160/212 [00:42<00:13,  3.83it/s] 76%|███████▌  | 161/212 [00:42<00:13,  3.77it/s] 76%|███████▋  | 162/212 [00:42<00:13,  3.72it/s] 77%|███████▋  | 163/212 [00:43<00:13,  3.66it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.79it/s] 78%|███████▊  | 165/212 [00:43<00:12,  3.82it/s] 78%|███████▊  | 166/212 [00:43<00:12,  3.75it/s] 79%|███████▉  | 167/212 [00:44<00:12,  3.75it/s] 79%|███████▉  | 168/212 [00:44<00:11,  3.67it/s] 80%|███████▉  | 169/212 [00:44<00:11,  3.79it/s] 80%|████████  | 170/212 [00:44<00:11,  3.82it/s] 81%|████████  | 171/212 [00:45<00:10,  3.74it/s] 81%|████████  | 172/212 [00:45<00:10,  3.75it/s] 82%|████████▏ | 173/212 [00:45<00:10,  3.69it/s] 82%|████████▏ | 174/212 [00:45<00:09,  3.82it/s] 83%|████████▎ | 175/212 [00:46<00:09,  3.77it/s] 83%|████████▎ | 176/212 [00:46<00:09,  3.74it/s] 83%|████████▎ | 177/212 [00:46<00:09,  3.78it/s] 84%|████████▍ | 178/212 [00:46<00:09,  3.72it/s] 84%|████████▍ | 179/212 [00:47<00:08,  3.83it/s] 85%|████████▍ | 180/212 [00:47<00:08,  3.77it/s] 85%|████████▌ | 181/212 [00:47<00:08,  3.73it/s] 86%|████████▌ | 182/212 [00:48<00:08,  3.66it/s] 86%|████████▋ | 183/212 [00:48<00:07,  3.79it/s] 87%|████████▋ | 184/212 [00:48<00:07,  3.81it/s] 87%|████████▋ | 185/212 [00:48<00:07,  3.76it/s] 88%|████████▊ | 186/212 [00:49<00:06,  3.73it/s] 88%|████████▊ | 187/212 [00:49<00:06,  3.66it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.78it/s] 89%|████████▉ | 189/212 [00:49<00:06,  3.81it/s] 90%|████████▉ | 190/212 [00:50<00:05,  3.73it/s] 90%|█████████ | 191/212 [00:50<00:05,  3.73it/s] 91%|█████████ | 192/212 [00:50<00:05,  3.66it/s] 91%|█████████ | 193/212 [00:50<00:05,  3.78it/s] 92%|█████████▏| 194/212 [00:51<00:04,  3.79it/s] 92%|█████████▏| 195/212 [00:51<00:04,  3.72it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.76it/s] 93%|█████████▎| 197/212 [00:52<00:04,  3.70it/s] 93%|█████████▎| 198/212 [00:52<00:03,  3.77it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.74it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.71it/s] 95%|█████████▍| 201/212 [00:53<00:03,  3.65it/s] 95%|█████████▌| 202/212 [00:53<00:02,  3.78it/s] 96%|█████████▌| 203/212 [00:53<00:02,  3.79it/s] 96%|█████████▌| 204/212 [00:53<00:02,  3.72it/s] 97%|█████████▋| 205/212 [00:54<00:01,  3.75it/s] 97%|█████████▋| 206/212 [00:54<00:01,  3.68it/s] 98%|█████████▊| 207/212 [00:54<00:01,  3.78it/s] 98%|█████████▊| 208/212 [00:54<00:01,  3.74it/s] 99%|█████████▊| 209/212 [00:55<00:00,  3.70it/s] 99%|█████████▉| 210/212 [00:55<00:00,  3.64it/s]100%|█████████▉| 211/212 [00:55<00:00,  3.77it/s]100%|██████████| 212/212 [00:56<00:00,  3.81it/s]accuracy:  0.8632075471698113
100%|██████████| 212/212 [00:59<00:00,  3.58it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 134
Training dataset size: 288, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.18s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:59:25,591] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 05:59:25,695] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 05:59:27,959] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<02:16,  2.90s/it]  4%|▍         | 2/48 [00:05<02:12,  2.88s/it]  6%|▋         | 3/48 [00:08<02:14,  3.00s/it]  8%|▊         | 4/48 [00:11<02:08,  2.92s/it] 10%|█         | 5/48 [00:14<01:58,  2.76s/it] 12%|█▎        | 6/48 [00:16<01:56,  2.77s/it] 15%|█▍        | 7/48 [00:19<01:54,  2.79s/it] 17%|█▋        | 8/48 [00:22<01:50,  2.76s/it] 19%|█▉        | 9/48 [00:24<01:43,  2.65s/it] 21%|██        | 10/48 [00:27<01:38,  2.60s/it]                                               {'loss': 0.7534, 'grad_norm': 6.690482139587402, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:27<01:38,  2.60s/it] 23%|██▎       | 11/48 [00:30<01:41,  2.75s/it] 25%|██▌       | 12/48 [00:32<01:36,  2.68s/it] 27%|██▋       | 13/48 [00:35<01:35,  2.72s/it] 29%|██▉       | 14/48 [00:38<01:32,  2.71s/it] 31%|███▏      | 15/48 [00:41<01:32,  2.80s/it] 33%|███▎      | 16/48 [00:44<01:29,  2.80s/it] 35%|███▌      | 17/48 [00:46<01:24,  2.71s/it] 38%|███▊      | 18/48 [00:49<01:21,  2.72s/it] 40%|███▉      | 19/48 [00:52<01:18,  2.70s/it] 42%|████▏     | 20/48 [00:54<01:16,  2.72s/it]                                               {'loss': 0.6093, 'grad_norm': 7.569230556488037, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:54<01:16,  2.72s/it] 44%|████▍     | 21/48 [00:57<01:15,  2.80s/it] 46%|████▌     | 22/48 [01:00<01:12,  2.79s/it] 48%|████▊     | 23/48 [01:03<01:06,  2.67s/it] 50%|█████     | 24/48 [01:05<01:02,  2.60s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:08<01:01,  2.67s/it] 54%|█████▍    | 26/48 [01:10<00:56,  2.58s/it] 56%|█████▋    | 27/48 [01:13<00:53,  2.57s/it] 58%|█████▊    | 28/48 [01:15<00:52,  2.60s/it] 60%|██████    | 29/48 [01:18<00:49,  2.60s/it] 62%|██████▎   | 30/48 [01:21<00:49,  2.73s/it]                                               {'loss': 0.6069, 'grad_norm': 5.785052299499512, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:21<00:49,  2.73s/it] 65%|██████▍   | 31/48 [01:24<00:45,  2.68s/it] 67%|██████▋   | 32/48 [01:26<00:41,  2.61s/it] 69%|██████▉   | 33/48 [01:29<00:40,  2.72s/it] 71%|███████   | 34/48 [01:32<00:38,  2.72s/it] 73%|███████▎  | 35/48 [01:35<00:35,  2.73s/it] 75%|███████▌  | 36/48 [01:37<00:31,  2.59s/it] 77%|███████▋  | 37/48 [01:39<00:28,  2.59s/it] 79%|███████▉  | 38/48 [01:42<00:26,  2.61s/it] 81%|████████▏ | 39/48 [01:45<00:25,  2.80s/it] 83%|████████▎ | 40/48 [01:48<00:21,  2.66s/it]                                               {'loss': 0.509, 'grad_norm': 4.435966968536377, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:48<00:21,  2.66s/it] 85%|████████▌ | 41/48 [01:51<00:19,  2.72s/it] 88%|████████▊ | 42/48 [01:53<00:16,  2.70s/it] 90%|████████▉ | 43/48 [01:56<00:13,  2.71s/it] 92%|█████████▏| 44/48 [01:58<00:10,  2.67s/it] 94%|█████████▍| 45/48 [02:01<00:07,  2.55s/it] 96%|█████████▌| 46/48 [02:03<00:04,  2.42s/it] 98%|█████████▊| 47/48 [02:05<00:02,  2.46s/it]100%|██████████| 48/48 [02:08<00:00,  2.51s/it]                                               {'train_runtime': 129.2218, 'train_samples_per_second': 4.457, 'train_steps_per_second': 0.371, 'train_loss': 0.6269106765588125, 'epoch': 2.0}
100%|██████████| 48/48 [02:09<00:00,  2.51s/it]100%|██████████| 48/48 [02:09<00:00,  2.69s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:52,  3.08it/s]  1%|          | 2/163 [00:00<00:49,  3.28it/s]  2%|▏         | 3/163 [00:00<00:45,  3.52it/s]  2%|▏         | 4/163 [00:01<00:44,  3.54it/s]  3%|▎         | 5/163 [00:01<00:42,  3.71it/s]  4%|▎         | 6/163 [00:01<00:42,  3.67it/s]  4%|▍         | 7/163 [00:01<00:42,  3.64it/s]  5%|▍         | 8/163 [00:02<00:42,  3.61it/s]  6%|▌         | 9/163 [00:02<00:40,  3.76it/s]  6%|▌         | 10/163 [00:02<00:40,  3.80it/s]  7%|▋         | 11/163 [00:03<00:41,  3.68it/s]  7%|▋         | 12/163 [00:03<00:41,  3.66it/s]  8%|▊         | 13/163 [00:03<00:39,  3.81it/s]  9%|▊         | 14/163 [00:03<00:38,  3.91it/s]  9%|▉         | 15/163 [00:04<00:37,  3.98it/s] 10%|▉         | 16/163 [00:04<00:36,  4.04it/s] 10%|█         | 17/163 [00:04<00:35,  4.08it/s] 11%|█         | 18/163 [00:04<00:35,  4.11it/s] 12%|█▏        | 19/163 [00:04<00:34,  4.13it/s] 12%|█▏        | 20/163 [00:05<00:34,  4.14it/s] 13%|█▎        | 21/163 [00:05<00:34,  4.15it/s] 13%|█▎        | 22/163 [00:05<00:33,  4.16it/s] 14%|█▍        | 23/163 [00:05<00:33,  4.17it/s] 15%|█▍        | 24/163 [00:06<00:33,  4.17it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.17it/s] 16%|█▌        | 26/163 [00:06<00:32,  4.16it/s] 17%|█▋        | 27/163 [00:06<00:32,  4.16it/s] 17%|█▋        | 28/163 [00:07<00:32,  4.15it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.16it/s] 18%|█▊        | 30/163 [00:07<00:31,  4.16it/s] 19%|█▉        | 31/163 [00:07<00:31,  4.16it/s] 20%|█▉        | 32/163 [00:08<00:31,  4.16it/s] 20%|██        | 33/163 [00:08<00:31,  4.16it/s] 21%|██        | 34/163 [00:08<00:31,  4.16it/s] 21%|██▏       | 35/163 [00:08<00:30,  4.15it/s] 22%|██▏       | 36/163 [00:09<00:30,  4.15it/s] 23%|██▎       | 37/163 [00:09<00:30,  4.14it/s] 23%|██▎       | 38/163 [00:09<00:30,  4.14it/s] 24%|██▍       | 39/163 [00:09<00:29,  4.13it/s] 25%|██▍       | 40/163 [00:10<00:29,  4.14it/s] 25%|██▌       | 41/163 [00:10<00:29,  4.14it/s] 26%|██▌       | 42/163 [00:10<00:29,  4.15it/s] 26%|██▋       | 43/163 [00:10<00:28,  4.15it/s] 27%|██▋       | 44/163 [00:10<00:28,  4.16it/s] 28%|██▊       | 45/163 [00:11<00:28,  4.16it/s] 28%|██▊       | 46/163 [00:11<00:28,  4.16it/s] 29%|██▉       | 47/163 [00:11<00:27,  4.16it/s] 29%|██▉       | 48/163 [00:11<00:27,  4.16it/s] 30%|███       | 49/163 [00:12<00:27,  4.16it/s] 31%|███       | 50/163 [00:12<00:27,  4.16it/s] 31%|███▏      | 51/163 [00:12<00:26,  4.16it/s] 32%|███▏      | 52/163 [00:12<00:26,  4.16it/s] 33%|███▎      | 53/163 [00:13<00:26,  4.16it/s] 33%|███▎      | 54/163 [00:13<00:26,  4.16it/s] 34%|███▎      | 55/163 [00:13<00:26,  4.15it/s] 34%|███▍      | 56/163 [00:13<00:25,  4.14it/s] 35%|███▍      | 57/163 [00:14<00:25,  4.14it/s] 36%|███▌      | 58/163 [00:14<00:25,  4.13it/s] 36%|███▌      | 59/163 [00:14<00:25,  4.12it/s] 37%|███▋      | 60/163 [00:14<00:24,  4.13it/s] 37%|███▋      | 61/163 [00:15<00:24,  4.13it/s] 38%|███▊      | 62/163 [00:15<00:24,  4.14it/s] 39%|███▊      | 63/163 [00:15<00:24,  4.14it/s] 39%|███▉      | 64/163 [00:15<00:23,  4.14it/s] 40%|███▉      | 65/163 [00:16<00:23,  4.14it/s] 40%|████      | 66/163 [00:16<00:23,  4.13it/s] 41%|████      | 67/163 [00:16<00:23,  4.12it/s] 42%|████▏     | 68/163 [00:16<00:23,  4.12it/s] 42%|████▏     | 69/163 [00:17<00:22,  4.12it/s] 43%|████▎     | 70/163 [00:17<00:22,  4.12it/s] 44%|████▎     | 71/163 [00:17<00:22,  4.13it/s] 44%|████▍     | 72/163 [00:17<00:22,  4.13it/s] 45%|████▍     | 73/163 [00:18<00:21,  4.13it/s] 45%|████▌     | 74/163 [00:18<00:21,  4.13it/s] 46%|████▌     | 75/163 [00:18<00:21,  4.12it/s] 47%|████▋     | 76/163 [00:18<00:21,  4.12it/s] 47%|████▋     | 77/163 [00:18<00:20,  4.12it/s] 48%|████▊     | 78/163 [00:19<00:20,  4.13it/s] 48%|████▊     | 79/163 [00:19<00:20,  4.14it/s] 49%|████▉     | 80/163 [00:19<00:20,  4.14it/s] 50%|████▉     | 81/163 [00:19<00:20,  4.10it/s] 50%|█████     | 82/163 [00:20<00:20,  3.96it/s] 51%|█████     | 83/163 [00:20<00:20,  3.85it/s] 52%|█████▏    | 84/163 [00:20<00:20,  3.93it/s] 52%|█████▏    | 85/163 [00:20<00:19,  3.91it/s] 53%|█████▎    | 86/163 [00:21<00:20,  3.74it/s] 53%|█████▎    | 87/163 [00:21<00:20,  3.76it/s] 54%|█████▍    | 88/163 [00:21<00:20,  3.70it/s] 55%|█████▍    | 89/163 [00:22<00:19,  3.79it/s] 55%|█████▌    | 90/163 [00:22<00:19,  3.72it/s] 56%|█████▌    | 91/163 [00:22<00:19,  3.66it/s] 56%|█████▋    | 92/163 [00:22<00:19,  3.61it/s] 57%|█████▋    | 93/163 [00:23<00:18,  3.75it/s] 58%|█████▊    | 94/163 [00:23<00:18,  3.78it/s] 58%|█████▊    | 95/163 [00:23<00:18,  3.66it/s] 59%|█████▉    | 96/163 [00:23<00:18,  3.70it/s] 60%|█████▉    | 97/163 [00:24<00:18,  3.66it/s] 60%|██████    | 98/163 [00:24<00:17,  3.75it/s] 61%|██████    | 99/163 [00:24<00:17,  3.68it/s] 61%|██████▏   | 100/163 [00:25<00:17,  3.63it/s] 62%|██████▏   | 101/163 [00:25<00:17,  3.59it/s] 63%|██████▎   | 102/163 [00:25<00:16,  3.74it/s] 63%|██████▎   | 103/163 [00:25<00:15,  3.77it/s] 64%|██████▍   | 104/163 [00:26<00:16,  3.65it/s] 64%|██████▍   | 105/163 [00:26<00:15,  3.70it/s] 65%|██████▌   | 106/163 [00:26<00:15,  3.66it/s] 66%|██████▌   | 107/163 [00:26<00:14,  3.77it/s] 66%|██████▋   | 108/163 [00:27<00:14,  3.70it/s] 67%|██████▋   | 109/163 [00:27<00:14,  3.65it/s] 67%|██████▋   | 110/163 [00:27<00:14,  3.60it/s] 68%|██████▊   | 111/163 [00:28<00:13,  3.75it/s] 69%|██████▊   | 112/163 [00:28<00:13,  3.78it/s] 69%|██████▉   | 113/163 [00:28<00:13,  3.66it/s] 70%|██████▉   | 114/163 [00:28<00:13,  3.70it/s] 71%|███████   | 115/163 [00:29<00:13,  3.65it/s] 71%|███████   | 116/163 [00:29<00:12,  3.76it/s] 72%|███████▏  | 117/163 [00:29<00:12,  3.69it/s] 72%|███████▏  | 118/163 [00:29<00:12,  3.64it/s] 73%|███████▎  | 119/163 [00:30<00:12,  3.59it/s] 74%|███████▎  | 120/163 [00:30<00:11,  3.74it/s] 74%|███████▍  | 121/163 [00:30<00:11,  3.76it/s] 75%|███████▍  | 122/163 [00:31<00:11,  3.64it/s] 75%|███████▌  | 123/163 [00:31<00:10,  3.69it/s] 76%|███████▌  | 124/163 [00:31<00:10,  3.66it/s] 77%|███████▋  | 125/163 [00:31<00:10,  3.79it/s] 77%|███████▋  | 126/163 [00:32<00:09,  3.70it/s] 78%|███████▊  | 127/163 [00:32<00:09,  3.64it/s] 79%|███████▊  | 128/163 [00:32<00:09,  3.59it/s] 79%|███████▉  | 129/163 [00:32<00:09,  3.74it/s] 80%|███████▉  | 130/163 [00:33<00:08,  3.78it/s] 80%|████████  | 131/163 [00:33<00:08,  3.70it/s] 81%|████████  | 132/163 [00:33<00:08,  3.65it/s] 82%|████████▏ | 133/163 [00:34<00:08,  3.60it/s] 82%|████████▏ | 134/163 [00:34<00:07,  3.74it/s] 83%|████████▎ | 135/163 [00:34<00:07,  3.77it/s] 83%|████████▎ | 136/163 [00:34<00:07,  3.65it/s] 84%|████████▍ | 137/163 [00:35<00:07,  3.69it/s] 85%|████████▍ | 138/163 [00:35<00:06,  3.65it/s] 85%|████████▌ | 139/163 [00:35<00:06,  3.76it/s] 86%|████████▌ | 140/163 [00:35<00:06,  3.69it/s] 87%|████████▋ | 141/163 [00:36<00:06,  3.64it/s] 87%|████████▋ | 142/163 [00:36<00:05,  3.59it/s] 88%|████████▊ | 143/163 [00:36<00:05,  3.73it/s] 88%|████████▊ | 144/163 [00:36<00:05,  3.76it/s] 89%|████████▉ | 145/163 [00:37<00:04,  3.64it/s] 90%|████████▉ | 146/163 [00:37<00:04,  3.63it/s] 90%|█████████ | 147/163 [00:37<00:04,  3.76it/s] 91%|█████████ | 148/163 [00:38<00:03,  3.86it/s] 91%|█████████▏| 149/163 [00:38<00:03,  3.93it/s] 92%|█████████▏| 150/163 [00:38<00:03,  3.98it/s] 93%|█████████▎| 151/163 [00:38<00:02,  4.00it/s] 93%|█████████▎| 152/163 [00:39<00:02,  4.04it/s] 94%|█████████▍| 153/163 [00:39<00:02,  4.06it/s] 94%|█████████▍| 154/163 [00:39<00:02,  4.08it/s] 95%|█████████▌| 155/163 [00:39<00:01,  4.08it/s] 96%|█████████▌| 156/163 [00:40<00:01,  4.08it/s] 96%|█████████▋| 157/163 [00:40<00:01,  4.09it/s] 97%|█████████▋| 158/163 [00:40<00:01,  4.10it/s] 98%|█████████▊| 159/163 [00:40<00:00,  4.10it/s] 98%|█████████▊| 160/163 [00:40<00:00,  4.09it/s] 99%|█████████▉| 161/163 [00:41<00:00,  4.09it/s] 99%|█████████▉| 162/163 [00:41<00:00,  4.10it/s]100%|██████████| 163/163 [00:41<00:00,  4.11it/s]accuracy:  0.803680981595092
100%|██████████| 163/163 [00:44<00:00,  3.68it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 171
Training dataset size: 288, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:02:47,169] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 06:02:47,229] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:02:49,076] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:03<02:25,  3.11s/it]  4%|▍         | 2/48 [00:05<02:09,  2.81s/it]  6%|▋         | 3/48 [00:07<01:53,  2.51s/it]  8%|▊         | 4/48 [00:10<01:48,  2.47s/it] 10%|█         | 5/48 [00:12<01:41,  2.36s/it] 12%|█▎        | 6/48 [00:15<01:43,  2.46s/it] 15%|█▍        | 7/48 [00:17<01:41,  2.47s/it] 17%|█▋        | 8/48 [00:20<01:41,  2.54s/it] 19%|█▉        | 9/48 [00:22<01:40,  2.57s/it] 21%|██        | 10/48 [00:25<01:40,  2.63s/it]                                               {'loss': 0.4983, 'grad_norm': 3.1983656883239746, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:25<01:40,  2.63s/it] 23%|██▎       | 11/48 [00:28<01:42,  2.77s/it] 25%|██▌       | 12/48 [00:31<01:39,  2.77s/it] 27%|██▋       | 13/48 [00:33<01:31,  2.62s/it] 29%|██▉       | 14/48 [00:36<01:33,  2.74s/it] 31%|███▏      | 15/48 [00:39<01:27,  2.64s/it] 33%|███▎      | 16/48 [00:41<01:24,  2.63s/it] 35%|███▌      | 17/48 [00:44<01:18,  2.52s/it] 38%|███▊      | 18/48 [00:46<01:13,  2.44s/it] 40%|███▉      | 19/48 [00:49<01:16,  2.64s/it] 42%|████▏     | 20/48 [00:52<01:15,  2.71s/it]                                               {'loss': 0.6155, 'grad_norm': 6.489279270172119, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:52<01:15,  2.71s/it] 44%|████▍     | 21/48 [00:55<01:16,  2.84s/it] 46%|████▌     | 22/48 [00:57<01:11,  2.74s/it] 48%|████▊     | 23/48 [01:00<01:05,  2.63s/it] 50%|█████     | 24/48 [01:02<01:01,  2.54s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:05<01:00,  2.61s/it] 54%|█████▍    | 26/48 [01:08<00:59,  2.71s/it] 56%|█████▋    | 27/48 [01:11<00:57,  2.73s/it] 58%|█████▊    | 28/48 [01:13<00:52,  2.63s/it] 60%|██████    | 29/48 [01:16<00:48,  2.57s/it] 62%|██████▎   | 30/48 [01:18<00:45,  2.55s/it]                                               {'loss': 0.546, 'grad_norm': 1.6957030296325684, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:18<00:45,  2.55s/it] 65%|██████▍   | 31/48 [01:20<00:42,  2.52s/it] 67%|██████▋   | 32/48 [01:23<00:40,  2.52s/it] 69%|██████▉   | 33/48 [01:25<00:36,  2.46s/it] 71%|███████   | 34/48 [01:28<00:35,  2.53s/it] 73%|███████▎  | 35/48 [01:30<00:32,  2.50s/it] 75%|███████▌  | 36/48 [01:33<00:30,  2.58s/it] 77%|███████▋  | 37/48 [01:36<00:30,  2.74s/it] 79%|███████▉  | 38/48 [01:39<00:26,  2.63s/it] 81%|████████▏ | 39/48 [01:42<00:25,  2.79s/it] 83%|████████▎ | 40/48 [01:44<00:20,  2.54s/it]                                               {'loss': 0.4592, 'grad_norm': 3.6430857181549072, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:44<00:20,  2.54s/it] 85%|████████▌ | 41/48 [01:47<00:18,  2.61s/it] 88%|████████▊ | 42/48 [01:49<00:15,  2.61s/it] 90%|████████▉ | 43/48 [01:52<00:13,  2.71s/it] 92%|█████████▏| 44/48 [01:55<00:10,  2.69s/it] 94%|█████████▍| 45/48 [01:57<00:07,  2.67s/it] 96%|█████████▌| 46/48 [02:00<00:05,  2.70s/it] 98%|█████████▊| 47/48 [02:03<00:02,  2.67s/it]100%|██████████| 48/48 [02:06<00:00,  2.71s/it]                                               {'train_runtime': 126.7479, 'train_samples_per_second': 4.544, 'train_steps_per_second': 0.379, 'train_loss': 0.4935476283232371, 'epoch': 2.0}
100%|██████████| 48/48 [02:06<00:00,  2.71s/it]100%|██████████| 48/48 [02:06<00:00,  2.64s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:12,  2.97it/s]  1%|          | 2/217 [00:00<01:00,  3.55it/s]  1%|▏         | 3/217 [00:00<00:59,  3.62it/s]  2%|▏         | 4/217 [00:01<00:58,  3.63it/s]  2%|▏         | 5/217 [00:01<00:58,  3.61it/s]  3%|▎         | 6/217 [00:01<00:55,  3.78it/s]  3%|▎         | 7/217 [00:01<00:54,  3.82it/s]  4%|▎         | 8/217 [00:02<00:55,  3.77it/s]  4%|▍         | 9/217 [00:02<00:54,  3.79it/s]  5%|▍         | 10/217 [00:02<00:55,  3.73it/s]  5%|▌         | 11/217 [00:02<00:53,  3.85it/s]  6%|▌         | 12/217 [00:03<00:54,  3.79it/s]  6%|▌         | 13/217 [00:03<00:54,  3.75it/s]  6%|▋         | 14/217 [00:03<00:55,  3.68it/s]  7%|▋         | 15/217 [00:04<00:52,  3.81it/s]  7%|▋         | 16/217 [00:04<00:52,  3.85it/s]  8%|▊         | 17/217 [00:04<00:52,  3.79it/s]  8%|▊         | 18/217 [00:04<00:52,  3.78it/s]  9%|▉         | 19/217 [00:05<00:53,  3.71it/s]  9%|▉         | 20/217 [00:05<00:51,  3.85it/s] 10%|▉         | 21/217 [00:05<00:50,  3.87it/s] 10%|█         | 22/217 [00:05<00:51,  3.80it/s] 11%|█         | 23/217 [00:06<00:50,  3.81it/s] 11%|█         | 24/217 [00:06<00:51,  3.73it/s] 12%|█▏        | 25/217 [00:06<00:49,  3.84it/s] 12%|█▏        | 26/217 [00:06<00:50,  3.78it/s] 12%|█▏        | 27/217 [00:07<00:50,  3.75it/s] 13%|█▎        | 28/217 [00:07<00:51,  3.70it/s] 13%|█▎        | 29/217 [00:07<00:49,  3.83it/s] 14%|█▍        | 30/217 [00:07<00:48,  3.87it/s] 14%|█▍        | 31/217 [00:08<00:49,  3.79it/s] 15%|█▍        | 32/217 [00:08<00:48,  3.79it/s] 15%|█▌        | 33/217 [00:08<00:49,  3.72it/s] 16%|█▌        | 34/217 [00:09<00:47,  3.84it/s] 16%|█▌        | 35/217 [00:09<00:47,  3.85it/s] 17%|█▋        | 36/217 [00:09<00:48,  3.77it/s] 17%|█▋        | 37/217 [00:09<00:47,  3.81it/s] 18%|█▊        | 38/217 [00:10<00:47,  3.74it/s] 18%|█▊        | 39/217 [00:10<00:46,  3.85it/s] 18%|█▊        | 40/217 [00:10<00:46,  3.79it/s] 19%|█▉        | 41/217 [00:10<00:46,  3.76it/s] 19%|█▉        | 42/217 [00:11<00:47,  3.69it/s] 20%|█▉        | 43/217 [00:11<00:45,  3.81it/s] 20%|██        | 44/217 [00:11<00:45,  3.84it/s] 21%|██        | 45/217 [00:11<00:45,  3.78it/s] 21%|██        | 46/217 [00:12<00:45,  3.77it/s] 22%|██▏       | 47/217 [00:12<00:45,  3.72it/s] 22%|██▏       | 48/217 [00:12<00:44,  3.83it/s] 23%|██▎       | 49/217 [00:12<00:44,  3.82it/s] 23%|██▎       | 50/217 [00:13<00:44,  3.76it/s] 24%|██▎       | 51/217 [00:13<00:43,  3.80it/s] 24%|██▍       | 52/217 [00:13<00:44,  3.73it/s] 24%|██▍       | 53/217 [00:14<00:42,  3.85it/s] 25%|██▍       | 54/217 [00:14<00:43,  3.79it/s] 25%|██▌       | 55/217 [00:14<00:43,  3.76it/s] 26%|██▌       | 56/217 [00:14<00:43,  3.69it/s] 26%|██▋       | 57/217 [00:15<00:41,  3.82it/s] 27%|██▋       | 58/217 [00:15<00:41,  3.85it/s] 27%|██▋       | 59/217 [00:15<00:41,  3.77it/s] 28%|██▊       | 60/217 [00:15<00:41,  3.77it/s] 28%|██▊       | 61/217 [00:16<00:41,  3.72it/s] 29%|██▊       | 62/217 [00:16<00:40,  3.84it/s] 29%|██▉       | 63/217 [00:16<00:40,  3.85it/s] 29%|██▉       | 64/217 [00:16<00:40,  3.77it/s] 30%|██▉       | 65/217 [00:17<00:39,  3.80it/s] 30%|███       | 66/217 [00:17<00:40,  3.74it/s] 31%|███       | 67/217 [00:17<00:39,  3.84it/s] 31%|███▏      | 68/217 [00:18<00:39,  3.78it/s] 32%|███▏      | 69/217 [00:18<00:39,  3.74it/s] 32%|███▏      | 70/217 [00:18<00:39,  3.68it/s] 33%|███▎      | 71/217 [00:18<00:38,  3.81it/s] 33%|███▎      | 72/217 [00:19<00:37,  3.84it/s] 34%|███▎      | 73/217 [00:19<00:38,  3.77it/s] 34%|███▍      | 74/217 [00:19<00:37,  3.77it/s] 35%|███▍      | 75/217 [00:19<00:38,  3.71it/s] 35%|███▌      | 76/217 [00:20<00:36,  3.83it/s] 35%|███▌      | 77/217 [00:20<00:36,  3.85it/s] 36%|███▌      | 78/217 [00:20<00:36,  3.77it/s] 36%|███▋      | 79/217 [00:20<00:36,  3.80it/s] 37%|███▋      | 80/217 [00:21<00:36,  3.73it/s] 37%|███▋      | 81/217 [00:21<00:35,  3.82it/s] 38%|███▊      | 82/217 [00:21<00:35,  3.77it/s] 38%|███▊      | 83/217 [00:21<00:35,  3.73it/s] 39%|███▊      | 84/217 [00:22<00:36,  3.67it/s] 39%|███▉      | 85/217 [00:22<00:34,  3.79it/s] 40%|███▉      | 86/217 [00:22<00:34,  3.81it/s] 40%|████      | 87/217 [00:23<00:34,  3.75it/s] 41%|████      | 88/217 [00:23<00:34,  3.74it/s] 41%|████      | 89/217 [00:23<00:34,  3.70it/s] 41%|████▏     | 90/217 [00:23<00:33,  3.82it/s] 42%|████▏     | 91/217 [00:24<00:33,  3.81it/s] 42%|████▏     | 92/217 [00:24<00:33,  3.76it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.75it/s] 43%|████▎     | 94/217 [00:24<00:32,  3.74it/s] 44%|████▍     | 95/217 [00:25<00:31,  3.81it/s] 44%|████▍     | 96/217 [00:25<00:32,  3.76it/s] 45%|████▍     | 97/217 [00:25<00:32,  3.74it/s] 45%|████▌     | 98/217 [00:26<00:32,  3.68it/s] 46%|████▌     | 99/217 [00:26<00:31,  3.80it/s] 46%|████▌     | 100/217 [00:26<00:30,  3.82it/s] 47%|████▋     | 101/217 [00:26<00:30,  3.75it/s] 47%|████▋     | 102/217 [00:27<00:30,  3.76it/s] 47%|████▋     | 103/217 [00:27<00:30,  3.71it/s] 48%|████▊     | 104/217 [00:27<00:29,  3.83it/s] 48%|████▊     | 105/217 [00:27<00:29,  3.77it/s] 49%|████▉     | 106/217 [00:28<00:29,  3.75it/s] 49%|████▉     | 107/217 [00:28<00:29,  3.79it/s] 50%|████▉     | 108/217 [00:28<00:29,  3.73it/s] 50%|█████     | 109/217 [00:28<00:28,  3.81it/s] 51%|█████     | 110/217 [00:29<00:28,  3.77it/s] 51%|█████     | 111/217 [00:29<00:28,  3.73it/s] 52%|█████▏    | 112/217 [00:29<00:28,  3.66it/s] 52%|█████▏    | 113/217 [00:29<00:27,  3.80it/s] 53%|█████▎    | 114/217 [00:30<00:26,  3.82it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.75it/s] 53%|█████▎    | 116/217 [00:30<00:27,  3.74it/s] 54%|█████▍    | 117/217 [00:31<00:27,  3.70it/s] 54%|█████▍    | 118/217 [00:31<00:25,  3.82it/s] 55%|█████▍    | 119/217 [00:31<00:25,  3.81it/s] 55%|█████▌    | 120/217 [00:31<00:25,  3.74it/s] 56%|█████▌    | 121/217 [00:32<00:25,  3.78it/s] 56%|█████▌    | 122/217 [00:32<00:25,  3.73it/s] 57%|█████▋    | 123/217 [00:32<00:24,  3.83it/s] 57%|█████▋    | 124/217 [00:32<00:24,  3.77it/s] 58%|█████▊    | 125/217 [00:33<00:24,  3.73it/s] 58%|█████▊    | 126/217 [00:33<00:24,  3.67it/s] 59%|█████▊    | 127/217 [00:33<00:23,  3.80it/s] 59%|█████▉    | 128/217 [00:33<00:23,  3.82it/s] 59%|█████▉    | 129/217 [00:34<00:23,  3.75it/s] 60%|█████▉    | 130/217 [00:34<00:23,  3.73it/s] 60%|██████    | 131/217 [00:34<00:23,  3.69it/s] 61%|██████    | 132/217 [00:35<00:22,  3.80it/s] 61%|██████▏   | 133/217 [00:35<00:22,  3.79it/s] 62%|██████▏   | 134/217 [00:35<00:22,  3.73it/s] 62%|██████▏   | 135/217 [00:35<00:22,  3.72it/s] 63%|██████▎   | 136/217 [00:36<00:21,  3.72it/s] 63%|██████▎   | 137/217 [00:36<00:21,  3.80it/s] 64%|██████▎   | 138/217 [00:36<00:21,  3.76it/s] 64%|██████▍   | 139/217 [00:36<00:20,  3.72it/s] 65%|██████▍   | 140/217 [00:37<00:21,  3.66it/s] 65%|██████▍   | 141/217 [00:37<00:20,  3.79it/s] 65%|██████▌   | 142/217 [00:37<00:19,  3.81it/s] 66%|██████▌   | 143/217 [00:37<00:19,  3.74it/s] 66%|██████▋   | 144/217 [00:38<00:19,  3.75it/s] 67%|██████▋   | 145/217 [00:38<00:19,  3.70it/s] 67%|██████▋   | 146/217 [00:38<00:18,  3.82it/s] 68%|██████▊   | 147/217 [00:39<00:18,  3.77it/s] 68%|██████▊   | 148/217 [00:39<00:18,  3.74it/s] 69%|██████▊   | 149/217 [00:39<00:18,  3.77it/s] 69%|██████▉   | 150/217 [00:39<00:18,  3.71it/s] 70%|██████▉   | 151/217 [00:40<00:17,  3.79it/s] 70%|███████   | 152/217 [00:40<00:17,  3.74it/s] 71%|███████   | 153/217 [00:40<00:17,  3.71it/s] 71%|███████   | 154/217 [00:40<00:17,  3.65it/s] 71%|███████▏  | 155/217 [00:41<00:16,  3.77it/s] 72%|███████▏  | 156/217 [00:41<00:16,  3.80it/s] 72%|███████▏  | 157/217 [00:41<00:16,  3.73it/s] 73%|███████▎  | 158/217 [00:41<00:15,  3.73it/s] 73%|███████▎  | 159/217 [00:42<00:15,  3.68it/s] 74%|███████▎  | 160/217 [00:42<00:15,  3.79it/s] 74%|███████▍  | 161/217 [00:42<00:14,  3.80it/s] 75%|███████▍  | 162/217 [00:43<00:14,  3.72it/s] 75%|███████▌  | 163/217 [00:43<00:14,  3.76it/s] 76%|███████▌  | 164/217 [00:43<00:14,  3.69it/s] 76%|███████▌  | 165/217 [00:43<00:13,  3.80it/s] 76%|███████▋  | 166/217 [00:44<00:13,  3.75it/s] 77%|███████▋  | 167/217 [00:44<00:13,  3.71it/s] 77%|███████▋  | 168/217 [00:44<00:13,  3.65it/s] 78%|███████▊  | 169/217 [00:44<00:12,  3.78it/s] 78%|███████▊  | 170/217 [00:45<00:12,  3.81it/s] 79%|███████▉  | 171/217 [00:45<00:12,  3.73it/s] 79%|███████▉  | 172/217 [00:45<00:12,  3.74it/s] 80%|███████▉  | 173/217 [00:45<00:11,  3.68it/s] 80%|████████  | 174/217 [00:46<00:11,  3.80it/s] 81%|████████  | 175/217 [00:46<00:11,  3.81it/s] 81%|████████  | 176/217 [00:46<00:10,  3.73it/s] 82%|████████▏ | 177/217 [00:47<00:10,  3.74it/s] 82%|████████▏ | 178/217 [00:47<00:10,  3.69it/s] 82%|████████▏ | 179/217 [00:47<00:09,  3.81it/s] 83%|████████▎ | 180/217 [00:47<00:09,  3.75it/s] 83%|████████▎ | 181/217 [00:48<00:09,  3.71it/s] 84%|████████▍ | 182/217 [00:48<00:09,  3.69it/s] 84%|████████▍ | 183/217 [00:48<00:09,  3.75it/s] 85%|████████▍ | 184/217 [00:48<00:08,  3.82it/s] 85%|████████▌ | 185/217 [00:49<00:08,  3.76it/s] 86%|████████▌ | 186/217 [00:49<00:08,  3.72it/s] 86%|████████▌ | 187/217 [00:49<00:08,  3.65it/s] 87%|████████▋ | 188/217 [00:49<00:07,  3.77it/s] 87%|████████▋ | 189/217 [00:50<00:07,  3.80it/s] 88%|████████▊ | 190/217 [00:50<00:07,  3.72it/s] 88%|████████▊ | 191/217 [00:50<00:06,  3.72it/s] 88%|████████▊ | 192/217 [00:51<00:06,  3.66it/s] 89%|████████▉ | 193/217 [00:51<00:06,  3.77it/s] 89%|████████▉ | 194/217 [00:51<00:06,  3.80it/s] 90%|████████▉ | 195/217 [00:51<00:05,  3.71it/s] 90%|█████████ | 196/217 [00:52<00:05,  3.72it/s] 91%|█████████ | 197/217 [00:52<00:05,  3.66it/s] 91%|█████████ | 198/217 [00:52<00:05,  3.77it/s] 92%|█████████▏| 199/217 [00:52<00:04,  3.78it/s] 92%|█████████▏| 200/217 [00:53<00:04,  3.71it/s] 93%|█████████▎| 201/217 [00:53<00:04,  3.75it/s] 93%|█████████▎| 202/217 [00:53<00:04,  3.69it/s] 94%|█████████▎| 203/217 [00:53<00:03,  3.80it/s] 94%|█████████▍| 204/217 [00:54<00:03,  3.74it/s] 94%|█████████▍| 205/217 [00:54<00:03,  3.71it/s] 95%|█████████▍| 206/217 [00:54<00:03,  3.65it/s] 95%|█████████▌| 207/217 [00:55<00:02,  3.78it/s] 96%|█████████▌| 208/217 [00:55<00:02,  3.80it/s] 96%|█████████▋| 209/217 [00:55<00:02,  3.73it/s] 97%|█████████▋| 210/217 [00:55<00:01,  3.72it/s] 97%|█████████▋| 211/217 [00:56<00:01,  3.66it/s] 98%|█████████▊| 212/217 [00:56<00:01,  3.78it/s] 98%|█████████▊| 213/217 [00:56<00:01,  3.80it/s] 99%|█████████▊| 214/217 [00:56<00:00,  3.74it/s] 99%|█████████▉| 215/217 [00:57<00:00,  3.76it/s]100%|█████████▉| 216/217 [00:57<00:00,  3.70it/s]100%|██████████| 217/217 [00:57<00:00,  3.82it/s]accuracy:  0.815668202764977
100%|██████████| 217/217 [01:01<00:00,  3.55it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 103
Training dataset size: 288, validation dataset size: 103
Training dataset size: 288, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.63s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:06:26,300] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:06:27,172] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 06:06:27,281] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<02:09,  2.76s/it]  4%|▍         | 2/48 [00:05<01:57,  2.55s/it]  6%|▋         | 3/48 [00:07<01:49,  2.43s/it]  8%|▊         | 4/48 [00:09<01:42,  2.33s/it] 10%|█         | 5/48 [00:11<01:35,  2.23s/it] 12%|█▎        | 6/48 [00:14<01:41,  2.42s/it] 15%|█▍        | 7/48 [00:16<01:37,  2.37s/it] 17%|█▋        | 8/48 [00:18<01:28,  2.21s/it] 19%|█▉        | 9/48 [00:20<01:25,  2.20s/it] 21%|██        | 10/48 [00:22<01:22,  2.18s/it]                                               {'loss': 0.9748, 'grad_norm': 11.9567289352417, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:22<01:22,  2.18s/it] 23%|██▎       | 11/48 [00:24<01:16,  2.08s/it] 25%|██▌       | 12/48 [00:26<01:13,  2.03s/it] 27%|██▋       | 13/48 [00:28<01:13,  2.09s/it] 29%|██▉       | 14/48 [00:31<01:14,  2.18s/it] 31%|███▏      | 15/48 [00:33<01:16,  2.32s/it] 33%|███▎      | 16/48 [00:35<01:08,  2.14s/it] 35%|███▌      | 17/48 [00:37<01:07,  2.17s/it] 38%|███▊      | 18/48 [00:40<01:09,  2.31s/it] 40%|███▉      | 19/48 [00:42<01:06,  2.28s/it] 42%|████▏     | 20/48 [00:44<01:03,  2.26s/it]                                               {'loss': 0.7473, 'grad_norm': 6.890269756317139, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:44<01:03,  2.26s/it] 44%|████▍     | 21/48 [00:46<00:59,  2.19s/it] 46%|████▌     | 22/48 [00:49<01:02,  2.39s/it] 48%|████▊     | 23/48 [00:51<00:58,  2.32s/it] 50%|█████     | 24/48 [00:54<00:55,  2.32s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [00:57<00:56,  2.46s/it] 54%|█████▍    | 26/48 [00:59<00:52,  2.40s/it] 56%|█████▋    | 27/48 [01:01<00:47,  2.24s/it] 58%|█████▊    | 28/48 [01:03<00:43,  2.18s/it] 60%|██████    | 29/48 [01:06<00:45,  2.38s/it] 62%|██████▎   | 30/48 [01:08<00:40,  2.27s/it]                                               {'loss': 0.5618, 'grad_norm': 5.066120147705078, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:08<00:40,  2.27s/it] 65%|██████▍   | 31/48 [01:10<00:39,  2.30s/it] 67%|██████▋   | 32/48 [01:12<00:37,  2.32s/it] 69%|██████▉   | 33/48 [01:15<00:34,  2.28s/it] 71%|███████   | 34/48 [01:17<00:31,  2.23s/it] 73%|███████▎  | 35/48 [01:19<00:29,  2.24s/it] 75%|███████▌  | 36/48 [01:21<00:26,  2.24s/it] 77%|███████▋  | 37/48 [01:24<00:25,  2.31s/it] 79%|███████▉  | 38/48 [01:26<00:23,  2.35s/it] 81%|████████▏ | 39/48 [01:29<00:21,  2.41s/it] 83%|████████▎ | 40/48 [01:31<00:19,  2.41s/it]                                               {'loss': 0.5741, 'grad_norm': 4.58115816116333, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:31<00:19,  2.41s/it] 85%|████████▌ | 41/48 [01:33<00:15,  2.16s/it] 88%|████████▊ | 42/48 [01:35<00:13,  2.17s/it] 90%|████████▉ | 43/48 [01:37<00:10,  2.08s/it] 92%|█████████▏| 44/48 [01:39<00:08,  2.14s/it] 94%|█████████▍| 45/48 [01:41<00:06,  2.17s/it] 96%|█████████▌| 46/48 [01:44<00:04,  2.28s/it] 98%|█████████▊| 47/48 [01:46<00:02,  2.42s/it]100%|██████████| 48/48 [01:48<00:00,  2.27s/it]                                               {'train_runtime': 109.558, 'train_samples_per_second': 5.257, 'train_steps_per_second': 0.438, 'train_loss': 0.6887233455975851, 'epoch': 2.0}
100%|██████████| 48/48 [01:49<00:00,  2.27s/it]100%|██████████| 48/48 [01:49<00:00,  2.28s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:58,  2.55it/s]  1%|▏         | 2/150 [00:00<00:47,  3.14it/s]  2%|▏         | 3/150 [00:00<00:43,  3.39it/s]  3%|▎         | 4/150 [00:01<00:41,  3.48it/s]  3%|▎         | 5/150 [00:01<00:39,  3.67it/s]  4%|▍         | 6/150 [00:01<00:39,  3.68it/s]  5%|▍         | 7/150 [00:01<00:38,  3.67it/s]  5%|▌         | 8/150 [00:02<00:37,  3.76it/s]  6%|▌         | 9/150 [00:02<00:37,  3.80it/s]  7%|▋         | 10/150 [00:02<00:37,  3.77it/s]  7%|▋         | 11/150 [00:03<00:37,  3.69it/s]  8%|▊         | 12/150 [00:03<00:36,  3.82it/s]  9%|▊         | 13/150 [00:03<00:36,  3.78it/s]  9%|▉         | 14/150 [00:03<00:35,  3.78it/s] 10%|█         | 15/150 [00:04<00:36,  3.73it/s] 11%|█         | 16/150 [00:04<00:34,  3.83it/s] 11%|█▏        | 17/150 [00:04<00:35,  3.78it/s] 12%|█▏        | 18/150 [00:04<00:34,  3.80it/s] 13%|█▎        | 19/150 [00:05<00:34,  3.77it/s] 13%|█▎        | 20/150 [00:05<00:34,  3.81it/s] 14%|█▍        | 21/150 [00:05<00:34,  3.77it/s] 15%|█▍        | 22/150 [00:05<00:34,  3.70it/s] 15%|█▌        | 23/150 [00:06<00:33,  3.83it/s] 16%|█▌        | 24/150 [00:06<00:33,  3.80it/s] 17%|█▋        | 25/150 [00:06<00:33,  3.78it/s] 17%|█▋        | 26/150 [00:07<00:33,  3.73it/s] 18%|█▊        | 27/150 [00:07<00:32,  3.83it/s] 19%|█▊        | 28/150 [00:07<00:32,  3.78it/s] 19%|█▉        | 29/150 [00:07<00:32,  3.77it/s] 20%|██        | 30/150 [00:08<00:32,  3.72it/s] 21%|██        | 31/150 [00:08<00:31,  3.81it/s] 21%|██▏       | 32/150 [00:08<00:31,  3.77it/s] 22%|██▏       | 33/150 [00:08<00:31,  3.72it/s] 23%|██▎       | 34/150 [00:09<00:30,  3.78it/s] 23%|██▎       | 35/150 [00:09<00:30,  3.80it/s] 24%|██▍       | 36/150 [00:09<00:30,  3.77it/s] 25%|██▍       | 37/150 [00:09<00:30,  3.71it/s] 25%|██▌       | 38/150 [00:10<00:29,  3.84it/s] 26%|██▌       | 39/150 [00:10<00:29,  3.80it/s] 27%|██▋       | 40/150 [00:10<00:29,  3.78it/s] 27%|██▋       | 41/150 [00:10<00:29,  3.73it/s] 28%|██▊       | 42/150 [00:11<00:28,  3.81it/s] 29%|██▊       | 43/150 [00:11<00:28,  3.77it/s] 29%|██▉       | 44/150 [00:11<00:28,  3.72it/s] 30%|███       | 45/150 [00:12<00:27,  3.79it/s] 31%|███       | 46/150 [00:12<00:27,  3.81it/s] 31%|███▏      | 47/150 [00:12<00:27,  3.78it/s] 32%|███▏      | 48/150 [00:12<00:27,  3.70it/s] 33%|███▎      | 49/150 [00:13<00:26,  3.82it/s] 33%|███▎      | 50/150 [00:13<00:26,  3.79it/s] 34%|███▍      | 51/150 [00:13<00:26,  3.77it/s] 35%|███▍      | 52/150 [00:13<00:26,  3.72it/s] 35%|███▌      | 53/150 [00:14<00:25,  3.80it/s] 36%|███▌      | 54/150 [00:14<00:25,  3.76it/s] 37%|███▋      | 55/150 [00:14<00:25,  3.71it/s] 37%|███▋      | 56/150 [00:14<00:24,  3.78it/s] 38%|███▊      | 57/150 [00:15<00:24,  3.79it/s] 39%|███▊      | 58/150 [00:15<00:24,  3.76it/s] 39%|███▉      | 59/150 [00:15<00:24,  3.70it/s] 40%|████      | 60/150 [00:16<00:23,  3.83it/s] 41%|████      | 61/150 [00:16<00:23,  3.79it/s] 41%|████▏     | 62/150 [00:16<00:23,  3.77it/s] 42%|████▏     | 63/150 [00:16<00:23,  3.72it/s] 43%|████▎     | 64/150 [00:17<00:22,  3.81it/s] 43%|████▎     | 65/150 [00:17<00:22,  3.76it/s] 44%|████▍     | 66/150 [00:17<00:22,  3.71it/s] 45%|████▍     | 67/150 [00:17<00:22,  3.77it/s] 45%|████▌     | 68/150 [00:18<00:21,  3.78it/s] 46%|████▌     | 69/150 [00:18<00:21,  3.74it/s] 47%|████▋     | 70/150 [00:18<00:21,  3.68it/s] 47%|████▋     | 71/150 [00:18<00:20,  3.80it/s] 48%|████▊     | 72/150 [00:19<00:20,  3.76it/s] 49%|████▊     | 73/150 [00:19<00:20,  3.76it/s] 49%|████▉     | 74/150 [00:19<00:20,  3.70it/s] 50%|█████     | 75/150 [00:20<00:19,  3.80it/s] 51%|█████     | 76/150 [00:20<00:19,  3.75it/s] 51%|█████▏    | 77/150 [00:20<00:19,  3.70it/s] 52%|█████▏    | 78/150 [00:20<00:19,  3.77it/s] 53%|█████▎    | 79/150 [00:21<00:18,  3.77it/s] 53%|█████▎    | 80/150 [00:21<00:18,  3.73it/s] 54%|█████▍    | 81/150 [00:21<00:18,  3.67it/s] 55%|█████▍    | 82/150 [00:21<00:17,  3.79it/s] 55%|█████▌    | 83/150 [00:22<00:17,  3.88it/s] 56%|█████▌    | 84/150 [00:22<00:17,  3.82it/s] 57%|█████▋    | 85/150 [00:22<00:17,  3.79it/s] 57%|█████▋    | 86/150 [00:22<00:17,  3.72it/s] 58%|█████▊    | 87/150 [00:23<00:16,  3.80it/s] 59%|█████▊    | 88/150 [00:23<00:16,  3.76it/s] 59%|█████▉    | 89/150 [00:23<00:16,  3.71it/s] 60%|██████    | 90/150 [00:24<00:15,  3.77it/s] 61%|██████    | 91/150 [00:24<00:15,  3.78it/s] 61%|██████▏   | 92/150 [00:24<00:15,  3.74it/s] 62%|██████▏   | 93/150 [00:24<00:15,  3.67it/s] 63%|██████▎   | 94/150 [00:25<00:14,  3.79it/s] 63%|██████▎   | 95/150 [00:25<00:14,  3.75it/s] 64%|██████▍   | 96/150 [00:25<00:14,  3.74it/s] 65%|██████▍   | 97/150 [00:25<00:14,  3.70it/s] 65%|██████▌   | 98/150 [00:26<00:13,  3.79it/s] 66%|██████▌   | 99/150 [00:26<00:13,  3.75it/s] 67%|██████▋   | 100/150 [00:26<00:13,  3.69it/s] 67%|██████▋   | 101/150 [00:26<00:13,  3.76it/s] 68%|██████▊   | 102/150 [00:27<00:12,  3.77it/s] 69%|██████▊   | 103/150 [00:27<00:12,  3.73it/s] 69%|██████▉   | 104/150 [00:27<00:12,  3.67it/s] 70%|███████   | 105/150 [00:28<00:11,  3.79it/s] 71%|███████   | 106/150 [00:28<00:11,  3.75it/s] 71%|███████▏  | 107/150 [00:28<00:11,  3.74it/s] 72%|███████▏  | 108/150 [00:28<00:11,  3.69it/s] 73%|███████▎  | 109/150 [00:29<00:10,  3.79it/s] 73%|███████▎  | 110/150 [00:29<00:10,  3.76it/s] 74%|███████▍  | 111/150 [00:29<00:10,  3.72it/s] 75%|███████▍  | 112/150 [00:29<00:10,  3.75it/s] 75%|███████▌  | 113/150 [00:30<00:09,  3.78it/s] 76%|███████▌  | 114/150 [00:30<00:09,  3.74it/s] 77%|███████▋  | 115/150 [00:30<00:09,  3.67it/s] 77%|███████▋  | 116/150 [00:30<00:08,  3.79it/s] 78%|███████▊  | 117/150 [00:31<00:08,  3.76it/s] 79%|███████▊  | 118/150 [00:31<00:08,  3.74it/s] 79%|███████▉  | 119/150 [00:31<00:08,  3.70it/s] 80%|████████  | 120/150 [00:32<00:07,  3.79it/s] 81%|████████  | 121/150 [00:32<00:07,  3.74it/s] 81%|████████▏ | 122/150 [00:32<00:07,  3.69it/s] 82%|████████▏ | 123/150 [00:32<00:07,  3.76it/s] 83%|████████▎ | 124/150 [00:33<00:06,  3.76it/s] 83%|████████▎ | 125/150 [00:33<00:06,  3.73it/s] 84%|████████▍ | 126/150 [00:33<00:06,  3.66it/s] 85%|████████▍ | 127/150 [00:33<00:06,  3.79it/s] 85%|████████▌ | 128/150 [00:34<00:05,  3.77it/s] 86%|████████▌ | 129/150 [00:34<00:05,  3.74it/s] 87%|████████▋ | 130/150 [00:34<00:05,  3.70it/s] 87%|████████▋ | 131/150 [00:34<00:05,  3.80it/s] 88%|████████▊ | 132/150 [00:35<00:04,  3.74it/s] 89%|████████▊ | 133/150 [00:35<00:04,  3.70it/s] 89%|████████▉ | 134/150 [00:35<00:04,  3.76it/s] 90%|█████████ | 135/150 [00:36<00:03,  3.78it/s] 91%|█████████ | 136/150 [00:36<00:03,  3.74it/s] 91%|█████████▏| 137/150 [00:36<00:03,  3.66it/s] 92%|█████████▏| 138/150 [00:36<00:03,  3.79it/s] 93%|█████████▎| 139/150 [00:37<00:02,  3.75it/s] 93%|█████████▎| 140/150 [00:37<00:02,  3.73it/s] 94%|█████████▍| 141/150 [00:37<00:02,  3.69it/s] 95%|█████████▍| 142/150 [00:37<00:02,  3.78it/s] 95%|█████████▌| 143/150 [00:38<00:01,  3.73it/s] 96%|█████████▌| 144/150 [00:38<00:01,  3.67it/s] 97%|█████████▋| 145/150 [00:38<00:01,  3.74it/s] 97%|█████████▋| 146/150 [00:38<00:01,  3.76it/s] 98%|█████████▊| 147/150 [00:39<00:00,  3.72it/s] 99%|█████████▊| 148/150 [00:39<00:00,  3.65it/s] 99%|█████████▉| 149/150 [00:39<00:00,  3.78it/s]100%|██████████| 150/150 [00:40<00:00,  3.75it/s]accuracy:  0.64
100%|██████████| 150/150 [00:42<00:00,  3.54it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 145
Training dataset size: 288, validation dataset size: 145
Training dataset size: 288, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:09:26,682] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 06:09:26,719] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2025-03-12 06:09:26,789] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<01:54,  2.44s/it]  4%|▍         | 2/48 [00:04<01:47,  2.33s/it]  6%|▋         | 3/48 [00:06<01:43,  2.30s/it]  8%|▊         | 4/48 [00:09<01:40,  2.29s/it] 10%|█         | 5/48 [00:11<01:43,  2.41s/it] 12%|█▎        | 6/48 [00:14<01:41,  2.41s/it] 15%|█▍        | 7/48 [00:16<01:40,  2.45s/it] 17%|█▋        | 8/48 [00:19<01:38,  2.46s/it] 19%|█▉        | 9/48 [00:21<01:35,  2.44s/it] 21%|██        | 10/48 [00:24<01:32,  2.43s/it]                                               {'loss': 1.5306, 'grad_norm': 15.482975006103516, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:24<01:32,  2.43s/it] 23%|██▎       | 11/48 [00:26<01:27,  2.38s/it] 25%|██▌       | 12/48 [00:28<01:26,  2.40s/it] 27%|██▋       | 13/48 [00:31<01:26,  2.49s/it] 29%|██▉       | 14/48 [00:33<01:23,  2.46s/it] 31%|███▏      | 15/48 [00:36<01:21,  2.47s/it] 33%|███▎      | 16/48 [00:39<01:23,  2.59s/it] 35%|███▌      | 17/48 [00:41<01:19,  2.56s/it] 38%|███▊      | 18/48 [00:44<01:15,  2.52s/it] 40%|███▉      | 19/48 [00:46<01:11,  2.45s/it] 42%|████▏     | 20/48 [00:49<01:09,  2.49s/it]                                               {'loss': 1.5504, 'grad_norm': 14.819443702697754, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:49<01:09,  2.49s/it] 44%|████▍     | 21/48 [00:51<01:06,  2.46s/it] 46%|████▌     | 22/48 [00:54<01:06,  2.55s/it] 48%|████▊     | 23/48 [00:56<01:02,  2.50s/it] 50%|█████     | 24/48 [00:58<00:57,  2.42s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:01<00:57,  2.50s/it] 54%|█████▍    | 26/48 [01:04<00:55,  2.52s/it] 56%|█████▋    | 27/48 [01:06<00:54,  2.58s/it] 58%|█████▊    | 28/48 [01:09<00:50,  2.51s/it] 60%|██████    | 29/48 [01:11<00:44,  2.36s/it] 62%|██████▎   | 30/48 [01:13<00:44,  2.47s/it]                                               {'loss': 1.0895, 'grad_norm': 8.751823425292969, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:13<00:44,  2.47s/it] 65%|██████▍   | 31/48 [01:16<00:42,  2.50s/it] 67%|██████▋   | 32/48 [01:19<00:41,  2.62s/it] 69%|██████▉   | 33/48 [01:21<00:35,  2.38s/it] 71%|███████   | 34/48 [01:23<00:33,  2.43s/it] 73%|███████▎  | 35/48 [01:25<00:30,  2.36s/it] 75%|███████▌  | 36/48 [01:28<00:28,  2.35s/it] 77%|███████▋  | 37/48 [01:30<00:26,  2.45s/it] 79%|███████▉  | 38/48 [01:33<00:25,  2.51s/it] 81%|████████▏ | 39/48 [01:36<00:23,  2.56s/it] 83%|████████▎ | 40/48 [01:38<00:19,  2.44s/it]                                               {'loss': 0.8134, 'grad_norm': 6.68067741394043, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:38<00:19,  2.44s/it] 85%|████████▌ | 41/48 [01:40<00:17,  2.49s/it] 88%|████████▊ | 42/48 [01:43<00:15,  2.56s/it] 90%|████████▉ | 43/48 [01:45<00:11,  2.33s/it] 92%|█████████▏| 44/48 [01:48<00:09,  2.39s/it] 94%|█████████▍| 45/48 [01:49<00:06,  2.25s/it] 96%|█████████▌| 46/48 [01:52<00:04,  2.46s/it] 98%|█████████▊| 47/48 [01:54<00:02,  2.30s/it]100%|██████████| 48/48 [01:57<00:00,  2.28s/it]                                               {'train_runtime': 117.6874, 'train_samples_per_second': 4.894, 'train_steps_per_second': 0.408, 'train_loss': 1.1454230050245922, 'epoch': 2.0}
100%|██████████| 48/48 [01:57<00:00,  2.28s/it]100%|██████████| 48/48 [01:57<00:00,  2.45s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:06,  2.97it/s]  1%|          | 2/200 [00:00<00:58,  3.39it/s]  2%|▏         | 3/200 [00:00<00:55,  3.53it/s]  2%|▏         | 4/200 [00:01<00:52,  3.77it/s]  2%|▎         | 5/200 [00:01<00:49,  3.91it/s]  3%|▎         | 6/200 [00:01<00:48,  3.99it/s]  4%|▎         | 7/200 [00:01<00:47,  4.05it/s]  4%|▍         | 8/200 [00:02<00:46,  4.09it/s]  4%|▍         | 9/200 [00:02<00:46,  4.11it/s]  5%|▌         | 10/200 [00:02<00:46,  4.13it/s]  6%|▌         | 11/200 [00:02<00:45,  4.14it/s]  6%|▌         | 12/200 [00:03<00:45,  4.15it/s]  6%|▋         | 13/200 [00:03<00:44,  4.16it/s]  7%|▋         | 14/200 [00:03<00:44,  4.16it/s]  8%|▊         | 15/200 [00:03<00:44,  4.16it/s]  8%|▊         | 16/200 [00:03<00:44,  4.16it/s]  8%|▊         | 17/200 [00:04<00:43,  4.16it/s]  9%|▉         | 18/200 [00:04<00:43,  4.16it/s] 10%|▉         | 19/200 [00:04<00:43,  4.17it/s] 10%|█         | 20/200 [00:04<00:43,  4.16it/s] 10%|█         | 21/200 [00:05<00:43,  4.16it/s] 11%|█         | 22/200 [00:05<00:42,  4.17it/s] 12%|█▏        | 23/200 [00:05<00:42,  4.17it/s] 12%|█▏        | 24/200 [00:05<00:42,  4.17it/s] 12%|█▎        | 25/200 [00:06<00:41,  4.17it/s] 13%|█▎        | 26/200 [00:06<00:41,  4.17it/s] 14%|█▎        | 27/200 [00:06<00:41,  4.17it/s] 14%|█▍        | 28/200 [00:06<00:41,  4.17it/s] 14%|█▍        | 29/200 [00:07<00:41,  4.17it/s] 15%|█▌        | 30/200 [00:07<00:40,  4.17it/s] 16%|█▌        | 31/200 [00:07<00:40,  4.16it/s] 16%|█▌        | 32/200 [00:07<00:40,  4.17it/s] 16%|█▋        | 33/200 [00:08<00:40,  4.17it/s] 17%|█▋        | 34/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 35/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 36/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 37/200 [00:09<00:39,  4.12it/s] 19%|█▉        | 38/200 [00:09<00:40,  4.01it/s] 20%|█▉        | 39/200 [00:09<00:40,  3.93it/s] 20%|██        | 40/200 [00:09<00:40,  3.92it/s] 20%|██        | 41/200 [00:10<00:40,  3.94it/s] 21%|██        | 42/200 [00:10<00:41,  3.84it/s] 22%|██▏       | 43/200 [00:10<00:40,  3.86it/s] 22%|██▏       | 44/200 [00:10<00:41,  3.78it/s] 22%|██▎       | 45/200 [00:11<00:40,  3.82it/s] 23%|██▎       | 46/200 [00:11<00:41,  3.75it/s] 24%|██▎       | 47/200 [00:11<00:40,  3.79it/s] 24%|██▍       | 48/200 [00:11<00:40,  3.73it/s] 24%|██▍       | 49/200 [00:12<00:39,  3.79it/s] 25%|██▌       | 50/200 [00:12<00:40,  3.73it/s] 26%|██▌       | 51/200 [00:12<00:39,  3.77it/s] 26%|██▌       | 52/200 [00:13<00:39,  3.71it/s] 26%|██▋       | 53/200 [00:13<00:38,  3.78it/s] 27%|██▋       | 54/200 [00:13<00:39,  3.72it/s] 28%|██▊       | 55/200 [00:13<00:38,  3.77it/s] 28%|██▊       | 56/200 [00:14<00:38,  3.72it/s] 28%|██▊       | 57/200 [00:14<00:37,  3.79it/s] 29%|██▉       | 58/200 [00:14<00:38,  3.72it/s] 30%|██▉       | 59/200 [00:14<00:37,  3.76it/s] 30%|███       | 60/200 [00:15<00:37,  3.70it/s] 30%|███       | 61/200 [00:15<00:36,  3.77it/s] 31%|███       | 62/200 [00:15<00:37,  3.72it/s] 32%|███▏      | 63/200 [00:15<00:36,  3.77it/s] 32%|███▏      | 64/200 [00:16<00:36,  3.72it/s] 32%|███▎      | 65/200 [00:16<00:35,  3.78it/s] 33%|███▎      | 66/200 [00:16<00:36,  3.71it/s] 34%|███▎      | 67/200 [00:17<00:35,  3.76it/s] 34%|███▍      | 68/200 [00:17<00:35,  3.70it/s] 34%|███▍      | 69/200 [00:17<00:34,  3.76it/s] 35%|███▌      | 70/200 [00:17<00:35,  3.71it/s] 36%|███▌      | 71/200 [00:18<00:34,  3.76it/s] 36%|███▌      | 72/200 [00:18<00:34,  3.71it/s] 36%|███▋      | 73/200 [00:18<00:33,  3.79it/s] 37%|███▋      | 74/200 [00:18<00:33,  3.73it/s] 38%|███▊      | 75/200 [00:19<00:33,  3.77it/s] 38%|███▊      | 76/200 [00:19<00:33,  3.71it/s] 38%|███▊      | 77/200 [00:19<00:32,  3.77it/s] 39%|███▉      | 78/200 [00:19<00:32,  3.72it/s] 40%|███▉      | 79/200 [00:20<00:32,  3.77it/s] 40%|████      | 80/200 [00:20<00:32,  3.71it/s] 40%|████      | 81/200 [00:20<00:31,  3.78it/s] 41%|████      | 82/200 [00:21<00:31,  3.71it/s] 42%|████▏     | 83/200 [00:21<00:31,  3.76it/s] 42%|████▏     | 84/200 [00:21<00:31,  3.70it/s] 42%|████▎     | 85/200 [00:21<00:30,  3.76it/s] 43%|████▎     | 86/200 [00:22<00:30,  3.71it/s] 44%|████▎     | 87/200 [00:22<00:30,  3.75it/s] 44%|████▍     | 88/200 [00:22<00:30,  3.70it/s] 44%|████▍     | 89/200 [00:22<00:29,  3.78it/s] 45%|████▌     | 90/200 [00:23<00:29,  3.71it/s] 46%|████▌     | 91/200 [00:23<00:29,  3.73it/s] 46%|████▌     | 92/200 [00:23<00:29,  3.70it/s] 46%|████▋     | 93/200 [00:23<00:28,  3.80it/s] 47%|████▋     | 94/200 [00:24<00:28,  3.75it/s] 48%|████▊     | 95/200 [00:24<00:28,  3.72it/s] 48%|████▊     | 96/200 [00:24<00:28,  3.67it/s] 48%|████▊     | 97/200 [00:25<00:27,  3.80it/s] 49%|████▉     | 98/200 [00:25<00:27,  3.75it/s] 50%|████▉     | 99/200 [00:25<00:27,  3.71it/s] 50%|█████     | 100/200 [00:25<00:27,  3.66it/s] 50%|█████     | 101/200 [00:26<00:26,  3.79it/s] 51%|█████     | 102/200 [00:26<00:26,  3.77it/s] 52%|█████▏    | 103/200 [00:26<00:26,  3.71it/s] 52%|█████▏    | 104/200 [00:26<00:26,  3.68it/s] 52%|█████▎    | 105/200 [00:27<00:25,  3.75it/s] 53%|█████▎    | 106/200 [00:27<00:24,  3.78it/s] 54%|█████▎    | 107/200 [00:27<00:25,  3.71it/s] 54%|█████▍    | 108/200 [00:27<00:24,  3.76it/s] 55%|█████▍    | 109/200 [00:28<00:24,  3.70it/s] 55%|█████▌    | 110/200 [00:28<00:23,  3.76it/s] 56%|█████▌    | 111/200 [00:28<00:24,  3.71it/s] 56%|█████▌    | 112/200 [00:29<00:23,  3.75it/s] 56%|█████▋    | 113/200 [00:29<00:23,  3.70it/s] 57%|█████▋    | 114/200 [00:29<00:22,  3.75it/s] 57%|█████▊    | 115/200 [00:29<00:23,  3.69it/s] 58%|█████▊    | 116/200 [00:30<00:22,  3.74it/s] 58%|█████▊    | 117/200 [00:30<00:22,  3.69it/s] 59%|█████▉    | 118/200 [00:30<00:21,  3.75it/s] 60%|█████▉    | 119/200 [00:30<00:21,  3.68it/s] 60%|██████    | 120/200 [00:31<00:21,  3.72it/s] 60%|██████    | 121/200 [00:31<00:21,  3.68it/s] 61%|██████    | 122/200 [00:31<00:20,  3.75it/s] 62%|██████▏   | 123/200 [00:32<00:20,  3.68it/s] 62%|██████▏   | 124/200 [00:32<00:20,  3.73it/s] 62%|██████▎   | 125/200 [00:32<00:20,  3.68it/s] 63%|██████▎   | 126/200 [00:32<00:19,  3.75it/s] 64%|██████▎   | 127/200 [00:33<00:19,  3.68it/s] 64%|██████▍   | 128/200 [00:33<00:19,  3.72it/s] 64%|██████▍   | 129/200 [00:33<00:19,  3.68it/s] 65%|██████▌   | 130/200 [00:33<00:18,  3.74it/s] 66%|██████▌   | 131/200 [00:34<00:18,  3.68it/s] 66%|██████▌   | 132/200 [00:34<00:18,  3.72it/s] 66%|██████▋   | 133/200 [00:34<00:18,  3.68it/s] 67%|██████▋   | 134/200 [00:34<00:17,  3.74it/s] 68%|██████▊   | 135/200 [00:35<00:17,  3.68it/s] 68%|██████▊   | 136/200 [00:35<00:17,  3.73it/s] 68%|██████▊   | 137/200 [00:35<00:17,  3.68it/s] 69%|██████▉   | 138/200 [00:36<00:16,  3.74it/s] 70%|██████▉   | 139/200 [00:36<00:16,  3.68it/s] 70%|███████   | 140/200 [00:36<00:16,  3.72it/s] 70%|███████   | 141/200 [00:36<00:16,  3.68it/s] 71%|███████   | 142/200 [00:37<00:15,  3.74it/s] 72%|███████▏  | 143/200 [00:37<00:15,  3.68it/s] 72%|███████▏  | 144/200 [00:37<00:15,  3.73it/s] 72%|███████▎  | 145/200 [00:37<00:14,  3.68it/s] 73%|███████▎  | 146/200 [00:38<00:14,  3.74it/s] 74%|███████▎  | 147/200 [00:38<00:14,  3.68it/s] 74%|███████▍  | 148/200 [00:38<00:13,  3.73it/s] 74%|███████▍  | 149/200 [00:39<00:13,  3.68it/s] 75%|███████▌  | 150/200 [00:39<00:13,  3.74it/s] 76%|███████▌  | 151/200 [00:39<00:13,  3.68it/s] 76%|███████▌  | 152/200 [00:39<00:12,  3.72it/s] 76%|███████▋  | 153/200 [00:40<00:12,  3.68it/s] 77%|███████▋  | 154/200 [00:40<00:12,  3.75it/s] 78%|███████▊  | 155/200 [00:40<00:12,  3.68it/s] 78%|███████▊  | 156/200 [00:40<00:11,  3.73it/s] 78%|███████▊  | 157/200 [00:41<00:11,  3.68it/s] 79%|███████▉  | 158/200 [00:41<00:11,  3.75it/s] 80%|███████▉  | 159/200 [00:41<00:11,  3.68it/s] 80%|████████  | 160/200 [00:42<00:10,  3.72it/s] 80%|████████  | 161/200 [00:42<00:10,  3.67it/s] 81%|████████  | 162/200 [00:42<00:10,  3.73it/s] 82%|████████▏ | 163/200 [00:42<00:10,  3.67it/s] 82%|████████▏ | 164/200 [00:43<00:09,  3.69it/s] 82%|████████▎ | 165/200 [00:43<00:09,  3.68it/s] 83%|████████▎ | 166/200 [00:43<00:09,  3.74it/s] 84%|████████▎ | 167/200 [00:43<00:08,  3.68it/s] 84%|████████▍ | 168/200 [00:44<00:08,  3.72it/s] 84%|████████▍ | 169/200 [00:44<00:08,  3.69it/s] 85%|████████▌ | 170/200 [00:44<00:08,  3.73it/s] 86%|████████▌ | 171/200 [00:44<00:07,  3.68it/s] 86%|████████▌ | 172/200 [00:45<00:07,  3.65it/s] 86%|████████▋ | 173/200 [00:45<00:07,  3.72it/s] 87%|████████▋ | 174/200 [00:45<00:06,  3.74it/s] 88%|████████▊ | 175/200 [00:46<00:06,  3.69it/s] 88%|████████▊ | 176/200 [00:46<00:06,  3.68it/s] 88%|████████▊ | 177/200 [00:46<00:06,  3.71it/s] 89%|████████▉ | 178/200 [00:46<00:05,  3.74it/s] 90%|████████▉ | 179/200 [00:47<00:05,  3.69it/s] 90%|█████████ | 180/200 [00:47<00:05,  3.69it/s] 90%|█████████ | 181/200 [00:47<00:05,  3.71it/s] 91%|█████████ | 182/200 [00:47<00:04,  3.74it/s] 92%|█████████▏| 183/200 [00:48<00:04,  3.68it/s] 92%|█████████▏| 184/200 [00:48<00:04,  3.65it/s] 92%|█████████▎| 185/200 [00:48<00:04,  3.71it/s] 93%|█████████▎| 186/200 [00:49<00:03,  3.74it/s] 94%|█████████▎| 187/200 [00:49<00:03,  3.68it/s] 94%|█████████▍| 188/200 [00:49<00:03,  3.66it/s] 94%|█████████▍| 189/200 [00:49<00:02,  3.71it/s] 95%|█████████▌| 190/200 [00:50<00:02,  3.73it/s] 96%|█████████▌| 191/200 [00:50<00:02,  3.67it/s] 96%|█████████▌| 192/200 [00:50<00:02,  3.64it/s] 96%|█████████▋| 193/200 [00:50<00:01,  3.72it/s] 97%|█████████▋| 194/200 [00:51<00:01,  3.73it/s] 98%|█████████▊| 195/200 [00:51<00:01,  3.66it/s] 98%|█████████▊| 196/200 [00:51<00:01,  3.63it/s] 98%|█████████▊| 197/200 [00:52<00:00,  3.70it/s] 99%|█████████▉| 198/200 [00:52<00:00,  3.73it/s]100%|█████████▉| 199/200 [00:52<00:00,  3.67it/s]100%|██████████| 200/200 [00:52<00:00,  3.67it/s]accuracy:  0.58
100%|██████████| 200/200 [00:55<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 191
Training dataset size: 288, validation dataset size: 191
Training dataset size: 288, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:12:46,789] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[2025-03-12 06:12:46,889] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
training start
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:12:47,077] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<01:47,  2.30s/it]  4%|▍         | 2/48 [00:04<01:38,  2.15s/it]  6%|▋         | 3/48 [00:07<02:01,  2.71s/it]  8%|▊         | 4/48 [00:10<02:01,  2.76s/it] 10%|█         | 5/48 [00:12<01:53,  2.63s/it] 12%|█▎        | 6/48 [00:15<01:50,  2.64s/it] 15%|█▍        | 7/48 [00:18<01:51,  2.72s/it] 17%|█▋        | 8/48 [00:21<01:49,  2.74s/it] 19%|█▉        | 9/48 [00:23<01:45,  2.70s/it] 21%|██        | 10/48 [00:26<01:43,  2.72s/it]                                               {'loss': 1.662, 'grad_norm': 16.25267791748047, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:26<01:43,  2.72s/it] 23%|██▎       | 11/48 [00:29<01:37,  2.65s/it] 25%|██▌       | 12/48 [00:31<01:35,  2.66s/it] 27%|██▋       | 13/48 [00:34<01:32,  2.65s/it] 29%|██▉       | 14/48 [00:37<01:32,  2.73s/it] 31%|███▏      | 15/48 [00:40<01:30,  2.73s/it] 33%|███▎      | 16/48 [00:42<01:23,  2.61s/it] 35%|███▌      | 17/48 [00:45<01:23,  2.69s/it] 38%|███▊      | 18/48 [00:48<01:23,  2.80s/it] 40%|███▉      | 19/48 [00:50<01:19,  2.73s/it] 42%|████▏     | 20/48 [00:53<01:16,  2.72s/it]                                               {'loss': 1.2466, 'grad_norm': 12.012186050415039, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:53<01:16,  2.72s/it] 44%|████▍     | 21/48 [00:56<01:14,  2.75s/it] 46%|████▌     | 22/48 [00:58<01:08,  2.64s/it] 48%|████▊     | 23/48 [01:00<01:02,  2.50s/it] 50%|█████     | 24/48 [01:03<01:01,  2.58s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:06<01:00,  2.61s/it] 54%|█████▍    | 26/48 [01:09<01:01,  2.78s/it] 56%|█████▋    | 27/48 [01:12<00:56,  2.68s/it] 58%|█████▊    | 28/48 [01:14<00:49,  2.49s/it] 60%|██████    | 29/48 [01:16<00:46,  2.45s/it] 62%|██████▎   | 30/48 [01:18<00:43,  2.40s/it]                                               {'loss': 0.9466, 'grad_norm': 11.17199420928955, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:18<00:43,  2.40s/it] 65%|██████▍   | 31/48 [01:21<00:43,  2.56s/it] 67%|██████▋   | 32/48 [01:23<00:39,  2.48s/it] 69%|██████▉   | 33/48 [01:26<00:37,  2.50s/it] 71%|███████   | 34/48 [01:29<00:37,  2.67s/it] 73%|███████▎  | 35/48 [01:31<00:32,  2.48s/it] 75%|███████▌  | 36/48 [01:34<00:31,  2.62s/it] 77%|███████▋  | 37/48 [01:37<00:29,  2.67s/it] 79%|███████▉  | 38/48 [01:39<00:24,  2.50s/it] 81%|████████▏ | 39/48 [01:41<00:22,  2.51s/it] 83%|████████▎ | 40/48 [01:44<00:19,  2.47s/it]                                               {'loss': 0.8825, 'grad_norm': 6.383044719696045, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:44<00:19,  2.47s/it] 85%|████████▌ | 41/48 [01:46<00:16,  2.34s/it] 88%|████████▊ | 42/48 [01:48<00:13,  2.29s/it] 90%|████████▉ | 43/48 [01:50<00:11,  2.23s/it] 92%|█████████▏| 44/48 [01:53<00:09,  2.29s/it] 94%|█████████▍| 45/48 [01:55<00:07,  2.48s/it] 96%|█████████▌| 46/48 [01:58<00:05,  2.50s/it] 98%|█████████▊| 47/48 [02:01<00:02,  2.52s/it]100%|██████████| 48/48 [02:03<00:00,  2.54s/it]                                               {'train_runtime': 124.3069, 'train_samples_per_second': 4.634, 'train_steps_per_second': 0.386, 'train_loss': 1.1160323719183605, 'epoch': 2.0}
100%|██████████| 48/48 [02:04<00:00,  2.54s/it]100%|██████████| 48/48 [02:04<00:00,  2.59s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:49,  2.53it/s]  1%|          | 2/279 [00:00<01:28,  3.13it/s]  1%|          | 3/279 [00:00<01:22,  3.33it/s]  1%|▏         | 4/279 [00:01<01:16,  3.61it/s]  2%|▏         | 5/279 [00:01<01:14,  3.67it/s]  2%|▏         | 6/279 [00:01<01:13,  3.70it/s]  3%|▎         | 7/279 [00:01<01:12,  3.77it/s]  3%|▎         | 8/279 [00:02<01:13,  3.71it/s]  3%|▎         | 9/279 [00:02<01:10,  3.83it/s]  4%|▎         | 10/279 [00:02<01:11,  3.78it/s]  4%|▍         | 11/279 [00:03<01:11,  3.76it/s]  4%|▍         | 12/279 [00:03<01:11,  3.71it/s]  5%|▍         | 13/279 [00:03<01:09,  3.84it/s]  5%|▌         | 14/279 [00:03<01:08,  3.85it/s]  5%|▌         | 15/279 [00:04<01:09,  3.80it/s]  6%|▌         | 16/279 [00:04<01:08,  3.84it/s]  6%|▌         | 17/279 [00:04<01:09,  3.77it/s]  6%|▋         | 18/279 [00:04<01:08,  3.83it/s]  7%|▋         | 19/279 [00:05<01:08,  3.79it/s]  7%|▋         | 20/279 [00:05<01:07,  3.82it/s]  8%|▊         | 21/279 [00:05<01:08,  3.75it/s]  8%|▊         | 22/279 [00:05<01:06,  3.85it/s]  8%|▊         | 23/279 [00:06<01:07,  3.79it/s]  9%|▊         | 24/279 [00:06<01:07,  3.76it/s]  9%|▉         | 25/279 [00:06<01:08,  3.72it/s]  9%|▉         | 26/279 [00:06<01:05,  3.84it/s] 10%|▉         | 27/279 [00:07<01:05,  3.85it/s] 10%|█         | 28/279 [00:07<01:06,  3.79it/s] 10%|█         | 29/279 [00:07<01:06,  3.75it/s] 11%|█         | 30/279 [00:08<01:05,  3.79it/s] 11%|█         | 31/279 [00:08<01:04,  3.85it/s] 11%|█▏        | 32/279 [00:08<01:05,  3.80it/s] 12%|█▏        | 33/279 [00:08<01:05,  3.77it/s] 12%|█▏        | 34/279 [00:09<01:04,  3.78it/s] 13%|█▎        | 35/279 [00:09<01:03,  3.83it/s] 13%|█▎        | 36/279 [00:09<01:04,  3.79it/s] 13%|█▎        | 37/279 [00:09<01:03,  3.79it/s] 14%|█▎        | 38/279 [00:10<01:04,  3.75it/s] 14%|█▍        | 39/279 [00:10<01:02,  3.82it/s] 14%|█▍        | 40/279 [00:10<01:03,  3.78it/s] 15%|█▍        | 41/279 [00:10<01:02,  3.78it/s] 15%|█▌        | 42/279 [00:11<01:03,  3.73it/s] 15%|█▌        | 43/279 [00:11<01:01,  3.85it/s] 16%|█▌        | 44/279 [00:11<01:01,  3.79it/s] 16%|█▌        | 45/279 [00:11<01:02,  3.76it/s] 16%|█▋        | 46/279 [00:12<01:02,  3.72it/s] 17%|█▋        | 47/279 [00:12<01:00,  3.84it/s] 17%|█▋        | 48/279 [00:12<01:00,  3.79it/s] 18%|█▊        | 49/279 [00:13<01:01,  3.75it/s] 18%|█▊        | 50/279 [00:13<01:01,  3.70it/s] 18%|█▊        | 51/279 [00:13<00:59,  3.82it/s] 19%|█▊        | 52/279 [00:13<00:59,  3.80it/s] 19%|█▉        | 53/279 [00:14<01:00,  3.75it/s] 19%|█▉        | 54/279 [00:14<01:01,  3.68it/s] 20%|█▉        | 55/279 [00:14<00:58,  3.80it/s] 20%|██        | 56/279 [00:14<00:58,  3.81it/s] 20%|██        | 57/279 [00:15<00:59,  3.76it/s] 21%|██        | 58/279 [00:15<00:59,  3.71it/s] 21%|██        | 59/279 [00:15<00:58,  3.78it/s] 22%|██▏       | 60/279 [00:15<00:57,  3.82it/s] 22%|██▏       | 61/279 [00:16<00:57,  3.77it/s] 22%|██▏       | 62/279 [00:16<00:57,  3.74it/s] 23%|██▎       | 63/279 [00:16<00:57,  3.77it/s] 23%|██▎       | 64/279 [00:17<00:55,  3.84it/s] 23%|██▎       | 65/279 [00:17<00:56,  3.79it/s] 24%|██▎       | 66/279 [00:17<00:55,  3.81it/s] 24%|██▍       | 67/279 [00:17<00:56,  3.74it/s] 24%|██▍       | 68/279 [00:18<00:55,  3.83it/s] 25%|██▍       | 69/279 [00:18<00:55,  3.77it/s] 25%|██▌       | 70/279 [00:18<00:55,  3.75it/s] 25%|██▌       | 71/279 [00:18<00:56,  3.71it/s] 26%|██▌       | 72/279 [00:19<00:54,  3.82it/s] 26%|██▌       | 73/279 [00:19<00:54,  3.80it/s] 27%|██▋       | 74/279 [00:19<00:54,  3.76it/s] 27%|██▋       | 75/279 [00:19<00:55,  3.69it/s] 27%|██▋       | 76/279 [00:20<00:53,  3.81it/s] 28%|██▊       | 77/279 [00:20<00:52,  3.81it/s] 28%|██▊       | 78/279 [00:20<00:53,  3.76it/s] 28%|██▊       | 79/279 [00:21<00:54,  3.70it/s] 29%|██▊       | 80/279 [00:21<00:52,  3.77it/s] 29%|██▉       | 81/279 [00:21<00:51,  3.81it/s] 29%|██▉       | 82/279 [00:21<00:52,  3.77it/s] 30%|██▉       | 83/279 [00:22<00:52,  3.70it/s] 30%|███       | 84/279 [00:22<00:51,  3.77it/s] 30%|███       | 85/279 [00:22<00:50,  3.81it/s] 31%|███       | 86/279 [00:22<00:51,  3.77it/s] 31%|███       | 87/279 [00:23<00:51,  3.73it/s] 32%|███▏      | 88/279 [00:23<00:50,  3.75it/s] 32%|███▏      | 89/279 [00:23<00:49,  3.81it/s] 32%|███▏      | 90/279 [00:23<00:50,  3.77it/s] 33%|███▎      | 91/279 [00:24<00:49,  3.76it/s] 33%|███▎      | 92/279 [00:24<00:49,  3.74it/s] 33%|███▎      | 93/279 [00:24<00:48,  3.81it/s] 34%|███▎      | 94/279 [00:24<00:49,  3.77it/s] 34%|███▍      | 95/279 [00:25<00:48,  3.78it/s] 34%|███▍      | 96/279 [00:25<00:48,  3.74it/s] 35%|███▍      | 97/279 [00:25<00:47,  3.82it/s] 35%|███▌      | 98/279 [00:26<00:47,  3.77it/s] 35%|███▌      | 99/279 [00:26<00:48,  3.75it/s] 36%|███▌      | 100/279 [00:26<00:47,  3.74it/s] 36%|███▌      | 101/279 [00:26<00:46,  3.82it/s] 37%|███▋      | 102/279 [00:27<00:46,  3.77it/s] 37%|███▋      | 103/279 [00:27<00:46,  3.78it/s] 37%|███▋      | 104/279 [00:27<00:47,  3.72it/s] 38%|███▊      | 105/279 [00:27<00:45,  3.84it/s] 38%|███▊      | 106/279 [00:28<00:45,  3.78it/s] 38%|███▊      | 107/279 [00:28<00:45,  3.74it/s] 39%|███▊      | 108/279 [00:28<00:46,  3.69it/s] 39%|███▉      | 109/279 [00:28<00:44,  3.80it/s] 39%|███▉      | 110/279 [00:29<00:44,  3.78it/s] 40%|███▉      | 111/279 [00:29<00:44,  3.75it/s] 40%|████      | 112/279 [00:29<00:45,  3.67it/s] 41%|████      | 113/279 [00:30<00:43,  3.80it/s] 41%|████      | 114/279 [00:30<00:43,  3.82it/s] 41%|████      | 115/279 [00:30<00:43,  3.77it/s] 42%|████▏     | 116/279 [00:30<00:44,  3.70it/s] 42%|████▏     | 117/279 [00:31<00:43,  3.76it/s] 42%|████▏     | 118/279 [00:31<00:42,  3.78it/s] 43%|████▎     | 119/279 [00:31<00:42,  3.74it/s] 43%|████▎     | 120/279 [00:31<00:42,  3.70it/s] 43%|████▎     | 121/279 [00:32<00:42,  3.76it/s] 44%|████▎     | 122/279 [00:32<00:41,  3.81it/s] 44%|████▍     | 123/279 [00:32<00:41,  3.76it/s] 44%|████▍     | 124/279 [00:32<00:41,  3.72it/s] 45%|████▍     | 125/279 [00:33<00:41,  3.74it/s] 45%|████▌     | 126/279 [00:33<00:40,  3.80it/s] 46%|████▌     | 127/279 [00:33<00:40,  3.76it/s] 46%|████▌     | 128/279 [00:34<00:40,  3.73it/s] 46%|████▌     | 129/279 [00:34<00:40,  3.72it/s] 47%|████▋     | 130/279 [00:34<00:39,  3.80it/s] 47%|████▋     | 131/279 [00:34<00:39,  3.75it/s] 47%|████▋     | 132/279 [00:35<00:39,  3.74it/s] 48%|████▊     | 133/279 [00:35<00:39,  3.69it/s] 48%|████▊     | 134/279 [00:35<00:38,  3.80it/s] 48%|████▊     | 135/279 [00:35<00:38,  3.77it/s] 49%|████▊     | 136/279 [00:36<00:38,  3.73it/s] 49%|████▉     | 137/279 [00:36<00:38,  3.69it/s] 49%|████▉     | 138/279 [00:36<00:37,  3.81it/s] 50%|████▉     | 139/279 [00:36<00:37,  3.77it/s] 50%|█████     | 140/279 [00:37<00:37,  3.72it/s] 51%|█████     | 141/279 [00:37<00:37,  3.68it/s] 51%|█████     | 142/279 [00:37<00:36,  3.79it/s] 51%|█████▏    | 143/279 [00:38<00:36,  3.77it/s] 52%|█████▏    | 144/279 [00:38<00:36,  3.73it/s] 52%|█████▏    | 145/279 [00:38<00:36,  3.67it/s] 52%|█████▏    | 146/279 [00:38<00:35,  3.79it/s] 53%|█████▎    | 147/279 [00:39<00:34,  3.77it/s] 53%|█████▎    | 148/279 [00:39<00:34,  3.75it/s] 53%|█████▎    | 149/279 [00:39<00:35,  3.70it/s] 54%|█████▍    | 150/279 [00:39<00:34,  3.76it/s] 54%|█████▍    | 151/279 [00:40<00:33,  3.80it/s] 54%|█████▍    | 152/279 [00:40<00:33,  3.75it/s] 55%|█████▍    | 153/279 [00:40<00:34,  3.69it/s] 55%|█████▌    | 154/279 [00:40<00:33,  3.74it/s] 56%|█████▌    | 155/279 [00:41<00:32,  3.78it/s] 56%|█████▌    | 156/279 [00:41<00:32,  3.74it/s] 56%|█████▋    | 157/279 [00:41<00:33,  3.69it/s] 57%|█████▋    | 158/279 [00:42<00:32,  3.75it/s] 57%|█████▋    | 159/279 [00:42<00:31,  3.79it/s] 57%|█████▋    | 160/279 [00:42<00:31,  3.74it/s] 58%|█████▊    | 161/279 [00:42<00:32,  3.69it/s] 58%|█████▊    | 162/279 [00:43<00:31,  3.74it/s] 58%|█████▊    | 163/279 [00:43<00:30,  3.77it/s] 59%|█████▉    | 164/279 [00:43<00:30,  3.73it/s] 59%|█████▉    | 165/279 [00:43<00:30,  3.69it/s] 59%|█████▉    | 166/279 [00:44<00:30,  3.75it/s] 60%|█████▉    | 167/279 [00:44<00:29,  3.79it/s] 60%|██████    | 168/279 [00:44<00:29,  3.74it/s] 61%|██████    | 169/279 [00:44<00:29,  3.70it/s] 61%|██████    | 170/279 [00:45<00:29,  3.73it/s] 61%|██████▏   | 171/279 [00:45<00:28,  3.78it/s] 62%|██████▏   | 172/279 [00:45<00:28,  3.71it/s] 62%|██████▏   | 173/279 [00:46<00:28,  3.69it/s] 62%|██████▏   | 174/279 [00:46<00:27,  3.81it/s] 63%|██████▎   | 175/279 [00:46<00:26,  3.88it/s] 63%|██████▎   | 176/279 [00:46<00:26,  3.95it/s] 63%|██████▎   | 177/279 [00:47<00:25,  4.00it/s] 64%|██████▍   | 178/279 [00:47<00:25,  4.04it/s] 64%|██████▍   | 179/279 [00:47<00:24,  4.07it/s] 65%|██████▍   | 180/279 [00:47<00:24,  4.09it/s] 65%|██████▍   | 181/279 [00:48<00:23,  4.09it/s] 65%|██████▌   | 182/279 [00:48<00:23,  4.08it/s] 66%|██████▌   | 183/279 [00:48<00:23,  4.09it/s] 66%|██████▌   | 184/279 [00:48<00:23,  4.10it/s] 66%|██████▋   | 185/279 [00:48<00:22,  4.11it/s] 67%|██████▋   | 186/279 [00:49<00:22,  4.10it/s] 67%|██████▋   | 187/279 [00:49<00:22,  4.10it/s] 67%|██████▋   | 188/279 [00:49<00:22,  4.09it/s] 68%|██████▊   | 189/279 [00:49<00:21,  4.09it/s] 68%|██████▊   | 190/279 [00:50<00:21,  4.10it/s] 68%|██████▊   | 191/279 [00:50<00:21,  4.10it/s] 69%|██████▉   | 192/279 [00:50<00:21,  4.09it/s] 69%|██████▉   | 193/279 [00:50<00:21,  4.09it/s] 70%|██████▉   | 194/279 [00:51<00:20,  4.11it/s] 70%|██████▉   | 195/279 [00:51<00:20,  4.11it/s] 70%|███████   | 196/279 [00:51<00:20,  4.10it/s] 71%|███████   | 197/279 [00:51<00:20,  4.09it/s] 71%|███████   | 198/279 [00:52<00:19,  4.10it/s] 71%|███████▏  | 199/279 [00:52<00:19,  4.06it/s] 72%|███████▏  | 200/279 [00:52<00:20,  3.94it/s] 72%|███████▏  | 201/279 [00:52<00:20,  3.84it/s] 72%|███████▏  | 202/279 [00:53<00:19,  3.91it/s] 73%|███████▎  | 203/279 [00:53<00:19,  3.88it/s] 73%|███████▎  | 204/279 [00:53<00:19,  3.80it/s] 73%|███████▎  | 205/279 [00:54<00:19,  3.76it/s] 74%|███████▍  | 206/279 [00:54<00:19,  3.82it/s] 74%|███████▍  | 207/279 [00:54<00:18,  3.88it/s] 75%|███████▍  | 208/279 [00:54<00:18,  3.94it/s] 75%|███████▍  | 209/279 [00:55<00:17,  3.98it/s] 75%|███████▌  | 210/279 [00:55<00:17,  4.01it/s] 76%|███████▌  | 211/279 [00:55<00:16,  4.02it/s] 76%|███████▌  | 212/279 [00:55<00:16,  4.05it/s] 76%|███████▋  | 213/279 [00:55<00:16,  4.05it/s] 77%|███████▋  | 214/279 [00:56<00:16,  4.06it/s] 77%|███████▋  | 215/279 [00:56<00:15,  4.06it/s] 77%|███████▋  | 216/279 [00:56<00:15,  4.07it/s] 78%|███████▊  | 217/279 [00:56<00:15,  4.08it/s] 78%|███████▊  | 218/279 [00:57<00:14,  4.07it/s] 78%|███████▊  | 219/279 [00:57<00:14,  4.07it/s] 79%|███████▉  | 220/279 [00:57<00:14,  4.08it/s] 79%|███████▉  | 221/279 [00:57<00:14,  4.08it/s] 80%|███████▉  | 222/279 [00:58<00:13,  4.08it/s] 80%|███████▉  | 223/279 [00:58<00:13,  4.08it/s] 80%|████████  | 224/279 [00:58<00:13,  4.09it/s] 81%|████████  | 225/279 [00:58<00:13,  4.08it/s] 81%|████████  | 226/279 [00:59<00:13,  4.07it/s] 81%|████████▏ | 227/279 [00:59<00:12,  4.08it/s] 82%|████████▏ | 228/279 [00:59<00:12,  4.08it/s] 82%|████████▏ | 229/279 [00:59<00:12,  4.07it/s] 82%|████████▏ | 230/279 [01:00<00:12,  4.07it/s] 83%|████████▎ | 231/279 [01:00<00:11,  4.08it/s] 83%|████████▎ | 232/279 [01:00<00:11,  4.08it/s] 84%|████████▎ | 233/279 [01:00<00:11,  4.07it/s] 84%|████████▍ | 234/279 [01:01<00:11,  4.07it/s] 84%|████████▍ | 235/279 [01:01<00:10,  4.08it/s] 85%|████████▍ | 236/279 [01:01<00:10,  4.08it/s] 85%|████████▍ | 237/279 [01:01<00:10,  4.08it/s] 85%|████████▌ | 238/279 [01:02<00:10,  4.04it/s] 86%|████████▌ | 239/279 [01:02<00:10,  3.92it/s] 86%|████████▌ | 240/279 [01:02<00:10,  3.85it/s] 86%|████████▋ | 241/279 [01:02<00:09,  3.82it/s] 87%|████████▋ | 242/279 [01:03<00:09,  3.87it/s] 87%|████████▋ | 243/279 [01:03<00:09,  3.75it/s] 87%|████████▋ | 244/279 [01:03<00:09,  3.77it/s] 88%|████████▊ | 245/279 [01:04<00:09,  3.70it/s] 88%|████████▊ | 246/279 [01:04<00:08,  3.76it/s] 89%|████████▊ | 247/279 [01:04<00:08,  3.67it/s] 89%|████████▉ | 248/279 [01:04<00:08,  3.72it/s] 89%|████████▉ | 249/279 [01:05<00:08,  3.68it/s] 90%|████████▉ | 250/279 [01:05<00:07,  3.74it/s] 90%|████████▉ | 251/279 [01:05<00:07,  3.66it/s] 90%|█████████ | 252/279 [01:05<00:07,  3.69it/s] 91%|█████████ | 253/279 [01:06<00:07,  3.67it/s] 91%|█████████ | 254/279 [01:06<00:06,  3.72it/s] 91%|█████████▏| 255/279 [01:06<00:06,  3.64it/s] 92%|█████████▏| 256/279 [01:06<00:06,  3.69it/s] 92%|█████████▏| 257/279 [01:07<00:06,  3.65it/s] 92%|█████████▏| 258/279 [01:07<00:05,  3.72it/s] 93%|█████████▎| 259/279 [01:07<00:05,  3.65it/s] 93%|█████████▎| 260/279 [01:08<00:05,  3.71it/s] 94%|█████████▎| 261/279 [01:08<00:04,  3.66it/s] 94%|█████████▍| 262/279 [01:08<00:04,  3.73it/s] 94%|█████████▍| 263/279 [01:08<00:04,  3.65it/s] 95%|█████████▍| 264/279 [01:09<00:04,  3.67it/s] 95%|█████████▍| 265/279 [01:09<00:03,  3.67it/s] 95%|█████████▌| 266/279 [01:09<00:03,  3.71it/s] 96%|█████████▌| 267/279 [01:09<00:03,  3.64it/s] 96%|█████████▌| 268/279 [01:10<00:02,  3.67it/s] 96%|█████████▋| 269/279 [01:10<00:02,  3.66it/s] 97%|█████████▋| 270/279 [01:10<00:02,  3.76it/s] 97%|█████████▋| 271/279 [01:11<00:02,  3.70it/s] 97%|█████████▋| 272/279 [01:11<00:01,  3.66it/s] 98%|█████████▊| 273/279 [01:11<00:01,  3.63it/s] 98%|█████████▊| 274/279 [01:11<00:01,  3.75it/s] 99%|█████████▊| 275/279 [01:12<00:01,  3.69it/s] 99%|█████████▉| 276/279 [01:12<00:00,  3.66it/s] 99%|█████████▉| 277/279 [01:12<00:00,  3.62it/s]100%|█████████▉| 278/279 [01:12<00:00,  3.76it/s]100%|██████████| 279/279 [01:13<00:00,  3.76it/s]accuracy:  0.6021505376344086
100%|██████████| 279/279 [01:17<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 241
Training dataset size: 288, validation dataset size: 241
Training dataset size: 288, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  3.00s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 06:16:34,190] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 06:16:34,322] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 06:16:34,396] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
  0%|          | 0/48 [00:00<?, ?it/s]It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<02:03,  2.63s/it]  4%|▍         | 2/48 [00:04<01:52,  2.44s/it]  6%|▋         | 3/48 [00:07<01:51,  2.47s/it]  8%|▊         | 4/48 [00:09<01:40,  2.29s/it] 10%|█         | 5/48 [00:12<01:46,  2.48s/it] 12%|█▎        | 6/48 [00:14<01:40,  2.39s/it] 15%|█▍        | 7/48 [00:16<01:35,  2.34s/it] 17%|█▋        | 8/48 [00:18<01:30,  2.25s/it] 19%|█▉        | 9/48 [00:21<01:35,  2.45s/it] 21%|██        | 10/48 [00:24<01:40,  2.64s/it]                                               {'loss': 0.4741, 'grad_norm': 6.205101013183594, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:24<01:40,  2.64s/it] 23%|██▎       | 11/48 [00:27<01:41,  2.74s/it] 25%|██▌       | 12/48 [00:29<01:31,  2.55s/it] 27%|██▋       | 13/48 [00:32<01:25,  2.44s/it] 29%|██▉       | 14/48 [00:34<01:19,  2.34s/it] 31%|███▏      | 15/48 [00:36<01:16,  2.32s/it] 33%|███▎      | 16/48 [00:38<01:09,  2.16s/it] 35%|███▌      | 17/48 [00:40<01:06,  2.16s/it] 38%|███▊      | 18/48 [00:42<01:05,  2.17s/it] 40%|███▉      | 19/48 [00:45<01:08,  2.37s/it] 42%|████▏     | 20/48 [00:47<01:07,  2.41s/it]                                               {'loss': 0.3182, 'grad_norm': 6.984435081481934, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:47<01:07,  2.41s/it] 44%|████▍     | 21/48 [00:50<01:06,  2.48s/it] 46%|████▌     | 22/48 [00:53<01:05,  2.52s/it] 48%|████▊     | 23/48 [00:55<01:00,  2.44s/it] 50%|█████     | 24/48 [00:57<00:56,  2.37s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:00<00:57,  2.52s/it] 54%|█████▍    | 26/48 [01:02<00:54,  2.48s/it] 56%|█████▋    | 27/48 [01:05<00:51,  2.44s/it] 58%|█████▊    | 28/48 [01:07<00:50,  2.53s/it] 60%|██████    | 29/48 [01:10<00:48,  2.56s/it] 62%|██████▎   | 30/48 [01:13<00:46,  2.60s/it]                                               {'loss': 0.2813, 'grad_norm': 1.2263916730880737, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:13<00:46,  2.60s/it] 65%|██████▍   | 31/48 [01:15<00:43,  2.59s/it] 67%|██████▋   | 32/48 [01:18<00:40,  2.54s/it] 69%|██████▉   | 33/48 [01:20<00:37,  2.48s/it] 71%|███████   | 34/48 [01:23<00:34,  2.48s/it] 73%|███████▎  | 35/48 [01:25<00:31,  2.46s/it] 75%|███████▌  | 36/48 [01:28<00:32,  2.67s/it] 77%|███████▋  | 37/48 [01:30<00:26,  2.45s/it] 79%|███████▉  | 38/48 [01:32<00:23,  2.39s/it] 81%|████████▏ | 39/48 [01:34<00:20,  2.23s/it] 83%|████████▎ | 40/48 [01:36<00:17,  2.23s/it]                                               {'loss': 0.3878, 'grad_norm': 2.9699864387512207, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:36<00:17,  2.23s/it] 85%|████████▌ | 41/48 [01:39<00:16,  2.36s/it] 88%|████████▊ | 42/48 [01:41<00:13,  2.30s/it] 90%|████████▉ | 43/48 [01:44<00:12,  2.44s/it] 92%|█████████▏| 44/48 [01:46<00:09,  2.38s/it] 94%|█████████▍| 45/48 [01:49<00:07,  2.62s/it] 96%|█████████▌| 46/48 [01:52<00:05,  2.71s/it] 98%|█████████▊| 47/48 [01:55<00:02,  2.54s/it]100%|██████████| 48/48 [01:57<00:00,  2.48s/it]                                               {'train_runtime': 117.9717, 'train_samples_per_second': 4.883, 'train_steps_per_second': 0.407, 'train_loss': 0.33904508004585904, 'epoch': 2.0}
100%|██████████| 48/48 [01:57<00:00,  2.48s/it]100%|██████████| 48/48 [01:57<00:00,  2.45s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:38,  2.74it/s]  1%|          | 2/272 [00:00<01:19,  3.39it/s]  1%|          | 3/272 [00:00<01:16,  3.54it/s]  1%|▏         | 4/272 [00:01<01:13,  3.64it/s]  2%|▏         | 5/272 [00:01<01:13,  3.64it/s]  2%|▏         | 6/272 [00:01<01:10,  3.80it/s]  3%|▎         | 7/272 [00:01<01:10,  3.78it/s]  3%|▎         | 8/272 [00:02<01:10,  3.76it/s]  3%|▎         | 9/272 [00:02<01:10,  3.72it/s]  4%|▎         | 10/272 [00:02<01:08,  3.85it/s]  4%|▍         | 11/272 [00:02<01:08,  3.83it/s]  4%|▍         | 12/272 [00:03<01:08,  3.81it/s]  5%|▍         | 13/272 [00:03<01:09,  3.75it/s]  5%|▌         | 14/272 [00:03<01:07,  3.82it/s]  6%|▌         | 15/272 [00:04<01:06,  3.88it/s]  6%|▌         | 16/272 [00:04<01:06,  3.83it/s]  6%|▋         | 17/272 [00:04<01:06,  3.82it/s]  7%|▋         | 18/272 [00:04<01:07,  3.77it/s]  7%|▋         | 19/272 [00:05<01:05,  3.88it/s]  7%|▋         | 20/272 [00:05<01:05,  3.84it/s]  8%|▊         | 21/272 [00:05<01:06,  3.80it/s]  8%|▊         | 22/272 [00:05<01:06,  3.75it/s]  8%|▊         | 23/272 [00:06<01:04,  3.87it/s]  9%|▉         | 24/272 [00:06<01:04,  3.84it/s]  9%|▉         | 25/272 [00:06<01:04,  3.82it/s] 10%|▉         | 26/272 [00:06<01:05,  3.76it/s] 10%|▉         | 27/272 [00:07<01:04,  3.82it/s] 10%|█         | 28/272 [00:07<01:02,  3.89it/s] 11%|█         | 29/272 [00:07<01:03,  3.83it/s] 11%|█         | 30/272 [00:07<01:03,  3.82it/s] 11%|█▏        | 31/272 [00:08<01:03,  3.77it/s] 12%|█▏        | 32/272 [00:08<01:01,  3.88it/s] 12%|█▏        | 33/272 [00:08<01:02,  3.83it/s] 12%|█▎        | 34/272 [00:08<01:02,  3.79it/s] 13%|█▎        | 35/272 [00:09<01:03,  3.75it/s] 13%|█▎        | 36/272 [00:09<01:01,  3.86it/s] 14%|█▎        | 37/272 [00:09<01:01,  3.84it/s] 14%|█▍        | 38/272 [00:10<01:01,  3.81it/s] 14%|█▍        | 39/272 [00:10<01:02,  3.75it/s] 15%|█▍        | 40/272 [00:10<01:00,  3.82it/s] 15%|█▌        | 41/272 [00:10<00:59,  3.87it/s] 15%|█▌        | 42/272 [00:11<01:00,  3.82it/s] 16%|█▌        | 43/272 [00:11<01:00,  3.81it/s] 16%|█▌        | 44/272 [00:11<01:00,  3.75it/s] 17%|█▋        | 45/272 [00:11<00:58,  3.85it/s] 17%|█▋        | 46/272 [00:12<00:59,  3.80it/s] 17%|█▋        | 47/272 [00:12<00:59,  3.77it/s] 18%|█▊        | 48/272 [00:12<01:00,  3.71it/s] 18%|█▊        | 49/272 [00:12<00:58,  3.83it/s] 18%|█▊        | 50/272 [00:13<00:57,  3.84it/s] 19%|█▉        | 51/272 [00:13<00:58,  3.80it/s] 19%|█▉        | 52/272 [00:13<00:58,  3.74it/s] 19%|█▉        | 53/272 [00:13<00:57,  3.80it/s] 20%|█▉        | 54/272 [00:14<00:56,  3.85it/s] 20%|██        | 55/272 [00:14<00:57,  3.80it/s] 21%|██        | 56/272 [00:14<00:57,  3.76it/s] 21%|██        | 57/272 [00:15<00:56,  3.78it/s] 21%|██▏       | 58/272 [00:15<00:55,  3.84it/s] 22%|██▏       | 59/272 [00:15<00:56,  3.80it/s] 22%|██▏       | 60/272 [00:15<00:55,  3.82it/s] 22%|██▏       | 61/272 [00:16<00:56,  3.75it/s] 23%|██▎       | 62/272 [00:16<00:54,  3.84it/s] 23%|██▎       | 63/272 [00:16<00:55,  3.79it/s] 24%|██▎       | 64/272 [00:16<00:55,  3.77it/s] 24%|██▍       | 65/272 [00:17<00:55,  3.72it/s] 24%|██▍       | 66/272 [00:17<00:53,  3.84it/s] 25%|██▍       | 67/272 [00:17<00:53,  3.82it/s] 25%|██▌       | 68/272 [00:17<00:53,  3.79it/s] 25%|██▌       | 69/272 [00:18<00:54,  3.74it/s] 26%|██▌       | 70/272 [00:18<00:53,  3.80it/s] 26%|██▌       | 71/272 [00:18<00:52,  3.86it/s] 26%|██▋       | 72/272 [00:19<00:52,  3.81it/s] 27%|██▋       | 73/272 [00:19<00:52,  3.78it/s] 27%|██▋       | 74/272 [00:19<00:52,  3.77it/s] 28%|██▊       | 75/272 [00:19<00:51,  3.84it/s] 28%|██▊       | 76/272 [00:20<00:51,  3.79it/s] 28%|██▊       | 77/272 [00:20<00:51,  3.77it/s] 29%|██▊       | 78/272 [00:20<00:51,  3.73it/s] 29%|██▉       | 79/272 [00:20<00:50,  3.86it/s] 29%|██▉       | 80/272 [00:21<00:50,  3.83it/s] 30%|██▉       | 81/272 [00:21<00:50,  3.79it/s] 30%|███       | 82/272 [00:21<00:50,  3.73it/s] 31%|███       | 83/272 [00:21<00:49,  3.79it/s] 31%|███       | 84/272 [00:22<00:48,  3.84it/s] 31%|███▏      | 85/272 [00:22<00:49,  3.78it/s] 32%|███▏      | 86/272 [00:22<00:49,  3.77it/s] 32%|███▏      | 87/272 [00:22<00:49,  3.72it/s] 32%|███▏      | 88/272 [00:23<00:47,  3.84it/s] 33%|███▎      | 89/272 [00:23<00:48,  3.81it/s] 33%|███▎      | 90/272 [00:23<00:48,  3.77it/s] 33%|███▎      | 91/272 [00:24<00:48,  3.71it/s] 34%|███▍      | 92/272 [00:24<00:46,  3.83it/s] 34%|███▍      | 93/272 [00:24<00:46,  3.84it/s] 35%|███▍      | 94/272 [00:24<00:46,  3.79it/s] 35%|███▍      | 95/272 [00:25<00:47,  3.76it/s] 35%|███▌      | 96/272 [00:25<00:46,  3.79it/s] 36%|███▌      | 97/272 [00:25<00:45,  3.84it/s] 36%|███▌      | 98/272 [00:25<00:45,  3.79it/s] 36%|███▋      | 99/272 [00:26<00:46,  3.74it/s] 37%|███▋      | 100/272 [00:26<00:44,  3.85it/s] 37%|███▋      | 101/272 [00:26<00:43,  3.94it/s] 38%|███▊      | 102/272 [00:26<00:42,  4.00it/s] 38%|███▊      | 103/272 [00:27<00:41,  4.05it/s] 38%|███▊      | 104/272 [00:27<00:41,  4.08it/s] 39%|███▊      | 105/272 [00:27<00:40,  4.11it/s] 39%|███▉      | 106/272 [00:27<00:40,  4.12it/s] 39%|███▉      | 107/272 [00:28<00:39,  4.13it/s] 40%|███▉      | 108/272 [00:28<00:39,  4.13it/s] 40%|████      | 109/272 [00:28<00:39,  4.13it/s] 40%|████      | 110/272 [00:28<00:39,  4.13it/s] 41%|████      | 111/272 [00:29<00:39,  4.12it/s] 41%|████      | 112/272 [00:29<00:38,  4.12it/s] 42%|████▏     | 113/272 [00:29<00:38,  4.13it/s] 42%|████▏     | 114/272 [00:29<00:38,  4.14it/s] 42%|████▏     | 115/272 [00:29<00:37,  4.15it/s] 43%|████▎     | 116/272 [00:30<00:37,  4.15it/s] 43%|████▎     | 117/272 [00:30<00:37,  4.16it/s] 43%|████▎     | 118/272 [00:30<00:37,  4.16it/s] 44%|████▍     | 119/272 [00:30<00:36,  4.16it/s] 44%|████▍     | 120/272 [00:31<00:36,  4.16it/s] 44%|████▍     | 121/272 [00:31<00:36,  4.16it/s] 45%|████▍     | 122/272 [00:31<00:36,  4.16it/s] 45%|████▌     | 123/272 [00:31<00:35,  4.17it/s] 46%|████▌     | 124/272 [00:32<00:35,  4.16it/s] 46%|████▌     | 125/272 [00:32<00:35,  4.16it/s] 46%|████▋     | 126/272 [00:32<00:35,  4.16it/s] 47%|████▋     | 127/272 [00:32<00:34,  4.16it/s] 47%|████▋     | 128/272 [00:33<00:34,  4.17it/s] 47%|████▋     | 129/272 [00:33<00:34,  4.16it/s] 48%|████▊     | 130/272 [00:33<00:34,  4.16it/s] 48%|████▊     | 131/272 [00:33<00:33,  4.16it/s] 49%|████▊     | 132/272 [00:34<00:33,  4.16it/s] 49%|████▉     | 133/272 [00:34<00:33,  4.16it/s] 49%|████▉     | 134/272 [00:34<00:33,  4.11it/s] 50%|████▉     | 135/272 [00:34<00:34,  3.97it/s] 50%|█████     | 136/272 [00:35<00:35,  3.88it/s] 50%|█████     | 137/272 [00:35<00:34,  3.89it/s] 51%|█████     | 138/272 [00:35<00:34,  3.88it/s] 51%|█████     | 139/272 [00:35<00:35,  3.76it/s] 51%|█████▏    | 140/272 [00:36<00:35,  3.74it/s] 52%|█████▏    | 141/272 [00:36<00:35,  3.74it/s] 52%|█████▏    | 142/272 [00:36<00:34,  3.78it/s] 53%|█████▎    | 143/272 [00:36<00:34,  3.69it/s] 53%|█████▎    | 144/272 [00:37<00:34,  3.69it/s] 53%|█████▎    | 145/272 [00:37<00:33,  3.80it/s] 54%|█████▎    | 146/272 [00:37<00:32,  3.90it/s] 54%|█████▍    | 147/272 [00:37<00:31,  3.97it/s] 54%|█████▍    | 148/272 [00:38<00:30,  4.02it/s] 55%|█████▍    | 149/272 [00:38<00:30,  4.06it/s] 55%|█████▌    | 150/272 [00:38<00:29,  4.09it/s] 56%|█████▌    | 151/272 [00:38<00:29,  4.11it/s] 56%|█████▌    | 152/272 [00:39<00:29,  4.12it/s] 56%|█████▋    | 153/272 [00:39<00:28,  4.12it/s] 57%|█████▋    | 154/272 [00:39<00:28,  4.11it/s] 57%|█████▋    | 155/272 [00:39<00:28,  4.11it/s] 57%|█████▋    | 156/272 [00:40<00:28,  4.12it/s] 58%|█████▊    | 157/272 [00:40<00:27,  4.12it/s] 58%|█████▊    | 158/272 [00:40<00:27,  4.13it/s] 58%|█████▊    | 159/272 [00:40<00:27,  4.13it/s] 59%|█████▉    | 160/272 [00:41<00:27,  4.14it/s] 59%|█████▉    | 161/272 [00:41<00:26,  4.14it/s] 60%|█████▉    | 162/272 [00:41<00:26,  4.12it/s] 60%|█████▉    | 163/272 [00:41<00:26,  4.12it/s] 60%|██████    | 164/272 [00:42<00:26,  4.12it/s] 61%|██████    | 165/272 [00:42<00:25,  4.12it/s] 61%|██████    | 166/272 [00:42<00:25,  4.13it/s] 61%|██████▏   | 167/272 [00:42<00:25,  4.14it/s] 62%|██████▏   | 168/272 [00:43<00:25,  4.14it/s] 62%|██████▏   | 169/272 [00:43<00:24,  4.15it/s] 62%|██████▎   | 170/272 [00:43<00:24,  4.15it/s] 63%|██████▎   | 171/272 [00:43<00:24,  4.14it/s] 63%|██████▎   | 172/272 [00:44<00:24,  4.14it/s] 64%|██████▎   | 173/272 [00:44<00:23,  4.13it/s] 64%|██████▍   | 174/272 [00:44<00:23,  4.12it/s] 64%|██████▍   | 175/272 [00:44<00:23,  4.12it/s] 65%|██████▍   | 176/272 [00:45<00:23,  4.12it/s] 65%|██████▌   | 177/272 [00:45<00:23,  4.12it/s] 65%|██████▌   | 178/272 [00:45<00:22,  4.13it/s] 66%|██████▌   | 179/272 [00:45<00:22,  4.12it/s] 66%|██████▌   | 180/272 [00:45<00:22,  4.11it/s] 67%|██████▋   | 181/272 [00:46<00:22,  4.11it/s] 67%|██████▋   | 182/272 [00:46<00:21,  4.11it/s] 67%|██████▋   | 183/272 [00:46<00:21,  4.12it/s] 68%|██████▊   | 184/272 [00:46<00:21,  4.08it/s] 68%|██████▊   | 185/272 [00:47<00:21,  3.96it/s] 68%|██████▊   | 186/272 [00:47<00:22,  3.87it/s] 69%|██████▉   | 187/272 [00:47<00:22,  3.86it/s] 69%|██████▉   | 188/272 [00:48<00:21,  3.88it/s] 69%|██████▉   | 189/272 [00:48<00:22,  3.76it/s] 70%|██████▉   | 190/272 [00:48<00:21,  3.73it/s] 70%|███████   | 191/272 [00:48<00:21,  3.73it/s] 71%|███████   | 192/272 [00:49<00:21,  3.77it/s] 71%|███████   | 193/272 [00:49<00:21,  3.68it/s] 71%|███████▏  | 194/272 [00:49<00:21,  3.71it/s] 72%|███████▏  | 195/272 [00:49<00:20,  3.69it/s] 72%|███████▏  | 196/272 [00:50<00:20,  3.76it/s] 72%|███████▏  | 197/272 [00:50<00:20,  3.68it/s] 73%|███████▎  | 198/272 [00:50<00:19,  3.73it/s] 73%|███████▎  | 199/272 [00:51<00:19,  3.69it/s] 74%|███████▎  | 200/272 [00:51<00:19,  3.74it/s] 74%|███████▍  | 201/272 [00:51<00:19,  3.67it/s] 74%|███████▍  | 202/272 [00:51<00:19,  3.66it/s] 75%|███████▍  | 203/272 [00:52<00:18,  3.78it/s] 75%|███████▌  | 204/272 [00:52<00:17,  3.88it/s] 75%|███████▌  | 205/272 [00:52<00:16,  3.95it/s] 76%|███████▌  | 206/272 [00:52<00:16,  3.99it/s] 76%|███████▌  | 207/272 [00:53<00:16,  4.02it/s] 76%|███████▋  | 208/272 [00:53<00:15,  4.04it/s] 77%|███████▋  | 209/272 [00:53<00:15,  4.07it/s] 77%|███████▋  | 210/272 [00:53<00:15,  4.09it/s] 78%|███████▊  | 211/272 [00:54<00:14,  4.10it/s] 78%|███████▊  | 212/272 [00:54<00:14,  4.10it/s] 78%|███████▊  | 213/272 [00:54<00:14,  4.09it/s] 79%|███████▊  | 214/272 [00:54<00:14,  4.10it/s] 79%|███████▉  | 215/272 [00:54<00:13,  4.10it/s] 79%|███████▉  | 216/272 [00:55<00:13,  4.11it/s] 80%|███████▉  | 217/272 [00:55<00:13,  4.12it/s] 80%|████████  | 218/272 [00:55<00:13,  4.11it/s] 81%|████████  | 219/272 [00:55<00:12,  4.10it/s] 81%|████████  | 220/272 [00:56<00:12,  4.10it/s] 81%|████████▏ | 221/272 [00:56<00:12,  4.11it/s] 82%|████████▏ | 222/272 [00:56<00:12,  4.12it/s] 82%|████████▏ | 223/272 [00:56<00:11,  4.12it/s] 82%|████████▏ | 224/272 [00:57<00:11,  4.11it/s] 83%|████████▎ | 225/272 [00:57<00:11,  4.10it/s] 83%|████████▎ | 226/272 [00:57<00:11,  4.11it/s] 83%|████████▎ | 227/272 [00:57<00:10,  4.11it/s] 84%|████████▍ | 228/272 [00:58<00:10,  4.11it/s] 84%|████████▍ | 229/272 [00:58<00:10,  4.11it/s] 85%|████████▍ | 230/272 [00:58<00:10,  4.11it/s] 85%|████████▍ | 231/272 [00:58<00:10,  4.10it/s] 85%|████████▌ | 232/272 [00:59<00:09,  4.07it/s] 86%|████████▌ | 233/272 [00:59<00:09,  3.94it/s] 86%|████████▌ | 234/272 [00:59<00:09,  3.85it/s] 86%|████████▋ | 235/272 [00:59<00:09,  3.92it/s] 87%|████████▋ | 236/272 [01:00<00:09,  3.82it/s] 87%|████████▋ | 237/272 [01:00<00:09,  3.76it/s] 88%|████████▊ | 238/272 [01:00<00:09,  3.70it/s] 88%|████████▊ | 239/272 [01:01<00:08,  3.80it/s] 88%|████████▊ | 240/272 [01:01<00:08,  3.74it/s] 89%|████████▊ | 241/272 [01:01<00:08,  3.71it/s] 89%|████████▉ | 242/272 [01:01<00:08,  3.66it/s] 89%|████████▉ | 243/272 [01:02<00:07,  3.76it/s] 90%|████████▉ | 244/272 [01:02<00:07,  3.72it/s] 90%|█████████ | 245/272 [01:02<00:07,  3.70it/s] 90%|█████████ | 246/272 [01:02<00:07,  3.66it/s] 91%|█████████ | 247/272 [01:03<00:06,  3.75it/s] 91%|█████████ | 248/272 [01:03<00:06,  3.71it/s] 92%|█████████▏| 249/272 [01:03<00:06,  3.69it/s] 92%|█████████▏| 250/272 [01:03<00:05,  3.72it/s] 92%|█████████▏| 251/272 [01:04<00:05,  3.74it/s] 93%|█████████▎| 252/272 [01:04<00:05,  3.70it/s] 93%|█████████▎| 253/272 [01:04<00:05,  3.65it/s] 93%|█████████▎| 254/272 [01:05<00:04,  3.72it/s] 94%|█████████▍| 255/272 [01:05<00:04,  3.69it/s] 94%|█████████▍| 256/272 [01:05<00:04,  3.67it/s] 94%|█████████▍| 257/272 [01:05<00:04,  3.64it/s] 95%|█████████▍| 258/272 [01:06<00:03,  3.74it/s] 95%|█████████▌| 259/272 [01:06<00:03,  3.70it/s] 96%|█████████▌| 260/272 [01:06<00:03,  3.68it/s] 96%|█████████▌| 261/272 [01:06<00:03,  3.64it/s] 96%|█████████▋| 262/272 [01:07<00:02,  3.74it/s] 97%|█████████▋| 263/272 [01:07<00:02,  3.70it/s] 97%|█████████▋| 264/272 [01:07<00:02,  3.73it/s] 97%|█████████▋| 265/272 [01:08<00:01,  3.68it/s] 98%|█████████▊| 266/272 [01:08<00:01,  3.72it/s] 98%|█████████▊| 267/272 [01:08<00:01,  3.68it/s] 99%|█████████▊| 268/272 [01:08<00:01,  3.65it/s] 99%|█████████▉| 269/272 [01:09<00:00,  3.72it/s] 99%|█████████▉| 270/272 [01:09<00:00,  3.74it/s]100%|█████████▉| 271/272 [01:09<00:00,  3.70it/s]100%|██████████| 272/272 [01:09<00:00,  3.66it/s]accuracy:  0.9007352941176471
100%|██████████| 272/272 [01:14<00:00,  3.68it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 149
Training dataset size: 288, validation dataset size: 149
Training dataset size: 288, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.27s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:20:11,915] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:20:12,116] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:20:12,484] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:03<02:26,  3.12s/it]  4%|▍         | 2/48 [00:05<02:04,  2.70s/it]  6%|▋         | 3/48 [00:07<01:54,  2.53s/it]  8%|▊         | 4/48 [00:09<01:43,  2.35s/it] 10%|█         | 5/48 [00:12<01:42,  2.39s/it] 12%|█▎        | 6/48 [00:14<01:36,  2.29s/it] 15%|█▍        | 7/48 [00:17<01:37,  2.38s/it] 17%|█▋        | 8/48 [00:19<01:30,  2.25s/it] 19%|█▉        | 9/48 [00:21<01:33,  2.40s/it] 21%|██        | 10/48 [00:24<01:31,  2.42s/it]                                               {'loss': 0.7647, 'grad_norm': 14.398802757263184, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:24<01:31,  2.42s/it] 23%|██▎       | 11/48 [00:26<01:25,  2.30s/it] 25%|██▌       | 12/48 [00:28<01:20,  2.25s/it] 27%|██▋       | 13/48 [00:30<01:20,  2.29s/it] 29%|██▉       | 14/48 [00:33<01:21,  2.38s/it] 31%|███▏      | 15/48 [00:35<01:14,  2.27s/it] 33%|███▎      | 16/48 [00:37<01:12,  2.27s/it] 35%|███▌      | 17/48 [00:40<01:12,  2.35s/it] 38%|███▊      | 18/48 [00:43<01:17,  2.57s/it] 40%|███▉      | 19/48 [00:45<01:12,  2.49s/it] 42%|████▏     | 20/48 [00:48<01:13,  2.64s/it]                                               {'loss': 0.5439, 'grad_norm': 5.467897415161133, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:48<01:13,  2.64s/it] 44%|████▍     | 21/48 [00:50<01:06,  2.47s/it] 46%|████▌     | 22/48 [00:53<01:06,  2.54s/it] 48%|████▊     | 23/48 [00:55<01:02,  2.50s/it] 50%|█████     | 24/48 [00:58<01:02,  2.60s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:00<00:58,  2.54s/it] 54%|█████▍    | 26/48 [01:02<00:51,  2.34s/it] 56%|█████▋    | 27/48 [01:05<00:50,  2.39s/it] 58%|█████▊    | 28/48 [01:07<00:47,  2.36s/it] 60%|██████    | 29/48 [01:10<00:44,  2.37s/it] 62%|██████▎   | 30/48 [01:12<00:43,  2.43s/it]                                               {'loss': 0.4144, 'grad_norm': 3.398397922515869, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:12<00:43,  2.43s/it] 65%|██████▍   | 31/48 [01:14<00:38,  2.24s/it] 67%|██████▋   | 32/48 [01:16<00:35,  2.22s/it] 69%|██████▉   | 33/48 [01:18<00:34,  2.27s/it] 71%|███████   | 34/48 [01:21<00:32,  2.33s/it] 73%|███████▎  | 35/48 [01:23<00:29,  2.25s/it] 75%|███████▌  | 36/48 [01:25<00:26,  2.19s/it] 77%|███████▋  | 37/48 [01:27<00:23,  2.18s/it] 79%|███████▉  | 38/48 [01:29<00:21,  2.11s/it] 81%|████████▏ | 39/48 [01:32<00:19,  2.20s/it] 83%|████████▎ | 40/48 [01:34<00:18,  2.27s/it]                                               {'loss': 0.3436, 'grad_norm': 5.4952216148376465, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:34<00:18,  2.27s/it] 85%|████████▌ | 41/48 [01:37<00:16,  2.38s/it] 88%|████████▊ | 42/48 [01:39<00:14,  2.42s/it] 90%|████████▉ | 43/48 [01:41<00:11,  2.35s/it] 92%|█████████▏| 44/48 [01:44<00:09,  2.33s/it] 94%|█████████▍| 45/48 [01:46<00:07,  2.37s/it] 96%|█████████▌| 46/48 [01:49<00:04,  2.42s/it] 98%|█████████▊| 47/48 [01:51<00:02,  2.32s/it]100%|██████████| 48/48 [01:53<00:00,  2.30s/it]                                               {'train_runtime': 114.144, 'train_samples_per_second': 5.046, 'train_steps_per_second': 0.421, 'train_loss': 0.5279830892880758, 'epoch': 2.0}
100%|██████████| 48/48 [01:53<00:00,  2.30s/it]100%|██████████| 48/48 [01:53<00:00,  2.37s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<00:53,  3.25it/s]  1%|          | 2/175 [00:00<00:49,  3.47it/s]  2%|▏         | 3/175 [00:00<00:48,  3.56it/s]  2%|▏         | 4/175 [00:01<00:47,  3.60it/s]  3%|▎         | 5/175 [00:01<00:44,  3.79it/s]  3%|▎         | 6/175 [00:01<00:45,  3.73it/s]  4%|▍         | 7/175 [00:01<00:45,  3.71it/s]  5%|▍         | 8/175 [00:02<00:45,  3.68it/s]  5%|▌         | 9/175 [00:02<00:43,  3.82it/s]  6%|▌         | 10/175 [00:02<00:43,  3.76it/s]  6%|▋         | 11/175 [00:02<00:43,  3.74it/s]  7%|▋         | 12/175 [00:03<00:44,  3.70it/s]  7%|▋         | 13/175 [00:03<00:42,  3.83it/s]  8%|▊         | 14/175 [00:03<00:42,  3.79it/s]  9%|▊         | 15/175 [00:04<00:42,  3.75it/s]  9%|▉         | 16/175 [00:04<00:43,  3.69it/s] 10%|▉         | 17/175 [00:04<00:41,  3.82it/s] 10%|█         | 18/175 [00:04<00:41,  3.82it/s] 11%|█         | 19/175 [00:05<00:41,  3.73it/s] 11%|█▏        | 20/175 [00:05<00:42,  3.67it/s] 12%|█▏        | 21/175 [00:05<00:40,  3.82it/s] 13%|█▎        | 22/175 [00:05<00:39,  3.84it/s] 13%|█▎        | 23/175 [00:06<00:40,  3.75it/s] 14%|█▎        | 24/175 [00:06<00:41,  3.68it/s] 14%|█▍        | 25/175 [00:06<00:39,  3.82it/s] 15%|█▍        | 26/175 [00:06<00:38,  3.84it/s] 15%|█▌        | 27/175 [00:07<00:39,  3.74it/s] 16%|█▌        | 28/175 [00:07<00:39,  3.68it/s] 17%|█▋        | 29/175 [00:07<00:38,  3.82it/s] 17%|█▋        | 30/175 [00:08<00:37,  3.85it/s] 18%|█▊        | 31/175 [00:08<00:38,  3.74it/s] 18%|█▊        | 32/175 [00:08<00:38,  3.68it/s] 19%|█▉        | 33/175 [00:08<00:37,  3.81it/s] 19%|█▉        | 34/175 [00:09<00:36,  3.84it/s] 20%|██        | 35/175 [00:09<00:37,  3.74it/s] 21%|██        | 36/175 [00:09<00:37,  3.69it/s] 21%|██        | 37/175 [00:09<00:36,  3.80it/s] 22%|██▏       | 38/175 [00:10<00:35,  3.83it/s] 22%|██▏       | 39/175 [00:10<00:36,  3.74it/s] 23%|██▎       | 40/175 [00:10<00:35,  3.79it/s] 23%|██▎       | 41/175 [00:10<00:35,  3.74it/s] 24%|██▍       | 42/175 [00:11<00:35,  3.80it/s] 25%|██▍       | 43/175 [00:11<00:35,  3.73it/s] 25%|██▌       | 44/175 [00:11<00:34,  3.78it/s] 26%|██▌       | 45/175 [00:12<00:34,  3.73it/s] 26%|██▋       | 46/175 [00:12<00:33,  3.81it/s] 27%|██▋       | 47/175 [00:12<00:34,  3.73it/s] 27%|██▋       | 48/175 [00:12<00:33,  3.77it/s] 28%|██▊       | 49/175 [00:13<00:33,  3.72it/s] 29%|██▊       | 50/175 [00:13<00:33,  3.78it/s] 29%|██▉       | 51/175 [00:13<00:33,  3.72it/s] 30%|██▉       | 52/175 [00:13<00:32,  3.77it/s] 30%|███       | 53/175 [00:14<00:32,  3.72it/s] 31%|███       | 54/175 [00:14<00:31,  3.79it/s] 31%|███▏      | 55/175 [00:14<00:32,  3.73it/s] 32%|███▏      | 56/175 [00:14<00:31,  3.77it/s] 33%|███▎      | 57/175 [00:15<00:31,  3.72it/s] 33%|███▎      | 58/175 [00:15<00:30,  3.81it/s] 34%|███▎      | 59/175 [00:15<00:31,  3.73it/s] 34%|███▍      | 60/175 [00:15<00:30,  3.78it/s] 35%|███▍      | 61/175 [00:16<00:30,  3.72it/s] 35%|███▌      | 62/175 [00:16<00:29,  3.79it/s] 36%|███▌      | 63/175 [00:16<00:30,  3.72it/s] 37%|███▋      | 64/175 [00:17<00:29,  3.76it/s] 37%|███▋      | 65/175 [00:17<00:29,  3.71it/s] 38%|███▊      | 66/175 [00:17<00:28,  3.78it/s] 38%|███▊      | 67/175 [00:17<00:29,  3.71it/s] 39%|███▉      | 68/175 [00:18<00:28,  3.76it/s] 39%|███▉      | 69/175 [00:18<00:28,  3.76it/s] 40%|████      | 70/175 [00:18<00:27,  3.86it/s] 41%|████      | 71/175 [00:18<00:26,  3.94it/s] 41%|████      | 72/175 [00:19<00:25,  4.01it/s] 42%|████▏     | 73/175 [00:19<00:25,  4.05it/s] 42%|████▏     | 74/175 [00:19<00:24,  4.09it/s] 43%|████▎     | 75/175 [00:19<00:24,  4.11it/s] 43%|████▎     | 76/175 [00:20<00:23,  4.13it/s] 44%|████▍     | 77/175 [00:20<00:23,  4.14it/s] 45%|████▍     | 78/175 [00:20<00:23,  4.15it/s] 45%|████▌     | 79/175 [00:20<00:23,  4.16it/s] 46%|████▌     | 80/175 [00:21<00:22,  4.16it/s] 46%|████▋     | 81/175 [00:21<00:22,  4.16it/s] 47%|████▋     | 82/175 [00:21<00:22,  4.16it/s] 47%|████▋     | 83/175 [00:21<00:22,  4.16it/s] 48%|████▊     | 84/175 [00:22<00:21,  4.16it/s] 49%|████▊     | 85/175 [00:22<00:21,  4.16it/s] 49%|████▉     | 86/175 [00:22<00:21,  4.16it/s] 50%|████▉     | 87/175 [00:22<00:21,  4.17it/s] 50%|█████     | 88/175 [00:22<00:20,  4.17it/s] 51%|█████     | 89/175 [00:23<00:20,  4.16it/s] 51%|█████▏    | 90/175 [00:23<00:20,  4.17it/s] 52%|█████▏    | 91/175 [00:23<00:20,  4.17it/s] 53%|█████▎    | 92/175 [00:23<00:19,  4.16it/s] 53%|█████▎    | 93/175 [00:24<00:19,  4.16it/s] 54%|█████▎    | 94/175 [00:24<00:19,  4.16it/s] 54%|█████▍    | 95/175 [00:24<00:19,  4.16it/s] 55%|█████▍    | 96/175 [00:24<00:19,  4.16it/s] 55%|█████▌    | 97/175 [00:25<00:18,  4.15it/s] 56%|█████▌    | 98/175 [00:25<00:18,  4.15it/s] 57%|█████▋    | 99/175 [00:25<00:18,  4.15it/s] 57%|█████▋    | 100/175 [00:25<00:18,  4.14it/s] 58%|█████▊    | 101/175 [00:26<00:17,  4.13it/s] 58%|█████▊    | 102/175 [00:26<00:17,  4.13it/s] 59%|█████▉    | 103/175 [00:26<00:17,  4.13it/s] 59%|█████▉    | 104/175 [00:26<00:17,  4.14it/s] 60%|██████    | 105/175 [00:27<00:16,  4.14it/s] 61%|██████    | 106/175 [00:27<00:17,  4.04it/s] 61%|██████    | 107/175 [00:27<00:17,  3.93it/s] 62%|██████▏   | 108/175 [00:27<00:17,  3.85it/s] 62%|██████▏   | 109/175 [00:28<00:16,  3.91it/s] 63%|██████▎   | 110/175 [00:28<00:16,  3.91it/s] 63%|██████▎   | 111/175 [00:28<00:16,  3.81it/s] 64%|██████▍   | 112/175 [00:28<00:16,  3.82it/s] 65%|██████▍   | 113/175 [00:29<00:16,  3.75it/s] 65%|██████▌   | 114/175 [00:29<00:15,  3.82it/s] 66%|██████▌   | 115/175 [00:29<00:16,  3.75it/s] 66%|██████▋   | 116/175 [00:29<00:15,  3.75it/s] 67%|██████▋   | 117/175 [00:30<00:15,  3.70it/s] 67%|██████▋   | 118/175 [00:30<00:14,  3.81it/s] 68%|██████▊   | 119/175 [00:30<00:14,  3.75it/s] 69%|██████▊   | 120/175 [00:31<00:14,  3.73it/s] 69%|██████▉   | 121/175 [00:31<00:14,  3.66it/s] 70%|██████▉   | 122/175 [00:31<00:13,  3.79it/s] 70%|███████   | 123/175 [00:31<00:13,  3.77it/s] 71%|███████   | 124/175 [00:32<00:13,  3.71it/s] 71%|███████▏  | 125/175 [00:32<00:13,  3.77it/s] 72%|███████▏  | 126/175 [00:32<00:13,  3.72it/s] 73%|███████▎  | 127/175 [00:32<00:12,  3.77it/s] 73%|███████▎  | 128/175 [00:33<00:12,  3.72it/s] 74%|███████▎  | 129/175 [00:33<00:12,  3.75it/s] 74%|███████▍  | 130/175 [00:33<00:12,  3.70it/s] 75%|███████▍  | 131/175 [00:33<00:11,  3.77it/s] 75%|███████▌  | 132/175 [00:34<00:11,  3.74it/s] 76%|███████▌  | 133/175 [00:34<00:11,  3.71it/s] 77%|███████▋  | 134/175 [00:34<00:11,  3.65it/s] 77%|███████▋  | 135/175 [00:35<00:10,  3.78it/s] 78%|███████▊  | 136/175 [00:35<00:10,  3.75it/s] 78%|███████▊  | 137/175 [00:35<00:10,  3.72it/s] 79%|███████▉  | 138/175 [00:35<00:09,  3.78it/s] 79%|███████▉  | 139/175 [00:36<00:09,  3.72it/s] 80%|████████  | 140/175 [00:36<00:09,  3.77it/s] 81%|████████  | 141/175 [00:36<00:09,  3.72it/s] 81%|████████  | 142/175 [00:36<00:08,  3.75it/s] 82%|████████▏ | 143/175 [00:37<00:08,  3.70it/s] 82%|████████▏ | 144/175 [00:37<00:08,  3.78it/s] 83%|████████▎ | 145/175 [00:37<00:08,  3.73it/s] 83%|████████▎ | 146/175 [00:38<00:07,  3.73it/s] 84%|████████▍ | 147/175 [00:38<00:07,  3.65it/s] 85%|████████▍ | 148/175 [00:38<00:07,  3.78it/s] 85%|████████▌ | 149/175 [00:38<00:06,  3.78it/s] 86%|████████▌ | 150/175 [00:39<00:06,  3.72it/s] 86%|████████▋ | 151/175 [00:39<00:06,  3.74it/s] 87%|████████▋ | 152/175 [00:39<00:06,  3.72it/s] 87%|████████▋ | 153/175 [00:39<00:05,  3.77it/s] 88%|████████▊ | 154/175 [00:40<00:05,  3.71it/s] 89%|████████▊ | 155/175 [00:40<00:05,  3.74it/s] 89%|████████▉ | 156/175 [00:40<00:05,  3.69it/s] 90%|████████▉ | 157/175 [00:40<00:04,  3.77it/s] 90%|█████████ | 158/175 [00:41<00:04,  3.71it/s] 91%|█████████ | 159/175 [00:41<00:04,  3.73it/s] 91%|█████████▏| 160/175 [00:41<00:04,  3.67it/s] 92%|█████████▏| 161/175 [00:42<00:03,  3.80it/s] 93%|█████████▎| 162/175 [00:42<00:03,  3.76it/s] 93%|█████████▎| 163/175 [00:42<00:03,  3.72it/s] 94%|█████████▎| 164/175 [00:42<00:02,  3.70it/s] 94%|█████████▍| 165/175 [00:43<00:02,  3.75it/s] 95%|█████████▍| 166/175 [00:43<00:02,  3.78it/s] 95%|█████████▌| 167/175 [00:43<00:02,  3.71it/s] 96%|█████████▌| 168/175 [00:43<00:01,  3.75it/s] 97%|█████████▋| 169/175 [00:44<00:01,  3.69it/s] 97%|█████████▋| 170/175 [00:44<00:01,  3.77it/s] 98%|█████████▊| 171/175 [00:44<00:01,  3.71it/s] 98%|█████████▊| 172/175 [00:44<00:00,  3.74it/s] 99%|█████████▉| 173/175 [00:45<00:00,  3.68it/s] 99%|█████████▉| 174/175 [00:45<00:00,  3.77it/s]100%|██████████| 175/175 [00:45<00:00,  3.73it/s]accuracy:  0.8457142857142858
100%|██████████| 175/175 [00:48<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 288, validation dataset size: 119
Training dataset size: 288, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 288, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.14s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:23:21,009] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:23:21,263] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 06:23:21,363] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/48 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/48 [00:02<02:03,  2.62s/it]  4%|▍         | 2/48 [00:04<01:53,  2.46s/it]  6%|▋         | 3/48 [00:07<02:01,  2.71s/it]  8%|▊         | 4/48 [00:10<02:01,  2.76s/it] 10%|█         | 5/48 [00:13<01:54,  2.67s/it] 12%|█▎        | 6/48 [00:15<01:49,  2.61s/it] 15%|█▍        | 7/48 [00:18<01:45,  2.58s/it] 17%|█▋        | 8/48 [00:20<01:39,  2.50s/it] 19%|█▉        | 9/48 [00:22<01:35,  2.44s/it] 21%|██        | 10/48 [00:25<01:34,  2.50s/it]                                               {'loss': 1.1089, 'grad_norm': 7.3166279792785645, 'learning_rate': 9.272097022732444e-06, 'epoch': 0.42}
 21%|██        | 10/48 [00:25<01:34,  2.50s/it] 23%|██▎       | 11/48 [00:28<01:34,  2.55s/it] 25%|██▌       | 12/48 [00:30<01:28,  2.47s/it] 27%|██▋       | 13/48 [00:33<01:27,  2.51s/it] 29%|██▉       | 14/48 [00:35<01:23,  2.45s/it] 31%|███▏      | 15/48 [00:37<01:14,  2.27s/it] 33%|███▎      | 16/48 [00:39<01:12,  2.27s/it] 35%|███▌      | 17/48 [00:42<01:13,  2.37s/it] 38%|███▊      | 18/48 [00:45<01:16,  2.54s/it] 40%|███▉      | 19/48 [00:47<01:14,  2.55s/it] 42%|████▏     | 20/48 [00:50<01:10,  2.52s/it]                                               {'loss': 1.151, 'grad_norm': 12.792872428894043, 'learning_rate': 6.674398060854931e-06, 'epoch': 0.83}
 42%|████▏     | 20/48 [00:50<01:10,  2.52s/it] 44%|████▍     | 21/48 [00:53<01:11,  2.67s/it] 46%|████▌     | 22/48 [00:55<01:08,  2.65s/it] 48%|████▊     | 23/48 [00:58<01:05,  2.61s/it] 50%|█████     | 24/48 [01:00<01:02,  2.60s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 25/48 [01:03<01:01,  2.65s/it] 54%|█████▍    | 26/48 [01:06<00:57,  2.63s/it] 56%|█████▋    | 27/48 [01:08<00:52,  2.52s/it] 58%|█████▊    | 28/48 [01:11<00:51,  2.57s/it] 60%|██████    | 29/48 [01:13<00:48,  2.53s/it] 62%|██████▎   | 30/48 [01:16<00:44,  2.50s/it]                                               {'loss': 0.934, 'grad_norm': 7.184226036071777, 'learning_rate': 3.3256019391450696e-06, 'epoch': 1.25}
 62%|██████▎   | 30/48 [01:16<00:44,  2.50s/it] 65%|██████▍   | 31/48 [01:18<00:43,  2.55s/it] 67%|██████▋   | 32/48 [01:21<00:39,  2.49s/it] 69%|██████▉   | 33/48 [01:24<00:40,  2.68s/it] 71%|███████   | 34/48 [01:26<00:37,  2.66s/it] 73%|███████▎  | 35/48 [01:28<00:32,  2.51s/it] 75%|███████▌  | 36/48 [01:31<00:30,  2.53s/it] 77%|███████▋  | 37/48 [01:34<00:28,  2.59s/it] 79%|███████▉  | 38/48 [01:37<00:26,  2.67s/it] 81%|████████▏ | 39/48 [01:39<00:24,  2.71s/it] 83%|████████▎ | 40/48 [01:42<00:21,  2.68s/it]                                               {'loss': 0.8442, 'grad_norm': 5.619507789611816, 'learning_rate': 7.279029772675572e-07, 'epoch': 1.67}
 83%|████████▎ | 40/48 [01:42<00:21,  2.68s/it] 85%|████████▌ | 41/48 [01:44<00:17,  2.55s/it] 88%|████████▊ | 42/48 [01:47<00:14,  2.47s/it] 90%|████████▉ | 43/48 [01:50<00:13,  2.69s/it] 92%|█████████▏| 44/48 [01:52<00:10,  2.68s/it] 94%|█████████▍| 45/48 [01:55<00:07,  2.53s/it] 96%|█████████▌| 46/48 [01:57<00:04,  2.48s/it] 98%|█████████▊| 47/48 [01:59<00:02,  2.40s/it]100%|██████████| 48/48 [02:02<00:00,  2.46s/it]                                               {'train_runtime': 122.8934, 'train_samples_per_second': 4.687, 'train_steps_per_second': 0.391, 'train_loss': 0.9860749244689941, 'epoch': 2.0}
100%|██████████| 48/48 [02:02<00:00,  2.46s/it]100%|██████████| 48/48 [02:02<00:00,  2.56s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:51,  2.97it/s]  1%|▏         | 2/153 [00:00<00:45,  3.35it/s]  2%|▏         | 3/153 [00:00<00:43,  3.45it/s]  3%|▎         | 4/153 [00:01<00:40,  3.70it/s]  3%|▎         | 5/153 [00:01<00:39,  3.76it/s]  4%|▍         | 6/153 [00:01<00:39,  3.70it/s]  5%|▍         | 7/153 [00:01<00:38,  3.77it/s]  5%|▌         | 8/153 [00:02<00:38,  3.72it/s]  6%|▌         | 9/153 [00:02<00:38,  3.78it/s]  7%|▋         | 10/153 [00:02<00:38,  3.74it/s]  7%|▋         | 11/153 [00:02<00:37,  3.78it/s]  8%|▊         | 12/153 [00:03<00:37,  3.72it/s]  8%|▊         | 13/153 [00:03<00:36,  3.78it/s]  9%|▉         | 14/153 [00:03<00:37,  3.73it/s] 10%|▉         | 15/153 [00:04<00:36,  3.78it/s] 10%|█         | 16/153 [00:04<00:36,  3.73it/s] 11%|█         | 17/153 [00:04<00:35,  3.80it/s] 12%|█▏        | 18/153 [00:04<00:35,  3.75it/s] 12%|█▏        | 19/153 [00:05<00:35,  3.76it/s] 13%|█▎        | 20/153 [00:05<00:35,  3.77it/s] 14%|█▎        | 21/153 [00:05<00:34,  3.87it/s] 14%|█▍        | 22/153 [00:05<00:33,  3.94it/s] 15%|█▌        | 23/153 [00:06<00:32,  4.00it/s] 16%|█▌        | 24/153 [00:06<00:31,  4.05it/s] 16%|█▋        | 25/153 [00:06<00:31,  4.08it/s] 17%|█▋        | 26/153 [00:06<00:30,  4.10it/s] 18%|█▊        | 27/153 [00:07<00:30,  4.12it/s] 18%|█▊        | 28/153 [00:07<00:30,  4.13it/s] 19%|█▉        | 29/153 [00:07<00:29,  4.14it/s] 20%|█▉        | 30/153 [00:07<00:29,  4.15it/s] 20%|██        | 31/153 [00:08<00:29,  4.13it/s] 21%|██        | 32/153 [00:08<00:30,  4.00it/s] 22%|██▏       | 33/153 [00:08<00:30,  3.91it/s] 22%|██▏       | 34/153 [00:08<00:30,  3.89it/s] 23%|██▎       | 35/153 [00:09<00:29,  3.97it/s] 24%|██▎       | 36/153 [00:09<00:30,  3.88it/s] 24%|██▍       | 37/153 [00:09<00:30,  3.84it/s] 25%|██▍       | 38/153 [00:09<00:29,  3.86it/s] 25%|██▌       | 39/153 [00:10<00:30,  3.78it/s] 26%|██▌       | 40/153 [00:10<00:29,  3.85it/s] 27%|██▋       | 41/153 [00:10<00:29,  3.80it/s] 27%|██▋       | 42/153 [00:10<00:29,  3.76it/s] 28%|██▊       | 43/153 [00:11<00:29,  3.69it/s] 29%|██▉       | 44/153 [00:11<00:28,  3.81it/s] 29%|██▉       | 45/153 [00:11<00:28,  3.83it/s] 30%|███       | 46/153 [00:11<00:28,  3.77it/s] 31%|███       | 47/153 [00:12<00:27,  3.79it/s] 31%|███▏      | 48/153 [00:12<00:28,  3.73it/s] 32%|███▏      | 49/153 [00:12<00:27,  3.85it/s] 33%|███▎      | 50/153 [00:13<00:27,  3.79it/s] 33%|███▎      | 51/153 [00:13<00:27,  3.76it/s] 34%|███▍      | 52/153 [00:13<00:26,  3.80it/s] 35%|███▍      | 53/153 [00:13<00:26,  3.74it/s] 35%|███▌      | 54/153 [00:14<00:25,  3.84it/s] 36%|███▌      | 55/153 [00:14<00:25,  3.79it/s] 37%|███▋      | 56/153 [00:14<00:25,  3.75it/s] 37%|███▋      | 57/153 [00:14<00:26,  3.68it/s] 38%|███▊      | 58/153 [00:15<00:24,  3.80it/s] 39%|███▊      | 59/153 [00:15<00:24,  3.83it/s] 39%|███▉      | 60/153 [00:15<00:24,  3.76it/s] 40%|███▉      | 61/153 [00:15<00:24,  3.76it/s] 41%|████      | 62/153 [00:16<00:24,  3.71it/s] 41%|████      | 63/153 [00:16<00:23,  3.82it/s] 42%|████▏     | 64/153 [00:16<00:23,  3.83it/s] 42%|████▏     | 65/153 [00:17<00:23,  3.76it/s] 43%|████▎     | 66/153 [00:17<00:23,  3.72it/s] 44%|████▍     | 67/153 [00:17<00:22,  3.82it/s] 44%|████▍     | 68/153 [00:17<00:21,  3.90it/s] 45%|████▌     | 69/153 [00:18<00:21,  3.97it/s] 46%|████▌     | 70/153 [00:18<00:20,  4.02it/s] 46%|████▋     | 71/153 [00:18<00:20,  4.06it/s] 47%|████▋     | 72/153 [00:18<00:19,  4.08it/s] 48%|████▊     | 73/153 [00:18<00:19,  4.09it/s] 48%|████▊     | 74/153 [00:19<00:19,  4.09it/s] 49%|████▉     | 75/153 [00:19<00:19,  4.10it/s] 50%|████▉     | 76/153 [00:19<00:18,  4.11it/s] 50%|█████     | 77/153 [00:19<00:18,  4.12it/s] 51%|█████     | 78/153 [00:20<00:18,  4.13it/s] 52%|█████▏    | 79/153 [00:20<00:17,  4.14it/s] 52%|█████▏    | 80/153 [00:20<00:17,  4.14it/s] 53%|█████▎    | 81/153 [00:20<00:17,  4.15it/s] 54%|█████▎    | 82/153 [00:21<00:17,  4.15it/s] 54%|█████▍    | 83/153 [00:21<00:16,  4.15it/s] 55%|█████▍    | 84/153 [00:21<00:16,  4.15it/s] 56%|█████▌    | 85/153 [00:21<00:16,  4.14it/s] 56%|█████▌    | 86/153 [00:22<00:16,  4.13it/s] 57%|█████▋    | 87/153 [00:22<00:15,  4.13it/s] 58%|█████▊    | 88/153 [00:22<00:15,  4.13it/s] 58%|█████▊    | 89/153 [00:22<00:15,  4.13it/s] 59%|█████▉    | 90/153 [00:23<00:15,  4.13it/s] 59%|█████▉    | 91/153 [00:23<00:14,  4.13it/s] 60%|██████    | 92/153 [00:23<00:14,  4.14it/s] 61%|██████    | 93/153 [00:23<00:14,  4.14it/s] 61%|██████▏   | 94/153 [00:24<00:14,  4.13it/s] 62%|██████▏   | 95/153 [00:24<00:14,  4.13it/s] 63%|██████▎   | 96/153 [00:24<00:13,  4.12it/s] 63%|██████▎   | 97/153 [00:24<00:13,  4.12it/s] 64%|██████▍   | 98/153 [00:25<00:13,  4.13it/s] 65%|██████▍   | 99/153 [00:25<00:13,  4.13it/s] 65%|██████▌   | 100/153 [00:25<00:13,  3.98it/s] 66%|██████▌   | 101/153 [00:25<00:13,  3.90it/s] 67%|██████▋   | 102/153 [00:26<00:13,  3.88it/s] 67%|██████▋   | 103/153 [00:26<00:12,  3.94it/s] 68%|██████▊   | 104/153 [00:26<00:12,  3.84it/s] 69%|██████▊   | 105/153 [00:26<00:12,  3.77it/s] 69%|██████▉   | 106/153 [00:27<00:12,  3.72it/s] 70%|██████▉   | 107/153 [00:27<00:12,  3.83it/s] 71%|███████   | 108/153 [00:27<00:11,  3.77it/s] 71%|███████   | 109/153 [00:27<00:11,  3.71it/s] 72%|███████▏  | 110/153 [00:28<00:11,  3.64it/s] 73%|███████▎  | 111/153 [00:28<00:11,  3.78it/s] 73%|███████▎  | 112/153 [00:28<00:10,  3.81it/s] 74%|███████▍  | 113/153 [00:29<00:10,  3.72it/s] 75%|███████▍  | 114/153 [00:29<00:10,  3.75it/s] 75%|███████▌  | 115/153 [00:29<00:10,  3.73it/s] 76%|███████▌  | 116/153 [00:29<00:09,  3.80it/s] 76%|███████▋  | 117/153 [00:30<00:09,  3.71it/s] 77%|███████▋  | 118/153 [00:30<00:09,  3.75it/s] 78%|███████▊  | 119/153 [00:30<00:09,  3.68it/s] 78%|███████▊  | 120/153 [00:30<00:08,  3.77it/s] 79%|███████▉  | 121/153 [00:31<00:08,  3.68it/s] 80%|███████▉  | 122/153 [00:31<00:08,  3.72it/s] 80%|████████  | 123/153 [00:31<00:08,  3.67it/s] 81%|████████  | 124/153 [00:31<00:07,  3.76it/s] 82%|████████▏ | 125/153 [00:32<00:07,  3.67it/s] 82%|████████▏ | 126/153 [00:32<00:07,  3.72it/s] 83%|████████▎ | 127/153 [00:32<00:07,  3.67it/s] 84%|████████▎ | 128/153 [00:33<00:06,  3.75it/s] 84%|████████▍ | 129/153 [00:33<00:06,  3.67it/s] 85%|████████▍ | 130/153 [00:33<00:06,  3.72it/s] 86%|████████▌ | 131/153 [00:33<00:05,  3.68it/s] 86%|████████▋ | 132/153 [00:34<00:05,  3.76it/s] 87%|████████▋ | 133/153 [00:34<00:05,  3.67it/s] 88%|████████▊ | 134/153 [00:34<00:05,  3.72it/s] 88%|████████▊ | 135/153 [00:34<00:04,  3.67it/s] 89%|████████▉ | 136/153 [00:35<00:04,  3.75it/s] 90%|████████▉ | 137/153 [00:35<00:04,  3.67it/s] 90%|█████████ | 138/153 [00:35<00:04,  3.72it/s] 91%|█████████ | 139/153 [00:36<00:03,  3.67it/s] 92%|█████████▏| 140/153 [00:36<00:03,  3.75it/s] 92%|█████████▏| 141/153 [00:36<00:03,  3.67it/s] 93%|█████████▎| 142/153 [00:36<00:02,  3.70it/s] 93%|█████████▎| 143/153 [00:37<00:02,  3.68it/s] 94%|█████████▍| 144/153 [00:37<00:02,  3.76it/s] 95%|█████████▍| 145/153 [00:37<00:02,  3.67it/s] 95%|█████████▌| 146/153 [00:37<00:01,  3.72it/s] 96%|█████████▌| 147/153 [00:38<00:01,  3.67it/s] 97%|█████████▋| 148/153 [00:38<00:01,  3.76it/s] 97%|█████████▋| 149/153 [00:38<00:01,  3.68it/s] 98%|█████████▊| 150/153 [00:38<00:00,  3.72it/s] 99%|█████████▊| 151/153 [00:39<00:00,  3.67it/s] 99%|█████████▉| 152/153 [00:39<00:00,  3.75it/s]100%|██████████| 153/153 [00:39<00:00,  3.68it/s]accuracy:  0.6666666666666666
100%|██████████| 153/153 [00:42<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 152
Training dataset size: 336, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:26:31,818] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:26:32,012] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it][93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:26:33,065] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:09,  2.36s/it]  4%|▎         | 2/56 [00:04<02:07,  2.37s/it]  5%|▌         | 3/56 [00:08<02:27,  2.79s/it]  7%|▋         | 4/56 [00:10<02:19,  2.69s/it]  9%|▉         | 5/56 [00:12<02:06,  2.47s/it] 11%|█         | 6/56 [00:15<02:06,  2.54s/it] 12%|█▎        | 7/56 [00:17<01:50,  2.26s/it] 14%|█▍        | 8/56 [00:19<01:51,  2.32s/it] 16%|█▌        | 9/56 [00:21<01:45,  2.25s/it] 18%|█▊        | 10/56 [00:23<01:43,  2.24s/it]                                               {'loss': 1.0646, 'grad_norm': 10.608390808105469, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:23<01:43,  2.24s/it] 20%|█▉        | 11/56 [00:26<01:41,  2.25s/it] 21%|██▏       | 12/56 [00:28<01:42,  2.33s/it] 23%|██▎       | 13/56 [00:30<01:38,  2.29s/it] 25%|██▌       | 14/56 [00:32<01:34,  2.26s/it] 27%|██▋       | 15/56 [00:34<01:28,  2.16s/it] 29%|██▊       | 16/56 [00:36<01:23,  2.08s/it] 30%|███       | 17/56 [00:38<01:19,  2.05s/it] 32%|███▏      | 18/56 [00:41<01:24,  2.23s/it] 34%|███▍      | 19/56 [00:43<01:26,  2.34s/it] 36%|███▌      | 20/56 [00:46<01:24,  2.36s/it]                                               {'loss': 0.9165, 'grad_norm': 10.877429962158203, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:46<01:24,  2.36s/it] 38%|███▊      | 21/56 [00:48<01:22,  2.36s/it] 39%|███▉      | 22/56 [00:51<01:23,  2.45s/it] 41%|████      | 23/56 [00:53<01:20,  2.43s/it] 43%|████▎     | 24/56 [00:56<01:16,  2.39s/it] 45%|████▍     | 25/56 [00:58<01:10,  2.28s/it] 46%|████▋     | 26/56 [01:00<01:09,  2.31s/it] 48%|████▊     | 27/56 [01:03<01:09,  2.40s/it] 50%|█████     | 28/56 [01:05<01:09,  2.47s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:08<01:08,  2.55s/it] 54%|█████▎    | 30/56 [01:10<01:04,  2.49s/it]                                               {'loss': 0.8044, 'grad_norm': 9.804672241210938, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:10<01:04,  2.49s/it] 55%|█████▌    | 31/56 [01:13<00:59,  2.40s/it] 57%|█████▋    | 32/56 [01:15<00:56,  2.36s/it] 59%|█████▉    | 33/56 [01:17<00:53,  2.34s/it] 61%|██████    | 34/56 [01:20<00:55,  2.53s/it] 62%|██████▎   | 35/56 [01:22<00:50,  2.42s/it] 64%|██████▍   | 36/56 [01:25<00:48,  2.41s/it] 66%|██████▌   | 37/56 [01:27<00:45,  2.41s/it] 68%|██████▊   | 38/56 [01:29<00:41,  2.31s/it] 70%|██████▉   | 39/56 [01:31<00:39,  2.32s/it] 71%|███████▏  | 40/56 [01:34<00:38,  2.40s/it]                                               {'loss': 0.7342, 'grad_norm': 6.685554504394531, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:34<00:38,  2.40s/it] 73%|███████▎  | 41/56 [01:36<00:36,  2.42s/it] 75%|███████▌  | 42/56 [01:39<00:33,  2.38s/it] 77%|███████▋  | 43/56 [01:41<00:28,  2.20s/it] 79%|███████▊  | 44/56 [01:43<00:26,  2.23s/it] 80%|████████  | 45/56 [01:46<00:27,  2.47s/it] 82%|████████▏ | 46/56 [01:48<00:23,  2.33s/it] 84%|████████▍ | 47/56 [01:50<00:19,  2.15s/it] 86%|████████▌ | 48/56 [01:52<00:16,  2.08s/it] 88%|████████▊ | 49/56 [01:54<00:15,  2.21s/it] 89%|████████▉ | 50/56 [01:56<00:13,  2.24s/it]                                               {'loss': 0.659, 'grad_norm': 5.380307197570801, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [01:56<00:13,  2.24s/it] 91%|█████████ | 51/56 [01:58<00:10,  2.17s/it] 93%|█████████▎| 52/56 [02:00<00:08,  2.15s/it] 95%|█████████▍| 53/56 [02:03<00:06,  2.15s/it] 96%|█████████▋| 54/56 [02:05<00:04,  2.31s/it] 98%|█████████▊| 55/56 [02:08<00:02,  2.34s/it]100%|██████████| 56/56 [02:10<00:00,  2.37s/it]                                               {'train_runtime': 131.2509, 'train_samples_per_second': 5.12, 'train_steps_per_second': 0.427, 'train_loss': 0.8173087281840188, 'epoch': 2.0}
100%|██████████| 56/56 [02:11<00:00,  2.37s/it]100%|██████████| 56/56 [02:11<00:00,  2.34s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:04,  3.27it/s]  1%|          | 2/212 [00:00<00:58,  3.56it/s]  1%|▏         | 3/212 [00:00<00:57,  3.65it/s]  2%|▏         | 4/212 [00:01<00:56,  3.66it/s]  2%|▏         | 5/212 [00:01<00:55,  3.76it/s]  3%|▎         | 6/212 [00:01<00:53,  3.86it/s]  3%|▎         | 7/212 [00:01<00:53,  3.80it/s]  4%|▍         | 8/212 [00:02<00:53,  3.79it/s]  4%|▍         | 9/212 [00:02<00:54,  3.74it/s]  5%|▍         | 10/212 [00:02<00:52,  3.88it/s]  5%|▌         | 11/212 [00:02<00:51,  3.88it/s]  6%|▌         | 12/212 [00:03<00:52,  3.82it/s]  6%|▌         | 13/212 [00:03<00:52,  3.77it/s]  7%|▋         | 14/212 [00:03<00:51,  3.82it/s]  7%|▋         | 15/212 [00:03<00:50,  3.87it/s]  8%|▊         | 16/212 [00:04<00:51,  3.82it/s]  8%|▊         | 17/212 [00:04<00:51,  3.80it/s]  8%|▊         | 18/212 [00:04<00:51,  3.76it/s]  9%|▉         | 19/212 [00:05<00:49,  3.87it/s]  9%|▉         | 20/212 [00:05<00:49,  3.85it/s] 10%|▉         | 21/212 [00:05<00:50,  3.82it/s] 10%|█         | 22/212 [00:05<00:50,  3.77it/s] 11%|█         | 23/212 [00:06<00:49,  3.82it/s] 11%|█▏        | 24/212 [00:06<00:48,  3.87it/s] 12%|█▏        | 25/212 [00:06<00:48,  3.82it/s] 12%|█▏        | 26/212 [00:06<00:48,  3.80it/s] 13%|█▎        | 27/212 [00:07<00:49,  3.76it/s] 13%|█▎        | 28/212 [00:07<00:47,  3.87it/s] 14%|█▎        | 29/212 [00:07<00:47,  3.84it/s] 14%|█▍        | 30/212 [00:07<00:47,  3.82it/s] 15%|█▍        | 31/212 [00:08<00:47,  3.78it/s] 15%|█▌        | 32/212 [00:08<00:47,  3.81it/s] 16%|█▌        | 33/212 [00:08<00:46,  3.88it/s] 16%|█▌        | 34/212 [00:08<00:46,  3.83it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.80it/s] 17%|█▋        | 36/212 [00:09<00:46,  3.75it/s] 17%|█▋        | 37/212 [00:09<00:45,  3.86it/s] 18%|█▊        | 38/212 [00:09<00:45,  3.84it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.82it/s] 19%|█▉        | 40/212 [00:10<00:45,  3.76it/s] 19%|█▉        | 41/212 [00:10<00:44,  3.82it/s] 20%|█▉        | 42/212 [00:11<00:43,  3.87it/s] 20%|██        | 43/212 [00:11<00:44,  3.81it/s] 21%|██        | 44/212 [00:11<00:44,  3.79it/s] 21%|██        | 45/212 [00:11<00:44,  3.74it/s] 22%|██▏       | 46/212 [00:12<00:43,  3.86it/s] 22%|██▏       | 47/212 [00:12<00:43,  3.83it/s] 23%|██▎       | 48/212 [00:12<00:43,  3.80it/s] 23%|██▎       | 49/212 [00:12<00:43,  3.74it/s] 24%|██▎       | 50/212 [00:13<00:42,  3.81it/s] 24%|██▍       | 51/212 [00:13<00:41,  3.88it/s] 25%|██▍       | 52/212 [00:13<00:41,  3.81it/s] 25%|██▌       | 53/212 [00:13<00:41,  3.79it/s] 25%|██▌       | 54/212 [00:14<00:42,  3.74it/s] 26%|██▌       | 55/212 [00:14<00:40,  3.86it/s] 26%|██▋       | 56/212 [00:14<00:40,  3.84it/s] 27%|██▋       | 57/212 [00:14<00:40,  3.80it/s] 27%|██▋       | 58/212 [00:15<00:41,  3.74it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.81it/s] 28%|██▊       | 60/212 [00:15<00:39,  3.87it/s] 29%|██▉       | 61/212 [00:16<00:39,  3.82it/s] 29%|██▉       | 62/212 [00:16<00:39,  3.82it/s] 30%|██▉       | 63/212 [00:16<00:39,  3.76it/s] 30%|███       | 64/212 [00:16<00:38,  3.86it/s] 31%|███       | 65/212 [00:17<00:38,  3.81it/s] 31%|███       | 66/212 [00:17<00:38,  3.77it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.73it/s] 32%|███▏      | 68/212 [00:17<00:37,  3.85it/s] 33%|███▎      | 69/212 [00:18<00:37,  3.81it/s] 33%|███▎      | 70/212 [00:18<00:37,  3.79it/s] 33%|███▎      | 71/212 [00:18<00:37,  3.72it/s] 34%|███▍      | 72/212 [00:18<00:36,  3.79it/s] 34%|███▍      | 73/212 [00:19<00:36,  3.84it/s] 35%|███▍      | 74/212 [00:19<00:36,  3.79it/s] 35%|███▌      | 75/212 [00:19<00:35,  3.81it/s] 36%|███▌      | 76/212 [00:19<00:36,  3.74it/s] 36%|███▋      | 77/212 [00:20<00:35,  3.83it/s] 37%|███▋      | 78/212 [00:20<00:35,  3.79it/s] 37%|███▋      | 79/212 [00:20<00:35,  3.77it/s] 38%|███▊      | 80/212 [00:21<00:35,  3.72it/s] 38%|███▊      | 81/212 [00:21<00:34,  3.83it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.82it/s] 39%|███▉      | 83/212 [00:21<00:34,  3.79it/s] 40%|███▉      | 84/212 [00:22<00:34,  3.73it/s] 40%|████      | 85/212 [00:22<00:33,  3.80it/s] 41%|████      | 86/212 [00:22<00:32,  3.87it/s] 41%|████      | 87/212 [00:22<00:32,  3.81it/s] 42%|████▏     | 88/212 [00:23<00:32,  3.82it/s] 42%|████▏     | 89/212 [00:23<00:32,  3.75it/s] 42%|████▏     | 90/212 [00:23<00:31,  3.84it/s] 43%|████▎     | 91/212 [00:23<00:31,  3.80it/s] 43%|████▎     | 92/212 [00:24<00:31,  3.77it/s] 44%|████▍     | 93/212 [00:24<00:31,  3.72it/s] 44%|████▍     | 94/212 [00:24<00:30,  3.84it/s] 45%|████▍     | 95/212 [00:24<00:30,  3.84it/s] 45%|████▌     | 96/212 [00:25<00:30,  3.79it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.73it/s] 46%|████▌     | 98/212 [00:25<00:30,  3.79it/s] 47%|████▋     | 99/212 [00:26<00:29,  3.86it/s] 47%|████▋     | 100/212 [00:26<00:29,  3.80it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.80it/s] 48%|████▊     | 102/212 [00:26<00:29,  3.73it/s] 49%|████▊     | 103/212 [00:27<00:28,  3.85it/s] 49%|████▉     | 104/212 [00:27<00:28,  3.81it/s] 50%|████▉     | 105/212 [00:27<00:28,  3.76it/s] 50%|█████     | 106/212 [00:27<00:28,  3.72it/s] 50%|█████     | 107/212 [00:28<00:27,  3.83it/s] 51%|█████     | 108/212 [00:28<00:27,  3.81it/s] 51%|█████▏    | 109/212 [00:28<00:27,  3.78it/s] 52%|█████▏    | 110/212 [00:28<00:27,  3.73it/s] 52%|█████▏    | 111/212 [00:29<00:26,  3.79it/s] 53%|█████▎    | 112/212 [00:29<00:26,  3.84it/s] 53%|█████▎    | 113/212 [00:29<00:26,  3.79it/s] 54%|█████▍    | 114/212 [00:30<00:25,  3.81it/s] 54%|█████▍    | 115/212 [00:30<00:25,  3.75it/s] 55%|█████▍    | 116/212 [00:30<00:25,  3.82it/s] 55%|█████▌    | 117/212 [00:30<00:25,  3.77it/s] 56%|█████▌    | 118/212 [00:31<00:24,  3.76it/s] 56%|█████▌    | 119/212 [00:31<00:25,  3.72it/s] 57%|█████▋    | 120/212 [00:31<00:23,  3.84it/s] 57%|█████▋    | 121/212 [00:31<00:23,  3.82it/s] 58%|█████▊    | 122/212 [00:32<00:23,  3.79it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.72it/s] 58%|█████▊    | 124/212 [00:32<00:23,  3.78it/s] 59%|█████▉    | 125/212 [00:32<00:22,  3.83it/s] 59%|█████▉    | 126/212 [00:33<00:22,  3.79it/s] 60%|█████▉    | 127/212 [00:33<00:22,  3.76it/s] 60%|██████    | 128/212 [00:33<00:22,  3.75it/s] 61%|██████    | 129/212 [00:33<00:21,  3.82it/s] 61%|██████▏   | 130/212 [00:34<00:21,  3.78it/s] 62%|██████▏   | 131/212 [00:34<00:21,  3.79it/s] 62%|██████▏   | 132/212 [00:34<00:21,  3.73it/s] 63%|██████▎   | 133/212 [00:35<00:20,  3.84it/s] 63%|██████▎   | 134/212 [00:35<00:20,  3.79it/s] 64%|██████▎   | 135/212 [00:35<00:20,  3.75it/s] 64%|██████▍   | 136/212 [00:35<00:20,  3.70it/s] 65%|██████▍   | 137/212 [00:36<00:19,  3.82it/s] 65%|██████▌   | 138/212 [00:36<00:19,  3.80it/s] 66%|██████▌   | 139/212 [00:36<00:19,  3.78it/s] 66%|██████▌   | 140/212 [00:36<00:19,  3.72it/s] 67%|██████▋   | 141/212 [00:37<00:18,  3.78it/s] 67%|██████▋   | 142/212 [00:37<00:18,  3.82it/s] 67%|██████▋   | 143/212 [00:37<00:18,  3.77it/s] 68%|██████▊   | 144/212 [00:37<00:18,  3.73it/s] 68%|██████▊   | 145/212 [00:38<00:17,  3.78it/s] 69%|██████▉   | 146/212 [00:38<00:17,  3.84it/s] 69%|██████▉   | 147/212 [00:38<00:17,  3.79it/s] 70%|██████▉   | 148/212 [00:39<00:16,  3.78it/s] 70%|███████   | 149/212 [00:39<00:16,  3.72it/s] 71%|███████   | 150/212 [00:39<00:16,  3.83it/s] 71%|███████   | 151/212 [00:39<00:16,  3.81it/s] 72%|███████▏  | 152/212 [00:40<00:15,  3.76it/s] 72%|███████▏  | 153/212 [00:40<00:15,  3.71it/s] 73%|███████▎  | 154/212 [00:40<00:15,  3.77it/s] 73%|███████▎  | 155/212 [00:40<00:14,  3.84it/s] 74%|███████▎  | 156/212 [00:41<00:14,  3.79it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.77it/s] 75%|███████▍  | 158/212 [00:41<00:14,  3.71it/s] 75%|███████▌  | 159/212 [00:41<00:13,  3.83it/s] 75%|███████▌  | 160/212 [00:42<00:13,  3.80it/s] 76%|███████▌  | 161/212 [00:42<00:13,  3.75it/s] 76%|███████▋  | 162/212 [00:42<00:13,  3.69it/s] 77%|███████▋  | 163/212 [00:42<00:12,  3.81it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.82it/s] 78%|███████▊  | 165/212 [00:43<00:12,  3.77it/s] 78%|███████▊  | 166/212 [00:43<00:12,  3.72it/s] 79%|███████▉  | 167/212 [00:44<00:11,  3.76it/s] 79%|███████▉  | 168/212 [00:44<00:11,  3.82it/s] 80%|███████▉  | 169/212 [00:44<00:11,  3.76it/s] 80%|████████  | 170/212 [00:44<00:11,  3.74it/s] 81%|████████  | 171/212 [00:45<00:11,  3.70it/s] 81%|████████  | 172/212 [00:45<00:10,  3.81it/s] 82%|████████▏ | 173/212 [00:45<00:10,  3.79it/s] 82%|████████▏ | 174/212 [00:45<00:10,  3.76it/s] 83%|████████▎ | 175/212 [00:46<00:10,  3.65it/s] 83%|████████▎ | 176/212 [00:46<00:09,  3.78it/s] 83%|████████▎ | 177/212 [00:46<00:09,  3.70it/s] 84%|████████▍ | 178/212 [00:46<00:09,  3.70it/s] 84%|████████▍ | 179/212 [00:47<00:09,  3.61it/s] 85%|████████▍ | 180/212 [00:47<00:08,  3.75it/s] 85%|████████▌ | 181/212 [00:47<00:08,  3.79it/s] 86%|████████▌ | 182/212 [00:48<00:08,  3.75it/s] 86%|████████▋ | 183/212 [00:48<00:07,  3.70it/s] 87%|████████▋ | 184/212 [00:48<00:07,  3.76it/s] 87%|████████▋ | 185/212 [00:48<00:07,  3.82it/s] 88%|████████▊ | 186/212 [00:49<00:06,  3.76it/s] 88%|████████▊ | 187/212 [00:49<00:06,  3.71it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.76it/s] 89%|████████▉ | 189/212 [00:49<00:06,  3.81it/s] 90%|████████▉ | 190/212 [00:50<00:05,  3.76it/s] 90%|█████████ | 191/212 [00:50<00:05,  3.70it/s] 91%|█████████ | 192/212 [00:50<00:05,  3.74it/s] 91%|█████████ | 193/212 [00:50<00:05,  3.79it/s] 92%|█████████▏| 194/212 [00:51<00:04,  3.75it/s] 92%|█████████▏| 195/212 [00:51<00:04,  3.77it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.71it/s] 93%|█████████▎| 197/212 [00:52<00:03,  3.80it/s] 93%|█████████▎| 198/212 [00:52<00:03,  3.75it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.72it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.68it/s] 95%|█████████▍| 201/212 [00:53<00:02,  3.80it/s] 95%|█████████▌| 202/212 [00:53<00:02,  3.78it/s] 96%|█████████▌| 203/212 [00:53<00:02,  3.76it/s] 96%|█████████▌| 204/212 [00:53<00:02,  3.70it/s] 97%|█████████▋| 205/212 [00:54<00:01,  3.75it/s] 97%|█████████▋| 206/212 [00:54<00:01,  3.79it/s] 98%|█████████▊| 207/212 [00:54<00:01,  3.75it/s] 98%|█████████▊| 208/212 [00:54<00:01,  3.73it/s] 99%|█████████▊| 209/212 [00:55<00:00,  3.72it/s] 99%|█████████▉| 210/212 [00:55<00:00,  3.82it/s]100%|█████████▉| 211/212 [00:55<00:00,  3.89it/s]100%|██████████| 212/212 [00:56<00:00,  3.89it/s]accuracy:  0.6509433962264151
100%|██████████| 212/212 [00:59<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 135
Training dataset size: 336, validation dataset size: 135
Training dataset size: 336, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.75s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 06:30:09,461] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:30:09,740] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:30:09,792] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<01:55,  2.10s/it]  4%|▎         | 2/56 [00:04<01:59,  2.22s/it]  5%|▌         | 3/56 [00:06<02:06,  2.38s/it]  7%|▋         | 4/56 [00:08<01:52,  2.17s/it]  9%|▉         | 5/56 [00:11<01:58,  2.32s/it] 11%|█         | 6/56 [00:13<01:59,  2.39s/it] 12%|█▎        | 7/56 [00:16<01:58,  2.42s/it] 14%|█▍        | 8/56 [00:18<01:51,  2.33s/it] 16%|█▌        | 9/56 [00:20<01:50,  2.35s/it] 18%|█▊        | 10/56 [00:23<01:51,  2.42s/it]                                               {'loss': 0.5212, 'grad_norm': 3.735252618789673, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:23<01:51,  2.42s/it] 20%|█▉        | 11/56 [00:25<01:48,  2.41s/it] 21%|██▏       | 12/56 [00:27<01:38,  2.23s/it] 23%|██▎       | 13/56 [00:30<01:44,  2.43s/it] 25%|██▌       | 14/56 [00:33<01:44,  2.48s/it] 27%|██▋       | 15/56 [00:35<01:40,  2.44s/it] 29%|██▊       | 16/56 [00:37<01:34,  2.37s/it] 30%|███       | 17/56 [00:40<01:34,  2.41s/it] 32%|███▏      | 18/56 [00:43<01:36,  2.54s/it] 34%|███▍      | 19/56 [00:45<01:29,  2.43s/it] 36%|███▌      | 20/56 [00:47<01:28,  2.46s/it]                                               {'loss': 0.4528, 'grad_norm': 6.155102252960205, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:47<01:28,  2.46s/it] 38%|███▊      | 21/56 [00:49<01:22,  2.37s/it] 39%|███▉      | 22/56 [00:52<01:19,  2.35s/it] 41%|████      | 23/56 [00:54<01:19,  2.41s/it] 43%|████▎     | 24/56 [00:57<01:17,  2.41s/it] 45%|████▍     | 25/56 [00:59<01:10,  2.29s/it] 46%|████▋     | 26/56 [01:01<01:07,  2.24s/it] 48%|████▊     | 27/56 [01:03<01:04,  2.22s/it] 50%|█████     | 28/56 [01:05<01:02,  2.23s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:08<01:02,  2.33s/it] 54%|█████▎    | 30/56 [01:10<00:59,  2.29s/it]                                               {'loss': 0.5437, 'grad_norm': 6.0897345542907715, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:10<00:59,  2.29s/it] 55%|█████▌    | 31/56 [01:13<00:58,  2.36s/it] 57%|█████▋    | 32/56 [01:15<00:56,  2.37s/it] 59%|█████▉    | 33/56 [01:17<00:53,  2.31s/it] 61%|██████    | 34/56 [01:19<00:48,  2.21s/it] 62%|██████▎   | 35/56 [01:22<00:48,  2.32s/it] 64%|██████▍   | 36/56 [01:24<00:47,  2.38s/it] 66%|██████▌   | 37/56 [01:27<00:44,  2.36s/it] 68%|██████▊   | 38/56 [01:29<00:44,  2.45s/it] 70%|██████▉   | 39/56 [01:31<00:40,  2.40s/it] 71%|███████▏  | 40/56 [01:33<00:36,  2.28s/it]                                               {'loss': 0.4171, 'grad_norm': 5.893625736236572, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:33<00:36,  2.28s/it] 73%|███████▎  | 41/56 [01:36<00:33,  2.22s/it] 75%|███████▌  | 42/56 [01:38<00:32,  2.32s/it] 77%|███████▋  | 43/56 [01:40<00:28,  2.23s/it] 79%|███████▊  | 44/56 [01:43<00:28,  2.36s/it] 80%|████████  | 45/56 [01:45<00:25,  2.35s/it] 82%|████████▏ | 46/56 [01:47<00:23,  2.34s/it] 84%|████████▍ | 47/56 [01:50<00:20,  2.31s/it] 86%|████████▌ | 48/56 [01:52<00:18,  2.36s/it] 88%|████████▊ | 49/56 [01:54<00:15,  2.28s/it] 89%|████████▉ | 50/56 [01:56<00:13,  2.22s/it]                                               {'loss': 0.402, 'grad_norm': 2.3666417598724365, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [01:56<00:13,  2.22s/it] 91%|█████████ | 51/56 [01:59<00:11,  2.32s/it] 93%|█████████▎| 52/56 [02:01<00:09,  2.27s/it] 95%|█████████▍| 53/56 [02:03<00:06,  2.22s/it] 96%|█████████▋| 54/56 [02:05<00:04,  2.22s/it] 98%|█████████▊| 55/56 [02:08<00:02,  2.24s/it]100%|██████████| 56/56 [02:10<00:00,  2.19s/it]                                               {'train_runtime': 130.8472, 'train_samples_per_second': 5.136, 'train_steps_per_second': 0.428, 'train_loss': 0.4649347024304526, 'epoch': 2.0}
100%|██████████| 56/56 [02:10<00:00,  2.19s/it]100%|██████████| 56/56 [02:10<00:00,  2.33s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:53,  3.01it/s]  1%|          | 2/163 [00:00<00:47,  3.41it/s]  2%|▏         | 3/163 [00:00<00:45,  3.55it/s]  2%|▏         | 4/163 [00:01<00:43,  3.64it/s]  3%|▎         | 5/163 [00:01<00:42,  3.68it/s]  4%|▎         | 6/163 [00:01<00:41,  3.76it/s]  4%|▍         | 7/163 [00:01<00:41,  3.75it/s]  5%|▍         | 8/163 [00:02<00:41,  3.75it/s]  6%|▌         | 9/163 [00:02<00:41,  3.71it/s]  6%|▌         | 10/163 [00:02<00:39,  3.84it/s]  7%|▋         | 11/163 [00:02<00:39,  3.81it/s]  7%|▋         | 12/163 [00:03<00:40,  3.77it/s]  8%|▊         | 13/163 [00:03<00:40,  3.70it/s]  9%|▊         | 14/163 [00:03<00:38,  3.83it/s]  9%|▉         | 15/163 [00:04<00:38,  3.84it/s] 10%|▉         | 16/163 [00:04<00:38,  3.78it/s] 10%|█         | 17/163 [00:04<00:38,  3.78it/s] 11%|█         | 18/163 [00:04<00:38,  3.78it/s] 12%|█▏        | 19/163 [00:05<00:37,  3.84it/s] 12%|█▏        | 20/163 [00:05<00:37,  3.79it/s] 13%|█▎        | 21/163 [00:05<00:37,  3.78it/s] 13%|█▎        | 22/163 [00:05<00:37,  3.73it/s] 14%|█▍        | 23/163 [00:06<00:36,  3.85it/s] 15%|█▍        | 24/163 [00:06<00:36,  3.81it/s] 15%|█▌        | 25/163 [00:06<00:36,  3.77it/s] 16%|█▌        | 26/163 [00:06<00:37,  3.69it/s] 17%|█▋        | 27/163 [00:07<00:35,  3.81it/s] 17%|█▋        | 28/163 [00:07<00:35,  3.82it/s] 18%|█▊        | 29/163 [00:07<00:35,  3.77it/s] 18%|█▊        | 30/163 [00:08<00:35,  3.73it/s] 19%|█▉        | 31/163 [00:08<00:34,  3.78it/s] 20%|█▉        | 32/163 [00:08<00:34,  3.83it/s] 20%|██        | 33/163 [00:08<00:34,  3.78it/s] 21%|██        | 34/163 [00:09<00:33,  3.81it/s] 21%|██▏       | 35/163 [00:09<00:34,  3.76it/s] 22%|██▏       | 36/163 [00:09<00:33,  3.83it/s] 23%|██▎       | 37/163 [00:09<00:33,  3.78it/s] 23%|██▎       | 38/163 [00:10<00:32,  3.81it/s] 24%|██▍       | 39/163 [00:10<00:33,  3.74it/s] 25%|██▍       | 40/163 [00:10<00:32,  3.83it/s] 25%|██▌       | 41/163 [00:10<00:32,  3.78it/s] 26%|██▌       | 42/163 [00:11<00:32,  3.77it/s] 26%|██▋       | 43/163 [00:11<00:32,  3.73it/s] 27%|██▋       | 44/163 [00:11<00:30,  3.85it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.80it/s] 28%|██▊       | 46/163 [00:12<00:31,  3.76it/s] 29%|██▉       | 47/163 [00:12<00:31,  3.71it/s] 29%|██▉       | 48/163 [00:12<00:29,  3.84it/s] 30%|███       | 49/163 [00:13<00:29,  3.81it/s] 31%|███       | 50/163 [00:13<00:29,  3.77it/s] 31%|███▏      | 51/163 [00:13<00:30,  3.69it/s] 32%|███▏      | 52/163 [00:13<00:29,  3.81it/s] 33%|███▎      | 53/163 [00:14<00:28,  3.82it/s] 33%|███▎      | 54/163 [00:14<00:28,  3.78it/s] 34%|███▎      | 55/163 [00:14<00:28,  3.73it/s] 34%|███▍      | 56/163 [00:14<00:28,  3.78it/s] 35%|███▍      | 57/163 [00:15<00:27,  3.81it/s] 36%|███▌      | 58/163 [00:15<00:27,  3.76it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.72it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.77it/s] 37%|███▋      | 61/163 [00:16<00:26,  3.80it/s] 38%|███▊      | 62/163 [00:16<00:26,  3.75it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.71it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.77it/s] 40%|███▉      | 65/163 [00:17<00:25,  3.83it/s] 40%|████      | 66/163 [00:17<00:25,  3.77it/s] 41%|████      | 67/163 [00:17<00:25,  3.72it/s] 42%|████▏     | 68/163 [00:18<00:25,  3.77it/s] 42%|████▏     | 69/163 [00:18<00:24,  3.83it/s] 43%|████▎     | 70/163 [00:18<00:24,  3.77it/s] 44%|████▎     | 71/163 [00:18<00:24,  3.74it/s] 44%|████▍     | 72/163 [00:19<00:24,  3.75it/s] 45%|████▍     | 73/163 [00:19<00:23,  3.81it/s] 45%|████▌     | 74/163 [00:19<00:23,  3.76it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.74it/s] 47%|████▋     | 76/163 [00:20<00:23,  3.75it/s] 47%|████▋     | 77/163 [00:20<00:22,  3.80it/s] 48%|████▊     | 78/163 [00:20<00:22,  3.75it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.70it/s] 49%|████▉     | 80/163 [00:21<00:22,  3.76it/s] 50%|████▉     | 81/163 [00:21<00:21,  3.78it/s] 50%|█████     | 82/163 [00:21<00:21,  3.74it/s] 51%|█████     | 83/163 [00:22<00:21,  3.70it/s] 52%|█████▏    | 84/163 [00:22<00:20,  3.77it/s] 52%|█████▏    | 85/163 [00:22<00:20,  3.81it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.75it/s] 53%|█████▎    | 87/163 [00:23<00:20,  3.72it/s] 54%|█████▍    | 88/163 [00:23<00:19,  3.76it/s] 55%|█████▍    | 89/163 [00:23<00:19,  3.86it/s] 55%|█████▌    | 90/163 [00:23<00:18,  3.93it/s] 56%|█████▌    | 91/163 [00:24<00:18,  3.84it/s] 56%|█████▋    | 92/163 [00:24<00:18,  3.78it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.72it/s] 58%|█████▊    | 94/163 [00:24<00:17,  3.84it/s] 58%|█████▊    | 95/163 [00:25<00:17,  3.80it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.74it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.70it/s] 60%|██████    | 98/163 [00:26<00:17,  3.81it/s] 61%|██████    | 99/163 [00:26<00:16,  3.79it/s] 61%|██████▏   | 100/163 [00:26<00:16,  3.74it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.67it/s] 63%|██████▎   | 102/163 [00:27<00:16,  3.80it/s] 63%|██████▎   | 103/163 [00:27<00:15,  3.79it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.76it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.70it/s] 65%|██████▌   | 106/163 [00:28<00:15,  3.76it/s] 66%|██████▌   | 107/163 [00:28<00:14,  3.79it/s] 66%|██████▋   | 108/163 [00:28<00:14,  3.75it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.70it/s] 67%|██████▋   | 110/163 [00:29<00:14,  3.76it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.80it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.74it/s] 69%|██████▉   | 113/163 [00:30<00:13,  3.69it/s] 70%|██████▉   | 114/163 [00:30<00:13,  3.75it/s] 71%|███████   | 115/163 [00:30<00:12,  3.79it/s] 71%|███████   | 116/163 [00:30<00:12,  3.74it/s] 72%|███████▏  | 117/163 [00:31<00:12,  3.72it/s] 72%|███████▏  | 118/163 [00:31<00:12,  3.75it/s] 73%|███████▎  | 119/163 [00:31<00:11,  3.79it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.75it/s] 74%|███████▍  | 121/163 [00:32<00:11,  3.73it/s] 75%|███████▍  | 122/163 [00:32<00:10,  3.75it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.79it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.75it/s] 77%|███████▋  | 125/163 [00:33<00:10,  3.71it/s] 77%|███████▋  | 126/163 [00:33<00:09,  3.74it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.79it/s] 79%|███████▊  | 128/163 [00:34<00:09,  3.74it/s] 79%|███████▉  | 129/163 [00:34<00:09,  3.72it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.74it/s] 80%|████████  | 131/163 [00:34<00:08,  3.79it/s] 81%|████████  | 132/163 [00:35<00:08,  3.73it/s] 82%|████████▏ | 133/163 [00:35<00:08,  3.71it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.72it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.77it/s] 83%|████████▎ | 136/163 [00:36<00:07,  3.73it/s] 84%|████████▍ | 137/163 [00:36<00:06,  3.72it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.74it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.79it/s] 86%|████████▌ | 140/163 [00:37<00:06,  3.73it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.71it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.71it/s] 88%|████████▊ | 143/163 [00:38<00:05,  3.76it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.72it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.76it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.69it/s] 90%|█████████ | 147/163 [00:39<00:04,  3.74it/s] 91%|█████████ | 148/163 [00:39<00:04,  3.71it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.73it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.70it/s] 93%|█████████▎| 151/163 [00:40<00:03,  3.76it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.72it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.70it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.80it/s] 95%|█████████▌| 155/163 [00:41<00:02,  3.88it/s] 96%|█████████▌| 156/163 [00:41<00:01,  3.95it/s] 96%|█████████▋| 157/163 [00:41<00:01,  3.99it/s] 97%|█████████▋| 158/163 [00:41<00:01,  4.02it/s] 98%|█████████▊| 159/163 [00:42<00:00,  4.04it/s] 98%|█████████▊| 160/163 [00:42<00:00,  4.06it/s] 99%|█████████▉| 161/163 [00:42<00:00,  4.08it/s] 99%|█████████▉| 162/163 [00:42<00:00,  4.09it/s]100%|██████████| 163/163 [00:43<00:00,  4.09it/s]accuracy:  0.8773006134969326
100%|██████████| 163/163 [00:45<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 198
Training dataset size: 336, validation dataset size: 198
Training dataset size: 336, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:33:32,166] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 06:33:32,185] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:33:33,856] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<01:58,  2.15s/it]  4%|▎         | 2/56 [00:04<02:16,  2.53s/it]  5%|▌         | 3/56 [00:08<02:27,  2.79s/it]  7%|▋         | 4/56 [00:10<02:21,  2.72s/it]  9%|▉         | 5/56 [00:12<02:05,  2.47s/it] 11%|█         | 6/56 [00:15<02:06,  2.53s/it] 12%|█▎        | 7/56 [00:17<02:05,  2.57s/it] 14%|█▍        | 8/56 [00:20<02:04,  2.59s/it] 16%|█▌        | 9/56 [00:22<01:57,  2.49s/it] 18%|█▊        | 10/56 [00:25<01:54,  2.49s/it]                                               {'loss': 0.5452, 'grad_norm': 1.7487863302230835, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:25<01:54,  2.49s/it] 20%|█▉        | 11/56 [00:28<01:54,  2.55s/it] 21%|██▏       | 12/56 [00:30<01:51,  2.52s/it] 23%|██▎       | 13/56 [00:33<01:48,  2.53s/it] 25%|██▌       | 14/56 [00:35<01:48,  2.58s/it] 27%|██▋       | 15/56 [00:38<01:49,  2.67s/it] 29%|██▊       | 16/56 [00:41<01:46,  2.67s/it] 30%|███       | 17/56 [00:43<01:42,  2.62s/it] 32%|███▏      | 18/56 [00:46<01:35,  2.50s/it] 34%|███▍      | 19/56 [00:48<01:32,  2.49s/it] 36%|███▌      | 20/56 [00:50<01:26,  2.40s/it]                                               {'loss': 0.5153, 'grad_norm': 2.4533498287200928, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:50<01:26,  2.40s/it] 38%|███▊      | 21/56 [00:52<01:21,  2.34s/it] 39%|███▉      | 22/56 [00:55<01:17,  2.28s/it] 41%|████      | 23/56 [00:57<01:15,  2.30s/it] 43%|████▎     | 24/56 [00:59<01:14,  2.33s/it] 45%|████▍     | 25/56 [01:02<01:15,  2.43s/it] 46%|████▋     | 26/56 [01:04<01:10,  2.33s/it] 48%|████▊     | 27/56 [01:07<01:11,  2.47s/it] 50%|█████     | 28/56 [01:09<01:10,  2.52s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:12<01:07,  2.51s/it] 54%|█████▎    | 30/56 [01:14<01:01,  2.38s/it]                                               {'loss': 0.4893, 'grad_norm': 0.6797373294830322, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:14<01:01,  2.38s/it] 55%|█████▌    | 31/56 [01:16<00:59,  2.37s/it] 57%|█████▋    | 32/56 [01:19<00:58,  2.43s/it] 59%|█████▉    | 33/56 [01:21<00:53,  2.33s/it] 61%|██████    | 34/56 [01:23<00:52,  2.37s/it] 62%|██████▎   | 35/56 [01:26<00:52,  2.49s/it] 64%|██████▍   | 36/56 [01:29<00:52,  2.61s/it] 66%|██████▌   | 37/56 [01:32<00:49,  2.60s/it] 68%|██████▊   | 38/56 [01:35<00:47,  2.66s/it] 70%|██████▉   | 39/56 [01:37<00:46,  2.71s/it] 71%|███████▏  | 40/56 [01:40<00:44,  2.78s/it]                                               {'loss': 0.4332, 'grad_norm': 0.805118203163147, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:40<00:44,  2.78s/it] 73%|███████▎  | 41/56 [01:43<00:40,  2.72s/it] 75%|███████▌  | 42/56 [01:46<00:38,  2.78s/it] 77%|███████▋  | 43/56 [01:49<00:36,  2.79s/it] 79%|███████▊  | 44/56 [01:51<00:33,  2.75s/it] 80%|████████  | 45/56 [01:54<00:29,  2.70s/it] 82%|████████▏ | 46/56 [01:56<00:26,  2.66s/it] 84%|████████▍ | 47/56 [01:59<00:23,  2.63s/it] 86%|████████▌ | 48/56 [02:01<00:20,  2.51s/it] 88%|████████▊ | 49/56 [02:04<00:18,  2.62s/it] 89%|████████▉ | 50/56 [02:07<00:15,  2.59s/it]                                               {'loss': 0.7044, 'grad_norm': 6.6008076667785645, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:07<00:15,  2.59s/it] 91%|█████████ | 51/56 [02:10<00:13,  2.78s/it] 93%|█████████▎| 52/56 [02:12<00:10,  2.63s/it] 95%|█████████▍| 53/56 [02:15<00:08,  2.84s/it] 96%|█████████▋| 54/56 [02:18<00:05,  2.84s/it] 98%|█████████▊| 55/56 [02:20<00:02,  2.65s/it]100%|██████████| 56/56 [02:23<00:00,  2.47s/it]                                               {'train_runtime': 143.6655, 'train_samples_per_second': 4.678, 'train_steps_per_second': 0.39, 'train_loss': 0.5294548315661294, 'epoch': 2.0}
100%|██████████| 56/56 [02:23<00:00,  2.47s/it]100%|██████████| 56/56 [02:23<00:00,  2.56s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:11,  3.25it/s]  1%|          | 2/232 [00:00<01:01,  3.74it/s]  1%|▏         | 3/232 [00:00<00:58,  3.93it/s]  2%|▏         | 4/232 [00:01<00:56,  4.03it/s]  2%|▏         | 5/232 [00:01<00:55,  4.08it/s]  3%|▎         | 6/232 [00:01<00:55,  4.11it/s]  3%|▎         | 7/232 [00:01<00:54,  4.13it/s]  3%|▎         | 8/232 [00:01<00:54,  4.14it/s]  4%|▍         | 9/232 [00:02<00:53,  4.16it/s]  4%|▍         | 10/232 [00:02<00:53,  4.16it/s]  5%|▍         | 11/232 [00:02<00:53,  4.17it/s]  5%|▌         | 12/232 [00:02<00:52,  4.17it/s]  6%|▌         | 13/232 [00:03<00:52,  4.17it/s]  6%|▌         | 14/232 [00:03<00:52,  4.17it/s]  6%|▋         | 15/232 [00:03<00:53,  4.06it/s]  7%|▋         | 16/232 [00:03<00:54,  3.96it/s]  7%|▋         | 17/232 [00:04<00:55,  3.87it/s]  8%|▊         | 18/232 [00:04<00:54,  3.95it/s]  8%|▊         | 19/232 [00:04<00:54,  3.88it/s]  9%|▊         | 20/232 [00:05<00:55,  3.79it/s]  9%|▉         | 21/232 [00:05<00:56,  3.72it/s]  9%|▉         | 22/232 [00:05<00:54,  3.84it/s] 10%|▉         | 23/232 [00:05<00:54,  3.83it/s] 10%|█         | 24/232 [00:06<00:55,  3.75it/s] 11%|█         | 25/232 [00:06<00:56,  3.68it/s] 11%|█         | 26/232 [00:06<00:54,  3.81it/s] 12%|█▏        | 27/232 [00:06<00:53,  3.82it/s] 12%|█▏        | 28/232 [00:07<00:54,  3.73it/s] 12%|█▎        | 29/232 [00:07<00:55,  3.66it/s] 13%|█▎        | 30/232 [00:07<00:53,  3.80it/s] 13%|█▎        | 31/232 [00:07<00:52,  3.80it/s] 14%|█▍        | 32/232 [00:08<00:53,  3.72it/s] 14%|█▍        | 33/232 [00:08<00:53,  3.69it/s] 15%|█▍        | 34/232 [00:08<00:52,  3.79it/s] 15%|█▌        | 35/232 [00:08<00:51,  3.80it/s] 16%|█▌        | 36/232 [00:09<00:52,  3.72it/s] 16%|█▌        | 37/232 [00:09<00:52,  3.70it/s] 16%|█▋        | 38/232 [00:09<00:51,  3.78it/s] 17%|█▋        | 39/232 [00:10<00:50,  3.81it/s] 17%|█▋        | 40/232 [00:10<00:51,  3.71it/s] 18%|█▊        | 41/232 [00:10<00:52,  3.64it/s] 18%|█▊        | 42/232 [00:10<00:50,  3.78it/s] 19%|█▊        | 43/232 [00:11<00:49,  3.79it/s] 19%|█▉        | 44/232 [00:11<00:50,  3.71it/s] 19%|█▉        | 45/232 [00:11<00:51,  3.65it/s] 20%|█▉        | 46/232 [00:11<00:49,  3.79it/s] 20%|██        | 47/232 [00:12<00:48,  3.81it/s] 21%|██        | 48/232 [00:12<00:49,  3.73it/s] 21%|██        | 49/232 [00:12<00:49,  3.66it/s] 22%|██▏       | 50/232 [00:13<00:47,  3.80it/s] 22%|██▏       | 51/232 [00:13<00:47,  3.81it/s] 22%|██▏       | 52/232 [00:13<00:48,  3.73it/s] 23%|██▎       | 53/232 [00:13<00:48,  3.66it/s] 23%|██▎       | 54/232 [00:14<00:47,  3.78it/s] 24%|██▎       | 55/232 [00:14<00:46,  3.79it/s] 24%|██▍       | 56/232 [00:14<00:47,  3.71it/s] 25%|██▍       | 57/232 [00:14<00:47,  3.65it/s] 25%|██▌       | 58/232 [00:15<00:45,  3.78it/s] 25%|██▌       | 59/232 [00:15<00:45,  3.79it/s] 26%|██▌       | 60/232 [00:15<00:46,  3.71it/s] 26%|██▋       | 61/232 [00:15<00:46,  3.65it/s] 27%|██▋       | 62/232 [00:16<00:44,  3.79it/s] 27%|██▋       | 63/232 [00:16<00:44,  3.79it/s] 28%|██▊       | 64/232 [00:16<00:45,  3.72it/s] 28%|██▊       | 65/232 [00:17<00:45,  3.65it/s] 28%|██▊       | 66/232 [00:17<00:43,  3.79it/s] 29%|██▉       | 67/232 [00:17<00:43,  3.79it/s] 29%|██▉       | 68/232 [00:17<00:44,  3.70it/s] 30%|██▉       | 69/232 [00:18<00:44,  3.64it/s] 30%|███       | 70/232 [00:18<00:42,  3.79it/s] 31%|███       | 71/232 [00:18<00:42,  3.80it/s] 31%|███       | 72/232 [00:18<00:42,  3.72it/s] 31%|███▏      | 73/232 [00:19<00:43,  3.65it/s] 32%|███▏      | 74/232 [00:19<00:41,  3.79it/s] 32%|███▏      | 75/232 [00:19<00:41,  3.79it/s] 33%|███▎      | 76/232 [00:19<00:42,  3.71it/s] 33%|███▎      | 77/232 [00:20<00:42,  3.64it/s] 34%|███▎      | 78/232 [00:20<00:40,  3.78it/s] 34%|███▍      | 79/232 [00:20<00:40,  3.78it/s] 34%|███▍      | 80/232 [00:21<00:41,  3.70it/s] 35%|███▍      | 81/232 [00:21<00:41,  3.64it/s] 35%|███▌      | 82/232 [00:21<00:39,  3.78it/s] 36%|███▌      | 83/232 [00:21<00:39,  3.74it/s] 36%|███▌      | 84/232 [00:22<00:40,  3.70it/s] 37%|███▋      | 85/232 [00:22<00:40,  3.65it/s] 37%|███▋      | 86/232 [00:22<00:38,  3.78it/s] 38%|███▊      | 87/232 [00:22<00:38,  3.75it/s] 38%|███▊      | 88/232 [00:23<00:38,  3.70it/s] 38%|███▊      | 89/232 [00:23<00:39,  3.64it/s] 39%|███▉      | 90/232 [00:23<00:37,  3.77it/s] 39%|███▉      | 91/232 [00:24<00:37,  3.76it/s] 40%|███▉      | 92/232 [00:24<00:37,  3.69it/s] 40%|████      | 93/232 [00:24<00:38,  3.62it/s] 41%|████      | 94/232 [00:24<00:36,  3.77it/s] 41%|████      | 95/232 [00:25<00:36,  3.77it/s] 41%|████▏     | 96/232 [00:25<00:36,  3.68it/s] 42%|████▏     | 97/232 [00:25<00:37,  3.62it/s] 42%|████▏     | 98/232 [00:25<00:35,  3.76it/s] 43%|████▎     | 99/232 [00:26<00:35,  3.76it/s] 43%|████▎     | 100/232 [00:26<00:35,  3.68it/s] 44%|████▎     | 101/232 [00:26<00:36,  3.62it/s] 44%|████▍     | 102/232 [00:26<00:34,  3.76it/s] 44%|████▍     | 103/232 [00:27<00:34,  3.72it/s] 45%|████▍     | 104/232 [00:27<00:34,  3.69it/s] 45%|████▌     | 105/232 [00:27<00:34,  3.63it/s] 46%|████▌     | 106/232 [00:28<00:33,  3.77it/s] 46%|████▌     | 107/232 [00:28<00:33,  3.79it/s] 47%|████▋     | 108/232 [00:28<00:33,  3.70it/s] 47%|████▋     | 109/232 [00:28<00:33,  3.63it/s] 47%|████▋     | 110/232 [00:29<00:32,  3.77it/s] 48%|████▊     | 111/232 [00:29<00:32,  3.77it/s] 48%|████▊     | 112/232 [00:29<00:32,  3.69it/s] 49%|████▊     | 113/232 [00:29<00:32,  3.63it/s] 49%|████▉     | 114/232 [00:30<00:31,  3.76it/s] 50%|████▉     | 115/232 [00:30<00:31,  3.77it/s] 50%|█████     | 116/232 [00:30<00:31,  3.69it/s] 50%|█████     | 117/232 [00:31<00:31,  3.63it/s] 51%|█████     | 118/232 [00:31<00:30,  3.77it/s] 51%|█████▏    | 119/232 [00:31<00:29,  3.77it/s] 52%|█████▏    | 120/232 [00:31<00:30,  3.68it/s] 52%|█████▏    | 121/232 [00:32<00:30,  3.64it/s] 53%|█████▎    | 122/232 [00:32<00:29,  3.76it/s] 53%|█████▎    | 123/232 [00:32<00:28,  3.78it/s] 53%|█████▎    | 124/232 [00:32<00:29,  3.69it/s] 54%|█████▍    | 125/232 [00:33<00:29,  3.63it/s] 54%|█████▍    | 126/232 [00:33<00:28,  3.77it/s] 55%|█████▍    | 127/232 [00:33<00:27,  3.78it/s] 55%|█████▌    | 128/232 [00:33<00:28,  3.70it/s] 56%|█████▌    | 129/232 [00:34<00:28,  3.64it/s] 56%|█████▌    | 130/232 [00:34<00:27,  3.78it/s] 56%|█████▋    | 131/232 [00:34<00:26,  3.79it/s] 57%|█████▋    | 132/232 [00:35<00:26,  3.70it/s] 57%|█████▋    | 133/232 [00:35<00:27,  3.65it/s] 58%|█████▊    | 134/232 [00:35<00:26,  3.76it/s] 58%|█████▊    | 135/232 [00:35<00:25,  3.78it/s] 59%|█████▊    | 136/232 [00:36<00:26,  3.69it/s] 59%|█████▉    | 137/232 [00:36<00:25,  3.68it/s] 59%|█████▉    | 138/232 [00:36<00:25,  3.73it/s] 60%|█████▉    | 139/232 [00:36<00:24,  3.77it/s] 60%|██████    | 140/232 [00:37<00:25,  3.68it/s] 61%|██████    | 141/232 [00:37<00:25,  3.62it/s] 61%|██████    | 142/232 [00:37<00:24,  3.75it/s] 62%|██████▏   | 143/232 [00:38<00:23,  3.76it/s] 62%|██████▏   | 144/232 [00:38<00:23,  3.68it/s] 62%|██████▎   | 145/232 [00:38<00:24,  3.62it/s] 63%|██████▎   | 146/232 [00:38<00:22,  3.76it/s] 63%|██████▎   | 147/232 [00:39<00:22,  3.77it/s] 64%|██████▍   | 148/232 [00:39<00:22,  3.69it/s] 64%|██████▍   | 149/232 [00:39<00:22,  3.70it/s] 65%|██████▍   | 150/232 [00:39<00:21,  3.81it/s] 65%|██████▌   | 151/232 [00:40<00:20,  3.89it/s] 66%|██████▌   | 152/232 [00:40<00:20,  3.95it/s] 66%|██████▌   | 153/232 [00:40<00:19,  4.00it/s] 66%|██████▋   | 154/232 [00:40<00:19,  4.04it/s] 67%|██████▋   | 155/232 [00:41<00:18,  4.07it/s] 67%|██████▋   | 156/232 [00:41<00:18,  4.09it/s] 68%|██████▊   | 157/232 [00:41<00:18,  4.09it/s] 68%|██████▊   | 158/232 [00:41<00:18,  4.09it/s] 69%|██████▊   | 159/232 [00:42<00:17,  4.10it/s] 69%|██████▉   | 160/232 [00:42<00:17,  4.11it/s] 69%|██████▉   | 161/232 [00:42<00:17,  4.12it/s] 70%|██████▉   | 162/232 [00:42<00:16,  4.12it/s] 70%|███████   | 163/232 [00:43<00:16,  4.12it/s] 71%|███████   | 164/232 [00:43<00:16,  4.11it/s] 71%|███████   | 165/232 [00:43<00:16,  4.10it/s] 72%|███████▏  | 166/232 [00:43<00:16,  4.10it/s] 72%|███████▏  | 167/232 [00:44<00:15,  4.10it/s] 72%|███████▏  | 168/232 [00:44<00:15,  4.10it/s] 73%|███████▎  | 169/232 [00:44<00:15,  4.09it/s] 73%|███████▎  | 170/232 [00:44<00:15,  4.09it/s] 74%|███████▎  | 171/232 [00:44<00:14,  4.09it/s] 74%|███████▍  | 172/232 [00:45<00:14,  4.08it/s] 75%|███████▍  | 173/232 [00:45<00:14,  4.08it/s] 75%|███████▌  | 174/232 [00:45<00:14,  4.08it/s] 75%|███████▌  | 175/232 [00:45<00:13,  4.09it/s] 76%|███████▌  | 176/232 [00:46<00:13,  4.10it/s] 76%|███████▋  | 177/232 [00:46<00:13,  4.09it/s] 77%|███████▋  | 178/232 [00:46<00:13,  4.08it/s] 77%|███████▋  | 179/232 [00:46<00:12,  4.09it/s] 78%|███████▊  | 180/232 [00:47<00:12,  4.09it/s] 78%|███████▊  | 181/232 [00:47<00:12,  4.08it/s] 78%|███████▊  | 182/232 [00:47<00:12,  4.08it/s] 79%|███████▉  | 183/232 [00:47<00:12,  4.08it/s] 79%|███████▉  | 184/232 [00:48<00:12,  3.98it/s] 80%|███████▉  | 185/232 [00:48<00:12,  3.88it/s] 80%|████████  | 186/232 [00:48<00:12,  3.79it/s] 81%|████████  | 187/232 [00:48<00:11,  3.89it/s] 81%|████████  | 188/232 [00:49<00:11,  3.85it/s] 81%|████████▏ | 189/232 [00:49<00:11,  3.74it/s] 82%|████████▏ | 190/232 [00:49<00:11,  3.73it/s] 82%|████████▏ | 191/232 [00:50<00:10,  3.83it/s] 83%|████████▎ | 192/232 [00:50<00:10,  3.90it/s] 83%|████████▎ | 193/232 [00:50<00:09,  3.95it/s] 84%|████████▎ | 194/232 [00:50<00:09,  3.99it/s] 84%|████████▍ | 195/232 [00:51<00:09,  4.01it/s] 84%|████████▍ | 196/232 [00:51<00:08,  4.02it/s] 85%|████████▍ | 197/232 [00:51<00:08,  4.04it/s] 85%|████████▌ | 198/232 [00:51<00:08,  4.06it/s] 86%|████████▌ | 199/232 [00:52<00:08,  4.05it/s] 86%|████████▌ | 200/232 [00:52<00:07,  4.06it/s] 87%|████████▋ | 201/232 [00:52<00:07,  4.07it/s] 87%|████████▋ | 202/232 [00:52<00:07,  4.07it/s] 88%|████████▊ | 203/232 [00:53<00:07,  4.07it/s] 88%|████████▊ | 204/232 [00:53<00:06,  4.08it/s] 88%|████████▊ | 205/232 [00:53<00:06,  4.08it/s] 89%|████████▉ | 206/232 [00:53<00:06,  4.08it/s] 89%|████████▉ | 207/232 [00:53<00:06,  4.07it/s] 90%|████████▉ | 208/232 [00:54<00:05,  4.08it/s] 90%|█████████ | 209/232 [00:54<00:05,  4.09it/s] 91%|█████████ | 210/232 [00:54<00:05,  4.08it/s] 91%|█████████ | 211/232 [00:54<00:05,  4.08it/s] 91%|█████████▏| 212/232 [00:55<00:04,  4.08it/s] 92%|█████████▏| 213/232 [00:55<00:04,  4.10it/s] 92%|█████████▏| 214/232 [00:55<00:04,  4.11it/s] 93%|█████████▎| 215/232 [00:55<00:04,  4.10it/s] 93%|█████████▎| 216/232 [00:56<00:03,  4.08it/s] 94%|█████████▎| 217/232 [00:56<00:03,  4.09it/s] 94%|█████████▍| 218/232 [00:56<00:03,  4.08it/s] 94%|█████████▍| 219/232 [00:56<00:03,  4.08it/s] 95%|█████████▍| 220/232 [00:57<00:02,  4.08it/s] 95%|█████████▌| 221/232 [00:57<00:02,  4.08it/s] 96%|█████████▌| 222/232 [00:57<00:02,  4.09it/s] 96%|█████████▌| 223/232 [00:57<00:02,  4.08it/s] 97%|█████████▋| 224/232 [00:58<00:02,  3.98it/s] 97%|█████████▋| 225/232 [00:58<00:01,  3.88it/s] 97%|█████████▋| 226/232 [00:58<00:01,  3.77it/s] 98%|█████████▊| 227/232 [00:58<00:01,  3.86it/s] 98%|█████████▊| 228/232 [00:59<00:01,  3.78it/s] 99%|█████████▊| 229/232 [00:59<00:00,  3.72it/s] 99%|█████████▉| 230/232 [00:59<00:00,  3.65it/s]100%|█████████▉| 231/232 [01:00<00:00,  3.77it/s]100%|██████████| 232/232 [01:00<00:00,  3.73it/s]accuracy:  0.8879310344827587
100%|██████████| 232/232 [01:03<00:00,  3.64it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 164
Training dataset size: 336, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:37:27,666] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:37:28,266] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:37:28,309] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:12,  2.40s/it]  4%|▎         | 2/56 [00:04<02:10,  2.42s/it]  5%|▌         | 3/56 [00:07<02:22,  2.68s/it]  7%|▋         | 4/56 [00:10<02:17,  2.65s/it]  9%|▉         | 5/56 [00:12<02:01,  2.39s/it] 11%|█         | 6/56 [00:14<02:03,  2.47s/it] 12%|█▎        | 7/56 [00:17<02:02,  2.50s/it] 14%|█▍        | 8/56 [00:19<01:57,  2.45s/it] 16%|█▌        | 9/56 [00:21<01:49,  2.32s/it] 18%|█▊        | 10/56 [00:24<01:52,  2.46s/it]                                               {'loss': 1.004, 'grad_norm': 8.54952335357666, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:24<01:52,  2.46s/it] 20%|█▉        | 11/56 [00:27<01:49,  2.43s/it] 21%|██▏       | 12/56 [00:29<01:42,  2.33s/it] 23%|██▎       | 13/56 [00:31<01:35,  2.22s/it] 25%|██▌       | 14/56 [00:33<01:34,  2.25s/it] 27%|██▋       | 15/56 [00:36<01:37,  2.37s/it] 29%|██▊       | 16/56 [00:38<01:35,  2.40s/it] 30%|███       | 17/56 [00:40<01:33,  2.41s/it] 32%|███▏      | 18/56 [00:43<01:34,  2.48s/it] 34%|███▍      | 19/56 [00:45<01:30,  2.43s/it] 36%|███▌      | 20/56 [00:48<01:23,  2.33s/it]                                               {'loss': 1.0912, 'grad_norm': 8.647889137268066, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:48<01:23,  2.33s/it] 38%|███▊      | 21/56 [00:50<01:23,  2.38s/it] 39%|███▉      | 22/56 [00:53<01:23,  2.45s/it] 41%|████      | 23/56 [00:55<01:20,  2.43s/it] 43%|████▎     | 24/56 [00:57<01:13,  2.31s/it] 45%|████▍     | 25/56 [00:59<01:10,  2.28s/it] 46%|████▋     | 26/56 [01:02<01:08,  2.29s/it] 48%|████▊     | 27/56 [01:04<01:06,  2.28s/it] 50%|█████     | 28/56 [01:07<01:08,  2.44s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:10<01:10,  2.62s/it] 54%|█████▎    | 30/56 [01:12<01:04,  2.50s/it]                                               {'loss': 0.9047, 'grad_norm': 7.948088645935059, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:12<01:04,  2.50s/it] 55%|█████▌    | 31/56 [01:14<01:02,  2.52s/it] 57%|█████▋    | 32/56 [01:17<01:00,  2.51s/it] 59%|█████▉    | 33/56 [01:19<00:56,  2.47s/it] 61%|██████    | 34/56 [01:22<00:57,  2.61s/it] 62%|██████▎   | 35/56 [01:25<00:53,  2.53s/it] 64%|██████▍   | 36/56 [01:27<00:48,  2.41s/it] 66%|██████▌   | 37/56 [01:29<00:44,  2.35s/it] 68%|██████▊   | 38/56 [01:31<00:40,  2.26s/it] 70%|██████▉   | 39/56 [01:34<00:40,  2.35s/it] 71%|███████▏  | 40/56 [01:36<00:38,  2.39s/it]                                               {'loss': 0.8244, 'grad_norm': 4.929563045501709, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:36<00:38,  2.39s/it] 73%|███████▎  | 41/56 [01:39<00:38,  2.55s/it] 75%|███████▌  | 42/56 [01:41<00:34,  2.46s/it] 77%|███████▋  | 43/56 [01:44<00:31,  2.43s/it] 79%|███████▊  | 44/56 [01:46<00:29,  2.42s/it] 80%|████████  | 45/56 [01:48<00:26,  2.38s/it] 82%|████████▏ | 46/56 [01:50<00:23,  2.30s/it] 84%|████████▍ | 47/56 [01:53<00:20,  2.31s/it] 86%|████████▌ | 48/56 [01:55<00:18,  2.36s/it] 88%|████████▊ | 49/56 [01:58<00:16,  2.36s/it] 89%|████████▉ | 50/56 [02:00<00:14,  2.38s/it]                                               {'loss': 0.8184, 'grad_norm': 5.761335372924805, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:00<00:14,  2.38s/it] 91%|█████████ | 51/56 [02:02<00:11,  2.34s/it] 93%|█████████▎| 52/56 [02:04<00:09,  2.29s/it] 95%|█████████▍| 53/56 [02:07<00:06,  2.33s/it] 96%|█████████▋| 54/56 [02:09<00:04,  2.27s/it] 98%|█████████▊| 55/56 [02:11<00:02,  2.34s/it]100%|██████████| 56/56 [02:14<00:00,  2.28s/it]                                               {'train_runtime': 134.7201, 'train_samples_per_second': 4.988, 'train_steps_per_second': 0.416, 'train_loss': 0.9058078186852592, 'epoch': 2.0}
100%|██████████| 56/56 [02:14<00:00,  2.28s/it]100%|██████████| 56/56 [02:14<00:00,  2.40s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:15,  3.27it/s]  1%|          | 2/248 [00:00<01:07,  3.65it/s]  1%|          | 3/248 [00:00<01:06,  3.68it/s]  2%|▏         | 4/248 [00:01<01:05,  3.70it/s]  2%|▏         | 5/248 [00:01<01:06,  3.65it/s]  2%|▏         | 6/248 [00:01<01:03,  3.81it/s]  3%|▎         | 7/248 [00:01<01:02,  3.87it/s]  3%|▎         | 8/248 [00:02<01:02,  3.83it/s]  4%|▎         | 9/248 [00:02<01:03,  3.79it/s]  4%|▍         | 10/248 [00:02<01:03,  3.72it/s]  4%|▍         | 11/248 [00:02<01:01,  3.86it/s]  5%|▍         | 12/248 [00:03<01:00,  3.92it/s]  5%|▌         | 13/248 [00:03<01:00,  3.87it/s]  6%|▌         | 14/248 [00:03<01:01,  3.82it/s]  6%|▌         | 15/248 [00:03<01:02,  3.74it/s]  6%|▋         | 16/248 [00:04<01:00,  3.86it/s]  7%|▋         | 17/248 [00:04<00:58,  3.92it/s]  7%|▋         | 18/248 [00:04<00:59,  3.87it/s]  8%|▊         | 19/248 [00:05<00:59,  3.82it/s]  8%|▊         | 20/248 [00:05<01:01,  3.73it/s]  8%|▊         | 21/248 [00:05<00:59,  3.84it/s]  9%|▉         | 22/248 [00:05<00:57,  3.91it/s]  9%|▉         | 23/248 [00:06<00:58,  3.85it/s] 10%|▉         | 24/248 [00:06<00:58,  3.81it/s] 10%|█         | 25/248 [00:06<00:59,  3.73it/s] 10%|█         | 26/248 [00:06<00:57,  3.85it/s] 11%|█         | 27/248 [00:07<00:56,  3.90it/s] 11%|█▏        | 28/248 [00:07<00:57,  3.85it/s] 12%|█▏        | 29/248 [00:07<00:57,  3.81it/s] 12%|█▏        | 30/248 [00:07<00:58,  3.73it/s] 12%|█▎        | 31/248 [00:08<00:56,  3.86it/s] 13%|█▎        | 32/248 [00:08<00:55,  3.92it/s] 13%|█▎        | 33/248 [00:08<00:55,  3.85it/s] 14%|█▎        | 34/248 [00:08<00:56,  3.81it/s] 14%|█▍        | 35/248 [00:09<00:57,  3.73it/s] 15%|█▍        | 36/248 [00:09<00:55,  3.85it/s] 15%|█▍        | 37/248 [00:09<00:53,  3.92it/s] 15%|█▌        | 38/248 [00:09<00:54,  3.85it/s] 16%|█▌        | 39/248 [00:10<00:54,  3.81it/s] 16%|█▌        | 40/248 [00:10<00:55,  3.72it/s] 17%|█▋        | 41/248 [00:10<00:53,  3.84it/s] 17%|█▋        | 42/248 [00:10<00:52,  3.91it/s] 17%|█▋        | 43/248 [00:11<00:53,  3.85it/s] 18%|█▊        | 44/248 [00:11<00:53,  3.80it/s] 18%|█▊        | 45/248 [00:11<00:54,  3.72it/s] 19%|█▊        | 46/248 [00:12<00:52,  3.84it/s] 19%|█▉        | 47/248 [00:12<00:51,  3.91it/s] 19%|█▉        | 48/248 [00:12<00:51,  3.85it/s] 20%|█▉        | 49/248 [00:12<00:51,  3.83it/s] 20%|██        | 50/248 [00:13<00:51,  3.81it/s] 21%|██        | 51/248 [00:13<00:50,  3.90it/s] 21%|██        | 52/248 [00:13<00:50,  3.91it/s] 21%|██▏       | 53/248 [00:13<00:50,  3.85it/s] 22%|██▏       | 54/248 [00:14<00:51,  3.77it/s] 22%|██▏       | 55/248 [00:14<00:49,  3.88it/s] 23%|██▎       | 56/248 [00:14<00:49,  3.87it/s] 23%|██▎       | 57/248 [00:14<00:50,  3.80it/s] 23%|██▎       | 58/248 [00:15<00:50,  3.78it/s] 24%|██▍       | 59/248 [00:15<00:50,  3.78it/s] 24%|██▍       | 60/248 [00:15<00:48,  3.86it/s] 25%|██▍       | 61/248 [00:15<00:49,  3.81it/s] 25%|██▌       | 62/248 [00:16<00:49,  3.76it/s] 25%|██▌       | 63/248 [00:16<00:49,  3.72it/s] 26%|██▌       | 64/248 [00:16<00:47,  3.84it/s] 26%|██▌       | 65/248 [00:17<00:47,  3.83it/s] 27%|██▋       | 66/248 [00:17<00:48,  3.77it/s] 27%|██▋       | 67/248 [00:17<00:48,  3.76it/s] 27%|██▋       | 68/248 [00:17<00:47,  3.77it/s] 28%|██▊       | 69/248 [00:18<00:46,  3.84it/s] 28%|██▊       | 70/248 [00:18<00:47,  3.77it/s] 29%|██▊       | 71/248 [00:18<00:47,  3.76it/s] 29%|██▉       | 72/248 [00:18<00:47,  3.72it/s] 29%|██▉       | 73/248 [00:19<00:45,  3.83it/s] 30%|██▉       | 74/248 [00:19<00:45,  3.82it/s] 30%|███       | 75/248 [00:19<00:45,  3.77it/s] 31%|███       | 76/248 [00:19<00:45,  3.76it/s] 31%|███       | 77/248 [00:20<00:45,  3.76it/s] 31%|███▏      | 78/248 [00:20<00:44,  3.85it/s] 32%|███▏      | 79/248 [00:20<00:44,  3.80it/s] 32%|███▏      | 80/248 [00:21<00:44,  3.75it/s] 33%|███▎      | 81/248 [00:21<00:45,  3.71it/s] 33%|███▎      | 82/248 [00:21<00:43,  3.83it/s] 33%|███▎      | 83/248 [00:21<00:42,  3.84it/s] 34%|███▍      | 84/248 [00:22<00:43,  3.78it/s] 34%|███▍      | 85/248 [00:22<00:42,  3.81it/s] 35%|███▍      | 86/248 [00:22<00:43,  3.75it/s] 35%|███▌      | 87/248 [00:22<00:41,  3.85it/s] 35%|███▌      | 88/248 [00:23<00:42,  3.78it/s] 36%|███▌      | 89/248 [00:23<00:42,  3.74it/s] 36%|███▋      | 90/248 [00:23<00:42,  3.68it/s] 37%|███▋      | 91/248 [00:23<00:41,  3.80it/s] 37%|███▋      | 92/248 [00:24<00:40,  3.81it/s] 38%|███▊      | 93/248 [00:24<00:41,  3.75it/s] 38%|███▊      | 94/248 [00:24<00:41,  3.75it/s] 38%|███▊      | 95/248 [00:24<00:40,  3.74it/s] 39%|███▊      | 96/248 [00:25<00:39,  3.84it/s] 39%|███▉      | 97/248 [00:25<00:39,  3.79it/s] 40%|███▉      | 98/248 [00:25<00:40,  3.75it/s] 40%|███▉      | 99/248 [00:26<00:40,  3.70it/s] 40%|████      | 100/248 [00:26<00:38,  3.80it/s] 41%|████      | 101/248 [00:26<00:38,  3.81it/s] 41%|████      | 102/248 [00:26<00:38,  3.75it/s] 42%|████▏     | 103/248 [00:27<00:38,  3.73it/s] 42%|████▏     | 104/248 [00:27<00:37,  3.84it/s] 42%|████▏     | 105/248 [00:27<00:36,  3.92it/s] 43%|████▎     | 106/248 [00:27<00:35,  3.98it/s] 43%|████▎     | 107/248 [00:28<00:34,  4.03it/s] 44%|████▎     | 108/248 [00:28<00:34,  4.07it/s] 44%|████▍     | 109/248 [00:28<00:33,  4.09it/s] 44%|████▍     | 110/248 [00:28<00:33,  4.11it/s] 45%|████▍     | 111/248 [00:29<00:33,  4.13it/s] 45%|████▌     | 112/248 [00:29<00:32,  4.14it/s] 46%|████▌     | 113/248 [00:29<00:32,  4.14it/s] 46%|████▌     | 114/248 [00:29<00:32,  4.15it/s] 46%|████▋     | 115/248 [00:29<00:32,  4.15it/s] 47%|████▋     | 116/248 [00:30<00:31,  4.15it/s] 47%|████▋     | 117/248 [00:30<00:31,  4.15it/s] 48%|████▊     | 118/248 [00:30<00:31,  4.15it/s] 48%|████▊     | 119/248 [00:30<00:31,  4.14it/s] 48%|████▊     | 120/248 [00:31<00:30,  4.14it/s] 49%|████▉     | 121/248 [00:31<00:30,  4.13it/s] 49%|████▉     | 122/248 [00:31<00:30,  4.13it/s] 50%|████▉     | 123/248 [00:31<00:30,  4.13it/s] 50%|█████     | 124/248 [00:32<00:29,  4.14it/s] 50%|█████     | 125/248 [00:32<00:29,  4.14it/s] 51%|█████     | 126/248 [00:32<00:29,  4.14it/s] 51%|█████     | 127/248 [00:32<00:29,  4.14it/s] 52%|█████▏    | 128/248 [00:33<00:28,  4.14it/s] 52%|█████▏    | 129/248 [00:33<00:28,  4.14it/s] 52%|█████▏    | 130/248 [00:33<00:28,  4.13it/s] 53%|█████▎    | 131/248 [00:33<00:28,  4.12it/s] 53%|█████▎    | 132/248 [00:34<00:28,  4.12it/s] 54%|█████▎    | 133/248 [00:34<00:27,  4.13it/s] 54%|█████▍    | 134/248 [00:34<00:27,  4.14it/s] 54%|█████▍    | 135/248 [00:34<00:27,  4.14it/s] 55%|█████▍    | 136/248 [00:35<00:26,  4.15it/s] 55%|█████▌    | 137/248 [00:35<00:26,  4.15it/s] 56%|█████▌    | 138/248 [00:35<00:26,  4.15it/s] 56%|█████▌    | 139/248 [00:35<00:26,  4.15it/s] 56%|█████▋    | 140/248 [00:36<00:26,  4.15it/s] 57%|█████▋    | 141/248 [00:36<00:25,  4.14it/s] 57%|█████▋    | 142/248 [00:36<00:25,  4.14it/s] 58%|█████▊    | 143/248 [00:36<00:25,  4.13it/s] 58%|█████▊    | 144/248 [00:37<00:25,  4.13it/s] 58%|█████▊    | 145/248 [00:37<00:24,  4.12it/s] 59%|█████▉    | 146/248 [00:37<00:24,  4.13it/s] 59%|█████▉    | 147/248 [00:37<00:24,  4.11it/s] 60%|█████▉    | 148/248 [00:38<00:25,  3.98it/s] 60%|██████    | 149/248 [00:38<00:25,  3.90it/s] 60%|██████    | 150/248 [00:38<00:25,  3.86it/s] 61%|██████    | 151/248 [00:38<00:24,  3.92it/s] 61%|██████▏   | 152/248 [00:39<00:25,  3.82it/s] 62%|██████▏   | 153/248 [00:39<00:25,  3.76it/s] 62%|██████▏   | 154/248 [00:39<00:25,  3.68it/s] 62%|██████▎   | 155/248 [00:39<00:24,  3.80it/s] 63%|██████▎   | 156/248 [00:40<00:24,  3.75it/s] 63%|██████▎   | 157/248 [00:40<00:24,  3.71it/s] 64%|██████▎   | 158/248 [00:40<00:24,  3.64it/s] 64%|██████▍   | 159/248 [00:40<00:23,  3.78it/s] 65%|██████▍   | 160/248 [00:41<00:23,  3.78it/s] 65%|██████▍   | 161/248 [00:41<00:23,  3.71it/s] 65%|██████▌   | 162/248 [00:41<00:23,  3.68it/s] 66%|██████▌   | 163/248 [00:42<00:22,  3.74it/s] 66%|██████▌   | 164/248 [00:42<00:22,  3.77it/s] 67%|██████▋   | 165/248 [00:42<00:22,  3.70it/s] 67%|██████▋   | 166/248 [00:42<00:21,  3.76it/s] 67%|██████▋   | 167/248 [00:43<00:21,  3.71it/s] 68%|██████▊   | 168/248 [00:43<00:21,  3.76it/s] 68%|██████▊   | 169/248 [00:43<00:21,  3.69it/s] 69%|██████▊   | 170/248 [00:43<00:20,  3.74it/s] 69%|██████▉   | 171/248 [00:44<00:20,  3.67it/s] 69%|██████▉   | 172/248 [00:44<00:20,  3.74it/s] 70%|██████▉   | 173/248 [00:44<00:20,  3.69it/s] 70%|███████   | 174/248 [00:44<00:19,  3.73it/s] 71%|███████   | 175/248 [00:45<00:19,  3.67it/s] 71%|███████   | 176/248 [00:45<00:19,  3.75it/s] 71%|███████▏  | 177/248 [00:45<00:19,  3.69it/s] 72%|███████▏  | 178/248 [00:46<00:18,  3.73it/s] 72%|███████▏  | 179/248 [00:46<00:18,  3.67it/s] 73%|███████▎  | 180/248 [00:46<00:18,  3.74it/s] 73%|███████▎  | 181/248 [00:46<00:18,  3.68it/s] 73%|███████▎  | 182/248 [00:47<00:17,  3.72it/s] 74%|███████▍  | 183/248 [00:47<00:17,  3.67it/s] 74%|███████▍  | 184/248 [00:47<00:17,  3.75it/s] 75%|███████▍  | 185/248 [00:47<00:17,  3.70it/s] 75%|███████▌  | 186/248 [00:48<00:16,  3.72it/s] 75%|███████▌  | 187/248 [00:48<00:16,  3.68it/s] 76%|███████▌  | 188/248 [00:48<00:15,  3.78it/s] 76%|███████▌  | 189/248 [00:49<00:15,  3.73it/s] 77%|███████▋  | 190/248 [00:49<00:15,  3.70it/s] 77%|███████▋  | 191/248 [00:49<00:15,  3.64it/s] 77%|███████▋  | 192/248 [00:49<00:14,  3.77it/s] 78%|███████▊  | 193/248 [00:50<00:14,  3.74it/s] 78%|███████▊  | 194/248 [00:50<00:14,  3.71it/s] 79%|███████▊  | 195/248 [00:50<00:14,  3.71it/s] 79%|███████▉  | 196/248 [00:50<00:13,  3.72it/s] 79%|███████▉  | 197/248 [00:51<00:13,  3.76it/s] 80%|███████▉  | 198/248 [00:51<00:13,  3.69it/s] 80%|████████  | 199/248 [00:51<00:13,  3.74it/s] 81%|████████  | 200/248 [00:51<00:13,  3.69it/s] 81%|████████  | 201/248 [00:52<00:12,  3.74it/s] 81%|████████▏ | 202/248 [00:52<00:12,  3.69it/s] 82%|████████▏ | 203/248 [00:52<00:12,  3.73it/s] 82%|████████▏ | 204/248 [00:53<00:11,  3.68it/s] 83%|████████▎ | 205/248 [00:53<00:11,  3.73it/s] 83%|████████▎ | 206/248 [00:53<00:11,  3.68it/s] 83%|████████▎ | 207/248 [00:53<00:10,  3.73it/s] 84%|████████▍ | 208/248 [00:54<00:10,  3.67it/s] 84%|████████▍ | 209/248 [00:54<00:10,  3.72it/s] 85%|████████▍ | 210/248 [00:54<00:10,  3.68it/s] 85%|████████▌ | 211/248 [00:54<00:09,  3.73it/s] 85%|████████▌ | 212/248 [00:55<00:09,  3.67it/s] 86%|████████▌ | 213/248 [00:55<00:09,  3.72it/s] 86%|████████▋ | 214/248 [00:55<00:09,  3.67it/s] 87%|████████▋ | 215/248 [00:56<00:08,  3.71it/s] 87%|████████▋ | 216/248 [00:56<00:08,  3.65it/s] 88%|████████▊ | 217/248 [00:56<00:08,  3.74it/s] 88%|████████▊ | 218/248 [00:56<00:08,  3.68it/s] 88%|████████▊ | 219/248 [00:57<00:07,  3.72it/s] 89%|████████▊ | 220/248 [00:57<00:07,  3.66it/s] 89%|████████▉ | 221/248 [00:57<00:07,  3.74it/s] 90%|████████▉ | 222/248 [00:57<00:07,  3.68it/s] 90%|████████▉ | 223/248 [00:58<00:06,  3.71it/s] 90%|█████████ | 224/248 [00:58<00:06,  3.66it/s] 91%|█████████ | 225/248 [00:58<00:06,  3.74it/s] 91%|█████████ | 226/248 [00:59<00:05,  3.68it/s] 92%|█████████▏| 227/248 [00:59<00:05,  3.73it/s] 92%|█████████▏| 228/248 [00:59<00:05,  3.67it/s] 92%|█████████▏| 229/248 [00:59<00:05,  3.75it/s] 93%|█████████▎| 230/248 [01:00<00:04,  3.69it/s] 93%|█████████▎| 231/248 [01:00<00:04,  3.72it/s] 94%|█████████▎| 232/248 [01:00<00:04,  3.67it/s] 94%|█████████▍| 233/248 [01:00<00:04,  3.74it/s] 94%|█████████▍| 234/248 [01:01<00:03,  3.69it/s] 95%|█████████▍| 235/248 [01:01<00:03,  3.71it/s] 95%|█████████▌| 236/248 [01:01<00:03,  3.67it/s] 96%|█████████▌| 237/248 [01:01<00:02,  3.76it/s] 96%|█████████▌| 238/248 [01:02<00:02,  3.71it/s] 96%|█████████▋| 239/248 [01:02<00:02,  3.70it/s] 97%|█████████▋| 240/248 [01:02<00:02,  3.66it/s] 97%|█████████▋| 241/248 [01:03<00:01,  3.78it/s] 98%|█████████▊| 242/248 [01:03<00:01,  3.73it/s] 98%|█████████▊| 243/248 [01:03<00:01,  3.70it/s] 98%|█████████▊| 244/248 [01:03<00:01,  3.63it/s] 99%|█████████▉| 245/248 [01:04<00:00,  3.76it/s] 99%|█████████▉| 246/248 [01:04<00:00,  3.76it/s]100%|█████████▉| 247/248 [01:04<00:00,  3.68it/s]100%|██████████| 248/248 [01:04<00:00,  3.74it/s]accuracy:  0.6491935483870968
100%|██████████| 248/248 [01:08<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 236
Training dataset size: 336, validation dataset size: 236
Training dataset size: 336, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:41:18,948] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 06:41:19,035] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:41:19,262] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:32,  2.78s/it]  4%|▎         | 2/56 [00:04<01:50,  2.04s/it]  5%|▌         | 3/56 [00:06<02:02,  2.31s/it]  7%|▋         | 4/56 [00:09<02:05,  2.42s/it]  9%|▉         | 5/56 [00:11<02:03,  2.42s/it] 11%|█         | 6/56 [00:14<02:01,  2.43s/it] 12%|█▎        | 7/56 [00:16<01:59,  2.43s/it] 14%|█▍        | 8/56 [00:18<01:52,  2.34s/it] 16%|█▌        | 9/56 [00:21<01:47,  2.28s/it] 18%|█▊        | 10/56 [00:24<01:53,  2.47s/it]                                               {'loss': 0.7892, 'grad_norm': 11.24008846282959, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:24<01:53,  2.47s/it] 20%|█▉        | 11/56 [00:26<01:53,  2.52s/it] 21%|██▏       | 12/56 [00:29<01:50,  2.51s/it] 23%|██▎       | 13/56 [00:31<01:44,  2.44s/it] 25%|██▌       | 14/56 [00:33<01:37,  2.33s/it] 27%|██▋       | 15/56 [00:35<01:31,  2.24s/it] 29%|██▊       | 16/56 [00:37<01:30,  2.25s/it] 30%|███       | 17/56 [00:40<01:31,  2.33s/it] 32%|███▏      | 18/56 [00:42<01:26,  2.28s/it] 34%|███▍      | 19/56 [00:44<01:25,  2.32s/it] 36%|███▌      | 20/56 [00:46<01:20,  2.24s/it]                                               {'loss': 0.9412, 'grad_norm': 9.772536277770996, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:46<01:20,  2.24s/it] 38%|███▊      | 21/56 [00:49<01:20,  2.29s/it] 39%|███▉      | 22/56 [00:52<01:27,  2.56s/it] 41%|████      | 23/56 [00:54<01:19,  2.41s/it] 43%|████▎     | 24/56 [00:57<01:19,  2.47s/it] 45%|████▍     | 25/56 [00:59<01:16,  2.45s/it] 46%|████▋     | 26/56 [01:02<01:13,  2.44s/it] 48%|████▊     | 27/56 [01:04<01:10,  2.43s/it] 50%|█████     | 28/56 [01:06<01:04,  2.30s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:08<01:02,  2.31s/it] 54%|█████▎    | 30/56 [01:11<01:01,  2.35s/it]                                               {'loss': 0.7977, 'grad_norm': 5.60604190826416, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:11<01:01,  2.35s/it] 55%|█████▌    | 31/56 [01:13<01:00,  2.42s/it] 57%|█████▋    | 32/56 [01:16<00:58,  2.44s/it] 59%|█████▉    | 33/56 [01:18<00:53,  2.35s/it] 61%|██████    | 34/56 [01:20<00:51,  2.33s/it] 62%|██████▎   | 35/56 [01:22<00:47,  2.24s/it] 64%|██████▍   | 36/56 [01:25<00:46,  2.31s/it] 66%|██████▌   | 37/56 [01:27<00:44,  2.32s/it] 68%|██████▊   | 38/56 [01:29<00:39,  2.20s/it] 70%|██████▉   | 39/56 [01:31<00:36,  2.16s/it] 71%|███████▏  | 40/56 [01:34<00:36,  2.26s/it]                                               {'loss': 0.8352, 'grad_norm': 7.481600284576416, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:34<00:36,  2.26s/it] 73%|███████▎  | 41/56 [01:36<00:34,  2.28s/it] 75%|███████▌  | 42/56 [01:39<00:33,  2.42s/it] 77%|███████▋  | 43/56 [01:41<00:29,  2.29s/it] 79%|███████▊  | 44/56 [01:43<00:26,  2.18s/it] 80%|████████  | 45/56 [01:44<00:22,  2.05s/it] 82%|████████▏ | 46/56 [01:47<00:21,  2.14s/it] 84%|████████▍ | 47/56 [01:49<00:20,  2.24s/it] 86%|████████▌ | 48/56 [01:51<00:17,  2.15s/it] 88%|████████▊ | 49/56 [01:53<00:15,  2.19s/it] 89%|████████▉ | 50/56 [01:56<00:13,  2.32s/it]                                               {'loss': 0.6598, 'grad_norm': 7.651512145996094, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [01:56<00:13,  2.32s/it] 91%|█████████ | 51/56 [01:59<00:12,  2.40s/it] 93%|█████████▎| 52/56 [02:00<00:09,  2.26s/it] 95%|█████████▍| 53/56 [02:03<00:07,  2.43s/it] 96%|█████████▋| 54/56 [02:06<00:04,  2.41s/it] 98%|█████████▊| 55/56 [02:08<00:02,  2.37s/it]100%|██████████| 56/56 [02:11<00:00,  2.47s/it]                                               {'train_runtime': 131.7448, 'train_samples_per_second': 5.101, 'train_steps_per_second': 0.425, 'train_loss': 0.7889383605548314, 'epoch': 2.0}
100%|██████████| 56/56 [02:11<00:00,  2.47s/it]100%|██████████| 56/56 [02:11<00:00,  2.35s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<01:52,  3.21it/s]  1%|          | 2/361 [00:00<01:42,  3.49it/s]  1%|          | 3/361 [00:00<01:40,  3.58it/s]  1%|          | 4/361 [00:01<01:39,  3.61it/s]  1%|▏         | 5/361 [00:01<01:34,  3.76it/s]  2%|▏         | 6/361 [00:01<01:31,  3.89it/s]  2%|▏         | 7/361 [00:01<01:32,  3.83it/s]  2%|▏         | 8/361 [00:02<01:33,  3.78it/s]  2%|▏         | 9/361 [00:02<01:35,  3.70it/s]  3%|▎         | 10/361 [00:02<01:31,  3.84it/s]  3%|▎         | 11/361 [00:02<01:29,  3.92it/s]  3%|▎         | 12/361 [00:03<01:30,  3.86it/s]  4%|▎         | 13/361 [00:03<01:31,  3.81it/s]  4%|▍         | 14/361 [00:03<01:33,  3.73it/s]  4%|▍         | 15/361 [00:03<01:29,  3.84it/s]  4%|▍         | 16/361 [00:04<01:28,  3.91it/s]  5%|▍         | 17/361 [00:04<01:29,  3.84it/s]  5%|▍         | 18/361 [00:04<01:30,  3.80it/s]  5%|▌         | 19/361 [00:05<01:31,  3.74it/s]  6%|▌         | 20/361 [00:05<01:28,  3.84it/s]  6%|▌         | 21/361 [00:05<01:26,  3.94it/s]  6%|▌         | 22/361 [00:05<01:28,  3.85it/s]  6%|▋         | 23/361 [00:06<01:28,  3.83it/s]  7%|▋         | 24/361 [00:06<01:27,  3.85it/s]  7%|▋         | 25/361 [00:06<01:29,  3.77it/s]  7%|▋         | 26/361 [00:06<01:26,  3.87it/s]  7%|▋         | 27/361 [00:07<01:26,  3.87it/s]  8%|▊         | 28/361 [00:07<01:27,  3.82it/s]  8%|▊         | 29/361 [00:07<01:27,  3.79it/s]  8%|▊         | 30/361 [00:07<01:28,  3.73it/s]  9%|▊         | 31/361 [00:08<01:26,  3.83it/s]  9%|▉         | 32/361 [00:08<01:24,  3.88it/s]  9%|▉         | 33/361 [00:08<01:25,  3.83it/s]  9%|▉         | 34/361 [00:08<01:26,  3.78it/s] 10%|▉         | 35/361 [00:09<01:27,  3.72it/s] 10%|▉         | 36/361 [00:09<01:24,  3.83it/s] 10%|█         | 37/361 [00:09<01:22,  3.92it/s] 11%|█         | 38/361 [00:09<01:23,  3.85it/s] 11%|█         | 39/361 [00:10<01:24,  3.80it/s] 11%|█         | 40/361 [00:10<01:26,  3.73it/s] 11%|█▏        | 41/361 [00:10<01:23,  3.84it/s] 12%|█▏        | 42/361 [00:11<01:21,  3.90it/s] 12%|█▏        | 43/361 [00:11<01:22,  3.84it/s] 12%|█▏        | 44/361 [00:11<01:23,  3.80it/s] 12%|█▏        | 45/361 [00:11<01:24,  3.73it/s] 13%|█▎        | 46/361 [00:12<01:22,  3.84it/s] 13%|█▎        | 47/361 [00:12<01:20,  3.89it/s] 13%|█▎        | 48/361 [00:12<01:21,  3.84it/s] 14%|█▎        | 49/361 [00:12<01:22,  3.80it/s] 14%|█▍        | 50/361 [00:13<01:23,  3.73it/s] 14%|█▍        | 51/361 [00:13<01:20,  3.84it/s] 14%|█▍        | 52/361 [00:13<01:18,  3.92it/s] 15%|█▍        | 53/361 [00:13<01:20,  3.84it/s] 15%|█▍        | 54/361 [00:14<01:20,  3.80it/s] 15%|█▌        | 55/361 [00:14<01:20,  3.82it/s] 16%|█▌        | 56/361 [00:14<01:21,  3.75it/s] 16%|█▌        | 57/361 [00:14<01:18,  3.86it/s] 16%|█▌        | 58/361 [00:15<01:18,  3.84it/s] 16%|█▋        | 59/361 [00:15<01:19,  3.80it/s] 17%|█▋        | 60/361 [00:15<01:18,  3.81it/s] 17%|█▋        | 61/361 [00:16<01:20,  3.74it/s] 17%|█▋        | 62/361 [00:16<01:17,  3.85it/s] 17%|█▋        | 63/361 [00:16<01:17,  3.85it/s] 18%|█▊        | 64/361 [00:16<01:18,  3.79it/s] 18%|█▊        | 65/361 [00:17<01:17,  3.81it/s] 18%|█▊        | 66/361 [00:17<01:18,  3.75it/s] 19%|█▊        | 67/361 [00:17<01:16,  3.87it/s] 19%|█▉        | 68/361 [00:17<01:15,  3.87it/s] 19%|█▉        | 69/361 [00:18<01:16,  3.81it/s] 19%|█▉        | 70/361 [00:18<01:16,  3.82it/s] 20%|█▉        | 71/361 [00:18<01:17,  3.73it/s] 20%|█▉        | 72/361 [00:18<01:15,  3.84it/s] 20%|██        | 73/361 [00:19<01:15,  3.84it/s] 20%|██        | 74/361 [00:19<01:15,  3.79it/s] 21%|██        | 75/361 [00:19<01:15,  3.80it/s] 21%|██        | 76/361 [00:19<01:16,  3.73it/s] 21%|██▏       | 77/361 [00:20<01:14,  3.83it/s] 22%|██▏       | 78/361 [00:20<01:14,  3.82it/s] 22%|██▏       | 79/361 [00:20<01:14,  3.78it/s] 22%|██▏       | 80/361 [00:21<01:13,  3.80it/s] 22%|██▏       | 81/361 [00:21<01:15,  3.73it/s] 23%|██▎       | 82/361 [00:21<01:12,  3.84it/s] 23%|██▎       | 83/361 [00:21<01:12,  3.83it/s] 23%|██▎       | 84/361 [00:22<01:13,  3.78it/s] 24%|██▎       | 85/361 [00:22<01:12,  3.80it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.72it/s] 24%|██▍       | 87/361 [00:22<01:11,  3.83it/s] 24%|██▍       | 88/361 [00:23<01:12,  3.79it/s] 25%|██▍       | 89/361 [00:23<01:12,  3.77it/s] 25%|██▍       | 90/361 [00:23<01:11,  3.79it/s] 25%|██▌       | 91/361 [00:23<01:12,  3.72it/s] 25%|██▌       | 92/361 [00:24<01:10,  3.83it/s] 26%|██▌       | 93/361 [00:24<01:09,  3.83it/s] 26%|██▌       | 94/361 [00:24<01:10,  3.78it/s] 26%|██▋       | 95/361 [00:24<01:10,  3.80it/s] 27%|██▋       | 96/361 [00:25<01:11,  3.72it/s] 27%|██▋       | 97/361 [00:25<01:08,  3.83it/s] 27%|██▋       | 98/361 [00:25<01:08,  3.84it/s] 27%|██▋       | 99/361 [00:26<01:09,  3.78it/s] 28%|██▊       | 100/361 [00:26<01:08,  3.79it/s] 28%|██▊       | 101/361 [00:26<01:09,  3.72it/s] 28%|██▊       | 102/361 [00:26<01:07,  3.83it/s] 29%|██▊       | 103/361 [00:27<01:07,  3.83it/s] 29%|██▉       | 104/361 [00:27<01:07,  3.78it/s] 29%|██▉       | 105/361 [00:27<01:07,  3.81it/s] 29%|██▉       | 106/361 [00:27<01:08,  3.73it/s] 30%|██▉       | 107/361 [00:28<01:06,  3.83it/s] 30%|██▉       | 108/361 [00:28<01:06,  3.82it/s] 30%|███       | 109/361 [00:28<01:06,  3.77it/s] 30%|███       | 110/361 [00:28<01:06,  3.80it/s] 31%|███       | 111/361 [00:29<01:07,  3.72it/s] 31%|███       | 112/361 [00:29<01:05,  3.83it/s] 31%|███▏      | 113/361 [00:29<01:05,  3.81it/s] 32%|███▏      | 114/361 [00:29<01:05,  3.76it/s] 32%|███▏      | 115/361 [00:30<01:05,  3.78it/s] 32%|███▏      | 116/361 [00:30<01:06,  3.71it/s] 32%|███▏      | 117/361 [00:30<01:03,  3.82it/s] 33%|███▎      | 118/361 [00:31<01:03,  3.81it/s] 33%|███▎      | 119/361 [00:31<01:04,  3.78it/s] 33%|███▎      | 120/361 [00:31<01:03,  3.79it/s] 34%|███▎      | 121/361 [00:31<01:04,  3.72it/s] 34%|███▍      | 122/361 [00:32<01:02,  3.83it/s] 34%|███▍      | 123/361 [00:32<01:02,  3.82it/s] 34%|███▍      | 124/361 [00:32<01:03,  3.76it/s] 35%|███▍      | 125/361 [00:32<01:03,  3.73it/s] 35%|███▍      | 126/361 [00:33<01:01,  3.83it/s] 35%|███▌      | 127/361 [00:33<00:59,  3.91it/s] 35%|███▌      | 128/361 [00:33<00:58,  3.97it/s] 36%|███▌      | 129/361 [00:33<00:57,  4.02it/s] 36%|███▌      | 130/361 [00:34<00:56,  4.06it/s] 36%|███▋      | 131/361 [00:34<00:56,  4.08it/s] 37%|███▋      | 132/361 [00:34<00:56,  4.09it/s] 37%|███▋      | 133/361 [00:34<00:55,  4.09it/s] 37%|███▋      | 134/361 [00:35<00:55,  4.09it/s] 37%|███▋      | 135/361 [00:35<00:55,  4.10it/s] 38%|███▊      | 136/361 [00:35<00:54,  4.11it/s] 38%|███▊      | 137/361 [00:35<00:54,  4.11it/s] 38%|███▊      | 138/361 [00:36<00:54,  4.11it/s] 39%|███▊      | 139/361 [00:36<00:54,  4.10it/s] 39%|███▉      | 140/361 [00:36<00:53,  4.11it/s] 39%|███▉      | 141/361 [00:36<00:53,  4.12it/s] 39%|███▉      | 142/361 [00:37<00:53,  4.12it/s] 40%|███▉      | 143/361 [00:37<00:52,  4.13it/s] 40%|███▉      | 144/361 [00:37<00:52,  4.13it/s] 40%|████      | 145/361 [00:37<00:52,  4.13it/s] 40%|████      | 146/361 [00:38<00:53,  4.01it/s] 41%|████      | 147/361 [00:38<00:55,  3.89it/s] 41%|████      | 148/361 [00:38<00:54,  3.89it/s] 41%|████▏     | 149/361 [00:38<00:53,  3.96it/s] 42%|████▏     | 150/361 [00:39<00:54,  3.86it/s] 42%|████▏     | 151/361 [00:39<00:55,  3.80it/s] 42%|████▏     | 152/361 [00:39<00:56,  3.73it/s] 42%|████▏     | 153/361 [00:39<00:54,  3.83it/s] 43%|████▎     | 154/361 [00:40<00:54,  3.83it/s] 43%|████▎     | 155/361 [00:40<00:54,  3.78it/s] 43%|████▎     | 156/361 [00:40<00:54,  3.78it/s] 43%|████▎     | 157/361 [00:40<00:54,  3.74it/s] 44%|████▍     | 158/361 [00:41<00:53,  3.79it/s] 44%|████▍     | 159/361 [00:41<00:53,  3.75it/s] 44%|████▍     | 160/361 [00:41<00:53,  3.79it/s] 45%|████▍     | 161/361 [00:42<00:53,  3.72it/s] 45%|████▍     | 162/361 [00:42<00:52,  3.81it/s] 45%|████▌     | 163/361 [00:42<00:52,  3.75it/s] 45%|████▌     | 164/361 [00:42<00:52,  3.73it/s] 46%|████▌     | 165/361 [00:43<00:53,  3.68it/s] 46%|████▌     | 166/361 [00:43<00:51,  3.80it/s] 46%|████▋     | 167/361 [00:43<00:51,  3.80it/s] 47%|████▋     | 168/361 [00:43<00:51,  3.76it/s] 47%|████▋     | 169/361 [00:44<00:51,  3.76it/s] 47%|████▋     | 170/361 [00:44<00:51,  3.72it/s] 47%|████▋     | 171/361 [00:44<00:50,  3.78it/s] 48%|████▊     | 172/361 [00:44<00:50,  3.73it/s] 48%|████▊     | 173/361 [00:45<00:50,  3.74it/s] 48%|████▊     | 174/361 [00:45<00:50,  3.72it/s] 48%|████▊     | 175/361 [00:45<00:49,  3.78it/s] 49%|████▉     | 176/361 [00:46<00:49,  3.74it/s] 49%|████▉     | 177/361 [00:46<00:49,  3.75it/s] 49%|████▉     | 178/361 [00:46<00:49,  3.69it/s] 50%|████▉     | 179/361 [00:46<00:47,  3.81it/s] 50%|████▉     | 180/361 [00:47<00:48,  3.77it/s] 50%|█████     | 181/361 [00:47<00:48,  3.73it/s] 50%|█████     | 182/361 [00:47<00:48,  3.68it/s] 51%|█████     | 183/361 [00:47<00:47,  3.79it/s] 51%|█████     | 184/361 [00:48<00:47,  3.76it/s] 51%|█████     | 185/361 [00:48<00:46,  3.75it/s] 52%|█████▏    | 186/361 [00:48<00:47,  3.68it/s] 52%|█████▏    | 187/361 [00:48<00:46,  3.74it/s] 52%|█████▏    | 188/361 [00:49<00:45,  3.79it/s] 52%|█████▏    | 189/361 [00:49<00:45,  3.75it/s] 53%|█████▎    | 190/361 [00:49<00:45,  3.73it/s] 53%|█████▎    | 191/361 [00:50<00:45,  3.71it/s] 53%|█████▎    | 192/361 [00:50<00:44,  3.77it/s] 53%|█████▎    | 193/361 [00:50<00:44,  3.74it/s] 54%|█████▎    | 194/361 [00:50<00:44,  3.72it/s] 54%|█████▍    | 195/361 [00:51<00:45,  3.67it/s] 54%|█████▍    | 196/361 [00:51<00:43,  3.78it/s] 55%|█████▍    | 197/361 [00:51<00:43,  3.75it/s] 55%|█████▍    | 198/361 [00:51<00:43,  3.71it/s] 55%|█████▌    | 199/361 [00:52<00:44,  3.65it/s] 55%|█████▌    | 200/361 [00:52<00:42,  3.77it/s] 56%|█████▌    | 201/361 [00:52<00:42,  3.75it/s] 56%|█████▌    | 202/361 [00:52<00:42,  3.72it/s] 56%|█████▌    | 203/361 [00:53<00:43,  3.66it/s] 57%|█████▋    | 204/361 [00:53<00:42,  3.73it/s] 57%|█████▋    | 205/361 [00:53<00:41,  3.77it/s] 57%|█████▋    | 206/361 [00:54<00:41,  3.73it/s] 57%|█████▋    | 207/361 [00:54<00:41,  3.75it/s] 58%|█████▊    | 208/361 [00:54<00:41,  3.68it/s] 58%|█████▊    | 209/361 [00:54<00:40,  3.75it/s] 58%|█████▊    | 210/361 [00:55<00:40,  3.72it/s] 58%|█████▊    | 211/361 [00:55<00:40,  3.73it/s] 59%|█████▊    | 212/361 [00:55<00:40,  3.66it/s] 59%|█████▉    | 213/361 [00:55<00:39,  3.78it/s] 59%|█████▉    | 214/361 [00:56<00:38,  3.85it/s] 60%|█████▉    | 215/361 [00:56<00:38,  3.77it/s] 60%|█████▉    | 216/361 [00:56<00:38,  3.73it/s] 60%|██████    | 217/361 [00:56<00:39,  3.67it/s] 60%|██████    | 218/361 [00:57<00:37,  3.77it/s] 61%|██████    | 219/361 [00:57<00:37,  3.75it/s] 61%|██████    | 220/361 [00:57<00:37,  3.74it/s] 61%|██████    | 221/361 [00:58<00:37,  3.74it/s] 61%|██████▏   | 222/361 [00:58<00:37,  3.71it/s] 62%|██████▏   | 223/361 [00:58<00:36,  3.78it/s] 62%|██████▏   | 224/361 [00:58<00:36,  3.72it/s] 62%|██████▏   | 225/361 [00:59<00:36,  3.71it/s] 63%|██████▎   | 226/361 [00:59<00:36,  3.67it/s] 63%|██████▎   | 227/361 [00:59<00:35,  3.79it/s] 63%|██████▎   | 228/361 [00:59<00:35,  3.77it/s] 63%|██████▎   | 229/361 [01:00<00:35,  3.73it/s] 64%|██████▎   | 230/361 [01:00<00:35,  3.72it/s] 64%|██████▍   | 231/361 [01:00<00:35,  3.70it/s] 64%|██████▍   | 232/361 [01:00<00:34,  3.77it/s] 65%|██████▍   | 233/361 [01:01<00:34,  3.72it/s] 65%|██████▍   | 234/361 [01:01<00:34,  3.71it/s] 65%|██████▌   | 235/361 [01:01<00:34,  3.66it/s] 65%|██████▌   | 236/361 [01:02<00:33,  3.78it/s] 66%|██████▌   | 237/361 [01:02<00:32,  3.79it/s] 66%|██████▌   | 238/361 [01:02<00:32,  3.75it/s] 66%|██████▌   | 239/361 [01:02<00:32,  3.77it/s] 66%|██████▋   | 240/361 [01:03<00:32,  3.70it/s] 67%|██████▋   | 241/361 [01:03<00:31,  3.78it/s] 67%|██████▋   | 242/361 [01:03<00:31,  3.72it/s] 67%|██████▋   | 243/361 [01:03<00:31,  3.71it/s] 68%|██████▊   | 244/361 [01:04<00:31,  3.67it/s] 68%|██████▊   | 245/361 [01:04<00:30,  3.78it/s] 68%|██████▊   | 246/361 [01:04<00:30,  3.78it/s] 68%|██████▊   | 247/361 [01:05<00:30,  3.73it/s] 69%|██████▊   | 248/361 [01:05<00:30,  3.75it/s] 69%|██████▉   | 249/361 [01:05<00:30,  3.71it/s] 69%|██████▉   | 250/361 [01:05<00:29,  3.77it/s] 70%|██████▉   | 251/361 [01:06<00:29,  3.73it/s] 70%|██████▉   | 252/361 [01:06<00:29,  3.72it/s] 70%|███████   | 253/361 [01:06<00:29,  3.67it/s] 70%|███████   | 254/361 [01:06<00:28,  3.78it/s] 71%|███████   | 255/361 [01:07<00:28,  3.75it/s] 71%|███████   | 256/361 [01:07<00:28,  3.72it/s] 71%|███████   | 257/361 [01:07<00:28,  3.65it/s] 71%|███████▏  | 258/361 [01:07<00:27,  3.77it/s] 72%|███████▏  | 259/361 [01:08<00:27,  3.77it/s] 72%|███████▏  | 260/361 [01:08<00:27,  3.72it/s] 72%|███████▏  | 261/361 [01:08<00:26,  3.72it/s] 73%|███████▎  | 262/361 [01:09<00:26,  3.70it/s] 73%|███████▎  | 263/361 [01:09<00:26,  3.77it/s] 73%|███████▎  | 264/361 [01:09<00:26,  3.72it/s] 73%|███████▎  | 265/361 [01:09<00:25,  3.72it/s] 74%|███████▎  | 266/361 [01:10<00:25,  3.67it/s] 74%|███████▍  | 267/361 [01:10<00:24,  3.79it/s] 74%|███████▍  | 268/361 [01:10<00:24,  3.76it/s] 75%|███████▍  | 269/361 [01:10<00:24,  3.71it/s] 75%|███████▍  | 270/361 [01:11<00:25,  3.64it/s] 75%|███████▌  | 271/361 [01:11<00:23,  3.76it/s] 75%|███████▌  | 272/361 [01:11<00:23,  3.78it/s] 76%|███████▌  | 273/361 [01:11<00:23,  3.74it/s] 76%|███████▌  | 274/361 [01:12<00:23,  3.77it/s] 76%|███████▌  | 275/361 [01:12<00:23,  3.71it/s] 76%|███████▋  | 276/361 [01:12<00:22,  3.78it/s] 77%|███████▋  | 277/361 [01:13<00:22,  3.73it/s] 77%|███████▋  | 278/361 [01:13<00:22,  3.72it/s] 77%|███████▋  | 279/361 [01:13<00:22,  3.67it/s] 78%|███████▊  | 280/361 [01:13<00:21,  3.78it/s] 78%|███████▊  | 281/361 [01:14<00:21,  3.75it/s] 78%|███████▊  | 282/361 [01:14<00:21,  3.70it/s] 78%|███████▊  | 283/361 [01:14<00:21,  3.65it/s] 79%|███████▊  | 284/361 [01:14<00:20,  3.77it/s] 79%|███████▉  | 285/361 [01:15<00:20,  3.77it/s] 79%|███████▉  | 286/361 [01:15<00:20,  3.72it/s] 80%|███████▉  | 287/361 [01:15<00:19,  3.74it/s] 80%|███████▉  | 288/361 [01:16<00:19,  3.70it/s] 80%|████████  | 289/361 [01:16<00:19,  3.77it/s] 80%|████████  | 290/361 [01:16<00:19,  3.72it/s] 81%|████████  | 291/361 [01:16<00:18,  3.71it/s] 81%|████████  | 292/361 [01:17<00:18,  3.65it/s] 81%|████████  | 293/361 [01:17<00:18,  3.76it/s] 81%|████████▏ | 294/361 [01:17<00:17,  3.74it/s] 82%|████████▏ | 295/361 [01:17<00:17,  3.72it/s] 82%|████████▏ | 296/361 [01:18<00:17,  3.72it/s] 82%|████████▏ | 297/361 [01:18<00:17,  3.70it/s] 83%|████████▎ | 298/361 [01:18<00:16,  3.77it/s] 83%|████████▎ | 299/361 [01:18<00:16,  3.72it/s] 83%|████████▎ | 300/361 [01:19<00:16,  3.71it/s] 83%|████████▎ | 301/361 [01:19<00:16,  3.66it/s] 84%|████████▎ | 302/361 [01:19<00:15,  3.77it/s] 84%|████████▍ | 303/361 [01:20<00:15,  3.75it/s] 84%|████████▍ | 304/361 [01:20<00:15,  3.72it/s] 84%|████████▍ | 305/361 [01:20<00:14,  3.75it/s] 85%|████████▍ | 306/361 [01:20<00:14,  3.71it/s] 85%|████████▌ | 307/361 [01:21<00:14,  3.78it/s] 85%|████████▌ | 308/361 [01:21<00:14,  3.73it/s] 86%|████████▌ | 309/361 [01:21<00:13,  3.75it/s] 86%|████████▌ | 310/361 [01:21<00:13,  3.68it/s] 86%|████████▌ | 311/361 [01:22<00:13,  3.76it/s] 86%|████████▋ | 312/361 [01:22<00:13,  3.70it/s] 87%|████████▋ | 313/361 [01:22<00:12,  3.70it/s] 87%|████████▋ | 314/361 [01:23<00:12,  3.66it/s] 87%|████████▋ | 315/361 [01:23<00:12,  3.77it/s] 88%|████████▊ | 316/361 [01:23<00:11,  3.76it/s] 88%|████████▊ | 317/361 [01:23<00:11,  3.73it/s] 88%|████████▊ | 318/361 [01:24<00:11,  3.74it/s] 88%|████████▊ | 319/361 [01:24<00:11,  3.70it/s] 89%|████████▊ | 320/361 [01:24<00:10,  3.76it/s] 89%|████████▉ | 321/361 [01:24<00:10,  3.72it/s] 89%|████████▉ | 322/361 [01:25<00:10,  3.74it/s] 89%|████████▉ | 323/361 [01:25<00:10,  3.68it/s] 90%|████████▉ | 324/361 [01:25<00:09,  3.78it/s] 90%|█████████ | 325/361 [01:25<00:09,  3.72it/s] 90%|█████████ | 326/361 [01:26<00:09,  3.69it/s] 91%|█████████ | 327/361 [01:26<00:09,  3.64it/s] 91%|█████████ | 328/361 [01:26<00:08,  3.76it/s] 91%|█████████ | 329/361 [01:27<00:08,  3.77it/s] 91%|█████████▏| 330/361 [01:27<00:08,  3.73it/s] 92%|█████████▏| 331/361 [01:27<00:08,  3.69it/s] 92%|█████████▏| 332/361 [01:27<00:07,  3.73it/s] 92%|█████████▏| 333/361 [01:28<00:07,  3.78it/s] 93%|█████████▎| 334/361 [01:28<00:07,  3.73it/s] 93%|█████████▎| 335/361 [01:28<00:07,  3.71it/s] 93%|█████████▎| 336/361 [01:28<00:06,  3.70it/s] 93%|█████████▎| 337/361 [01:29<00:06,  3.79it/s] 94%|█████████▎| 338/361 [01:29<00:06,  3.75it/s] 94%|█████████▍| 339/361 [01:29<00:05,  3.71it/s] 94%|█████████▍| 340/361 [01:29<00:05,  3.66it/s] 94%|█████████▍| 341/361 [01:30<00:05,  3.78it/s] 95%|█████████▍| 342/361 [01:30<00:05,  3.76it/s] 95%|█████████▌| 343/361 [01:30<00:04,  3.70it/s] 95%|█████████▌| 344/361 [01:31<00:04,  3.63it/s] 96%|█████████▌| 345/361 [01:31<00:04,  3.76it/s] 96%|█████████▌| 346/361 [01:31<00:03,  3.77it/s] 96%|█████████▌| 347/361 [01:31<00:03,  3.73it/s] 96%|█████████▋| 348/361 [01:32<00:03,  3.76it/s] 97%|█████████▋| 349/361 [01:32<00:03,  3.71it/s] 97%|█████████▋| 350/361 [01:32<00:02,  3.79it/s] 97%|█████████▋| 351/361 [01:32<00:02,  3.72it/s] 98%|█████████▊| 352/361 [01:33<00:02,  3.71it/s] 98%|█████████▊| 353/361 [01:33<00:02,  3.66it/s] 98%|█████████▊| 354/361 [01:33<00:01,  3.77it/s] 98%|█████████▊| 355/361 [01:33<00:01,  3.75it/s] 99%|█████████▊| 356/361 [01:34<00:01,  3.73it/s] 99%|█████████▉| 357/361 [01:34<00:01,  3.77it/s] 99%|█████████▉| 358/361 [01:34<00:00,  3.69it/s] 99%|█████████▉| 359/361 [01:35<00:00,  3.77it/s]100%|█████████▉| 360/361 [01:35<00:00,  3.72it/s]100%|██████████| 361/361 [01:35<00:00,  3.72it/s]accuracy:  0.6232686980609419
100%|██████████| 361/361 [01:41<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 188
Training dataset size: 336, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:45:41,546] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:45:41,918] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:45:42,171] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:32,  2.78s/it]  4%|▎         | 2/56 [00:05<02:30,  2.78s/it]  5%|▌         | 3/56 [00:07<02:17,  2.60s/it]  7%|▋         | 4/56 [00:10<02:13,  2.56s/it]  9%|▉         | 5/56 [00:13<02:13,  2.61s/it] 11%|█         | 6/56 [00:15<02:06,  2.54s/it] 12%|█▎        | 7/56 [00:17<01:59,  2.43s/it] 14%|█▍        | 8/56 [00:20<02:01,  2.53s/it] 16%|█▌        | 9/56 [00:23<02:08,  2.74s/it] 18%|█▊        | 10/56 [00:26<02:12,  2.88s/it]                                               {'loss': 0.8804, 'grad_norm': 3.6541922092437744, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:26<02:12,  2.88s/it] 20%|█▉        | 11/56 [00:29<02:07,  2.84s/it] 21%|██▏       | 12/56 [00:32<02:05,  2.84s/it] 23%|██▎       | 13/56 [00:35<02:04,  2.90s/it] 25%|██▌       | 14/56 [00:38<02:00,  2.88s/it] 27%|██▋       | 15/56 [00:40<01:52,  2.75s/it] 29%|██▊       | 16/56 [00:43<01:49,  2.73s/it] 30%|███       | 17/56 [00:45<01:42,  2.62s/it] 32%|███▏      | 18/56 [00:48<01:38,  2.60s/it] 34%|███▍      | 19/56 [00:51<01:40,  2.72s/it] 36%|███▌      | 20/56 [00:53<01:32,  2.57s/it]                                               {'loss': 0.5876, 'grad_norm': 9.725271224975586, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:53<01:32,  2.57s/it] 38%|███▊      | 21/56 [00:56<01:32,  2.65s/it] 39%|███▉      | 22/56 [00:59<01:33,  2.75s/it] 41%|████      | 23/56 [01:02<01:31,  2.78s/it] 43%|████▎     | 24/56 [01:05<01:28,  2.77s/it] 45%|████▍     | 25/56 [01:08<01:29,  2.88s/it] 46%|████▋     | 26/56 [01:10<01:21,  2.71s/it] 48%|████▊     | 27/56 [01:13<01:17,  2.67s/it] 50%|█████     | 28/56 [01:16<01:18,  2.80s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:18<01:11,  2.66s/it] 54%|█████▎    | 30/56 [01:21<01:09,  2.69s/it]                                               {'loss': 0.4959, 'grad_norm': 1.7541004419326782, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:21<01:09,  2.69s/it] 55%|█████▌    | 31/56 [01:24<01:11,  2.85s/it] 57%|█████▋    | 32/56 [01:27<01:10,  2.92s/it] 59%|█████▉    | 33/56 [01:30<01:06,  2.91s/it] 61%|██████    | 34/56 [01:33<01:03,  2.89s/it] 62%|██████▎   | 35/56 [01:36<01:03,  3.01s/it] 64%|██████▍   | 36/56 [01:39<00:58,  2.95s/it] 66%|██████▌   | 37/56 [01:42<00:55,  2.91s/it] 68%|██████▊   | 38/56 [01:44<00:49,  2.74s/it] 70%|██████▉   | 39/56 [01:47<00:46,  2.76s/it] 71%|███████▏  | 40/56 [01:50<00:45,  2.82s/it]                                               {'loss': 0.4237, 'grad_norm': 5.588342666625977, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:50<00:45,  2.82s/it] 73%|███████▎  | 41/56 [01:52<00:40,  2.70s/it] 75%|███████▌  | 42/56 [01:55<00:38,  2.74s/it] 77%|███████▋  | 43/56 [01:58<00:36,  2.77s/it] 79%|███████▊  | 44/56 [02:01<00:34,  2.90s/it] 80%|████████  | 45/56 [02:04<00:32,  2.93s/it] 82%|████████▏ | 46/56 [02:07<00:29,  2.96s/it] 84%|████████▍ | 47/56 [02:10<00:26,  2.98s/it] 86%|████████▌ | 48/56 [02:13<00:22,  2.82s/it] 88%|████████▊ | 49/56 [02:15<00:19,  2.80s/it] 89%|████████▉ | 50/56 [02:18<00:16,  2.81s/it]                                               {'loss': 0.3653, 'grad_norm': 6.318709850311279, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:18<00:16,  2.81s/it] 91%|█████████ | 51/56 [02:21<00:14,  2.83s/it] 93%|█████████▎| 52/56 [02:24<00:10,  2.75s/it] 95%|█████████▍| 53/56 [02:26<00:08,  2.78s/it] 96%|█████████▋| 54/56 [02:28<00:05,  2.55s/it] 98%|█████████▊| 55/56 [02:32<00:02,  2.72s/it]100%|██████████| 56/56 [02:35<00:00,  2.78s/it]                                               {'train_runtime': 155.7203, 'train_samples_per_second': 4.315, 'train_steps_per_second': 0.36, 'train_loss': 0.5311925709247589, 'epoch': 2.0}
100%|██████████| 56/56 [02:35<00:00,  2.78s/it]100%|██████████| 56/56 [02:35<00:00,  2.78s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:11,  2.94it/s]  1%|          | 2/212 [00:00<01:03,  3.32it/s]  1%|▏         | 3/212 [00:00<01:01,  3.42it/s]  2%|▏         | 4/212 [00:01<00:56,  3.68it/s]  2%|▏         | 5/212 [00:01<00:55,  3.76it/s]  3%|▎         | 6/212 [00:01<00:55,  3.70it/s]  3%|▎         | 7/212 [00:01<00:54,  3.77it/s]  4%|▍         | 8/212 [00:02<00:54,  3.72it/s]  4%|▍         | 9/212 [00:02<00:53,  3.78it/s]  5%|▍         | 10/212 [00:02<00:54,  3.73it/s]  5%|▌         | 11/212 [00:02<00:53,  3.78it/s]  6%|▌         | 12/212 [00:03<00:53,  3.73it/s]  6%|▌         | 13/212 [00:03<00:52,  3.81it/s]  7%|▋         | 14/212 [00:03<00:52,  3.75it/s]  7%|▋         | 15/212 [00:04<00:51,  3.79it/s]  8%|▊         | 16/212 [00:04<00:52,  3.74it/s]  8%|▊         | 17/212 [00:04<00:51,  3.82it/s]  8%|▊         | 18/212 [00:04<00:51,  3.77it/s]  9%|▉         | 19/212 [00:05<00:51,  3.76it/s]  9%|▉         | 20/212 [00:05<00:51,  3.70it/s] 10%|▉         | 21/212 [00:05<00:50,  3.82it/s] 10%|█         | 22/212 [00:05<00:50,  3.77it/s] 11%|█         | 23/212 [00:06<00:50,  3.75it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.72it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.78it/s] 12%|█▏        | 26/212 [00:06<00:48,  3.83it/s] 13%|█▎        | 27/212 [00:07<00:49,  3.75it/s] 13%|█▎        | 28/212 [00:07<00:48,  3.79it/s] 14%|█▎        | 29/212 [00:07<00:49,  3.72it/s] 14%|█▍        | 30/212 [00:08<00:48,  3.78it/s] 15%|█▍        | 31/212 [00:08<00:48,  3.71it/s] 15%|█▌        | 32/212 [00:08<00:47,  3.76it/s] 16%|█▌        | 33/212 [00:08<00:48,  3.71it/s] 16%|█▌        | 34/212 [00:09<00:47,  3.78it/s] 17%|█▋        | 35/212 [00:09<00:47,  3.73it/s] 17%|█▋        | 36/212 [00:09<00:46,  3.77it/s] 17%|█▋        | 37/212 [00:09<00:47,  3.71it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.77it/s] 18%|█▊        | 39/212 [00:10<00:46,  3.73it/s] 19%|█▉        | 40/212 [00:10<00:45,  3.78it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.73it/s] 20%|█▉        | 42/212 [00:11<00:44,  3.80it/s] 20%|██        | 43/212 [00:11<00:45,  3.74it/s] 21%|██        | 44/212 [00:11<00:44,  3.79it/s] 21%|██        | 45/212 [00:12<00:44,  3.73it/s] 22%|██▏       | 46/212 [00:12<00:43,  3.79it/s] 22%|██▏       | 47/212 [00:12<00:44,  3.72it/s] 23%|██▎       | 48/212 [00:12<00:43,  3.77it/s] 23%|██▎       | 49/212 [00:13<00:44,  3.70it/s] 24%|██▎       | 50/212 [00:13<00:43,  3.76it/s] 24%|██▍       | 51/212 [00:13<00:43,  3.72it/s] 25%|██▍       | 52/212 [00:13<00:42,  3.77it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.72it/s] 25%|██▌       | 54/212 [00:14<00:41,  3.79it/s] 26%|██▌       | 55/212 [00:14<00:42,  3.73it/s] 26%|██▋       | 56/212 [00:14<00:41,  3.77it/s] 27%|██▋       | 57/212 [00:15<00:41,  3.72it/s] 27%|██▋       | 58/212 [00:15<00:40,  3.82it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.77it/s] 28%|██▊       | 60/212 [00:16<00:40,  3.74it/s] 29%|██▉       | 61/212 [00:16<00:40,  3.68it/s] 29%|██▉       | 62/212 [00:16<00:39,  3.81it/s] 30%|██▉       | 63/212 [00:16<00:39,  3.80it/s] 30%|███       | 64/212 [00:17<00:39,  3.73it/s] 31%|███       | 65/212 [00:17<00:39,  3.73it/s] 31%|███       | 66/212 [00:17<00:39,  3.74it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.78it/s] 32%|███▏      | 68/212 [00:18<00:38,  3.72it/s] 33%|███▎      | 69/212 [00:18<00:38,  3.73it/s] 33%|███▎      | 70/212 [00:18<00:38,  3.69it/s] 33%|███▎      | 71/212 [00:18<00:37,  3.80it/s] 34%|███▍      | 72/212 [00:19<00:37,  3.75it/s] 34%|███▍      | 73/212 [00:19<00:37,  3.72it/s] 35%|███▍      | 74/212 [00:19<00:37,  3.66it/s] 35%|███▌      | 75/212 [00:20<00:36,  3.79it/s] 36%|███▌      | 76/212 [00:20<00:35,  3.78it/s] 36%|███▋      | 77/212 [00:20<00:36,  3.71it/s] 37%|███▋      | 78/212 [00:20<00:36,  3.70it/s] 37%|███▋      | 79/212 [00:21<00:35,  3.74it/s] 38%|███▊      | 80/212 [00:21<00:34,  3.78it/s] 38%|███▊      | 81/212 [00:21<00:35,  3.71it/s] 39%|███▊      | 82/212 [00:21<00:34,  3.75it/s] 39%|███▉      | 83/212 [00:22<00:34,  3.70it/s] 40%|███▉      | 84/212 [00:22<00:33,  3.79it/s] 40%|████      | 85/212 [00:22<00:33,  3.75it/s] 41%|████      | 86/212 [00:22<00:33,  3.72it/s] 41%|████      | 87/212 [00:23<00:34,  3.66it/s] 42%|████▏     | 88/212 [00:23<00:32,  3.79it/s] 42%|████▏     | 89/212 [00:23<00:32,  3.75it/s] 42%|████▏     | 90/212 [00:24<00:32,  3.72it/s] 43%|████▎     | 91/212 [00:24<00:32,  3.73it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.74it/s] 44%|████▍     | 93/212 [00:24<00:31,  3.78it/s] 44%|████▍     | 94/212 [00:25<00:31,  3.71it/s] 45%|████▍     | 95/212 [00:25<00:31,  3.74it/s] 45%|████▌     | 96/212 [00:25<00:31,  3.69it/s] 46%|████▌     | 97/212 [00:25<00:30,  3.78it/s] 46%|████▌     | 98/212 [00:26<00:30,  3.75it/s] 47%|████▋     | 99/212 [00:26<00:30,  3.72it/s] 47%|████▋     | 100/212 [00:26<00:30,  3.66it/s] 48%|████▊     | 101/212 [00:27<00:29,  3.78it/s] 48%|████▊     | 102/212 [00:27<00:29,  3.74it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.71it/s] 49%|████▉     | 104/212 [00:27<00:29,  3.72it/s] 50%|████▉     | 105/212 [00:28<00:28,  3.74it/s] 50%|█████     | 106/212 [00:28<00:28,  3.78it/s] 50%|█████     | 107/212 [00:28<00:28,  3.72it/s] 51%|█████     | 108/212 [00:28<00:27,  3.76it/s] 51%|█████▏    | 109/212 [00:29<00:27,  3.70it/s] 52%|█████▏    | 110/212 [00:29<00:27,  3.77it/s] 52%|█████▏    | 111/212 [00:29<00:27,  3.70it/s] 53%|█████▎    | 112/212 [00:29<00:26,  3.74it/s] 53%|█████▎    | 113/212 [00:30<00:26,  3.68it/s] 54%|█████▍    | 114/212 [00:30<00:26,  3.75it/s] 54%|█████▍    | 115/212 [00:30<00:26,  3.69it/s] 55%|█████▍    | 116/212 [00:31<00:25,  3.73it/s] 55%|█████▌    | 117/212 [00:31<00:25,  3.67it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.74it/s] 56%|█████▌    | 119/212 [00:31<00:25,  3.68it/s] 57%|█████▋    | 120/212 [00:32<00:24,  3.72it/s] 57%|█████▋    | 121/212 [00:32<00:24,  3.67it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.74it/s] 58%|█████▊    | 123/212 [00:32<00:24,  3.69it/s] 58%|█████▊    | 124/212 [00:33<00:23,  3.73it/s] 59%|█████▉    | 125/212 [00:33<00:23,  3.67it/s] 59%|█████▉    | 126/212 [00:33<00:22,  3.74it/s] 60%|█████▉    | 127/212 [00:34<00:23,  3.69it/s] 60%|██████    | 128/212 [00:34<00:22,  3.74it/s] 61%|██████    | 129/212 [00:34<00:22,  3.67it/s] 61%|██████▏   | 130/212 [00:34<00:21,  3.74it/s] 62%|██████▏   | 131/212 [00:35<00:21,  3.69it/s] 62%|██████▏   | 132/212 [00:35<00:21,  3.73it/s] 63%|██████▎   | 133/212 [00:35<00:21,  3.67it/s] 63%|██████▎   | 134/212 [00:35<00:20,  3.73it/s] 64%|██████▎   | 135/212 [00:36<00:20,  3.68it/s] 64%|██████▍   | 136/212 [00:36<00:20,  3.72it/s] 65%|██████▍   | 137/212 [00:36<00:20,  3.68it/s] 65%|██████▌   | 138/212 [00:36<00:19,  3.76it/s] 66%|██████▌   | 139/212 [00:37<00:19,  3.73it/s] 66%|██████▌   | 140/212 [00:37<00:19,  3.70it/s] 67%|██████▋   | 141/212 [00:37<00:19,  3.65it/s] 67%|██████▋   | 142/212 [00:38<00:18,  3.78it/s] 67%|██████▋   | 143/212 [00:38<00:18,  3.73it/s] 68%|██████▊   | 144/212 [00:38<00:18,  3.70it/s] 68%|██████▊   | 145/212 [00:38<00:18,  3.63it/s] 69%|██████▉   | 146/212 [00:39<00:17,  3.77it/s] 69%|██████▉   | 147/212 [00:39<00:17,  3.77it/s] 70%|██████▉   | 148/212 [00:39<00:17,  3.70it/s] 70%|███████   | 149/212 [00:39<00:16,  3.73it/s] 71%|███████   | 150/212 [00:40<00:16,  3.70it/s] 71%|███████   | 151/212 [00:40<00:16,  3.76it/s] 72%|███████▏  | 152/212 [00:40<00:16,  3.70it/s] 72%|███████▏  | 153/212 [00:41<00:15,  3.74it/s] 73%|███████▎  | 154/212 [00:41<00:15,  3.68it/s] 73%|███████▎  | 155/212 [00:41<00:15,  3.76it/s] 74%|███████▎  | 156/212 [00:41<00:15,  3.70it/s] 74%|███████▍  | 157/212 [00:42<00:14,  3.73it/s] 75%|███████▍  | 158/212 [00:42<00:14,  3.68it/s] 75%|███████▌  | 159/212 [00:42<00:14,  3.77it/s] 75%|███████▌  | 160/212 [00:42<00:14,  3.71it/s] 76%|███████▌  | 161/212 [00:43<00:13,  3.71it/s] 76%|███████▋  | 162/212 [00:43<00:13,  3.65it/s] 77%|███████▋  | 163/212 [00:43<00:12,  3.77it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.77it/s] 78%|███████▊  | 165/212 [00:44<00:12,  3.69it/s] 78%|███████▊  | 166/212 [00:44<00:12,  3.67it/s] 79%|███████▉  | 167/212 [00:44<00:12,  3.72it/s] 79%|███████▉  | 168/212 [00:45<00:11,  3.76it/s] 80%|███████▉  | 169/212 [00:45<00:11,  3.69it/s] 80%|████████  | 170/212 [00:45<00:11,  3.73it/s] 81%|████████  | 171/212 [00:45<00:11,  3.67it/s] 81%|████████  | 172/212 [00:46<00:10,  3.73it/s] 82%|████████▏ | 173/212 [00:46<00:10,  3.68it/s] 82%|████████▏ | 174/212 [00:46<00:10,  3.73it/s] 83%|████████▎ | 175/212 [00:46<00:10,  3.68it/s] 83%|████████▎ | 176/212 [00:47<00:09,  3.75it/s] 83%|████████▎ | 177/212 [00:47<00:09,  3.69it/s] 84%|████████▍ | 178/212 [00:47<00:09,  3.73it/s] 84%|████████▍ | 179/212 [00:48<00:08,  3.67it/s] 85%|████████▍ | 180/212 [00:48<00:08,  3.74it/s] 85%|████████▌ | 181/212 [00:48<00:08,  3.69it/s] 86%|████████▌ | 182/212 [00:48<00:08,  3.72it/s] 86%|████████▋ | 183/212 [00:49<00:07,  3.67it/s] 87%|████████▋ | 184/212 [00:49<00:07,  3.75it/s] 87%|████████▋ | 185/212 [00:49<00:07,  3.68it/s] 88%|████████▊ | 186/212 [00:49<00:07,  3.69it/s] 88%|████████▊ | 187/212 [00:50<00:06,  3.64it/s] 89%|████████▊ | 188/212 [00:50<00:06,  3.75it/s] 89%|████████▉ | 189/212 [00:50<00:06,  3.70it/s] 90%|████████▉ | 190/212 [00:50<00:05,  3.67it/s] 90%|█████████ | 191/212 [00:51<00:05,  3.62it/s] 91%|█████████ | 192/212 [00:51<00:05,  3.75it/s] 91%|█████████ | 193/212 [00:51<00:05,  3.71it/s] 92%|█████████▏| 194/212 [00:52<00:04,  3.67it/s] 92%|█████████▏| 195/212 [00:52<00:04,  3.62it/s] 92%|█████████▏| 196/212 [00:52<00:04,  3.74it/s] 93%|█████████▎| 197/212 [00:52<00:04,  3.75it/s] 93%|█████████▎| 198/212 [00:53<00:03,  3.68it/s] 94%|█████████▍| 199/212 [00:53<00:03,  3.70it/s] 94%|█████████▍| 200/212 [00:53<00:03,  3.68it/s] 95%|█████████▍| 201/212 [00:53<00:02,  3.73it/s] 95%|█████████▌| 202/212 [00:54<00:02,  3.68it/s] 96%|█████████▌| 203/212 [00:54<00:02,  3.72it/s] 96%|█████████▌| 204/212 [00:54<00:02,  3.67it/s] 97%|█████████▋| 205/212 [00:55<00:01,  3.72it/s] 97%|█████████▋| 206/212 [00:55<00:01,  3.67it/s] 98%|█████████▊| 207/212 [00:55<00:01,  3.71it/s] 98%|█████████▊| 208/212 [00:55<00:01,  3.65it/s] 99%|█████████▊| 209/212 [00:56<00:00,  3.71it/s] 99%|█████████▉| 210/212 [00:56<00:00,  3.66it/s]100%|█████████▉| 211/212 [00:56<00:00,  3.70it/s]100%|██████████| 212/212 [00:56<00:00,  3.66it/s]accuracy:  0.8632075471698113
100%|██████████| 212/212 [01:00<00:00,  3.52it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 134
Training dataset size: 336, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:49:44,860] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:49:44,908] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:49:45,480] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:16,  2.48s/it]  4%|▎         | 2/56 [00:04<02:14,  2.49s/it]  5%|▌         | 3/56 [00:08<02:31,  2.85s/it]  7%|▋         | 4/56 [00:10<02:23,  2.76s/it]  9%|▉         | 5/56 [00:13<02:19,  2.74s/it] 11%|█         | 6/56 [00:16<02:16,  2.73s/it] 12%|█▎        | 7/56 [00:19<02:15,  2.77s/it] 14%|█▍        | 8/56 [00:21<02:07,  2.66s/it] 16%|█▌        | 9/56 [00:23<01:57,  2.50s/it] 18%|█▊        | 10/56 [00:26<02:01,  2.64s/it]                                               {'loss': 0.7345, 'grad_norm': 6.715723037719727, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:26<02:01,  2.64s/it] 20%|█▉        | 11/56 [00:29<01:57,  2.62s/it] 21%|██▏       | 12/56 [00:31<01:53,  2.58s/it] 23%|██▎       | 13/56 [00:34<01:52,  2.61s/it] 25%|██▌       | 14/56 [00:37<01:51,  2.66s/it] 27%|██▋       | 15/56 [00:40<01:55,  2.81s/it] 29%|██▊       | 16/56 [00:43<01:50,  2.77s/it] 30%|███       | 17/56 [00:45<01:48,  2.78s/it] 32%|███▏      | 18/56 [00:48<01:45,  2.78s/it] 34%|███▍      | 19/56 [00:51<01:41,  2.74s/it] 36%|███▌      | 20/56 [00:53<01:36,  2.69s/it]                                               {'loss': 0.8108, 'grad_norm': 6.746328353881836, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:53<01:36,  2.69s/it] 38%|███▊      | 21/56 [00:56<01:38,  2.82s/it] 39%|███▉      | 22/56 [00:59<01:30,  2.65s/it] 41%|████      | 23/56 [01:01<01:25,  2.59s/it] 43%|████▎     | 24/56 [01:04<01:24,  2.65s/it] 45%|████▍     | 25/56 [01:07<01:22,  2.68s/it] 46%|████▋     | 26/56 [01:10<01:25,  2.85s/it] 48%|████▊     | 27/56 [01:12<01:19,  2.73s/it] 50%|█████     | 28/56 [01:15<01:13,  2.61s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:18<01:14,  2.75s/it] 54%|█████▎    | 30/56 [01:20<01:10,  2.70s/it]                                               {'loss': 0.4575, 'grad_norm': 5.709532737731934, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:20<01:10,  2.70s/it] 55%|█████▌    | 31/56 [01:23<01:08,  2.72s/it] 57%|█████▋    | 32/56 [01:26<01:04,  2.70s/it] 59%|█████▉    | 33/56 [01:29<01:03,  2.75s/it] 61%|██████    | 34/56 [01:31<00:59,  2.70s/it] 62%|██████▎   | 35/56 [01:34<00:57,  2.73s/it] 64%|██████▍   | 36/56 [01:36<00:51,  2.58s/it] 66%|██████▌   | 37/56 [01:39<00:48,  2.57s/it] 68%|██████▊   | 38/56 [01:42<00:47,  2.66s/it] 70%|██████▉   | 39/56 [01:44<00:44,  2.63s/it] 71%|███████▏  | 40/56 [01:47<00:41,  2.62s/it]                                               {'loss': 0.5052, 'grad_norm': 4.9062299728393555, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:47<00:41,  2.62s/it] 73%|███████▎  | 41/56 [01:50<00:40,  2.69s/it] 75%|███████▌  | 42/56 [01:52<00:35,  2.55s/it] 77%|███████▋  | 43/56 [01:55<00:35,  2.71s/it] 79%|███████▊  | 44/56 [01:58<00:33,  2.81s/it] 80%|████████  | 45/56 [02:01<00:30,  2.78s/it] 82%|████████▏ | 46/56 [02:04<00:27,  2.78s/it] 84%|████████▍ | 47/56 [02:06<00:24,  2.71s/it] 86%|████████▌ | 48/56 [02:08<00:20,  2.60s/it] 88%|████████▊ | 49/56 [02:11<00:18,  2.71s/it] 89%|████████▉ | 50/56 [02:14<00:16,  2.73s/it]                                               {'loss': 0.5531, 'grad_norm': 9.749777793884277, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:14<00:16,  2.73s/it] 91%|█████████ | 51/56 [02:17<00:13,  2.72s/it] 93%|█████████▎| 52/56 [02:19<00:10,  2.66s/it] 95%|█████████▍| 53/56 [02:22<00:07,  2.52s/it] 96%|█████████▋| 54/56 [02:24<00:04,  2.48s/it] 98%|█████████▊| 55/56 [02:27<00:02,  2.59s/it]100%|██████████| 56/56 [02:30<00:00,  2.68s/it]                                               {'train_runtime': 150.8733, 'train_samples_per_second': 4.454, 'train_steps_per_second': 0.371, 'train_loss': 0.5939328755651202, 'epoch': 2.0}
100%|██████████| 56/56 [02:30<00:00,  2.68s/it]100%|██████████| 56/56 [02:30<00:00,  2.69s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:50,  3.23it/s]  1%|          | 2/163 [00:00<00:46,  3.49it/s]  2%|▏         | 3/163 [00:00<00:44,  3.59it/s]  2%|▏         | 4/163 [00:01<00:44,  3.61it/s]  3%|▎         | 5/163 [00:01<00:41,  3.78it/s]  4%|▎         | 6/163 [00:01<00:41,  3.75it/s]  4%|▍         | 7/163 [00:01<00:41,  3.73it/s]  5%|▍         | 8/163 [00:02<00:41,  3.70it/s]  6%|▌         | 9/163 [00:02<00:40,  3.83it/s]  6%|▌         | 10/163 [00:02<00:40,  3.78it/s]  7%|▋         | 11/163 [00:02<00:40,  3.76it/s]  7%|▋         | 12/163 [00:03<00:40,  3.71it/s]  8%|▊         | 13/163 [00:03<00:39,  3.83it/s]  9%|▊         | 14/163 [00:03<00:39,  3.78it/s]  9%|▉         | 15/163 [00:04<00:39,  3.78it/s] 10%|▉         | 16/163 [00:04<00:38,  3.78it/s] 10%|█         | 17/163 [00:04<00:37,  3.88it/s] 11%|█         | 18/163 [00:04<00:36,  3.95it/s] 12%|█▏        | 19/163 [00:05<00:35,  4.00it/s] 12%|█▏        | 20/163 [00:05<00:35,  4.05it/s] 13%|█▎        | 21/163 [00:05<00:34,  4.08it/s] 13%|█▎        | 22/163 [00:05<00:34,  4.11it/s] 14%|█▍        | 23/163 [00:05<00:33,  4.12it/s] 15%|█▍        | 24/163 [00:06<00:33,  4.14it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.15it/s] 16%|█▌        | 26/163 [00:06<00:33,  4.15it/s] 17%|█▋        | 27/163 [00:06<00:32,  4.15it/s] 17%|█▋        | 28/163 [00:07<00:32,  4.15it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.16it/s] 18%|█▊        | 30/163 [00:07<00:31,  4.16it/s] 19%|█▉        | 31/163 [00:07<00:31,  4.15it/s] 20%|█▉        | 32/163 [00:08<00:31,  4.15it/s] 20%|██        | 33/163 [00:08<00:31,  4.16it/s] 21%|██        | 34/163 [00:08<00:31,  4.16it/s] 21%|██▏       | 35/163 [00:08<00:30,  4.17it/s] 22%|██▏       | 36/163 [00:09<00:30,  4.17it/s] 23%|██▎       | 37/163 [00:09<00:30,  4.17it/s] 23%|██▎       | 38/163 [00:09<00:29,  4.18it/s] 24%|██▍       | 39/163 [00:09<00:29,  4.17it/s] 25%|██▍       | 40/163 [00:10<00:29,  4.17it/s] 25%|██▌       | 41/163 [00:10<00:29,  4.17it/s] 26%|██▌       | 42/163 [00:10<00:29,  4.17it/s] 26%|██▋       | 43/163 [00:10<00:28,  4.17it/s] 27%|██▋       | 44/163 [00:11<00:28,  4.17it/s] 28%|██▊       | 45/163 [00:11<00:28,  4.18it/s] 28%|██▊       | 46/163 [00:11<00:28,  4.18it/s] 29%|██▉       | 47/163 [00:11<00:27,  4.17it/s] 29%|██▉       | 48/163 [00:11<00:27,  4.17it/s] 30%|███       | 49/163 [00:12<00:27,  4.16it/s] 31%|███       | 50/163 [00:12<00:27,  4.16it/s] 31%|███▏      | 51/163 [00:12<00:26,  4.16it/s] 32%|███▏      | 52/163 [00:12<00:26,  4.16it/s] 33%|███▎      | 53/163 [00:13<00:26,  4.13it/s] 33%|███▎      | 54/163 [00:13<00:27,  4.00it/s] 34%|███▎      | 55/163 [00:13<00:27,  3.91it/s] 34%|███▍      | 56/163 [00:13<00:27,  3.89it/s] 35%|███▍      | 57/163 [00:14<00:27,  3.91it/s] 36%|███▌      | 58/163 [00:14<00:27,  3.81it/s] 36%|███▌      | 59/163 [00:14<00:27,  3.82it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.74it/s] 37%|███▋      | 61/163 [00:15<00:26,  3.80it/s] 38%|███▊      | 62/163 [00:15<00:27,  3.72it/s] 39%|███▊      | 63/163 [00:15<00:26,  3.76it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.70it/s] 40%|███▉      | 65/163 [00:16<00:26,  3.76it/s] 40%|████      | 66/163 [00:16<00:26,  3.70it/s] 41%|████      | 67/163 [00:16<00:25,  3.74it/s] 42%|████▏     | 68/163 [00:17<00:25,  3.68it/s] 42%|████▏     | 69/163 [00:17<00:25,  3.74it/s] 43%|████▎     | 70/163 [00:17<00:25,  3.68it/s] 44%|████▎     | 71/163 [00:18<00:24,  3.73it/s] 44%|████▍     | 72/163 [00:18<00:24,  3.67it/s] 45%|████▍     | 73/163 [00:18<00:23,  3.76it/s] 45%|████▌     | 74/163 [00:18<00:24,  3.71it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.74it/s] 47%|████▋     | 76/163 [00:19<00:23,  3.69it/s] 47%|████▋     | 77/163 [00:19<00:22,  3.78it/s] 48%|████▊     | 78/163 [00:19<00:22,  3.74it/s] 48%|████▊     | 79/163 [00:20<00:22,  3.71it/s] 49%|████▉     | 80/163 [00:20<00:22,  3.67it/s] 50%|████▉     | 81/163 [00:20<00:21,  3.79it/s] 50%|█████     | 82/163 [00:20<00:21,  3.74it/s] 51%|█████     | 83/163 [00:21<00:21,  3.71it/s] 52%|█████▏    | 84/163 [00:21<00:21,  3.67it/s] 52%|█████▏    | 85/163 [00:21<00:20,  3.80it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.73it/s] 53%|█████▎    | 87/163 [00:22<00:20,  3.71it/s] 54%|█████▍    | 88/163 [00:22<00:20,  3.65it/s] 55%|█████▍    | 89/163 [00:22<00:19,  3.78it/s] 55%|█████▌    | 90/163 [00:23<00:19,  3.74it/s] 56%|█████▌    | 91/163 [00:23<00:19,  3.72it/s] 56%|█████▋    | 92/163 [00:23<00:19,  3.68it/s] 57%|█████▋    | 93/163 [00:23<00:18,  3.75it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.78it/s] 58%|█████▊    | 95/163 [00:24<00:18,  3.71it/s] 59%|█████▉    | 96/163 [00:24<00:18,  3.72it/s] 60%|█████▉    | 97/163 [00:24<00:17,  3.71it/s] 60%|██████    | 98/163 [00:25<00:17,  3.76it/s] 61%|██████    | 99/163 [00:25<00:17,  3.70it/s] 61%|██████▏   | 100/163 [00:25<00:16,  3.74it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.69it/s] 63%|██████▎   | 102/163 [00:26<00:16,  3.77it/s] 63%|██████▎   | 103/163 [00:26<00:16,  3.70it/s] 64%|██████▍   | 104/163 [00:26<00:15,  3.74it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.68it/s] 65%|██████▌   | 106/163 [00:27<00:15,  3.75it/s] 66%|██████▌   | 107/163 [00:27<00:15,  3.68it/s] 66%|██████▋   | 108/163 [00:27<00:14,  3.73it/s] 67%|██████▋   | 109/163 [00:28<00:14,  3.67it/s] 67%|██████▋   | 110/163 [00:28<00:14,  3.75it/s] 68%|██████▊   | 111/163 [00:28<00:14,  3.69it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.73it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.67it/s] 70%|██████▉   | 114/163 [00:29<00:13,  3.74it/s] 71%|███████   | 115/163 [00:29<00:13,  3.68it/s] 71%|███████   | 116/163 [00:30<00:12,  3.73it/s] 72%|███████▏  | 117/163 [00:30<00:12,  3.67it/s] 72%|███████▏  | 118/163 [00:30<00:12,  3.73it/s] 73%|███████▎  | 119/163 [00:30<00:11,  3.67it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.71it/s] 74%|███████▍  | 121/163 [00:31<00:11,  3.67it/s] 75%|███████▍  | 122/163 [00:31<00:10,  3.73it/s] 75%|███████▌  | 123/163 [00:32<00:10,  3.67it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.72it/s] 77%|███████▋  | 125/163 [00:32<00:10,  3.68it/s] 77%|███████▋  | 126/163 [00:32<00:09,  3.74it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.67it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.70it/s] 79%|███████▉  | 129/163 [00:33<00:09,  3.69it/s] 80%|███████▉  | 130/163 [00:33<00:08,  3.75it/s] 80%|████████  | 131/163 [00:34<00:08,  3.69it/s] 81%|████████  | 132/163 [00:34<00:08,  3.73it/s] 82%|████████▏ | 133/163 [00:34<00:08,  3.68it/s] 82%|████████▏ | 134/163 [00:34<00:07,  3.74it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.68it/s] 83%|████████▎ | 136/163 [00:35<00:07,  3.72it/s] 84%|████████▍ | 137/163 [00:35<00:07,  3.67it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.75it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.68it/s] 86%|████████▌ | 140/163 [00:36<00:06,  3.72it/s] 87%|████████▋ | 141/163 [00:36<00:05,  3.67it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.75it/s] 88%|████████▊ | 143/163 [00:37<00:05,  3.68it/s] 88%|████████▊ | 144/163 [00:37<00:05,  3.73it/s] 89%|████████▉ | 145/163 [00:37<00:04,  3.68it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.76it/s] 90%|█████████ | 147/163 [00:38<00:04,  3.69it/s] 91%|█████████ | 148/163 [00:38<00:04,  3.72it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.67it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.75it/s] 93%|█████████▎| 151/163 [00:39<00:03,  3.68it/s] 93%|█████████▎| 152/163 [00:39<00:02,  3.74it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.73it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.84it/s] 95%|█████████▌| 155/163 [00:40<00:02,  3.92it/s] 96%|█████████▌| 156/163 [00:40<00:01,  3.97it/s] 96%|█████████▋| 157/163 [00:41<00:01,  4.00it/s] 97%|█████████▋| 158/163 [00:41<00:01,  4.03it/s] 98%|█████████▊| 159/163 [00:41<00:00,  4.06it/s] 98%|█████████▊| 160/163 [00:41<00:00,  4.07it/s] 99%|█████████▉| 161/163 [00:42<00:00,  4.08it/s] 99%|█████████▉| 162/163 [00:42<00:00,  4.08it/s]100%|██████████| 163/163 [00:42<00:00,  4.09it/s]accuracy:  0.8220858895705522
100%|██████████| 163/163 [00:45<00:00,  3.62it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 171
Training dataset size: 336, validation dataset size: 171
Training dataset size: 336, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:53:27,395] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.46s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:53:27,589] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:53:28,679] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:15,  2.46s/it]  4%|▎         | 2/56 [00:04<02:03,  2.29s/it]  5%|▌         | 3/56 [00:07<02:09,  2.44s/it]  7%|▋         | 4/56 [00:09<02:10,  2.51s/it]  9%|▉         | 5/56 [00:12<02:08,  2.51s/it] 11%|█         | 6/56 [00:14<01:59,  2.40s/it] 12%|█▎        | 7/56 [00:17<01:58,  2.42s/it] 14%|█▍        | 8/56 [00:19<01:58,  2.48s/it] 16%|█▌        | 9/56 [00:22<02:02,  2.62s/it] 18%|█▊        | 10/56 [00:25<02:02,  2.66s/it]                                               {'loss': 0.4188, 'grad_norm': 1.6496220827102661, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:25<02:02,  2.66s/it] 20%|█▉        | 11/56 [00:27<01:59,  2.65s/it] 21%|██▏       | 12/56 [00:30<01:56,  2.65s/it] 23%|██▎       | 13/56 [00:32<01:48,  2.53s/it] 25%|██▌       | 14/56 [00:35<01:43,  2.47s/it] 27%|██▋       | 15/56 [00:37<01:40,  2.46s/it] 29%|██▊       | 16/56 [00:40<01:39,  2.48s/it] 30%|███       | 17/56 [00:42<01:35,  2.44s/it] 32%|███▏      | 18/56 [00:45<01:37,  2.56s/it] 34%|███▍      | 19/56 [00:47<01:33,  2.53s/it] 36%|███▌      | 20/56 [00:50<01:35,  2.66s/it]                                               {'loss': 0.725, 'grad_norm': 4.209773540496826, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:50<01:35,  2.66s/it] 38%|███▊      | 21/56 [00:53<01:35,  2.73s/it] 39%|███▉      | 22/56 [00:56<01:31,  2.70s/it] 41%|████      | 23/56 [00:58<01:29,  2.70s/it] 43%|████▎     | 24/56 [01:01<01:28,  2.75s/it] 45%|████▍     | 25/56 [01:04<01:22,  2.66s/it] 46%|████▋     | 26/56 [01:06<01:18,  2.62s/it] 48%|████▊     | 27/56 [01:09<01:16,  2.64s/it] 50%|█████     | 28/56 [01:12<01:14,  2.66s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:15<01:13,  2.73s/it] 54%|█████▎    | 30/56 [01:17<01:08,  2.65s/it]                                               {'loss': 0.2987, 'grad_norm': 7.644037246704102, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:17<01:08,  2.65s/it] 55%|█████▌    | 31/56 [01:19<01:03,  2.55s/it] 57%|█████▋    | 32/56 [01:22<01:01,  2.55s/it] 59%|█████▉    | 33/56 [01:25<00:59,  2.57s/it] 61%|██████    | 34/56 [01:27<00:55,  2.51s/it] 62%|██████▎   | 35/56 [01:29<00:52,  2.48s/it] 64%|██████▍   | 36/56 [01:32<00:50,  2.51s/it] 66%|██████▌   | 37/56 [01:34<00:45,  2.38s/it] 68%|██████▊   | 38/56 [01:36<00:41,  2.29s/it] 70%|██████▉   | 39/56 [01:39<00:40,  2.40s/it] 71%|███████▏  | 40/56 [01:41<00:39,  2.46s/it]                                               {'loss': 0.297, 'grad_norm': 1.804928183555603, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:41<00:39,  2.46s/it] 73%|███████▎  | 41/56 [01:44<00:38,  2.54s/it] 75%|███████▌  | 42/56 [01:47<00:36,  2.60s/it] 77%|███████▋  | 43/56 [01:50<00:34,  2.66s/it] 79%|███████▊  | 44/56 [01:52<00:31,  2.62s/it] 80%|████████  | 45/56 [01:55<00:29,  2.64s/it] 82%|████████▏ | 46/56 [01:57<00:25,  2.59s/it] 84%|████████▍ | 47/56 [02:00<00:23,  2.59s/it] 86%|████████▌ | 48/56 [02:03<00:20,  2.62s/it] 88%|████████▊ | 49/56 [02:05<00:19,  2.72s/it] 89%|████████▉ | 50/56 [02:07<00:14,  2.49s/it]                                               {'loss': 0.2968, 'grad_norm': 7.707608699798584, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:07<00:14,  2.49s/it] 91%|█████████ | 51/56 [02:09<00:11,  2.28s/it] 93%|█████████▎| 52/56 [02:12<00:09,  2.35s/it] 95%|█████████▍| 53/56 [02:15<00:07,  2.52s/it] 96%|█████████▋| 54/56 [02:17<00:04,  2.42s/it] 98%|█████████▊| 55/56 [02:20<00:02,  2.52s/it]100%|██████████| 56/56 [02:22<00:00,  2.61s/it]                                               {'train_runtime': 143.5768, 'train_samples_per_second': 4.68, 'train_steps_per_second': 0.39, 'train_loss': 0.4036714477198465, 'epoch': 2.0}
100%|██████████| 56/56 [02:23<00:00,  2.61s/it]100%|██████████| 56/56 [02:23<00:00,  2.56s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<01:12,  2.98it/s]  1%|          | 2/217 [00:00<01:04,  3.35it/s]  1%|▏         | 3/217 [00:00<01:01,  3.47it/s]  2%|▏         | 4/217 [00:01<00:57,  3.71it/s]  2%|▏         | 5/217 [00:01<00:57,  3.71it/s]  3%|▎         | 6/217 [00:01<00:57,  3.70it/s]  3%|▎         | 7/217 [00:01<00:57,  3.68it/s]  4%|▎         | 8/217 [00:02<00:54,  3.83it/s]  4%|▍         | 9/217 [00:02<00:54,  3.79it/s]  5%|▍         | 10/217 [00:02<00:55,  3.75it/s]  5%|▌         | 11/217 [00:02<00:55,  3.72it/s]  6%|▌         | 12/217 [00:03<00:53,  3.84it/s]  6%|▌         | 13/217 [00:03<00:53,  3.78it/s]  6%|▋         | 14/217 [00:03<00:54,  3.75it/s]  7%|▋         | 15/217 [00:04<00:54,  3.71it/s]  7%|▋         | 16/217 [00:04<00:52,  3.84it/s]  8%|▊         | 17/217 [00:04<00:52,  3.79it/s]  8%|▊         | 18/217 [00:04<00:52,  3.76it/s]  9%|▉         | 19/217 [00:05<00:53,  3.72it/s]  9%|▉         | 20/217 [00:05<00:51,  3.84it/s] 10%|▉         | 21/217 [00:05<00:51,  3.79it/s] 10%|█         | 22/217 [00:05<00:51,  3.75it/s] 11%|█         | 23/217 [00:06<00:52,  3.72it/s] 11%|█         | 24/217 [00:06<00:50,  3.84it/s] 12%|█▏        | 25/217 [00:06<00:50,  3.78it/s] 12%|█▏        | 26/217 [00:06<00:50,  3.75it/s] 12%|█▏        | 27/217 [00:07<00:51,  3.72it/s] 13%|█▎        | 28/217 [00:07<00:49,  3.84it/s] 13%|█▎        | 29/217 [00:07<00:49,  3.79it/s] 14%|█▍        | 30/217 [00:08<00:49,  3.76it/s] 14%|█▍        | 31/217 [00:08<00:50,  3.72it/s] 15%|█▍        | 32/217 [00:08<00:48,  3.84it/s] 15%|█▌        | 33/217 [00:08<00:48,  3.78it/s] 16%|█▌        | 34/217 [00:09<00:48,  3.74it/s] 16%|█▌        | 35/217 [00:09<00:49,  3.71it/s] 17%|█▋        | 36/217 [00:09<00:47,  3.83it/s] 17%|█▋        | 37/217 [00:09<00:47,  3.78it/s] 18%|█▊        | 38/217 [00:10<00:47,  3.75it/s] 18%|█▊        | 39/217 [00:10<00:48,  3.71it/s] 18%|█▊        | 40/217 [00:10<00:46,  3.82it/s] 19%|█▉        | 41/217 [00:10<00:46,  3.78it/s] 19%|█▉        | 42/217 [00:11<00:46,  3.75it/s] 20%|█▉        | 43/217 [00:11<00:46,  3.70it/s] 20%|██        | 44/217 [00:11<00:45,  3.82it/s] 21%|██        | 45/217 [00:12<00:45,  3.76it/s] 21%|██        | 46/217 [00:12<00:45,  3.75it/s] 22%|██▏       | 47/217 [00:12<00:45,  3.70it/s] 22%|██▏       | 48/217 [00:12<00:44,  3.81it/s] 23%|██▎       | 49/217 [00:13<00:44,  3.76it/s] 23%|██▎       | 50/217 [00:13<00:44,  3.73it/s] 24%|██▎       | 51/217 [00:13<00:44,  3.70it/s] 24%|██▍       | 52/217 [00:13<00:43,  3.83it/s] 24%|██▍       | 53/217 [00:14<00:43,  3.77it/s] 25%|██▍       | 54/217 [00:14<00:43,  3.74it/s] 25%|██▌       | 55/217 [00:14<00:43,  3.70it/s] 26%|██▌       | 56/217 [00:14<00:42,  3.82it/s] 26%|██▋       | 57/217 [00:15<00:42,  3.78it/s] 27%|██▋       | 58/217 [00:15<00:42,  3.75it/s] 27%|██▋       | 59/217 [00:15<00:42,  3.71it/s] 28%|██▊       | 60/217 [00:15<00:41,  3.81it/s] 28%|██▊       | 61/217 [00:16<00:41,  3.76it/s] 29%|██▊       | 62/217 [00:16<00:41,  3.74it/s] 29%|██▉       | 63/217 [00:16<00:41,  3.70it/s] 29%|██▉       | 64/217 [00:17<00:40,  3.81it/s] 30%|██▉       | 65/217 [00:17<00:40,  3.76it/s] 30%|███       | 66/217 [00:17<00:40,  3.74it/s] 31%|███       | 67/217 [00:17<00:40,  3.70it/s] 31%|███▏      | 68/217 [00:18<00:39,  3.79it/s] 32%|███▏      | 69/217 [00:18<00:39,  3.74it/s] 32%|███▏      | 70/217 [00:18<00:39,  3.73it/s] 33%|███▎      | 71/217 [00:18<00:38,  3.75it/s] 33%|███▎      | 72/217 [00:19<00:38,  3.78it/s] 34%|███▎      | 73/217 [00:19<00:38,  3.74it/s] 34%|███▍      | 74/217 [00:19<00:38,  3.69it/s] 35%|███▍      | 75/217 [00:20<00:37,  3.76it/s] 35%|███▌      | 76/217 [00:20<00:37,  3.78it/s] 35%|███▌      | 77/217 [00:20<00:37,  3.74it/s] 36%|███▌      | 78/217 [00:20<00:37,  3.76it/s] 36%|███▋      | 79/217 [00:21<00:36,  3.74it/s] 37%|███▋      | 80/217 [00:21<00:36,  3.79it/s] 37%|███▋      | 81/217 [00:21<00:36,  3.74it/s] 38%|███▊      | 82/217 [00:21<00:36,  3.70it/s] 38%|███▊      | 83/217 [00:22<00:35,  3.76it/s] 39%|███▊      | 84/217 [00:22<00:35,  3.79it/s] 39%|███▉      | 85/217 [00:22<00:35,  3.74it/s] 40%|███▉      | 86/217 [00:22<00:35,  3.69it/s] 40%|████      | 87/217 [00:23<00:34,  3.76it/s] 41%|████      | 88/217 [00:23<00:34,  3.78it/s] 41%|████      | 89/217 [00:23<00:34,  3.73it/s] 41%|████▏     | 90/217 [00:24<00:34,  3.70it/s] 42%|████▏     | 91/217 [00:24<00:33,  3.76it/s] 42%|████▏     | 92/217 [00:24<00:32,  3.79it/s] 43%|████▎     | 93/217 [00:24<00:33,  3.74it/s] 43%|████▎     | 94/217 [00:25<00:32,  3.74it/s] 44%|████▍     | 95/217 [00:25<00:31,  3.85it/s] 44%|████▍     | 96/217 [00:25<00:30,  3.93it/s] 45%|████▍     | 97/217 [00:25<00:30,  3.99it/s] 45%|████▌     | 98/217 [00:26<00:29,  4.03it/s] 46%|████▌     | 99/217 [00:26<00:29,  4.06it/s] 46%|████▌     | 100/217 [00:26<00:28,  4.07it/s] 47%|████▋     | 101/217 [00:26<00:28,  4.08it/s] 47%|████▋     | 102/217 [00:27<00:28,  4.10it/s] 47%|████▋     | 103/217 [00:27<00:27,  4.11it/s] 48%|████▊     | 104/217 [00:27<00:27,  4.12it/s] 48%|████▊     | 105/217 [00:27<00:27,  4.10it/s] 49%|████▉     | 106/217 [00:28<00:27,  3.98it/s] 49%|████▉     | 107/217 [00:28<00:28,  3.90it/s] 50%|████▉     | 108/217 [00:28<00:28,  3.86it/s] 50%|█████     | 109/217 [00:28<00:27,  3.93it/s] 51%|█████     | 110/217 [00:29<00:27,  3.90it/s] 51%|█████     | 111/217 [00:29<00:27,  3.82it/s] 52%|█████▏    | 112/217 [00:29<00:27,  3.82it/s] 52%|█████▏    | 113/217 [00:29<00:27,  3.74it/s] 53%|█████▎    | 114/217 [00:30<00:26,  3.84it/s] 53%|█████▎    | 115/217 [00:30<00:27,  3.77it/s] 53%|█████▎    | 116/217 [00:30<00:26,  3.75it/s] 54%|█████▍    | 117/217 [00:30<00:26,  3.75it/s] 54%|█████▍    | 118/217 [00:31<00:26,  3.74it/s] 55%|█████▍    | 119/217 [00:31<00:25,  3.85it/s] 55%|█████▌    | 120/217 [00:31<00:25,  3.82it/s] 56%|█████▌    | 121/217 [00:31<00:25,  3.76it/s] 56%|█████▌    | 122/217 [00:32<00:25,  3.79it/s] 57%|█████▋    | 123/217 [00:32<00:25,  3.73it/s] 57%|█████▋    | 124/217 [00:32<00:24,  3.84it/s] 58%|█████▊    | 125/217 [00:33<00:24,  3.78it/s] 58%|█████▊    | 126/217 [00:33<00:24,  3.74it/s] 59%|█████▊    | 127/217 [00:33<00:24,  3.66it/s] 59%|█████▉    | 128/217 [00:33<00:23,  3.79it/s] 59%|█████▉    | 129/217 [00:34<00:22,  3.85it/s] 60%|█████▉    | 130/217 [00:34<00:22,  3.80it/s] 60%|██████    | 131/217 [00:34<00:22,  3.76it/s] 61%|██████    | 132/217 [00:34<00:23,  3.68it/s] 61%|██████▏   | 133/217 [00:35<00:22,  3.81it/s] 62%|██████▏   | 134/217 [00:35<00:21,  3.87it/s] 62%|██████▏   | 135/217 [00:35<00:21,  3.81it/s] 63%|██████▎   | 136/217 [00:35<00:21,  3.77it/s] 63%|██████▎   | 137/217 [00:36<00:21,  3.69it/s] 64%|██████▎   | 138/217 [00:36<00:20,  3.81it/s] 64%|██████▍   | 139/217 [00:36<00:20,  3.85it/s] 65%|██████▍   | 140/217 [00:37<00:20,  3.77it/s] 65%|██████▍   | 141/217 [00:37<00:20,  3.74it/s] 65%|██████▌   | 142/217 [00:37<00:20,  3.67it/s] 66%|██████▌   | 143/217 [00:37<00:19,  3.79it/s] 66%|██████▋   | 144/217 [00:38<00:19,  3.82it/s] 67%|██████▋   | 145/217 [00:38<00:19,  3.75it/s] 67%|██████▋   | 146/217 [00:38<00:19,  3.74it/s] 68%|██████▊   | 147/217 [00:38<00:19,  3.66it/s] 68%|██████▊   | 148/217 [00:39<00:18,  3.79it/s] 69%|██████▊   | 149/217 [00:39<00:17,  3.83it/s] 69%|██████▉   | 150/217 [00:39<00:17,  3.78it/s] 70%|██████▉   | 151/217 [00:39<00:17,  3.74it/s] 70%|███████   | 152/217 [00:40<00:17,  3.66it/s] 71%|███████   | 153/217 [00:40<00:16,  3.78it/s] 71%|███████   | 154/217 [00:40<00:16,  3.86it/s] 71%|███████▏  | 155/217 [00:40<00:16,  3.80it/s] 72%|███████▏  | 156/217 [00:41<00:16,  3.75it/s] 72%|███████▏  | 157/217 [00:41<00:16,  3.67it/s] 73%|███████▎  | 158/217 [00:41<00:15,  3.79it/s] 73%|███████▎  | 159/217 [00:42<00:15,  3.86it/s] 74%|███████▎  | 160/217 [00:42<00:15,  3.80it/s] 74%|███████▍  | 161/217 [00:42<00:14,  3.76it/s] 75%|███████▍  | 162/217 [00:42<00:14,  3.68it/s] 75%|███████▌  | 163/217 [00:43<00:14,  3.80it/s] 76%|███████▌  | 164/217 [00:43<00:13,  3.88it/s] 76%|███████▌  | 165/217 [00:43<00:13,  3.80it/s] 76%|███████▋  | 166/217 [00:43<00:13,  3.76it/s] 77%|███████▋  | 167/217 [00:44<00:13,  3.68it/s] 77%|███████▋  | 168/217 [00:44<00:12,  3.79it/s] 78%|███████▊  | 169/217 [00:44<00:12,  3.83it/s] 78%|███████▊  | 170/217 [00:44<00:12,  3.78it/s] 79%|███████▉  | 171/217 [00:45<00:12,  3.73it/s] 79%|███████▉  | 172/217 [00:45<00:12,  3.67it/s] 80%|███████▉  | 173/217 [00:45<00:11,  3.79it/s] 80%|████████  | 174/217 [00:46<00:11,  3.85it/s] 81%|████████  | 175/217 [00:46<00:11,  3.79it/s] 81%|████████  | 176/217 [00:46<00:10,  3.75it/s] 82%|████████▏ | 177/217 [00:46<00:10,  3.67it/s] 82%|████████▏ | 178/217 [00:47<00:10,  3.79it/s] 82%|████████▏ | 179/217 [00:47<00:09,  3.87it/s] 83%|████████▎ | 180/217 [00:47<00:09,  3.79it/s] 83%|████████▎ | 181/217 [00:47<00:09,  3.75it/s] 84%|████████▍ | 182/217 [00:48<00:09,  3.67it/s] 84%|████████▍ | 183/217 [00:48<00:09,  3.77it/s] 85%|████████▍ | 184/217 [00:48<00:08,  3.82it/s] 85%|████████▌ | 185/217 [00:48<00:08,  3.76it/s] 86%|████████▌ | 186/217 [00:49<00:08,  3.72it/s] 86%|████████▌ | 187/217 [00:49<00:08,  3.66it/s] 87%|████████▋ | 188/217 [00:49<00:07,  3.77it/s] 87%|████████▋ | 189/217 [00:50<00:07,  3.82it/s] 88%|████████▊ | 190/217 [00:50<00:07,  3.76it/s] 88%|████████▊ | 191/217 [00:50<00:06,  3.73it/s] 88%|████████▊ | 192/217 [00:50<00:06,  3.65it/s] 89%|████████▉ | 193/217 [00:51<00:06,  3.78it/s] 89%|████████▉ | 194/217 [00:51<00:06,  3.81it/s] 90%|████████▉ | 195/217 [00:51<00:05,  3.76it/s] 90%|█████████ | 196/217 [00:51<00:05,  3.73it/s] 91%|█████████ | 197/217 [00:52<00:05,  3.66it/s] 91%|█████████ | 198/217 [00:52<00:05,  3.77it/s] 92%|█████████▏| 199/217 [00:52<00:04,  3.82it/s] 92%|█████████▏| 200/217 [00:52<00:04,  3.77it/s] 93%|█████████▎| 201/217 [00:53<00:04,  3.72it/s] 93%|█████████▎| 202/217 [00:53<00:04,  3.65it/s] 94%|█████████▎| 203/217 [00:53<00:03,  3.77it/s] 94%|█████████▍| 204/217 [00:54<00:03,  3.82it/s] 94%|█████████▍| 205/217 [00:54<00:03,  3.76it/s] 95%|█████████▍| 206/217 [00:54<00:02,  3.74it/s] 95%|█████████▌| 207/217 [00:54<00:02,  3.66it/s] 96%|█████████▌| 208/217 [00:55<00:02,  3.78it/s] 96%|█████████▋| 209/217 [00:55<00:02,  3.81it/s] 97%|█████████▋| 210/217 [00:55<00:01,  3.75it/s] 97%|█████████▋| 211/217 [00:55<00:01,  3.72it/s] 98%|█████████▊| 212/217 [00:56<00:01,  3.65it/s] 98%|█████████▊| 213/217 [00:56<00:01,  3.78it/s] 99%|█████████▊| 214/217 [00:56<00:00,  3.82it/s] 99%|█████████▉| 215/217 [00:56<00:00,  3.77it/s]100%|█████████▉| 216/217 [00:57<00:00,  3.73it/s]100%|██████████| 217/217 [00:57<00:00,  3.66it/s]accuracy:  0.8525345622119815
100%|██████████| 217/217 [01:00<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 103
Training dataset size: 336, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.38s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:57:19,862] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 06:57:20,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 06:57:20,242] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:01,  2.21s/it]  4%|▎         | 2/56 [00:05<02:29,  2.76s/it]  5%|▌         | 3/56 [00:07<02:07,  2.40s/it]  7%|▋         | 4/56 [00:09<02:07,  2.45s/it]  9%|▉         | 5/56 [00:12<02:09,  2.53s/it] 11%|█         | 6/56 [00:15<02:09,  2.59s/it] 12%|█▎        | 7/56 [00:16<01:53,  2.32s/it] 14%|█▍        | 8/56 [00:18<01:42,  2.14s/it] 16%|█▌        | 9/56 [00:20<01:35,  2.03s/it] 18%|█▊        | 10/56 [00:22<01:34,  2.05s/it]                                               {'loss': 0.8582, 'grad_norm': 9.254538536071777, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:22<01:34,  2.05s/it] 20%|█▉        | 11/56 [00:24<01:34,  2.11s/it] 21%|██▏       | 12/56 [00:26<01:32,  2.10s/it] 23%|██▎       | 13/56 [00:28<01:29,  2.08s/it] 25%|██▌       | 14/56 [00:31<01:33,  2.22s/it] 27%|██▋       | 15/56 [00:33<01:31,  2.23s/it] 29%|██▊       | 16/56 [00:36<01:34,  2.37s/it] 30%|███       | 17/56 [00:38<01:33,  2.40s/it] 32%|███▏      | 18/56 [00:41<01:29,  2.37s/it] 34%|███▍      | 19/56 [00:44<01:32,  2.51s/it] 36%|███▌      | 20/56 [00:46<01:31,  2.55s/it]                                               {'loss': 0.5599, 'grad_norm': 11.264467239379883, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:46<01:31,  2.55s/it] 38%|███▊      | 21/56 [00:49<01:31,  2.63s/it] 39%|███▉      | 22/56 [00:51<01:20,  2.36s/it] 41%|████      | 23/56 [00:52<01:10,  2.14s/it] 43%|████▎     | 24/56 [00:54<01:06,  2.09s/it] 45%|████▍     | 25/56 [00:56<01:02,  2.02s/it] 46%|████▋     | 26/56 [00:58<01:01,  2.06s/it] 48%|████▊     | 27/56 [01:01<01:01,  2.10s/it] 50%|█████     | 28/56 [01:03<00:59,  2.11s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:05<00:58,  2.17s/it] 54%|█████▎    | 30/56 [01:07<00:52,  2.03s/it]                                               {'loss': 0.4081, 'grad_norm': 5.014700412750244, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:07<00:52,  2.03s/it] 55%|█████▌    | 31/56 [01:09<00:54,  2.19s/it] 57%|█████▋    | 32/56 [01:11<00:50,  2.10s/it] 59%|█████▉    | 33/56 [01:14<00:50,  2.18s/it] 61%|██████    | 34/56 [01:15<00:45,  2.05s/it] 62%|██████▎   | 35/56 [01:18<00:46,  2.22s/it] 64%|██████▍   | 36/56 [01:20<00:43,  2.20s/it] 66%|██████▌   | 37/56 [01:22<00:41,  2.20s/it] 68%|██████▊   | 38/56 [01:24<00:37,  2.09s/it] 70%|██████▉   | 39/56 [01:26<00:35,  2.07s/it] 71%|███████▏  | 40/56 [01:28<00:33,  2.08s/it]                                               {'loss': 0.429, 'grad_norm': 8.014572143554688, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:28<00:33,  2.08s/it] 73%|███████▎  | 41/56 [01:30<00:30,  2.04s/it] 75%|███████▌  | 42/56 [01:32<00:28,  2.01s/it] 77%|███████▋  | 43/56 [01:34<00:24,  1.92s/it] 79%|███████▊  | 44/56 [01:36<00:22,  1.91s/it] 80%|████████  | 45/56 [01:38<00:21,  1.99s/it] 82%|████████▏ | 46/56 [01:40<00:20,  2.01s/it] 84%|████████▍ | 47/56 [01:42<00:17,  1.98s/it] 86%|████████▌ | 48/56 [01:44<00:16,  2.03s/it] 88%|████████▊ | 49/56 [01:46<00:14,  2.09s/it] 89%|████████▉ | 50/56 [01:48<00:12,  2.14s/it]                                               {'loss': 0.5938, 'grad_norm': 7.428457736968994, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [01:48<00:12,  2.14s/it] 91%|█████████ | 51/56 [01:51<00:10,  2.19s/it] 93%|█████████▎| 52/56 [01:52<00:08,  2.04s/it] 95%|█████████▍| 53/56 [01:55<00:06,  2.04s/it] 96%|█████████▋| 54/56 [01:58<00:04,  2.35s/it] 98%|█████████▊| 55/56 [02:00<00:02,  2.42s/it]100%|██████████| 56/56 [02:03<00:00,  2.48s/it]                                               {'train_runtime': 123.9364, 'train_samples_per_second': 5.422, 'train_steps_per_second': 0.452, 'train_loss': 0.5702333109719413, 'epoch': 2.0}
100%|██████████| 56/56 [02:03<00:00,  2.48s/it]100%|██████████| 56/56 [02:03<00:00,  2.21s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:47,  3.13it/s]  1%|▏         | 2/150 [00:00<00:42,  3.45it/s]  2%|▏         | 3/150 [00:00<00:41,  3.58it/s]  3%|▎         | 4/150 [00:01<00:40,  3.60it/s]  3%|▎         | 5/150 [00:01<00:38,  3.78it/s]  4%|▍         | 6/150 [00:01<00:38,  3.77it/s]  5%|▍         | 7/150 [00:01<00:38,  3.75it/s]  5%|▌         | 8/150 [00:02<00:38,  3.71it/s]  6%|▌         | 9/150 [00:02<00:36,  3.84it/s]  7%|▋         | 10/150 [00:02<00:36,  3.83it/s]  7%|▋         | 11/150 [00:02<00:36,  3.81it/s]  8%|▊         | 12/150 [00:03<00:37,  3.72it/s]  9%|▊         | 13/150 [00:03<00:35,  3.84it/s]  9%|▉         | 14/150 [00:03<00:35,  3.86it/s] 10%|█         | 15/150 [00:03<00:35,  3.82it/s] 11%|█         | 16/150 [00:04<00:35,  3.76it/s] 11%|█▏        | 17/150 [00:04<00:34,  3.81it/s] 12%|█▏        | 18/150 [00:04<00:34,  3.86it/s] 13%|█▎        | 19/150 [00:05<00:34,  3.82it/s] 13%|█▎        | 20/150 [00:05<00:33,  3.85it/s] 14%|█▍        | 21/150 [00:05<00:34,  3.79it/s] 15%|█▍        | 22/150 [00:05<00:33,  3.87it/s] 15%|█▌        | 23/150 [00:06<00:33,  3.83it/s] 16%|█▌        | 24/150 [00:06<00:33,  3.77it/s] 17%|█▋        | 25/150 [00:06<00:32,  3.81it/s] 17%|█▋        | 26/150 [00:06<00:32,  3.86it/s] 18%|█▊        | 27/150 [00:07<00:32,  3.82it/s] 19%|█▊        | 28/150 [00:07<00:32,  3.76it/s] 19%|█▉        | 29/150 [00:07<00:31,  3.81it/s] 20%|██        | 30/150 [00:07<00:31,  3.84it/s] 21%|██        | 31/150 [00:08<00:31,  3.81it/s] 21%|██▏       | 32/150 [00:08<00:31,  3.75it/s] 22%|██▏       | 33/150 [00:08<00:30,  3.79it/s] 23%|██▎       | 34/150 [00:08<00:30,  3.85it/s] 23%|██▎       | 35/150 [00:09<00:30,  3.81it/s] 24%|██▍       | 36/150 [00:09<00:30,  3.75it/s] 25%|██▍       | 37/150 [00:09<00:29,  3.80it/s] 25%|██▌       | 38/150 [00:10<00:28,  3.88it/s] 26%|██▌       | 39/150 [00:10<00:28,  3.83it/s] 27%|██▋       | 40/150 [00:10<00:29,  3.76it/s] 27%|██▋       | 41/150 [00:10<00:28,  3.81it/s] 28%|██▊       | 42/150 [00:11<00:28,  3.83it/s] 29%|██▊       | 43/150 [00:11<00:28,  3.80it/s] 29%|██▉       | 44/150 [00:11<00:28,  3.74it/s] 30%|███       | 45/150 [00:11<00:27,  3.80it/s] 31%|███       | 46/150 [00:12<00:27,  3.85it/s] 31%|███▏      | 47/150 [00:12<00:27,  3.81it/s] 32%|███▏      | 48/150 [00:12<00:27,  3.75it/s] 33%|███▎      | 49/150 [00:12<00:26,  3.80it/s] 33%|███▎      | 50/150 [00:13<00:25,  3.88it/s] 34%|███▍      | 51/150 [00:13<00:25,  3.83it/s] 35%|███▍      | 52/150 [00:13<00:25,  3.81it/s] 35%|███▌      | 53/150 [00:13<00:25,  3.75it/s] 36%|███▌      | 54/150 [00:14<00:24,  3.85it/s] 37%|███▋      | 55/150 [00:14<00:24,  3.82it/s] 37%|███▋      | 56/150 [00:14<00:24,  3.79it/s] 38%|███▊      | 57/150 [00:15<00:24,  3.73it/s] 39%|███▊      | 58/150 [00:15<00:23,  3.84it/s] 39%|███▉      | 59/150 [00:15<00:23,  3.82it/s] 40%|████      | 60/150 [00:15<00:23,  3.79it/s] 41%|████      | 61/150 [00:16<00:24,  3.69it/s] 41%|████▏     | 62/150 [00:16<00:23,  3.81it/s] 42%|████▏     | 63/150 [00:16<00:22,  3.85it/s] 43%|████▎     | 64/150 [00:16<00:22,  3.81it/s] 43%|████▎     | 65/150 [00:17<00:22,  3.75it/s] 44%|████▍     | 66/150 [00:17<00:22,  3.80it/s] 45%|████▍     | 67/150 [00:17<00:21,  3.84it/s] 45%|████▌     | 68/150 [00:17<00:21,  3.80it/s] 46%|████▌     | 69/150 [00:18<00:21,  3.73it/s] 47%|████▋     | 70/150 [00:18<00:21,  3.79it/s] 47%|████▋     | 71/150 [00:18<00:20,  3.84it/s] 48%|████▊     | 72/150 [00:18<00:20,  3.79it/s] 49%|████▊     | 73/150 [00:19<00:20,  3.74it/s] 49%|████▉     | 74/150 [00:19<00:20,  3.79it/s] 50%|█████     | 75/150 [00:19<00:19,  3.85it/s] 51%|█████     | 76/150 [00:20<00:19,  3.80it/s] 51%|█████▏    | 77/150 [00:20<00:19,  3.74it/s] 52%|█████▏    | 78/150 [00:20<00:19,  3.79it/s] 53%|█████▎    | 79/150 [00:20<00:18,  3.83it/s] 53%|█████▎    | 80/150 [00:21<00:18,  3.79it/s] 54%|█████▍    | 81/150 [00:21<00:18,  3.76it/s] 55%|█████▍    | 82/150 [00:21<00:18,  3.77it/s] 55%|█████▌    | 83/150 [00:21<00:17,  3.82it/s] 56%|█████▌    | 84/150 [00:22<00:17,  3.78it/s] 57%|█████▋    | 85/150 [00:22<00:17,  3.73it/s] 57%|█████▋    | 86/150 [00:22<00:16,  3.78it/s] 58%|█████▊    | 87/150 [00:22<00:16,  3.84it/s] 59%|█████▊    | 88/150 [00:23<00:16,  3.80it/s] 59%|█████▉    | 89/150 [00:23<00:16,  3.74it/s] 60%|██████    | 90/150 [00:23<00:15,  3.78it/s] 61%|██████    | 91/150 [00:24<00:15,  3.82it/s] 61%|██████▏   | 92/150 [00:24<00:15,  3.79it/s] 62%|██████▏   | 93/150 [00:24<00:15,  3.73it/s] 63%|██████▎   | 94/150 [00:24<00:14,  3.77it/s] 63%|██████▎   | 95/150 [00:25<00:14,  3.83it/s] 64%|██████▍   | 96/150 [00:25<00:14,  3.79it/s] 65%|██████▍   | 97/150 [00:25<00:14,  3.72it/s] 65%|██████▌   | 98/150 [00:25<00:13,  3.76it/s] 66%|██████▌   | 99/150 [00:26<00:13,  3.81it/s] 67%|██████▋   | 100/150 [00:26<00:13,  3.77it/s] 67%|██████▋   | 101/150 [00:26<00:13,  3.72it/s] 68%|██████▊   | 102/150 [00:26<00:12,  3.77it/s] 69%|██████▊   | 103/150 [00:27<00:12,  3.82it/s] 69%|██████▉   | 104/150 [00:27<00:12,  3.79it/s] 70%|███████   | 105/150 [00:27<00:12,  3.73it/s] 71%|███████   | 106/150 [00:27<00:11,  3.77it/s] 71%|███████▏  | 107/150 [00:28<00:11,  3.80it/s] 72%|███████▏  | 108/150 [00:28<00:11,  3.76it/s] 73%|███████▎  | 109/150 [00:28<00:11,  3.70it/s] 73%|███████▎  | 110/150 [00:29<00:10,  3.77it/s] 74%|███████▍  | 111/150 [00:29<00:10,  3.82it/s] 75%|███████▍  | 112/150 [00:29<00:10,  3.77it/s] 75%|███████▌  | 113/150 [00:29<00:09,  3.72it/s] 76%|███████▌  | 114/150 [00:30<00:09,  3.77it/s] 77%|███████▋  | 115/150 [00:30<00:09,  3.82it/s] 77%|███████▋  | 116/150 [00:30<00:09,  3.77it/s] 78%|███████▊  | 117/150 [00:30<00:08,  3.72it/s] 79%|███████▊  | 118/150 [00:31<00:08,  3.77it/s] 79%|███████▉  | 119/150 [00:31<00:08,  3.80it/s] 80%|████████  | 120/150 [00:31<00:07,  3.77it/s] 81%|████████  | 121/150 [00:31<00:07,  3.71it/s] 81%|████████▏ | 122/150 [00:32<00:07,  3.76it/s] 82%|████████▏ | 123/150 [00:32<00:07,  3.79it/s] 83%|████████▎ | 124/150 [00:32<00:06,  3.77it/s] 83%|████████▎ | 125/150 [00:33<00:06,  3.71it/s] 84%|████████▍ | 126/150 [00:33<00:06,  3.76it/s] 85%|████████▍ | 127/150 [00:33<00:06,  3.82it/s] 85%|████████▌ | 128/150 [00:33<00:05,  3.78it/s] 86%|████████▌ | 129/150 [00:34<00:05,  3.73it/s] 87%|████████▋ | 130/150 [00:34<00:05,  3.76it/s] 87%|████████▋ | 131/150 [00:34<00:05,  3.80it/s] 88%|████████▊ | 132/150 [00:34<00:04,  3.76it/s] 89%|████████▊ | 133/150 [00:35<00:04,  3.72it/s] 89%|████████▉ | 134/150 [00:35<00:04,  3.77it/s] 90%|█████████ | 135/150 [00:35<00:03,  3.80it/s] 91%|█████████ | 136/150 [00:35<00:03,  3.77it/s] 91%|█████████▏| 137/150 [00:36<00:03,  3.72it/s] 92%|█████████▏| 138/150 [00:36<00:03,  3.77it/s] 93%|█████████▎| 139/150 [00:36<00:02,  3.82it/s] 93%|█████████▎| 140/150 [00:37<00:02,  3.77it/s] 94%|█████████▍| 141/150 [00:37<00:02,  3.71it/s] 95%|█████████▍| 142/150 [00:37<00:02,  3.77it/s] 95%|█████████▌| 143/150 [00:37<00:01,  3.83it/s] 96%|█████████▌| 144/150 [00:38<00:01,  3.78it/s] 97%|█████████▋| 145/150 [00:38<00:01,  3.65it/s] 97%|█████████▋| 146/150 [00:38<00:01,  3.79it/s] 98%|█████████▊| 147/150 [00:38<00:00,  3.82it/s] 99%|█████████▊| 148/150 [00:39<00:00,  3.78it/s] 99%|█████████▉| 149/150 [00:39<00:00,  3.73it/s]100%|██████████| 150/150 [00:39<00:00,  3.77it/s]accuracy:  0.62
100%|██████████| 150/150 [00:42<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:00:33,046] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]training start
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:00:33,587] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:00:34,788] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:39,  2.91s/it]  4%|▎         | 2/56 [00:05<02:16,  2.53s/it]  5%|▌         | 3/56 [00:07<02:10,  2.46s/it]  7%|▋         | 4/56 [00:10<02:13,  2.56s/it]  9%|▉         | 5/56 [00:12<02:06,  2.47s/it] 11%|█         | 6/56 [00:15<02:09,  2.59s/it] 12%|█▎        | 7/56 [00:18<02:08,  2.63s/it] 14%|█▍        | 8/56 [00:20<02:03,  2.57s/it] 16%|█▌        | 9/56 [00:22<01:57,  2.50s/it] 18%|█▊        | 10/56 [00:25<01:52,  2.44s/it]                                               {'loss': 1.4507, 'grad_norm': 10.140883445739746, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:25<01:52,  2.44s/it] 20%|█▉        | 11/56 [00:27<01:45,  2.34s/it] 21%|██▏       | 12/56 [00:29<01:42,  2.33s/it] 23%|██▎       | 13/56 [00:32<01:45,  2.45s/it] 25%|██▌       | 14/56 [00:34<01:39,  2.37s/it] 27%|██▋       | 15/56 [00:37<01:41,  2.47s/it] 29%|██▊       | 16/56 [00:39<01:37,  2.44s/it] 30%|███       | 17/56 [00:41<01:34,  2.42s/it] 32%|███▏      | 18/56 [00:44<01:33,  2.46s/it] 34%|███▍      | 19/56 [00:46<01:29,  2.41s/it] 36%|███▌      | 20/56 [00:48<01:22,  2.30s/it]                                               {'loss': 1.2321, 'grad_norm': 5.920921325683594, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:48<01:22,  2.30s/it] 38%|███▊      | 21/56 [00:51<01:20,  2.31s/it] 39%|███▉      | 22/56 [00:53<01:21,  2.41s/it] 41%|████      | 23/56 [00:56<01:17,  2.35s/it] 43%|████▎     | 24/56 [00:58<01:13,  2.30s/it] 45%|████▍     | 25/56 [01:00<01:10,  2.27s/it] 46%|████▋     | 26/56 [01:02<01:06,  2.21s/it] 48%|████▊     | 27/56 [01:05<01:06,  2.31s/it] 50%|█████     | 28/56 [01:07<01:06,  2.37s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:10<01:06,  2.47s/it] 54%|█████▎    | 30/56 [01:12<01:04,  2.47s/it]                                               {'loss': 0.9184, 'grad_norm': 11.062339782714844, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:12<01:04,  2.47s/it] 55%|█████▌    | 31/56 [01:15<01:03,  2.53s/it] 57%|█████▋    | 32/56 [01:17<00:58,  2.42s/it] 59%|█████▉    | 33/56 [01:20<00:56,  2.45s/it] 61%|██████    | 34/56 [01:22<00:55,  2.50s/it] 62%|██████▎   | 35/56 [01:25<00:51,  2.47s/it] 64%|██████▍   | 36/56 [01:27<00:50,  2.51s/it] 66%|██████▌   | 37/56 [01:30<00:48,  2.53s/it] 68%|██████▊   | 38/56 [01:32<00:42,  2.37s/it] 70%|██████▉   | 39/56 [01:34<00:40,  2.37s/it] 71%|███████▏  | 40/56 [01:36<00:37,  2.34s/it]                                               {'loss': 0.8796, 'grad_norm': 9.52933120727539, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:36<00:37,  2.34s/it] 73%|███████▎  | 41/56 [01:39<00:36,  2.45s/it] 75%|███████▌  | 42/56 [01:41<00:33,  2.38s/it] 77%|███████▋  | 43/56 [01:44<00:30,  2.36s/it] 79%|███████▊  | 44/56 [01:46<00:27,  2.32s/it] 80%|████████  | 45/56 [01:48<00:25,  2.33s/it] 82%|████████▏ | 46/56 [01:51<00:23,  2.36s/it] 84%|████████▍ | 47/56 [01:53<00:21,  2.35s/it] 86%|████████▌ | 48/56 [01:56<00:19,  2.43s/it] 88%|████████▊ | 49/56 [01:58<00:16,  2.41s/it] 89%|████████▉ | 50/56 [02:00<00:14,  2.40s/it]                                               {'loss': 0.8255, 'grad_norm': 4.40981912612915, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:00<00:14,  2.40s/it] 91%|█████████ | 51/56 [02:03<00:11,  2.33s/it] 93%|█████████▎| 52/56 [02:05<00:09,  2.46s/it] 95%|█████████▍| 53/56 [02:08<00:07,  2.51s/it] 96%|█████████▋| 54/56 [02:11<00:05,  2.53s/it] 98%|█████████▊| 55/56 [02:13<00:02,  2.50s/it]100%|██████████| 56/56 [02:16<00:00,  2.55s/it]                                               {'train_runtime': 136.745, 'train_samples_per_second': 4.914, 'train_steps_per_second': 0.41, 'train_loss': 1.0179455493177687, 'epoch': 2.0}
100%|██████████| 56/56 [02:16<00:00,  2.55s/it]100%|██████████| 56/56 [02:16<00:00,  2.44s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:02,  3.17it/s]  1%|          | 2/200 [00:00<00:56,  3.50it/s]  2%|▏         | 3/200 [00:00<00:54,  3.62it/s]  2%|▏         | 4/200 [00:01<00:52,  3.70it/s]  2%|▎         | 5/200 [00:01<00:50,  3.85it/s]  3%|▎         | 6/200 [00:01<00:51,  3.76it/s]  4%|▎         | 7/200 [00:01<00:51,  3.71it/s]  4%|▍         | 8/200 [00:02<00:50,  3.77it/s]  4%|▍         | 9/200 [00:02<00:51,  3.72it/s]  5%|▌         | 10/200 [00:02<00:49,  3.84it/s]  6%|▌         | 11/200 [00:02<00:50,  3.76it/s]  6%|▌         | 12/200 [00:03<00:50,  3.70it/s]  6%|▋         | 13/200 [00:03<00:51,  3.64it/s]  7%|▋         | 14/200 [00:03<00:48,  3.80it/s]  8%|▊         | 15/200 [00:04<00:48,  3.85it/s]  8%|▊         | 16/200 [00:04<00:48,  3.78it/s]  8%|▊         | 17/200 [00:04<00:49,  3.71it/s]  9%|▉         | 18/200 [00:04<00:49,  3.65it/s] 10%|▉         | 19/200 [00:05<00:47,  3.80it/s] 10%|█         | 20/200 [00:05<00:47,  3.83it/s] 10%|█         | 21/200 [00:05<00:47,  3.75it/s] 11%|█         | 22/200 [00:05<00:48,  3.69it/s] 12%|█▏        | 23/200 [00:06<00:48,  3.64it/s] 12%|█▏        | 24/200 [00:06<00:46,  3.79it/s] 12%|█▎        | 25/200 [00:06<00:45,  3.82it/s] 13%|█▎        | 26/200 [00:06<00:46,  3.76it/s] 14%|█▎        | 27/200 [00:07<00:46,  3.70it/s] 14%|█▍        | 28/200 [00:07<00:47,  3.64it/s] 14%|█▍        | 29/200 [00:07<00:45,  3.78it/s] 15%|█▌        | 30/200 [00:08<00:44,  3.81it/s] 16%|█▌        | 31/200 [00:08<00:45,  3.73it/s] 16%|█▌        | 32/200 [00:08<00:45,  3.69it/s] 16%|█▋        | 33/200 [00:08<00:45,  3.63it/s] 17%|█▋        | 34/200 [00:09<00:43,  3.78it/s] 18%|█▊        | 35/200 [00:09<00:43,  3.83it/s] 18%|█▊        | 36/200 [00:09<00:43,  3.76it/s] 18%|█▊        | 37/200 [00:09<00:44,  3.70it/s] 19%|█▉        | 38/200 [00:10<00:44,  3.64it/s] 20%|█▉        | 39/200 [00:10<00:42,  3.78it/s] 20%|██        | 40/200 [00:10<00:41,  3.84it/s] 20%|██        | 41/200 [00:10<00:42,  3.77it/s] 21%|██        | 42/200 [00:11<00:42,  3.71it/s] 22%|██▏       | 43/200 [00:11<00:43,  3.65it/s] 22%|██▏       | 44/200 [00:11<00:41,  3.79it/s] 22%|██▎       | 45/200 [00:12<00:40,  3.82it/s] 23%|██▎       | 46/200 [00:12<00:41,  3.76it/s] 24%|██▎       | 47/200 [00:12<00:41,  3.69it/s] 24%|██▍       | 48/200 [00:12<00:41,  3.64it/s] 24%|██▍       | 49/200 [00:13<00:40,  3.77it/s] 25%|██▌       | 50/200 [00:13<00:39,  3.80it/s] 26%|██▌       | 51/200 [00:13<00:40,  3.67it/s] 26%|██▌       | 52/200 [00:13<00:39,  3.72it/s] 26%|██▋       | 53/200 [00:14<00:39,  3.69it/s] 27%|██▋       | 54/200 [00:14<00:38,  3.82it/s] 28%|██▊       | 55/200 [00:14<00:38,  3.74it/s] 28%|██▊       | 56/200 [00:15<00:39,  3.69it/s] 28%|██▊       | 57/200 [00:15<00:38,  3.74it/s] 29%|██▉       | 58/200 [00:15<00:38,  3.69it/s] 30%|██▉       | 59/200 [00:15<00:37,  3.78it/s] 30%|███       | 60/200 [00:16<00:37,  3.73it/s] 30%|███       | 61/200 [00:16<00:37,  3.68it/s] 31%|███       | 62/200 [00:16<00:38,  3.62it/s] 32%|███▏      | 63/200 [00:16<00:36,  3.77it/s] 32%|███▏      | 64/200 [00:17<00:35,  3.80it/s] 32%|███▎      | 65/200 [00:17<00:36,  3.73it/s] 33%|███▎      | 66/200 [00:17<00:36,  3.67it/s] 34%|███▎      | 67/200 [00:17<00:36,  3.62it/s] 34%|███▍      | 68/200 [00:18<00:35,  3.76it/s] 34%|███▍      | 69/200 [00:18<00:34,  3.80it/s] 35%|███▌      | 70/200 [00:18<00:34,  3.73it/s] 36%|███▌      | 71/200 [00:19<00:35,  3.68it/s] 36%|███▌      | 72/200 [00:19<00:35,  3.63it/s] 36%|███▋      | 73/200 [00:19<00:33,  3.77it/s] 37%|███▋      | 74/200 [00:19<00:33,  3.81it/s] 38%|███▊      | 75/200 [00:20<00:33,  3.74it/s] 38%|███▊      | 76/200 [00:20<00:33,  3.68it/s] 38%|███▊      | 77/200 [00:20<00:33,  3.63it/s] 39%|███▉      | 78/200 [00:20<00:32,  3.78it/s] 40%|███▉      | 79/200 [00:21<00:31,  3.81it/s] 40%|████      | 80/200 [00:21<00:32,  3.68it/s] 40%|████      | 81/200 [00:21<00:32,  3.69it/s] 41%|████      | 82/200 [00:22<00:32,  3.67it/s] 42%|████▏     | 83/200 [00:22<00:30,  3.80it/s] 42%|████▏     | 84/200 [00:22<00:30,  3.80it/s] 42%|████▎     | 85/200 [00:22<00:31,  3.68it/s] 43%|████▎     | 86/200 [00:23<00:30,  3.69it/s] 44%|████▎     | 87/200 [00:23<00:30,  3.67it/s] 44%|████▍     | 88/200 [00:23<00:29,  3.79it/s] 44%|████▍     | 89/200 [00:23<00:29,  3.72it/s] 45%|████▌     | 90/200 [00:24<00:29,  3.68it/s] 46%|████▌     | 91/200 [00:24<00:29,  3.73it/s] 46%|████▌     | 92/200 [00:24<00:29,  3.69it/s] 46%|████▋     | 93/200 [00:24<00:28,  3.78it/s] 47%|████▋     | 94/200 [00:25<00:28,  3.72it/s] 48%|████▊     | 95/200 [00:25<00:28,  3.66it/s] 48%|████▊     | 96/200 [00:25<00:28,  3.61it/s] 48%|████▊     | 97/200 [00:26<00:27,  3.75it/s] 49%|████▉     | 98/200 [00:26<00:26,  3.79it/s] 50%|████▉     | 99/200 [00:26<00:27,  3.72it/s] 50%|█████     | 100/200 [00:26<00:27,  3.67it/s] 50%|█████     | 101/200 [00:27<00:27,  3.62it/s] 51%|█████     | 102/200 [00:27<00:26,  3.76it/s] 52%|█████▏    | 103/200 [00:27<00:25,  3.80it/s] 52%|█████▏    | 104/200 [00:27<00:25,  3.74it/s] 52%|█████▎    | 105/200 [00:28<00:25,  3.67it/s] 53%|█████▎    | 106/200 [00:28<00:25,  3.62it/s] 54%|█████▎    | 107/200 [00:28<00:24,  3.75it/s] 54%|█████▍    | 108/200 [00:28<00:24,  3.78it/s] 55%|█████▍    | 109/200 [00:29<00:24,  3.72it/s] 55%|█████▌    | 110/200 [00:29<00:24,  3.66it/s] 56%|█████▌    | 111/200 [00:29<00:24,  3.61it/s] 56%|█████▌    | 112/200 [00:30<00:23,  3.75it/s] 56%|█████▋    | 113/200 [00:30<00:22,  3.79it/s] 57%|█████▋    | 114/200 [00:30<00:23,  3.73it/s] 57%|█████▊    | 115/200 [00:30<00:23,  3.67it/s] 58%|█████▊    | 116/200 [00:31<00:23,  3.61it/s] 58%|█████▊    | 117/200 [00:31<00:22,  3.75it/s] 59%|█████▉    | 118/200 [00:31<00:21,  3.78it/s] 60%|█████▉    | 119/200 [00:31<00:22,  3.65it/s] 60%|██████    | 120/200 [00:32<00:21,  3.67it/s] 60%|██████    | 121/200 [00:32<00:21,  3.62it/s] 61%|██████    | 122/200 [00:32<00:20,  3.77it/s] 62%|██████▏   | 123/200 [00:33<00:20,  3.81it/s] 62%|██████▏   | 124/200 [00:33<00:20,  3.72it/s] 62%|██████▎   | 125/200 [00:33<00:20,  3.67it/s] 63%|██████▎   | 126/200 [00:33<00:20,  3.65it/s] 64%|██████▎   | 127/200 [00:34<00:19,  3.74it/s] 64%|██████▍   | 128/200 [00:34<00:19,  3.77it/s] 64%|██████▍   | 129/200 [00:34<00:19,  3.71it/s] 65%|██████▌   | 130/200 [00:34<00:19,  3.65it/s] 66%|██████▌   | 131/200 [00:35<00:19,  3.60it/s] 66%|██████▌   | 132/200 [00:35<00:18,  3.75it/s] 66%|██████▋   | 133/200 [00:35<00:17,  3.79it/s] 67%|██████▋   | 134/200 [00:36<00:18,  3.66it/s] 68%|██████▊   | 135/200 [00:36<00:17,  3.68it/s] 68%|██████▊   | 136/200 [00:36<00:17,  3.66it/s] 68%|██████▊   | 137/200 [00:36<00:16,  3.79it/s] 69%|██████▉   | 138/200 [00:37<00:16,  3.79it/s] 70%|██████▉   | 139/200 [00:37<00:16,  3.66it/s] 70%|███████   | 140/200 [00:37<00:16,  3.71it/s] 70%|███████   | 141/200 [00:37<00:16,  3.67it/s] 71%|███████   | 142/200 [00:38<00:15,  3.80it/s] 72%|███████▏  | 143/200 [00:38<00:15,  3.73it/s] 72%|███████▏  | 144/200 [00:38<00:15,  3.67it/s] 72%|███████▎  | 145/200 [00:38<00:14,  3.73it/s] 73%|███████▎  | 146/200 [00:39<00:14,  3.68it/s] 74%|███████▎  | 147/200 [00:39<00:14,  3.78it/s] 74%|███████▍  | 148/200 [00:39<00:14,  3.71it/s] 74%|███████▍  | 149/200 [00:40<00:13,  3.65it/s] 75%|███████▌  | 150/200 [00:40<00:13,  3.60it/s] 76%|███████▌  | 151/200 [00:40<00:13,  3.74it/s] 76%|███████▌  | 152/200 [00:40<00:12,  3.78it/s] 76%|███████▋  | 153/200 [00:41<00:12,  3.72it/s] 77%|███████▋  | 154/200 [00:41<00:12,  3.66it/s] 78%|███████▊  | 155/200 [00:41<00:12,  3.60it/s] 78%|███████▊  | 156/200 [00:41<00:11,  3.74it/s] 78%|███████▊  | 157/200 [00:42<00:11,  3.78it/s] 79%|███████▉  | 158/200 [00:42<00:11,  3.72it/s] 80%|███████▉  | 159/200 [00:42<00:11,  3.66it/s] 80%|████████  | 160/200 [00:43<00:11,  3.61it/s] 80%|████████  | 161/200 [00:43<00:10,  3.74it/s] 81%|████████  | 162/200 [00:43<00:10,  3.77it/s] 82%|████████▏ | 163/200 [00:43<00:09,  3.70it/s] 82%|████████▏ | 164/200 [00:44<00:09,  3.64it/s] 82%|████████▎ | 165/200 [00:44<00:09,  3.59it/s] 83%|████████▎ | 166/200 [00:44<00:09,  3.74it/s] 84%|████████▎ | 167/200 [00:44<00:08,  3.77it/s] 84%|████████▍ | 168/200 [00:45<00:08,  3.69it/s] 84%|████████▍ | 169/200 [00:45<00:08,  3.64it/s] 85%|████████▌ | 170/200 [00:45<00:08,  3.59it/s] 86%|████████▌ | 171/200 [00:46<00:07,  3.74it/s] 86%|████████▌ | 172/200 [00:46<00:07,  3.78it/s] 86%|████████▋ | 173/200 [00:46<00:07,  3.72it/s] 87%|████████▋ | 174/200 [00:46<00:07,  3.65it/s] 88%|████████▊ | 175/200 [00:47<00:06,  3.60it/s] 88%|████████▊ | 176/200 [00:47<00:06,  3.73it/s] 88%|████████▊ | 177/200 [00:47<00:06,  3.77it/s] 89%|████████▉ | 178/200 [00:47<00:05,  3.69it/s] 90%|████████▉ | 179/200 [00:48<00:05,  3.65it/s] 90%|█████████ | 180/200 [00:48<00:05,  3.59it/s] 90%|█████████ | 181/200 [00:48<00:05,  3.73it/s] 91%|█████████ | 182/200 [00:48<00:04,  3.76it/s] 92%|█████████▏| 183/200 [00:49<00:04,  3.69it/s] 92%|█████████▏| 184/200 [00:49<00:04,  3.64it/s] 92%|█████████▎| 185/200 [00:49<00:04,  3.59it/s] 93%|█████████▎| 186/200 [00:50<00:03,  3.72it/s] 94%|█████████▎| 187/200 [00:50<00:03,  3.76it/s] 94%|█████████▍| 188/200 [00:50<00:03,  3.69it/s] 94%|█████████▍| 189/200 [00:50<00:03,  3.64it/s] 95%|█████████▌| 190/200 [00:51<00:02,  3.59it/s] 96%|█████████▌| 191/200 [00:51<00:02,  3.72it/s] 96%|█████████▌| 192/200 [00:51<00:02,  3.76it/s] 96%|█████████▋| 193/200 [00:51<00:01,  3.69it/s] 97%|█████████▋| 194/200 [00:52<00:01,  3.64it/s] 98%|█████████▊| 195/200 [00:52<00:01,  3.59it/s] 98%|█████████▊| 196/200 [00:52<00:01,  3.72it/s] 98%|█████████▊| 197/200 [00:53<00:00,  3.76it/s] 99%|█████████▉| 198/200 [00:53<00:00,  3.68it/s]100%|█████████▉| 199/200 [00:53<00:00,  3.64it/s]100%|██████████| 200/200 [00:53<00:00,  3.58it/s]accuracy:  0.605
100%|██████████| 200/200 [00:57<00:00,  3.51it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 191
Training dataset size: 336, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
[2025-03-12 07:04:15,171] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[2025-03-12 07:04:15,334] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 07:04:15,644] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:17,  2.51s/it]  4%|▎         | 2/56 [00:05<02:25,  2.69s/it]  5%|▌         | 3/56 [00:07<02:17,  2.59s/it]  7%|▋         | 4/56 [00:10<02:07,  2.44s/it]  9%|▉         | 5/56 [00:12<02:00,  2.36s/it] 11%|█         | 6/56 [00:14<01:58,  2.37s/it] 12%|█▎        | 7/56 [00:17<01:58,  2.42s/it] 14%|█▍        | 8/56 [00:20<02:06,  2.63s/it] 16%|█▌        | 9/56 [00:23<02:08,  2.73s/it] 18%|█▊        | 10/56 [00:26<02:13,  2.91s/it]                                               {'loss': 1.5578, 'grad_norm': 13.970656394958496, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:26<02:13,  2.91s/it] 20%|█▉        | 11/56 [00:29<02:07,  2.84s/it] 21%|██▏       | 12/56 [00:31<01:58,  2.70s/it] 23%|██▎       | 13/56 [00:33<01:48,  2.53s/it] 25%|██▌       | 14/56 [00:36<01:47,  2.55s/it] 27%|██▋       | 15/56 [00:38<01:44,  2.56s/it] 29%|██▊       | 16/56 [00:41<01:44,  2.62s/it] 30%|███       | 17/56 [00:44<01:39,  2.56s/it] 32%|███▏      | 18/56 [00:46<01:37,  2.56s/it] 34%|███▍      | 19/56 [00:49<01:36,  2.60s/it] 36%|███▌      | 20/56 [00:52<01:36,  2.67s/it]                                               {'loss': 1.0431, 'grad_norm': 11.05863094329834, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:52<01:36,  2.67s/it] 38%|███▊      | 21/56 [00:54<01:32,  2.64s/it] 39%|███▉      | 22/56 [00:57<01:31,  2.68s/it] 41%|████      | 23/56 [01:00<01:30,  2.74s/it] 43%|████▎     | 24/56 [01:03<01:27,  2.75s/it] 45%|████▍     | 25/56 [01:05<01:24,  2.73s/it] 46%|████▋     | 26/56 [01:08<01:19,  2.66s/it] 48%|████▊     | 27/56 [01:11<01:19,  2.75s/it] 50%|█████     | 28/56 [01:13<01:16,  2.73s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:17<01:18,  2.90s/it] 54%|█████▎    | 30/56 [01:19<01:13,  2.82s/it]                                               {'loss': 0.8547, 'grad_norm': 4.400151252746582, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:19<01:13,  2.82s/it] 55%|█████▌    | 31/56 [01:22<01:09,  2.77s/it] 57%|█████▋    | 32/56 [01:25<01:07,  2.82s/it] 59%|█████▉    | 33/56 [01:28<01:04,  2.81s/it] 61%|██████    | 34/56 [01:30<00:57,  2.59s/it] 62%|██████▎   | 35/56 [01:32<00:54,  2.60s/it] 64%|██████▍   | 36/56 [01:35<00:50,  2.51s/it] 66%|██████▌   | 37/56 [01:37<00:48,  2.53s/it] 68%|██████▊   | 38/56 [01:40<00:45,  2.54s/it] 70%|██████▉   | 39/56 [01:43<00:43,  2.59s/it] 71%|███████▏  | 40/56 [01:46<00:43,  2.72s/it]                                               {'loss': 0.8529, 'grad_norm': 10.599655151367188, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:46<00:43,  2.72s/it] 73%|███████▎  | 41/56 [01:48<00:38,  2.60s/it] 75%|███████▌  | 42/56 [01:50<00:34,  2.44s/it] 77%|███████▋  | 43/56 [01:52<00:31,  2.44s/it] 79%|███████▊  | 44/56 [01:55<00:29,  2.46s/it] 80%|████████  | 45/56 [01:57<00:25,  2.35s/it] 82%|████████▏ | 46/56 [01:59<00:23,  2.38s/it] 84%|████████▍ | 47/56 [02:02<00:22,  2.46s/it] 86%|████████▌ | 48/56 [02:05<00:20,  2.58s/it] 88%|████████▊ | 49/56 [02:08<00:18,  2.66s/it] 89%|████████▉ | 50/56 [02:10<00:15,  2.62s/it]                                               {'loss': 0.7989, 'grad_norm': 7.151137351989746, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:10<00:15,  2.62s/it] 91%|█████████ | 51/56 [02:13<00:13,  2.61s/it] 93%|█████████▎| 52/56 [02:15<00:10,  2.53s/it] 95%|█████████▍| 53/56 [02:18<00:07,  2.52s/it] 96%|█████████▋| 54/56 [02:20<00:04,  2.49s/it] 98%|█████████▊| 55/56 [02:23<00:02,  2.53s/it]100%|██████████| 56/56 [02:25<00:00,  2.46s/it]                                               {'train_runtime': 146.2673, 'train_samples_per_second': 4.594, 'train_steps_per_second': 0.383, 'train_loss': 0.9970107844897679, 'epoch': 2.0}
100%|██████████| 56/56 [02:26<00:00,  2.46s/it]100%|██████████| 56/56 [02:26<00:00,  2.61s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<01:36,  2.88it/s]  1%|          | 2/279 [00:00<01:20,  3.42it/s]  1%|          | 3/279 [00:00<01:18,  3.50it/s]  1%|▏         | 4/279 [00:01<01:15,  3.64it/s]  2%|▏         | 5/279 [00:01<01:15,  3.62it/s]  2%|▏         | 6/279 [00:01<01:13,  3.72it/s]  3%|▎         | 7/279 [00:01<01:13,  3.69it/s]  3%|▎         | 8/279 [00:02<01:12,  3.74it/s]  3%|▎         | 9/279 [00:02<01:12,  3.70it/s]  4%|▎         | 10/279 [00:02<01:11,  3.79it/s]  4%|▍         | 11/279 [00:03<01:11,  3.74it/s]  4%|▍         | 12/279 [00:03<01:10,  3.77it/s]  5%|▍         | 13/279 [00:03<01:11,  3.73it/s]  5%|▌         | 14/279 [00:03<01:09,  3.82it/s]  5%|▌         | 15/279 [00:04<01:09,  3.78it/s]  6%|▌         | 16/279 [00:04<01:10,  3.75it/s]  6%|▌         | 17/279 [00:04<01:11,  3.68it/s]  6%|▋         | 18/279 [00:04<01:08,  3.80it/s]  7%|▋         | 19/279 [00:05<01:09,  3.76it/s]  7%|▋         | 20/279 [00:05<01:09,  3.74it/s]  8%|▊         | 21/279 [00:05<01:08,  3.76it/s]  8%|▊         | 22/279 [00:05<01:08,  3.74it/s]  8%|▊         | 23/279 [00:06<01:07,  3.80it/s]  9%|▊         | 24/279 [00:06<01:08,  3.75it/s]  9%|▉         | 25/279 [00:06<01:07,  3.79it/s]  9%|▉         | 26/279 [00:06<01:07,  3.73it/s] 10%|▉         | 27/279 [00:07<01:06,  3.80it/s] 10%|█         | 28/279 [00:07<01:07,  3.74it/s] 10%|█         | 29/279 [00:07<01:06,  3.78it/s] 11%|█         | 30/279 [00:08<01:06,  3.73it/s] 11%|█         | 31/279 [00:08<01:04,  3.82it/s] 11%|█▏        | 32/279 [00:08<01:05,  3.75it/s] 12%|█▏        | 33/279 [00:08<01:04,  3.79it/s] 12%|█▏        | 34/279 [00:09<01:05,  3.73it/s] 13%|█▎        | 35/279 [00:09<01:04,  3.81it/s] 13%|█▎        | 36/279 [00:09<01:04,  3.76it/s] 13%|█▎        | 37/279 [00:09<01:04,  3.75it/s] 14%|█▎        | 38/279 [00:10<01:05,  3.68it/s] 14%|█▍        | 39/279 [00:10<01:03,  3.80it/s] 14%|█▍        | 40/279 [00:10<01:03,  3.76it/s] 15%|█▍        | 41/279 [00:10<01:03,  3.74it/s] 15%|█▌        | 42/279 [00:11<01:02,  3.76it/s] 15%|█▌        | 43/279 [00:11<01:03,  3.74it/s] 16%|█▌        | 44/279 [00:11<01:02,  3.78it/s] 16%|█▌        | 45/279 [00:12<01:02,  3.72it/s] 16%|█▋        | 46/279 [00:12<01:02,  3.76it/s] 17%|█▋        | 47/279 [00:12<01:02,  3.70it/s] 17%|█▋        | 48/279 [00:12<01:01,  3.76it/s] 18%|█▊        | 49/279 [00:13<01:02,  3.71it/s] 18%|█▊        | 50/279 [00:13<01:00,  3.76it/s] 18%|█▊        | 51/279 [00:13<01:01,  3.71it/s] 19%|█▊        | 52/279 [00:13<00:59,  3.79it/s] 19%|█▉        | 53/279 [00:14<01:00,  3.73it/s] 19%|█▉        | 54/279 [00:14<01:00,  3.74it/s] 20%|█▉        | 55/279 [00:14<01:00,  3.70it/s] 20%|██        | 56/279 [00:14<00:58,  3.81it/s] 20%|██        | 57/279 [00:15<00:59,  3.75it/s] 21%|██        | 58/279 [00:15<00:59,  3.72it/s] 21%|██        | 59/279 [00:15<01:00,  3.66it/s] 22%|██▏       | 60/279 [00:16<00:57,  3.78it/s] 22%|██▏       | 61/279 [00:16<00:57,  3.79it/s] 22%|██▏       | 62/279 [00:16<00:58,  3.72it/s] 23%|██▎       | 63/279 [00:16<00:57,  3.77it/s] 23%|██▎       | 64/279 [00:17<00:57,  3.71it/s] 23%|██▎       | 65/279 [00:17<00:56,  3.77it/s] 24%|██▎       | 66/279 [00:17<00:57,  3.71it/s] 24%|██▍       | 67/279 [00:17<00:56,  3.74it/s] 24%|██▍       | 68/279 [00:18<00:57,  3.69it/s] 25%|██▍       | 69/279 [00:18<00:55,  3.78it/s] 25%|██▌       | 70/279 [00:18<00:55,  3.74it/s] 25%|██▌       | 71/279 [00:19<00:56,  3.71it/s] 26%|██▌       | 72/279 [00:19<00:56,  3.68it/s] 26%|██▌       | 73/279 [00:19<00:54,  3.79it/s] 27%|██▋       | 74/279 [00:19<00:54,  3.74it/s] 27%|██▋       | 75/279 [00:20<00:55,  3.71it/s] 27%|██▋       | 76/279 [00:20<00:55,  3.66it/s] 28%|██▊       | 77/279 [00:20<00:53,  3.79it/s] 28%|██▊       | 78/279 [00:20<00:53,  3.74it/s] 28%|██▊       | 79/279 [00:21<00:53,  3.71it/s] 29%|██▊       | 80/279 [00:21<00:54,  3.64it/s] 29%|██▉       | 81/279 [00:21<00:52,  3.78it/s] 29%|██▉       | 82/279 [00:21<00:52,  3.78it/s] 30%|██▉       | 83/279 [00:22<00:52,  3.71it/s] 30%|███       | 84/279 [00:22<00:52,  3.75it/s] 30%|███       | 85/279 [00:22<00:52,  3.72it/s] 31%|███       | 86/279 [00:23<00:51,  3.77it/s] 31%|███       | 87/279 [00:23<00:51,  3.73it/s] 32%|███▏      | 88/279 [00:23<00:50,  3.76it/s] 32%|███▏      | 89/279 [00:23<00:51,  3.70it/s] 32%|███▏      | 90/279 [00:24<00:50,  3.75it/s] 33%|███▎      | 91/279 [00:24<00:50,  3.69it/s] 33%|███▎      | 92/279 [00:24<00:50,  3.74it/s] 33%|███▎      | 93/279 [00:24<00:50,  3.69it/s] 34%|███▎      | 94/279 [00:25<00:49,  3.76it/s] 34%|███▍      | 95/279 [00:25<00:49,  3.70it/s] 34%|███▍      | 96/279 [00:25<00:48,  3.74it/s] 35%|███▍      | 97/279 [00:25<00:49,  3.69it/s] 35%|███▌      | 98/279 [00:26<00:48,  3.74it/s] 35%|███▌      | 99/279 [00:26<00:48,  3.69it/s] 36%|███▌      | 100/279 [00:26<00:47,  3.73it/s] 36%|███▌      | 101/279 [00:27<00:48,  3.67it/s] 37%|███▋      | 102/279 [00:27<00:47,  3.75it/s] 37%|███▋      | 103/279 [00:27<00:47,  3.69it/s] 37%|███▋      | 104/279 [00:27<00:46,  3.74it/s] 38%|███▊      | 105/279 [00:28<00:47,  3.68it/s] 38%|███▊      | 106/279 [00:28<00:46,  3.74it/s] 38%|███▊      | 107/279 [00:28<00:46,  3.69it/s] 39%|███▊      | 108/279 [00:28<00:45,  3.73it/s] 39%|███▉      | 109/279 [00:29<00:46,  3.67it/s] 39%|███▉      | 110/279 [00:29<00:45,  3.73it/s] 40%|███▉      | 111/279 [00:29<00:45,  3.68it/s] 40%|████      | 112/279 [00:30<00:44,  3.73it/s] 41%|████      | 113/279 [00:30<00:45,  3.67it/s] 41%|████      | 114/279 [00:30<00:44,  3.74it/s] 41%|████      | 115/279 [00:30<00:44,  3.69it/s] 42%|████▏     | 116/279 [00:31<00:43,  3.74it/s] 42%|████▏     | 117/279 [00:31<00:44,  3.68it/s] 42%|████▏     | 118/279 [00:31<00:43,  3.74it/s] 43%|████▎     | 119/279 [00:31<00:43,  3.69it/s] 43%|████▎     | 120/279 [00:32<00:42,  3.74it/s] 43%|████▎     | 121/279 [00:32<00:42,  3.69it/s] 44%|████▎     | 122/279 [00:32<00:41,  3.77it/s] 44%|████▍     | 123/279 [00:32<00:42,  3.70it/s] 44%|████▍     | 124/279 [00:33<00:41,  3.74it/s] 45%|████▍     | 125/279 [00:33<00:41,  3.68it/s] 45%|████▌     | 126/279 [00:33<00:40,  3.76it/s] 46%|████▌     | 127/279 [00:34<00:41,  3.70it/s] 46%|████▌     | 128/279 [00:34<00:40,  3.74it/s] 46%|████▌     | 129/279 [00:34<00:40,  3.68it/s] 47%|████▋     | 130/279 [00:34<00:39,  3.75it/s] 47%|████▋     | 131/279 [00:35<00:40,  3.68it/s] 47%|████▋     | 132/279 [00:35<00:39,  3.72it/s] 48%|████▊     | 133/279 [00:35<00:39,  3.67it/s] 48%|████▊     | 134/279 [00:35<00:38,  3.75it/s] 48%|████▊     | 135/279 [00:36<00:39,  3.68it/s] 49%|████▊     | 136/279 [00:36<00:38,  3.73it/s] 49%|████▉     | 137/279 [00:36<00:38,  3.67it/s] 49%|████▉     | 138/279 [00:37<00:37,  3.74it/s] 50%|████▉     | 139/279 [00:37<00:38,  3.67it/s] 50%|█████     | 140/279 [00:37<00:37,  3.72it/s] 51%|█████     | 141/279 [00:37<00:37,  3.67it/s] 51%|█████     | 142/279 [00:38<00:36,  3.73it/s] 51%|█████▏    | 143/279 [00:38<00:37,  3.67it/s] 52%|█████▏    | 144/279 [00:38<00:36,  3.72it/s] 52%|█████▏    | 145/279 [00:38<00:36,  3.67it/s] 52%|█████▏    | 146/279 [00:39<00:35,  3.74it/s] 53%|█████▎    | 147/279 [00:39<00:35,  3.68it/s] 53%|█████▎    | 148/279 [00:39<00:35,  3.72it/s] 53%|█████▎    | 149/279 [00:40<00:35,  3.67it/s] 54%|█████▍    | 150/279 [00:40<00:34,  3.75it/s] 54%|█████▍    | 151/279 [00:40<00:34,  3.69it/s] 54%|█████▍    | 152/279 [00:40<00:34,  3.72it/s] 55%|█████▍    | 153/279 [00:41<00:34,  3.67it/s] 55%|█████▌    | 154/279 [00:41<00:33,  3.74it/s] 56%|█████▌    | 155/279 [00:41<00:33,  3.68it/s] 56%|█████▌    | 156/279 [00:41<00:33,  3.73it/s] 56%|█████▋    | 157/279 [00:42<00:33,  3.67it/s] 57%|█████▋    | 158/279 [00:42<00:32,  3.74it/s] 57%|█████▋    | 159/279 [00:42<00:32,  3.68it/s] 57%|█████▋    | 160/279 [00:42<00:31,  3.72it/s] 58%|█████▊    | 161/279 [00:43<00:32,  3.67it/s] 58%|█████▊    | 162/279 [00:43<00:31,  3.75it/s] 58%|█████▊    | 163/279 [00:43<00:31,  3.68it/s] 59%|█████▉    | 164/279 [00:44<00:30,  3.73it/s] 59%|█████▉    | 165/279 [00:44<00:31,  3.67it/s] 59%|█████▉    | 166/279 [00:44<00:30,  3.73it/s] 60%|█████▉    | 167/279 [00:44<00:30,  3.67it/s] 60%|██████    | 168/279 [00:45<00:29,  3.72it/s] 61%|██████    | 169/279 [00:45<00:29,  3.67it/s] 61%|██████    | 170/279 [00:45<00:29,  3.74it/s] 61%|██████▏   | 171/279 [00:45<00:29,  3.68it/s] 62%|██████▏   | 172/279 [00:46<00:28,  3.72it/s] 62%|██████▏   | 173/279 [00:46<00:28,  3.67it/s] 62%|██████▏   | 174/279 [00:46<00:28,  3.73it/s] 63%|██████▎   | 175/279 [00:47<00:28,  3.68it/s] 63%|██████▎   | 176/279 [00:47<00:27,  3.73it/s] 63%|██████▎   | 177/279 [00:47<00:27,  3.67it/s] 64%|██████▍   | 178/279 [00:47<00:26,  3.74it/s] 64%|██████▍   | 179/279 [00:48<00:27,  3.69it/s] 65%|██████▍   | 180/279 [00:48<00:26,  3.74it/s] 65%|██████▍   | 181/279 [00:48<00:26,  3.68it/s] 65%|██████▌   | 182/279 [00:48<00:25,  3.75it/s] 66%|██████▌   | 183/279 [00:49<00:26,  3.69it/s] 66%|██████▌   | 184/279 [00:49<00:25,  3.73it/s] 66%|██████▋   | 185/279 [00:49<00:25,  3.67it/s] 67%|██████▋   | 186/279 [00:49<00:24,  3.73it/s] 67%|██████▋   | 187/279 [00:50<00:25,  3.67it/s] 67%|██████▋   | 188/279 [00:50<00:24,  3.72it/s] 68%|██████▊   | 189/279 [00:50<00:24,  3.67it/s] 68%|██████▊   | 190/279 [00:51<00:23,  3.73it/s] 68%|██████▊   | 191/279 [00:51<00:23,  3.68it/s] 69%|██████▉   | 192/279 [00:51<00:23,  3.72it/s] 69%|██████▉   | 193/279 [00:51<00:23,  3.67it/s] 70%|██████▉   | 194/279 [00:52<00:22,  3.73it/s] 70%|██████▉   | 195/279 [00:52<00:22,  3.67it/s] 70%|███████   | 196/279 [00:52<00:22,  3.72it/s] 71%|███████   | 197/279 [00:52<00:22,  3.66it/s] 71%|███████   | 198/279 [00:53<00:21,  3.71it/s] 71%|███████▏  | 199/279 [00:53<00:21,  3.65it/s] 72%|███████▏  | 200/279 [00:53<00:21,  3.69it/s] 72%|███████▏  | 201/279 [00:54<00:21,  3.64it/s] 72%|███████▏  | 202/279 [00:54<00:20,  3.71it/s] 73%|███████▎  | 203/279 [00:54<00:20,  3.65it/s] 73%|███████▎  | 204/279 [00:54<00:20,  3.71it/s] 73%|███████▎  | 205/279 [00:55<00:20,  3.66it/s] 74%|███████▍  | 206/279 [00:55<00:19,  3.71it/s] 74%|███████▍  | 207/279 [00:55<00:19,  3.65it/s] 75%|███████▍  | 208/279 [00:55<00:19,  3.69it/s] 75%|███████▍  | 209/279 [00:56<00:19,  3.64it/s] 75%|███████▌  | 210/279 [00:56<00:18,  3.71it/s] 76%|███████▌  | 211/279 [00:56<00:18,  3.66it/s] 76%|███████▌  | 212/279 [00:57<00:18,  3.70it/s] 76%|███████▋  | 213/279 [00:57<00:18,  3.65it/s] 77%|███████▋  | 214/279 [00:57<00:17,  3.72it/s] 77%|███████▋  | 215/279 [00:57<00:17,  3.67it/s] 77%|███████▋  | 216/279 [00:58<00:17,  3.67it/s] 78%|███████▊  | 217/279 [00:58<00:16,  3.74it/s] 78%|███████▊  | 218/279 [00:58<00:15,  3.83it/s] 78%|███████▊  | 219/279 [00:58<00:15,  3.90it/s] 79%|███████▉  | 220/279 [00:59<00:14,  3.96it/s] 79%|███████▉  | 221/279 [00:59<00:14,  3.99it/s] 80%|███████▉  | 222/279 [00:59<00:14,  4.02it/s] 80%|███████▉  | 223/279 [00:59<00:13,  4.04it/s] 80%|████████  | 224/279 [01:00<00:13,  4.05it/s] 81%|████████  | 225/279 [01:00<00:13,  4.05it/s] 81%|████████  | 226/279 [01:00<00:13,  4.05it/s] 81%|████████▏ | 227/279 [01:00<00:12,  4.07it/s] 82%|████████▏ | 228/279 [01:01<00:12,  4.07it/s] 82%|████████▏ | 229/279 [01:01<00:12,  4.07it/s] 82%|████████▏ | 230/279 [01:01<00:12,  4.07it/s] 83%|████████▎ | 231/279 [01:01<00:11,  4.08it/s] 83%|████████▎ | 232/279 [01:02<00:11,  4.09it/s] 84%|████████▎ | 233/279 [01:02<00:11,  4.08it/s] 84%|████████▍ | 234/279 [01:02<00:11,  4.08it/s] 84%|████████▍ | 235/279 [01:02<00:10,  4.09it/s] 85%|████████▍ | 236/279 [01:03<00:10,  4.08it/s] 85%|████████▍ | 237/279 [01:03<00:10,  4.08it/s] 85%|████████▌ | 238/279 [01:03<00:10,  4.07it/s] 86%|████████▌ | 239/279 [01:03<00:09,  4.08it/s] 86%|████████▌ | 240/279 [01:04<00:09,  4.07it/s] 86%|████████▋ | 241/279 [01:04<00:09,  4.07it/s] 87%|████████▋ | 242/279 [01:04<00:09,  4.07it/s] 87%|████████▋ | 243/279 [01:04<00:08,  4.08it/s] 87%|████████▋ | 244/279 [01:05<00:08,  4.07it/s] 88%|████████▊ | 245/279 [01:05<00:08,  4.07it/s] 88%|████████▊ | 246/279 [01:05<00:08,  4.08it/s] 89%|████████▊ | 247/279 [01:05<00:07,  4.08it/s] 89%|████████▉ | 248/279 [01:06<00:07,  4.07it/s] 89%|████████▉ | 249/279 [01:06<00:07,  4.07it/s] 90%|████████▉ | 250/279 [01:06<00:07,  4.07it/s] 90%|████████▉ | 251/279 [01:06<00:06,  4.06it/s] 90%|█████████ | 252/279 [01:06<00:06,  4.07it/s] 91%|█████████ | 253/279 [01:07<00:06,  4.08it/s] 91%|█████████ | 254/279 [01:07<00:06,  4.08it/s] 91%|█████████▏| 255/279 [01:07<00:05,  4.08it/s] 92%|█████████▏| 256/279 [01:07<00:05,  4.02it/s] 92%|█████████▏| 257/279 [01:08<00:05,  3.91it/s] 92%|█████████▏| 258/279 [01:08<00:05,  3.81it/s] 93%|█████████▎| 259/279 [01:08<00:05,  3.89it/s] 93%|█████████▎| 260/279 [01:09<00:04,  3.89it/s] 94%|█████████▎| 261/279 [01:09<00:04,  3.82it/s] 94%|█████████▍| 262/279 [01:09<00:04,  3.76it/s] 94%|█████████▍| 263/279 [01:09<00:04,  3.68it/s] 95%|█████████▍| 264/279 [01:10<00:03,  3.79it/s] 95%|█████████▍| 265/279 [01:10<00:03,  3.81it/s] 95%|█████████▌| 266/279 [01:10<00:03,  3.77it/s] 96%|█████████▌| 267/279 [01:10<00:03,  3.73it/s] 96%|█████████▌| 268/279 [01:11<00:02,  3.79it/s] 96%|█████████▋| 269/279 [01:11<00:02,  3.88it/s] 97%|█████████▋| 270/279 [01:11<00:02,  3.93it/s] 97%|█████████▋| 271/279 [01:11<00:02,  3.97it/s] 97%|█████████▋| 272/279 [01:12<00:01,  4.00it/s] 98%|█████████▊| 273/279 [01:12<00:01,  4.02it/s] 98%|█████████▊| 274/279 [01:12<00:01,  4.03it/s] 99%|█████████▊| 275/279 [01:12<00:00,  4.04it/s] 99%|█████████▉| 276/279 [01:13<00:00,  4.05it/s] 99%|█████████▉| 277/279 [01:13<00:00,  4.05it/s]100%|█████████▉| 278/279 [01:13<00:00,  4.06it/s]100%|██████████| 279/279 [01:13<00:00,  4.00it/s]accuracy:  0.6200716845878136
100%|██████████| 279/279 [01:18<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:08:27,304] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Training dataset size: 336, validation dataset size: 241
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:08:32,069] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Training dataset size: 336, validation dataset size: 241
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:08:37,665] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:33,  2.79s/it]  4%|▎         | 2/56 [00:05<02:20,  2.59s/it]  5%|▌         | 3/56 [00:07<02:14,  2.53s/it]  7%|▋         | 4/56 [00:10<02:14,  2.59s/it]  9%|▉         | 5/56 [00:12<02:06,  2.48s/it] 11%|█         | 6/56 [00:15<02:07,  2.55s/it] 12%|█▎        | 7/56 [00:17<01:58,  2.41s/it] 14%|█▍        | 8/56 [00:19<01:54,  2.39s/it] 16%|█▌        | 9/56 [00:22<01:50,  2.34s/it] 18%|█▊        | 10/56 [00:25<01:57,  2.56s/it]                                               {'loss': 0.3931, 'grad_norm': 8.688969612121582, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:25<01:57,  2.56s/it] 20%|█▉        | 11/56 [00:27<01:52,  2.50s/it] 21%|██▏       | 12/56 [00:29<01:41,  2.32s/it] 23%|██▎       | 13/56 [00:32<01:45,  2.44s/it] 25%|██▌       | 14/56 [00:33<01:33,  2.23s/it] 27%|██▋       | 15/56 [00:36<01:41,  2.48s/it] 29%|██▊       | 16/56 [00:39<01:41,  2.54s/it] 30%|███       | 17/56 [00:42<01:38,  2.54s/it] 32%|███▏      | 18/56 [00:44<01:30,  2.39s/it] 34%|███▍      | 19/56 [00:46<01:30,  2.44s/it] 36%|███▌      | 20/56 [00:49<01:31,  2.54s/it]                                               {'loss': 0.2319, 'grad_norm': 0.6259786486625671, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:49<01:31,  2.54s/it] 38%|███▊      | 21/56 [00:51<01:25,  2.43s/it] 39%|███▉      | 22/56 [00:54<01:23,  2.46s/it] 41%|████      | 23/56 [00:56<01:17,  2.33s/it] 43%|████▎     | 24/56 [00:58<01:09,  2.19s/it] 45%|████▍     | 25/56 [01:00<01:10,  2.28s/it] 46%|████▋     | 26/56 [01:02<01:07,  2.26s/it] 48%|████▊     | 27/56 [01:04<01:03,  2.20s/it] 50%|█████     | 28/56 [01:07<01:03,  2.25s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:09<00:59,  2.22s/it] 54%|█████▎    | 30/56 [01:11<00:59,  2.27s/it]                                               {'loss': 0.3953, 'grad_norm': 6.088827133178711, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:11<00:59,  2.27s/it] 55%|█████▌    | 31/56 [01:14<00:57,  2.30s/it] 57%|█████▋    | 32/56 [01:16<00:52,  2.21s/it] 59%|█████▉    | 33/56 [01:18<00:54,  2.35s/it] 61%|██████    | 34/56 [01:21<00:54,  2.50s/it] 62%|██████▎   | 35/56 [01:24<00:51,  2.46s/it] 64%|██████▍   | 36/56 [01:26<00:49,  2.47s/it] 66%|██████▌   | 37/56 [01:29<00:47,  2.52s/it] 68%|██████▊   | 38/56 [01:31<00:45,  2.55s/it] 70%|██████▉   | 39/56 [01:34<00:42,  2.47s/it] 71%|███████▏  | 40/56 [01:36<00:38,  2.40s/it]                                               {'loss': 0.2668, 'grad_norm': 2.3936784267425537, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:36<00:38,  2.40s/it] 73%|███████▎  | 41/56 [01:38<00:36,  2.43s/it] 75%|███████▌  | 42/56 [01:40<00:32,  2.32s/it] 77%|███████▋  | 43/56 [01:42<00:29,  2.24s/it] 79%|███████▊  | 44/56 [01:45<00:27,  2.27s/it] 80%|████████  | 45/56 [01:47<00:24,  2.25s/it] 82%|████████▏ | 46/56 [01:50<00:24,  2.47s/it] 84%|████████▍ | 47/56 [01:52<00:21,  2.43s/it] 86%|████████▌ | 48/56 [01:55<00:19,  2.45s/it] 88%|████████▊ | 49/56 [01:57<00:17,  2.50s/it] 89%|████████▉ | 50/56 [02:00<00:14,  2.49s/it]                                               {'loss': 0.2606, 'grad_norm': 0.8143904209136963, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:00<00:14,  2.49s/it] 91%|█████████ | 51/56 [02:03<00:12,  2.60s/it] 93%|█████████▎| 52/56 [02:06<00:10,  2.69s/it] 95%|█████████▍| 53/56 [02:08<00:07,  2.54s/it] 96%|█████████▋| 54/56 [02:10<00:05,  2.53s/it] 98%|█████████▊| 55/56 [02:13<00:02,  2.47s/it]100%|██████████| 56/56 [02:15<00:00,  2.43s/it]                                               {'train_runtime': 136.1179, 'train_samples_per_second': 4.937, 'train_steps_per_second': 0.411, 'train_loss': 0.28968185292822973, 'epoch': 2.0}
100%|██████████| 56/56 [02:15<00:00,  2.43s/it]100%|██████████| 56/56 [02:15<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:22,  3.28it/s]  1%|          | 2/272 [00:00<01:12,  3.74it/s]  1%|          | 3/272 [00:00<01:08,  3.92it/s]  1%|▏         | 4/272 [00:01<01:06,  4.01it/s]  2%|▏         | 5/272 [00:01<01:05,  4.06it/s]  2%|▏         | 6/272 [00:01<01:04,  4.09it/s]  3%|▎         | 7/272 [00:01<01:04,  4.12it/s]  3%|▎         | 8/272 [00:01<01:03,  4.13it/s]  3%|▎         | 9/272 [00:02<01:03,  4.14it/s]  4%|▎         | 10/272 [00:02<01:03,  4.14it/s]  4%|▍         | 11/272 [00:02<01:02,  4.15it/s]  4%|▍         | 12/272 [00:02<01:02,  4.16it/s]  5%|▍         | 13/272 [00:03<01:02,  4.16it/s]  5%|▌         | 14/272 [00:03<01:01,  4.16it/s]  6%|▌         | 15/272 [00:03<01:01,  4.17it/s]  6%|▌         | 16/272 [00:03<01:01,  4.17it/s]  6%|▋         | 17/272 [00:04<01:01,  4.16it/s]  7%|▋         | 18/272 [00:04<01:00,  4.17it/s]  7%|▋         | 19/272 [00:04<01:00,  4.16it/s]  7%|▋         | 20/272 [00:04<01:00,  4.17it/s]  8%|▊         | 21/272 [00:05<01:00,  4.17it/s]  8%|▊         | 22/272 [00:05<00:59,  4.17it/s]  8%|▊         | 23/272 [00:05<00:59,  4.17it/s]  9%|▉         | 24/272 [00:05<00:59,  4.17it/s]  9%|▉         | 25/272 [00:06<00:59,  4.17it/s] 10%|▉         | 26/272 [00:06<00:58,  4.17it/s] 10%|▉         | 27/272 [00:06<00:58,  4.17it/s] 10%|█         | 28/272 [00:06<00:58,  4.17it/s] 11%|█         | 29/272 [00:07<01:00,  4.04it/s] 11%|█         | 30/272 [00:07<01:01,  3.95it/s] 11%|█▏        | 31/272 [00:07<01:02,  3.86it/s] 12%|█▏        | 32/272 [00:07<01:00,  3.95it/s] 12%|█▏        | 33/272 [00:08<01:01,  3.91it/s] 12%|█▎        | 34/272 [00:08<01:03,  3.78it/s] 13%|█▎        | 35/272 [00:08<01:04,  3.70it/s] 13%|█▎        | 36/272 [00:08<01:01,  3.82it/s] 14%|█▎        | 37/272 [00:09<01:01,  3.82it/s] 14%|█▍        | 38/272 [00:09<01:02,  3.72it/s] 14%|█▍        | 39/272 [00:09<01:03,  3.65it/s] 15%|█▍        | 40/272 [00:09<01:01,  3.79it/s] 15%|█▌        | 41/272 [00:10<01:00,  3.81it/s] 15%|█▌        | 42/272 [00:10<01:01,  3.72it/s] 16%|█▌        | 43/272 [00:10<01:02,  3.65it/s] 16%|█▌        | 44/272 [00:11<01:00,  3.80it/s] 17%|█▋        | 45/272 [00:11<00:59,  3.82it/s] 17%|█▋        | 46/272 [00:11<01:00,  3.72it/s] 17%|█▋        | 47/272 [00:11<01:01,  3.65it/s] 18%|█▊        | 48/272 [00:12<00:59,  3.79it/s] 18%|█▊        | 49/272 [00:12<00:58,  3.81it/s] 18%|█▊        | 50/272 [00:12<00:59,  3.73it/s] 19%|█▉        | 51/272 [00:12<00:59,  3.68it/s] 19%|█▉        | 52/272 [00:13<00:58,  3.78it/s] 19%|█▉        | 53/272 [00:13<00:57,  3.81it/s] 20%|█▉        | 54/272 [00:13<00:58,  3.71it/s] 20%|██        | 55/272 [00:13<00:57,  3.75it/s] 21%|██        | 56/272 [00:14<00:58,  3.72it/s] 21%|██        | 57/272 [00:14<00:57,  3.77it/s] 21%|██▏       | 58/272 [00:14<00:57,  3.69it/s] 22%|██▏       | 59/272 [00:15<00:56,  3.74it/s] 22%|██▏       | 60/272 [00:15<00:57,  3.71it/s] 22%|██▏       | 61/272 [00:15<00:55,  3.77it/s] 23%|██▎       | 62/272 [00:15<00:56,  3.70it/s] 23%|██▎       | 63/272 [00:16<00:55,  3.73it/s] 24%|██▎       | 64/272 [00:16<00:55,  3.72it/s] 24%|██▍       | 65/272 [00:16<00:54,  3.77it/s] 24%|██▍       | 66/272 [00:16<00:55,  3.70it/s] 25%|██▍       | 67/272 [00:17<00:55,  3.69it/s] 25%|██▌       | 68/272 [00:17<00:53,  3.81it/s] 25%|██▌       | 69/272 [00:17<00:52,  3.89it/s] 26%|██▌       | 70/272 [00:17<00:50,  3.96it/s] 26%|██▌       | 71/272 [00:18<00:50,  4.02it/s] 26%|██▋       | 72/272 [00:18<00:49,  4.06it/s] 27%|██▋       | 73/272 [00:18<00:48,  4.08it/s] 27%|██▋       | 74/272 [00:18<00:48,  4.10it/s] 28%|██▊       | 75/272 [00:19<00:47,  4.12it/s] 28%|██▊       | 76/272 [00:19<00:47,  4.13it/s] 28%|██▊       | 77/272 [00:19<00:47,  4.14it/s] 29%|██▊       | 78/272 [00:19<00:46,  4.14it/s] 29%|██▉       | 79/272 [00:20<00:46,  4.14it/s] 29%|██▉       | 80/272 [00:20<00:46,  4.14it/s] 30%|██▉       | 81/272 [00:20<00:46,  4.13it/s] 30%|███       | 82/272 [00:20<00:46,  4.12it/s] 31%|███       | 83/272 [00:21<00:45,  4.12it/s] 31%|███       | 84/272 [00:21<00:45,  4.13it/s] 31%|███▏      | 85/272 [00:21<00:45,  4.13it/s] 32%|███▏      | 86/272 [00:21<00:44,  4.14it/s] 32%|███▏      | 87/272 [00:22<00:44,  4.14it/s] 32%|███▏      | 88/272 [00:22<00:44,  4.14it/s] 33%|███▎      | 89/272 [00:22<00:44,  4.13it/s] 33%|███▎      | 90/272 [00:22<00:44,  4.12it/s] 33%|███▎      | 91/272 [00:23<00:43,  4.12it/s] 34%|███▍      | 92/272 [00:23<00:44,  4.04it/s] 34%|███▍      | 93/272 [00:23<00:45,  3.92it/s] 35%|███▍      | 94/272 [00:23<00:45,  3.88it/s] 35%|███▍      | 95/272 [00:24<00:44,  3.94it/s] 35%|███▌      | 96/272 [00:24<00:45,  3.87it/s] 36%|███▌      | 97/272 [00:24<00:45,  3.82it/s] 36%|███▌      | 98/272 [00:24<00:46,  3.75it/s] 36%|███▋      | 99/272 [00:25<00:44,  3.84it/s] 37%|███▋      | 100/272 [00:25<00:45,  3.79it/s] 37%|███▋      | 101/272 [00:25<00:45,  3.77it/s] 38%|███▊      | 102/272 [00:25<00:45,  3.72it/s] 38%|███▊      | 103/272 [00:26<00:44,  3.83it/s] 38%|███▊      | 104/272 [00:26<00:44,  3.80it/s] 39%|███▊      | 105/272 [00:26<00:44,  3.76it/s] 39%|███▉      | 106/272 [00:27<00:44,  3.72it/s] 39%|███▉      | 107/272 [00:27<00:43,  3.83it/s] 40%|███▉      | 108/272 [00:27<00:43,  3.78it/s] 40%|████      | 109/272 [00:27<00:43,  3.76it/s] 40%|████      | 110/272 [00:28<00:43,  3.71it/s] 41%|████      | 111/272 [00:28<00:42,  3.83it/s] 41%|████      | 112/272 [00:28<00:42,  3.79it/s] 42%|████▏     | 113/272 [00:28<00:42,  3.76it/s] 42%|████▏     | 114/272 [00:29<00:42,  3.71it/s] 42%|████▏     | 115/272 [00:29<00:41,  3.83it/s] 43%|████▎     | 116/272 [00:29<00:41,  3.78it/s] 43%|████▎     | 117/272 [00:29<00:41,  3.76it/s] 43%|████▎     | 118/272 [00:30<00:41,  3.70it/s] 44%|████▍     | 119/272 [00:30<00:40,  3.82it/s] 44%|████▍     | 120/272 [00:30<00:40,  3.77it/s] 44%|████▍     | 121/272 [00:30<00:40,  3.76it/s] 45%|████▍     | 122/272 [00:31<00:40,  3.70it/s] 45%|████▌     | 123/272 [00:31<00:39,  3.81it/s] 46%|████▌     | 124/272 [00:31<00:39,  3.77it/s] 46%|████▌     | 125/272 [00:32<00:39,  3.76it/s] 46%|████▋     | 126/272 [00:32<00:39,  3.70it/s] 47%|████▋     | 127/272 [00:32<00:38,  3.81it/s] 47%|████▋     | 128/272 [00:32<00:38,  3.77it/s] 47%|████▋     | 129/272 [00:33<00:38,  3.76it/s] 48%|████▊     | 130/272 [00:33<00:38,  3.71it/s] 48%|████▊     | 131/272 [00:33<00:36,  3.82it/s] 49%|████▊     | 132/272 [00:33<00:37,  3.78it/s] 49%|████▉     | 133/272 [00:34<00:36,  3.76it/s] 49%|████▉     | 134/272 [00:34<00:37,  3.71it/s] 50%|████▉     | 135/272 [00:34<00:35,  3.82it/s] 50%|█████     | 136/272 [00:34<00:36,  3.76it/s] 50%|█████     | 137/272 [00:35<00:36,  3.74it/s] 51%|█████     | 138/272 [00:35<00:36,  3.69it/s] 51%|█████     | 139/272 [00:35<00:34,  3.81it/s] 51%|█████▏    | 140/272 [00:36<00:34,  3.77it/s] 52%|█████▏    | 141/272 [00:36<00:34,  3.75it/s] 52%|█████▏    | 142/272 [00:36<00:35,  3.69it/s] 53%|█████▎    | 143/272 [00:36<00:33,  3.80it/s] 53%|█████▎    | 144/272 [00:37<00:34,  3.76it/s] 53%|█████▎    | 145/272 [00:37<00:33,  3.74it/s] 54%|█████▎    | 146/272 [00:37<00:34,  3.69it/s] 54%|█████▍    | 147/272 [00:37<00:32,  3.80it/s] 54%|█████▍    | 148/272 [00:38<00:32,  3.76it/s] 55%|█████▍    | 149/272 [00:38<00:32,  3.74it/s] 55%|█████▌    | 150/272 [00:38<00:33,  3.69it/s] 56%|█████▌    | 151/272 [00:38<00:31,  3.80it/s] 56%|█████▌    | 152/272 [00:39<00:31,  3.76it/s] 56%|█████▋    | 153/272 [00:39<00:31,  3.74it/s] 57%|█████▋    | 154/272 [00:39<00:31,  3.69it/s] 57%|█████▋    | 155/272 [00:40<00:30,  3.80it/s] 57%|█████▋    | 156/272 [00:40<00:30,  3.77it/s] 58%|█████▊    | 157/272 [00:40<00:30,  3.75it/s] 58%|█████▊    | 158/272 [00:40<00:30,  3.70it/s] 58%|█████▊    | 159/272 [00:41<00:29,  3.81it/s] 59%|█████▉    | 160/272 [00:41<00:29,  3.76it/s] 59%|█████▉    | 161/272 [00:41<00:29,  3.74it/s] 60%|█████▉    | 162/272 [00:41<00:29,  3.69it/s] 60%|█████▉    | 163/272 [00:42<00:28,  3.80it/s] 60%|██████    | 164/272 [00:42<00:28,  3.76it/s] 61%|██████    | 165/272 [00:42<00:28,  3.75it/s] 61%|██████    | 166/272 [00:42<00:28,  3.69it/s] 61%|██████▏   | 167/272 [00:43<00:27,  3.80it/s] 62%|██████▏   | 168/272 [00:43<00:27,  3.76it/s] 62%|██████▏   | 169/272 [00:43<00:27,  3.74it/s] 62%|██████▎   | 170/272 [00:44<00:27,  3.69it/s] 63%|██████▎   | 171/272 [00:44<00:26,  3.80it/s] 63%|██████▎   | 172/272 [00:44<00:26,  3.76it/s] 64%|██████▎   | 173/272 [00:44<00:26,  3.74it/s] 64%|██████▍   | 174/272 [00:45<00:26,  3.69it/s] 64%|██████▍   | 175/272 [00:45<00:25,  3.79it/s] 65%|██████▍   | 176/272 [00:45<00:25,  3.73it/s] 65%|██████▌   | 177/272 [00:45<00:25,  3.72it/s] 65%|██████▌   | 178/272 [00:46<00:25,  3.67it/s] 66%|██████▌   | 179/272 [00:46<00:24,  3.79it/s] 66%|██████▌   | 180/272 [00:46<00:24,  3.75it/s] 67%|██████▋   | 181/272 [00:46<00:24,  3.74it/s] 67%|██████▋   | 182/272 [00:47<00:24,  3.69it/s] 67%|██████▋   | 183/272 [00:47<00:23,  3.80it/s] 68%|██████▊   | 184/272 [00:47<00:23,  3.76it/s] 68%|██████▊   | 185/272 [00:48<00:23,  3.74it/s] 68%|██████▊   | 186/272 [00:48<00:23,  3.70it/s] 69%|██████▉   | 187/272 [00:48<00:22,  3.81it/s] 69%|██████▉   | 188/272 [00:48<00:22,  3.76it/s] 69%|██████▉   | 189/272 [00:49<00:22,  3.74it/s] 70%|██████▉   | 190/272 [00:49<00:22,  3.69it/s] 70%|███████   | 191/272 [00:49<00:21,  3.78it/s] 71%|███████   | 192/272 [00:49<00:21,  3.74it/s] 71%|███████   | 193/272 [00:50<00:21,  3.72it/s] 71%|███████▏  | 194/272 [00:50<00:21,  3.68it/s] 72%|███████▏  | 195/272 [00:50<00:20,  3.80it/s] 72%|███████▏  | 196/272 [00:50<00:20,  3.76it/s] 72%|███████▏  | 197/272 [00:51<00:20,  3.73it/s] 73%|███████▎  | 198/272 [00:51<00:20,  3.68it/s] 73%|███████▎  | 199/272 [00:51<00:19,  3.79it/s] 74%|███████▎  | 200/272 [00:52<00:19,  3.74it/s] 74%|███████▍  | 201/272 [00:52<00:19,  3.73it/s] 74%|███████▍  | 202/272 [00:52<00:18,  3.68it/s] 75%|███████▍  | 203/272 [00:52<00:18,  3.80it/s] 75%|███████▌  | 204/272 [00:53<00:18,  3.76it/s] 75%|███████▌  | 205/272 [00:53<00:17,  3.73it/s] 76%|███████▌  | 206/272 [00:53<00:17,  3.68it/s] 76%|███████▌  | 207/272 [00:53<00:17,  3.79it/s] 76%|███████▋  | 208/272 [00:54<00:17,  3.74it/s] 77%|███████▋  | 209/272 [00:54<00:16,  3.73it/s] 77%|███████▋  | 210/272 [00:54<00:16,  3.68it/s] 78%|███████▊  | 211/272 [00:54<00:16,  3.80it/s] 78%|███████▊  | 212/272 [00:55<00:15,  3.77it/s] 78%|███████▊  | 213/272 [00:55<00:15,  3.74it/s] 79%|███████▊  | 214/272 [00:55<00:15,  3.68it/s] 79%|███████▉  | 215/272 [00:56<00:15,  3.79it/s] 79%|███████▉  | 216/272 [00:56<00:14,  3.75it/s] 80%|███████▉  | 217/272 [00:56<00:14,  3.73it/s] 80%|████████  | 218/272 [00:56<00:14,  3.68it/s] 81%|████████  | 219/272 [00:57<00:13,  3.80it/s] 81%|████████  | 220/272 [00:57<00:13,  3.76it/s] 81%|████████▏ | 221/272 [00:57<00:13,  3.73it/s] 82%|████████▏ | 222/272 [00:57<00:13,  3.67it/s] 82%|████████▏ | 223/272 [00:58<00:12,  3.78it/s] 82%|████████▏ | 224/272 [00:58<00:12,  3.75it/s] 83%|████████▎ | 225/272 [00:58<00:12,  3.73it/s] 83%|████████▎ | 226/272 [00:59<00:12,  3.68it/s] 83%|████████▎ | 227/272 [00:59<00:11,  3.80it/s] 84%|████████▍ | 228/272 [00:59<00:11,  3.75it/s] 84%|████████▍ | 229/272 [00:59<00:11,  3.73it/s] 85%|████████▍ | 230/272 [01:00<00:11,  3.67it/s] 85%|████████▍ | 231/272 [01:00<00:10,  3.78it/s] 85%|████████▌ | 232/272 [01:00<00:10,  3.74it/s] 86%|████████▌ | 233/272 [01:00<00:10,  3.72it/s] 86%|████████▌ | 234/272 [01:01<00:10,  3.67it/s] 86%|████████▋ | 235/272 [01:01<00:09,  3.78it/s] 87%|████████▋ | 236/272 [01:01<00:09,  3.74it/s] 87%|████████▋ | 237/272 [01:01<00:09,  3.73it/s] 88%|████████▊ | 238/272 [01:02<00:09,  3.67it/s] 88%|████████▊ | 239/272 [01:02<00:08,  3.79it/s] 88%|████████▊ | 240/272 [01:02<00:08,  3.76it/s] 89%|████████▊ | 241/272 [01:03<00:08,  3.73it/s] 89%|████████▉ | 242/272 [01:03<00:08,  3.67it/s] 89%|████████▉ | 243/272 [01:03<00:07,  3.78it/s] 90%|████████▉ | 244/272 [01:03<00:07,  3.73it/s] 90%|█████████ | 245/272 [01:04<00:07,  3.72it/s] 90%|█████████ | 246/272 [01:04<00:07,  3.67it/s] 91%|█████████ | 247/272 [01:04<00:06,  3.79it/s] 91%|█████████ | 248/272 [01:04<00:06,  3.74it/s] 92%|█████████▏| 249/272 [01:05<00:06,  3.73it/s] 92%|█████████▏| 250/272 [01:05<00:05,  3.67it/s] 92%|█████████▏| 251/272 [01:05<00:05,  3.79it/s] 93%|█████████▎| 252/272 [01:05<00:05,  3.75it/s] 93%|█████████▎| 253/272 [01:06<00:05,  3.72it/s] 93%|█████████▎| 254/272 [01:06<00:04,  3.66it/s] 94%|█████████▍| 255/272 [01:06<00:04,  3.77it/s] 94%|█████████▍| 256/272 [01:07<00:04,  3.73it/s] 94%|█████████▍| 257/272 [01:07<00:04,  3.72it/s] 95%|█████████▍| 258/272 [01:07<00:03,  3.66it/s] 95%|█████████▌| 259/272 [01:07<00:03,  3.77it/s] 96%|█████████▌| 260/272 [01:08<00:03,  3.72it/s] 96%|█████████▌| 261/272 [01:08<00:02,  3.70it/s] 96%|█████████▋| 262/272 [01:08<00:02,  3.65it/s] 97%|█████████▋| 263/272 [01:08<00:02,  3.77it/s] 97%|█████████▋| 264/272 [01:09<00:02,  3.72it/s] 97%|█████████▋| 265/272 [01:09<00:01,  3.70it/s] 98%|█████████▊| 266/272 [01:09<00:01,  3.66it/s] 98%|█████████▊| 267/272 [01:09<00:01,  3.78it/s] 99%|█████████▊| 268/272 [01:10<00:01,  3.75it/s] 99%|█████████▉| 269/272 [01:10<00:00,  3.73it/s] 99%|█████████▉| 270/272 [01:10<00:00,  3.68it/s]100%|█████████▉| 271/272 [01:11<00:00,  3.79it/s]100%|██████████| 272/272 [01:11<00:00,  3.76it/s]accuracy:  0.8933823529411765
100%|██████████| 272/272 [01:15<00:00,  3.61it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 149
Training dataset size: 336, validation dataset size: 149
Training dataset size: 336, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:12:35,013] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:12:35,098] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2025-03-12 07:12:35,117] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:37,  2.87s/it]  4%|▎         | 2/56 [00:05<02:26,  2.71s/it]  5%|▌         | 3/56 [00:07<02:06,  2.38s/it]  7%|▋         | 4/56 [00:09<02:06,  2.43s/it]  9%|▉         | 5/56 [00:12<02:02,  2.40s/it] 11%|█         | 6/56 [00:14<02:02,  2.46s/it] 12%|█▎        | 7/56 [00:17<01:59,  2.43s/it] 14%|█▍        | 8/56 [00:19<01:57,  2.44s/it] 16%|█▌        | 9/56 [00:21<01:49,  2.32s/it] 18%|█▊        | 10/56 [00:24<01:51,  2.42s/it]                                               {'loss': 0.732, 'grad_norm': 6.302196502685547, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:24<01:51,  2.42s/it] 20%|█▉        | 11/56 [00:26<01:47,  2.39s/it] 21%|██▏       | 12/56 [00:29<01:45,  2.41s/it] 23%|██▎       | 13/56 [00:31<01:44,  2.44s/it] 25%|██▌       | 14/56 [00:33<01:37,  2.32s/it] 27%|██▋       | 15/56 [00:36<01:36,  2.35s/it] 29%|██▊       | 16/56 [00:38<01:35,  2.38s/it] 30%|███       | 17/56 [00:40<01:31,  2.35s/it] 32%|███▏      | 18/56 [00:44<01:38,  2.60s/it] 34%|███▍      | 19/56 [00:46<01:32,  2.50s/it] 36%|███▌      | 20/56 [00:49<01:33,  2.58s/it]                                               {'loss': 0.5352, 'grad_norm': 8.9885892868042, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:49<01:33,  2.58s/it] 38%|███▊      | 21/56 [00:51<01:26,  2.47s/it] 39%|███▉      | 22/56 [00:54<01:29,  2.65s/it] 41%|████      | 23/56 [00:56<01:26,  2.62s/it] 43%|████▎     | 24/56 [00:59<01:19,  2.48s/it] 45%|████▍     | 25/56 [01:00<01:10,  2.27s/it] 46%|████▋     | 26/56 [01:03<01:11,  2.38s/it] 48%|████▊     | 27/56 [01:05<01:08,  2.35s/it] 50%|█████     | 28/56 [01:07<01:04,  2.30s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:10<01:07,  2.49s/it] 54%|█████▎    | 30/56 [01:13<01:09,  2.66s/it]                                               {'loss': 0.4762, 'grad_norm': 5.30767822265625, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:13<01:09,  2.66s/it] 55%|█████▌    | 31/56 [01:16<01:07,  2.71s/it] 57%|█████▋    | 32/56 [01:19<01:03,  2.63s/it] 59%|█████▉    | 33/56 [01:22<01:01,  2.67s/it] 61%|██████    | 34/56 [01:24<00:57,  2.61s/it] 62%|██████▎   | 35/56 [01:27<00:55,  2.63s/it] 64%|██████▍   | 36/56 [01:29<00:50,  2.52s/it] 66%|██████▌   | 37/56 [01:31<00:46,  2.46s/it] 68%|██████▊   | 38/56 [01:33<00:41,  2.30s/it] 70%|██████▉   | 39/56 [01:35<00:37,  2.19s/it] 71%|███████▏  | 40/56 [01:37<00:35,  2.21s/it]                                               {'loss': 0.4009, 'grad_norm': 7.137707710266113, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:37<00:35,  2.21s/it] 73%|███████▎  | 41/56 [01:40<00:33,  2.26s/it] 75%|███████▌  | 42/56 [01:42<00:32,  2.30s/it] 77%|███████▋  | 43/56 [01:45<00:32,  2.52s/it] 79%|███████▊  | 44/56 [01:48<00:30,  2.53s/it] 80%|████████  | 45/56 [01:50<00:28,  2.55s/it] 82%|████████▏ | 46/56 [01:52<00:24,  2.40s/it] 84%|████████▍ | 47/56 [01:54<00:19,  2.18s/it] 86%|████████▌ | 48/56 [01:57<00:18,  2.35s/it] 88%|████████▊ | 49/56 [01:59<00:16,  2.38s/it] 89%|████████▉ | 50/56 [02:02<00:14,  2.48s/it]                                               {'loss': 0.5091, 'grad_norm': 7.400136947631836, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:02<00:14,  2.48s/it] 91%|█████████ | 51/56 [02:04<00:11,  2.34s/it] 93%|█████████▎| 52/56 [02:06<00:08,  2.19s/it] 95%|█████████▍| 53/56 [02:08<00:06,  2.21s/it] 96%|█████████▋| 54/56 [02:10<00:04,  2.11s/it] 98%|█████████▊| 55/56 [02:12<00:02,  2.17s/it]100%|██████████| 56/56 [02:14<00:00,  2.12s/it]                                               {'train_runtime': 135.3955, 'train_samples_per_second': 4.963, 'train_steps_per_second': 0.414, 'train_loss': 0.49941726454666685, 'epoch': 2.0}
100%|██████████| 56/56 [02:15<00:00,  2.12s/it]100%|██████████| 56/56 [02:15<00:00,  2.41s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<00:53,  3.26it/s]  1%|          | 2/175 [00:00<00:46,  3.75it/s]  2%|▏         | 3/175 [00:00<00:43,  3.94it/s]  2%|▏         | 4/175 [00:01<00:42,  4.04it/s]  3%|▎         | 5/175 [00:01<00:41,  4.10it/s]  3%|▎         | 6/175 [00:01<00:40,  4.13it/s]  4%|▍         | 7/175 [00:01<00:40,  4.15it/s]  5%|▍         | 8/175 [00:01<00:40,  4.16it/s]  5%|▌         | 9/175 [00:02<00:39,  4.17it/s]  6%|▌         | 10/175 [00:02<00:40,  4.09it/s]  6%|▋         | 11/175 [00:02<00:41,  3.97it/s]  7%|▋         | 12/175 [00:03<00:42,  3.86it/s]  7%|▋         | 13/175 [00:03<00:41,  3.95it/s]  8%|▊         | 14/175 [00:03<00:40,  3.95it/s]  9%|▊         | 15/175 [00:03<00:41,  3.83it/s]  9%|▉         | 16/175 [00:04<00:41,  3.82it/s] 10%|▉         | 17/175 [00:04<00:41,  3.80it/s] 10%|█         | 18/175 [00:04<00:40,  3.88it/s] 11%|█         | 19/175 [00:04<00:40,  3.81it/s] 11%|█▏        | 20/175 [00:05<00:41,  3.77it/s] 12%|█▏        | 21/175 [00:05<00:41,  3.73it/s] 13%|█▎        | 22/175 [00:05<00:39,  3.86it/s] 13%|█▎        | 23/175 [00:05<00:39,  3.86it/s] 14%|█▎        | 24/175 [00:06<00:39,  3.78it/s] 14%|█▍        | 25/175 [00:06<00:40,  3.70it/s] 15%|█▍        | 26/175 [00:06<00:39,  3.82it/s] 15%|█▌        | 27/175 [00:06<00:38,  3.86it/s] 16%|█▌        | 28/175 [00:07<00:39,  3.76it/s] 17%|█▋        | 29/175 [00:07<00:38,  3.79it/s] 17%|█▋        | 30/175 [00:07<00:38,  3.74it/s] 18%|█▊        | 31/175 [00:07<00:37,  3.82it/s] 18%|█▊        | 32/175 [00:08<00:37,  3.78it/s] 19%|█▉        | 33/175 [00:08<00:37,  3.75it/s] 19%|█▉        | 34/175 [00:08<00:37,  3.72it/s] 20%|██        | 35/175 [00:09<00:36,  3.84it/s] 21%|██        | 36/175 [00:09<00:36,  3.83it/s] 21%|██        | 37/175 [00:09<00:36,  3.75it/s] 22%|██▏       | 38/175 [00:09<00:37,  3.67it/s] 22%|██▏       | 39/175 [00:10<00:35,  3.81it/s] 23%|██▎       | 40/175 [00:10<00:34,  3.86it/s] 23%|██▎       | 41/175 [00:10<00:35,  3.78it/s] 24%|██▍       | 42/175 [00:10<00:34,  3.80it/s] 25%|██▍       | 43/175 [00:11<00:35,  3.74it/s] 25%|██▌       | 44/175 [00:11<00:34,  3.82it/s] 26%|██▌       | 45/175 [00:11<00:34,  3.74it/s] 26%|██▋       | 46/175 [00:11<00:34,  3.76it/s] 27%|██▋       | 47/175 [00:12<00:34,  3.72it/s] 27%|██▋       | 48/175 [00:12<00:32,  3.85it/s] 28%|██▊       | 49/175 [00:12<00:32,  3.85it/s] 29%|██▊       | 50/175 [00:13<00:33,  3.77it/s] 29%|██▉       | 51/175 [00:13<00:33,  3.72it/s] 30%|██▉       | 52/175 [00:13<00:32,  3.78it/s] 30%|███       | 53/175 [00:13<00:31,  3.87it/s] 31%|███       | 54/175 [00:14<00:31,  3.80it/s] 31%|███▏      | 55/175 [00:14<00:31,  3.75it/s] 32%|███▏      | 56/175 [00:14<00:32,  3.71it/s] 33%|███▎      | 57/175 [00:14<00:30,  3.84it/s] 33%|███▎      | 58/175 [00:15<00:30,  3.84it/s] 34%|███▎      | 59/175 [00:15<00:30,  3.76it/s] 34%|███▍      | 60/175 [00:15<00:31,  3.67it/s] 35%|███▍      | 61/175 [00:15<00:29,  3.80it/s] 35%|███▌      | 62/175 [00:16<00:29,  3.85it/s] 36%|███▌      | 63/175 [00:16<00:29,  3.77it/s] 37%|███▋      | 64/175 [00:16<00:29,  3.79it/s] 37%|███▋      | 65/175 [00:17<00:29,  3.72it/s] 38%|███▊      | 66/175 [00:17<00:28,  3.84it/s] 38%|███▊      | 67/175 [00:17<00:28,  3.77it/s] 39%|███▉      | 68/175 [00:17<00:28,  3.73it/s] 39%|███▉      | 69/175 [00:18<00:28,  3.69it/s] 40%|████      | 70/175 [00:18<00:27,  3.81it/s] 41%|████      | 71/175 [00:18<00:27,  3.83it/s] 41%|████      | 72/175 [00:18<00:27,  3.73it/s] 42%|████▏     | 73/175 [00:19<00:27,  3.73it/s] 42%|████▏     | 74/175 [00:19<00:26,  3.85it/s] 43%|████▎     | 75/175 [00:19<00:25,  3.94it/s] 43%|████▎     | 76/175 [00:19<00:24,  4.01it/s] 44%|████▍     | 77/175 [00:20<00:24,  4.05it/s] 45%|████▍     | 78/175 [00:20<00:23,  4.08it/s] 45%|████▌     | 79/175 [00:20<00:23,  4.09it/s] 46%|████▌     | 80/175 [00:20<00:23,  4.10it/s] 46%|████▋     | 81/175 [00:21<00:22,  4.11it/s] 47%|████▋     | 82/175 [00:21<00:22,  4.11it/s] 47%|████▋     | 83/175 [00:21<00:22,  4.12it/s] 48%|████▊     | 84/175 [00:21<00:22,  4.12it/s] 49%|████▊     | 85/175 [00:22<00:21,  4.13it/s] 49%|████▉     | 86/175 [00:22<00:21,  4.14it/s] 50%|████▉     | 87/175 [00:22<00:21,  4.15it/s] 50%|█████     | 88/175 [00:22<00:21,  4.07it/s] 51%|█████     | 89/175 [00:23<00:21,  3.95it/s] 51%|█████▏    | 90/175 [00:23<00:21,  3.88it/s] 52%|█████▏    | 91/175 [00:23<00:21,  3.95it/s] 53%|█████▎    | 92/175 [00:23<00:21,  3.94it/s] 53%|█████▎    | 93/175 [00:24<00:21,  3.87it/s] 54%|█████▎    | 94/175 [00:24<00:20,  3.87it/s] 54%|█████▍    | 95/175 [00:24<00:21,  3.78it/s] 55%|█████▍    | 96/175 [00:24<00:20,  3.87it/s] 55%|█████▌    | 97/175 [00:25<00:20,  3.82it/s] 56%|█████▌    | 98/175 [00:25<00:20,  3.78it/s] 57%|█████▋    | 99/175 [00:25<00:20,  3.71it/s] 57%|█████▋    | 100/175 [00:25<00:19,  3.83it/s] 58%|█████▊    | 101/175 [00:26<00:19,  3.87it/s] 58%|█████▊    | 102/175 [00:26<00:19,  3.82it/s] 59%|█████▉    | 103/175 [00:26<00:18,  3.79it/s] 59%|█████▉    | 104/175 [00:26<00:19,  3.71it/s] 60%|██████    | 105/175 [00:27<00:18,  3.83it/s] 61%|██████    | 106/175 [00:27<00:17,  3.89it/s] 61%|██████    | 107/175 [00:27<00:17,  3.84it/s] 62%|██████▏   | 108/175 [00:28<00:17,  3.80it/s] 62%|██████▏   | 109/175 [00:28<00:17,  3.72it/s] 63%|██████▎   | 110/175 [00:28<00:16,  3.83it/s] 63%|██████▎   | 111/175 [00:28<00:16,  3.86it/s] 64%|██████▍   | 112/175 [00:29<00:16,  3.83it/s] 65%|██████▍   | 113/175 [00:29<00:16,  3.79it/s] 65%|██████▌   | 114/175 [00:29<00:16,  3.71it/s] 66%|██████▌   | 115/175 [00:29<00:15,  3.84it/s] 66%|██████▋   | 116/175 [00:30<00:15,  3.86it/s] 67%|██████▋   | 117/175 [00:30<00:15,  3.82it/s] 67%|██████▋   | 118/175 [00:30<00:15,  3.80it/s] 68%|██████▊   | 119/175 [00:30<00:15,  3.71it/s] 69%|██████▊   | 120/175 [00:31<00:14,  3.83it/s] 69%|██████▉   | 121/175 [00:31<00:13,  3.87it/s] 70%|██████▉   | 122/175 [00:31<00:13,  3.82it/s] 70%|███████   | 123/175 [00:31<00:13,  3.79it/s] 71%|███████   | 124/175 [00:32<00:13,  3.71it/s] 71%|███████▏  | 125/175 [00:32<00:13,  3.82it/s] 72%|███████▏  | 126/175 [00:32<00:12,  3.85it/s] 73%|███████▎  | 127/175 [00:33<00:12,  3.81it/s] 73%|███████▎  | 128/175 [00:33<00:12,  3.78it/s] 74%|███████▎  | 129/175 [00:33<00:12,  3.70it/s] 74%|███████▍  | 130/175 [00:33<00:11,  3.82it/s] 75%|███████▍  | 131/175 [00:34<00:11,  3.86it/s] 75%|███████▌  | 132/175 [00:34<00:11,  3.82it/s] 76%|███████▌  | 133/175 [00:34<00:11,  3.78it/s] 77%|███████▋  | 134/175 [00:34<00:11,  3.70it/s] 77%|███████▋  | 135/175 [00:35<00:10,  3.81it/s] 78%|███████▊  | 136/175 [00:35<00:10,  3.87it/s] 78%|███████▊  | 137/175 [00:35<00:09,  3.82it/s] 79%|███████▉  | 138/175 [00:35<00:09,  3.79it/s] 79%|███████▉  | 139/175 [00:36<00:09,  3.83it/s] 80%|████████  | 140/175 [00:36<00:08,  3.92it/s] 81%|████████  | 141/175 [00:36<00:08,  3.99it/s] 81%|████████  | 142/175 [00:36<00:08,  4.03it/s] 82%|████████▏ | 143/175 [00:37<00:07,  4.07it/s] 82%|████████▏ | 144/175 [00:37<00:07,  4.09it/s] 83%|████████▎ | 145/175 [00:37<00:07,  4.10it/s] 83%|████████▎ | 146/175 [00:37<00:07,  4.10it/s] 84%|████████▍ | 147/175 [00:38<00:06,  4.10it/s] 85%|████████▍ | 148/175 [00:38<00:06,  4.11it/s] 85%|████████▌ | 149/175 [00:38<00:06,  4.12it/s] 86%|████████▌ | 150/175 [00:38<00:06,  4.13it/s] 86%|████████▋ | 151/175 [00:39<00:05,  4.14it/s] 87%|████████▋ | 152/175 [00:39<00:05,  4.14it/s] 87%|████████▋ | 153/175 [00:39<00:05,  4.14it/s] 88%|████████▊ | 154/175 [00:39<00:05,  4.14it/s] 89%|████████▊ | 155/175 [00:40<00:04,  4.14it/s] 89%|████████▉ | 156/175 [00:40<00:04,  4.14it/s] 90%|████████▉ | 157/175 [00:40<00:04,  4.13it/s] 90%|█████████ | 158/175 [00:40<00:04,  4.12it/s] 91%|█████████ | 159/175 [00:41<00:03,  4.12it/s] 91%|█████████▏| 160/175 [00:41<00:03,  4.13it/s] 92%|█████████▏| 161/175 [00:41<00:03,  4.13it/s] 93%|█████████▎| 162/175 [00:41<00:03,  4.14it/s] 93%|█████████▎| 163/175 [00:41<00:02,  4.14it/s] 94%|█████████▎| 164/175 [00:42<00:02,  4.14it/s] 94%|█████████▍| 165/175 [00:42<00:02,  4.13it/s] 95%|█████████▍| 166/175 [00:42<00:02,  4.13it/s] 95%|█████████▌| 167/175 [00:42<00:01,  4.12it/s] 96%|█████████▌| 168/175 [00:43<00:01,  4.12it/s] 97%|█████████▋| 169/175 [00:43<00:01,  4.12it/s] 97%|█████████▋| 170/175 [00:43<00:01,  4.13it/s] 98%|█████████▊| 171/175 [00:43<00:00,  4.13it/s] 98%|█████████▊| 172/175 [00:44<00:00,  4.13it/s] 99%|█████████▉| 173/175 [00:44<00:00,  4.13it/s] 99%|█████████▉| 174/175 [00:44<00:00,  4.12it/s]100%|██████████| 175/175 [00:44<00:00,  4.12it/s]accuracy:  0.8457142857142858
100%|██████████| 175/175 [00:47<00:00,  3.67it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 336, validation dataset size: 119
Training dataset size: 336, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 336, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:16:04,813] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.08s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:16:06,428] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:16:07,752] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/56 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:02<02:34,  2.81s/it]  4%|▎         | 2/56 [00:05<02:21,  2.62s/it]  5%|▌         | 3/56 [00:08<02:25,  2.74s/it]  7%|▋         | 4/56 [00:10<02:16,  2.63s/it]  9%|▉         | 5/56 [00:13<02:15,  2.65s/it] 11%|█         | 6/56 [00:15<02:05,  2.51s/it] 12%|█▎        | 7/56 [00:18<02:04,  2.53s/it] 14%|█▍        | 8/56 [00:20<01:56,  2.42s/it] 16%|█▌        | 9/56 [00:22<01:52,  2.39s/it] 18%|█▊        | 10/56 [00:25<01:52,  2.44s/it]                                               {'loss': 1.3934, 'grad_norm': 9.828916549682617, 'learning_rate': 9.468163201617063e-06, 'epoch': 0.36}
 18%|█▊        | 10/56 [00:25<01:52,  2.44s/it] 20%|█▉        | 11/56 [00:27<01:46,  2.37s/it] 21%|██▏       | 12/56 [00:29<01:43,  2.34s/it] 23%|██▎       | 13/56 [00:32<01:45,  2.45s/it] 25%|██▌       | 14/56 [00:34<01:42,  2.44s/it] 27%|██▋       | 15/56 [00:37<01:40,  2.44s/it] 29%|██▊       | 16/56 [00:40<01:43,  2.60s/it] 30%|███       | 17/56 [00:43<01:47,  2.76s/it] 32%|███▏      | 18/56 [00:45<01:42,  2.70s/it] 34%|███▍      | 19/56 [00:48<01:42,  2.76s/it] 36%|███▌      | 20/56 [00:51<01:35,  2.65s/it]                                               {'loss': 1.0904, 'grad_norm': 15.185863494873047, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.71}
 36%|███▌      | 20/56 [00:51<01:35,  2.65s/it] 38%|███▊      | 21/56 [00:54<01:36,  2.75s/it] 39%|███▉      | 22/56 [00:56<01:32,  2.73s/it] 41%|████      | 23/56 [00:59<01:26,  2.61s/it] 43%|████▎     | 24/56 [01:01<01:18,  2.45s/it] 45%|████▍     | 25/56 [01:04<01:19,  2.56s/it] 46%|████▋     | 26/56 [01:06<01:14,  2.48s/it] 48%|████▊     | 27/56 [01:09<01:14,  2.57s/it] 50%|█████     | 28/56 [01:11<01:10,  2.51s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 29/56 [01:14<01:09,  2.58s/it] 54%|█████▎    | 30/56 [01:16<01:06,  2.54s/it]                                               {'loss': 0.8117, 'grad_norm': 7.349271774291992, 'learning_rate': 4.7092758554476215e-06, 'epoch': 1.07}
 54%|█████▎    | 30/56 [01:16<01:06,  2.54s/it] 55%|█████▌    | 31/56 [01:18<01:00,  2.43s/it] 57%|█████▋    | 32/56 [01:21<01:01,  2.58s/it] 59%|█████▉    | 33/56 [01:24<00:57,  2.50s/it] 61%|██████    | 34/56 [01:26<00:56,  2.58s/it] 62%|██████▎   | 35/56 [01:29<00:53,  2.55s/it] 64%|██████▍   | 36/56 [01:31<00:51,  2.56s/it] 66%|██████▌   | 37/56 [01:34<00:48,  2.57s/it] 68%|██████▊   | 38/56 [01:37<00:47,  2.64s/it] 70%|██████▉   | 39/56 [01:39<00:43,  2.55s/it] 71%|███████▏  | 40/56 [01:42<00:40,  2.53s/it]                                               {'loss': 0.6828, 'grad_norm': 7.437148094177246, 'learning_rate': 2.0142070414860704e-06, 'epoch': 1.43}
 71%|███████▏  | 40/56 [01:42<00:40,  2.53s/it] 73%|███████▎  | 41/56 [01:45<00:39,  2.63s/it] 75%|███████▌  | 42/56 [01:47<00:37,  2.69s/it] 77%|███████▋  | 43/56 [01:50<00:34,  2.65s/it] 79%|███████▊  | 44/56 [01:52<00:30,  2.50s/it] 80%|████████  | 45/56 [01:54<00:27,  2.47s/it] 82%|████████▏ | 46/56 [01:57<00:24,  2.47s/it] 84%|████████▍ | 47/56 [02:00<00:23,  2.56s/it] 86%|████████▌ | 48/56 [02:02<00:20,  2.61s/it] 88%|████████▊ | 49/56 [02:05<00:18,  2.69s/it] 89%|████████▉ | 50/56 [02:08<00:15,  2.61s/it]                                               {'loss': 0.6895, 'grad_norm': 5.0351104736328125, 'learning_rate': 3.015368960704584e-07, 'epoch': 1.79}
 89%|████████▉ | 50/56 [02:08<00:15,  2.61s/it] 91%|█████████ | 51/56 [02:11<00:13,  2.67s/it] 93%|█████████▎| 52/56 [02:13<00:10,  2.64s/it] 95%|█████████▍| 53/56 [02:15<00:07,  2.54s/it] 96%|█████████▋| 54/56 [02:18<00:04,  2.48s/it] 98%|█████████▊| 55/56 [02:20<00:02,  2.51s/it]100%|██████████| 56/56 [02:23<00:00,  2.57s/it]                                               {'train_runtime': 144.1693, 'train_samples_per_second': 4.661, 'train_steps_per_second': 0.388, 'train_loss': 0.9230939320155552, 'epoch': 2.0}
100%|██████████| 56/56 [02:23<00:00,  2.57s/it]100%|██████████| 56/56 [02:23<00:00,  2.57s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<00:52,  2.90it/s]  1%|▏         | 2/153 [00:00<00:43,  3.48it/s]  2%|▏         | 3/153 [00:00<00:42,  3.55it/s]  3%|▎         | 4/153 [00:01<00:41,  3.57it/s]  3%|▎         | 5/153 [00:01<00:41,  3.56it/s]  4%|▍         | 6/153 [00:01<00:39,  3.76it/s]  5%|▍         | 7/153 [00:01<00:38,  3.82it/s]  5%|▌         | 8/153 [00:02<00:38,  3.74it/s]  6%|▌         | 9/153 [00:02<00:39,  3.69it/s]  7%|▋         | 10/153 [00:02<00:39,  3.64it/s]  7%|▋         | 11/153 [00:02<00:37,  3.79it/s]  8%|▊         | 12/153 [00:03<00:36,  3.89it/s]  8%|▊         | 13/153 [00:03<00:36,  3.87it/s]  9%|▉         | 14/153 [00:03<00:37,  3.72it/s] 10%|▉         | 15/153 [00:04<00:36,  3.78it/s] 10%|█         | 16/153 [00:04<00:36,  3.73it/s] 11%|█         | 17/153 [00:04<00:35,  3.84it/s] 12%|█▏        | 18/153 [00:04<00:35,  3.76it/s] 12%|█▏        | 19/153 [00:05<00:36,  3.70it/s] 13%|█▎        | 20/153 [00:05<00:36,  3.65it/s] 14%|█▎        | 21/153 [00:05<00:34,  3.79it/s] 14%|█▍        | 22/153 [00:05<00:34,  3.83it/s] 15%|█▌        | 23/153 [00:06<00:35,  3.69it/s] 16%|█▌        | 24/153 [00:06<00:34,  3.71it/s] 16%|█▋        | 25/153 [00:06<00:34,  3.66it/s] 17%|█▋        | 26/153 [00:06<00:33,  3.81it/s] 18%|█▊        | 27/153 [00:07<00:32,  3.84it/s] 18%|█▊        | 28/153 [00:07<00:33,  3.76it/s] 19%|█▉        | 29/153 [00:07<00:33,  3.70it/s] 20%|█▉        | 30/153 [00:08<00:33,  3.65it/s] 20%|██        | 31/153 [00:08<00:32,  3.79it/s] 21%|██        | 32/153 [00:08<00:31,  3.83it/s] 22%|██▏       | 33/153 [00:08<00:31,  3.75it/s] 22%|██▏       | 34/153 [00:09<00:32,  3.70it/s] 23%|██▎       | 35/153 [00:09<00:32,  3.65it/s] 24%|██▎       | 36/153 [00:09<00:30,  3.79it/s] 24%|██▍       | 37/153 [00:09<00:30,  3.82it/s] 25%|██▍       | 38/153 [00:10<00:31,  3.69it/s] 25%|██▌       | 39/153 [00:10<00:30,  3.74it/s] 26%|██▌       | 40/153 [00:10<00:30,  3.69it/s] 27%|██▋       | 41/153 [00:10<00:29,  3.79it/s] 27%|██▋       | 42/153 [00:11<00:29,  3.73it/s] 28%|██▊       | 43/153 [00:11<00:29,  3.68it/s] 29%|██▉       | 44/153 [00:11<00:30,  3.63it/s] 29%|██▉       | 45/153 [00:12<00:28,  3.77it/s] 30%|███       | 46/153 [00:12<00:28,  3.81it/s] 31%|███       | 47/153 [00:12<00:28,  3.73it/s] 31%|███▏      | 48/153 [00:12<00:28,  3.68it/s] 32%|███▏      | 49/153 [00:13<00:28,  3.63it/s] 33%|███▎      | 50/153 [00:13<00:27,  3.77it/s] 33%|███▎      | 51/153 [00:13<00:26,  3.81it/s] 34%|███▍      | 52/153 [00:13<00:26,  3.74it/s] 35%|███▍      | 53/153 [00:14<00:27,  3.69it/s] 35%|███▌      | 54/153 [00:14<00:27,  3.64it/s] 36%|███▌      | 55/153 [00:14<00:25,  3.79it/s] 37%|███▋      | 56/153 [00:15<00:25,  3.83it/s] 37%|███▋      | 57/153 [00:15<00:25,  3.76it/s] 38%|███▊      | 58/153 [00:15<00:25,  3.69it/s] 39%|███▊      | 59/153 [00:15<00:25,  3.64it/s] 39%|███▉      | 60/153 [00:16<00:24,  3.78it/s] 40%|███▉      | 61/153 [00:16<00:24,  3.81it/s] 41%|████      | 62/153 [00:16<00:24,  3.73it/s] 41%|████      | 63/153 [00:16<00:24,  3.68it/s] 42%|████▏     | 64/153 [00:17<00:24,  3.63it/s] 42%|████▏     | 65/153 [00:17<00:23,  3.77it/s] 43%|████▎     | 66/153 [00:17<00:22,  3.82it/s] 44%|████▍     | 67/153 [00:17<00:23,  3.74it/s] 44%|████▍     | 68/153 [00:18<00:23,  3.68it/s] 45%|████▌     | 69/153 [00:18<00:23,  3.63it/s] 46%|████▌     | 70/153 [00:18<00:21,  3.78it/s] 46%|████▋     | 71/153 [00:19<00:21,  3.81it/s] 47%|████▋     | 72/153 [00:19<00:21,  3.73it/s] 48%|████▊     | 73/153 [00:19<00:21,  3.68it/s] 48%|████▊     | 74/153 [00:19<00:21,  3.63it/s] 49%|████▉     | 75/153 [00:20<00:20,  3.77it/s] 50%|████▉     | 76/153 [00:20<00:20,  3.82it/s] 50%|█████     | 77/153 [00:20<00:20,  3.75it/s] 51%|█████     | 78/153 [00:20<00:20,  3.69it/s] 52%|█████▏    | 79/153 [00:21<00:20,  3.63it/s] 52%|█████▏    | 80/153 [00:21<00:19,  3.78it/s] 53%|█████▎    | 81/153 [00:21<00:18,  3.81it/s] 54%|█████▎    | 82/153 [00:21<00:19,  3.73it/s] 54%|█████▍    | 83/153 [00:22<00:19,  3.68it/s] 55%|█████▍    | 84/153 [00:22<00:19,  3.63it/s] 56%|█████▌    | 85/153 [00:22<00:18,  3.77it/s] 56%|█████▌    | 86/153 [00:23<00:17,  3.82it/s] 57%|█████▋    | 87/153 [00:23<00:17,  3.75it/s] 58%|█████▊    | 88/153 [00:23<00:17,  3.69it/s] 58%|█████▊    | 89/153 [00:23<00:17,  3.64it/s] 59%|█████▉    | 90/153 [00:24<00:16,  3.78it/s] 59%|█████▉    | 91/153 [00:24<00:16,  3.81it/s] 60%|██████    | 92/153 [00:24<00:16,  3.73it/s] 61%|██████    | 93/153 [00:24<00:16,  3.67it/s] 61%|██████▏   | 94/153 [00:25<00:16,  3.62it/s] 62%|██████▏   | 95/153 [00:25<00:15,  3.76it/s] 63%|██████▎   | 96/153 [00:25<00:15,  3.80it/s] 63%|██████▎   | 97/153 [00:26<00:15,  3.72it/s] 64%|██████▍   | 98/153 [00:26<00:14,  3.67it/s] 65%|██████▍   | 99/153 [00:26<00:14,  3.62it/s] 65%|██████▌   | 100/153 [00:26<00:14,  3.77it/s] 66%|██████▌   | 101/153 [00:27<00:13,  3.81it/s] 67%|██████▋   | 102/153 [00:27<00:13,  3.72it/s] 67%|██████▋   | 103/153 [00:27<00:13,  3.67it/s] 68%|██████▊   | 104/153 [00:27<00:13,  3.62it/s] 69%|██████▊   | 105/153 [00:28<00:12,  3.76it/s] 69%|██████▉   | 106/153 [00:28<00:12,  3.79it/s] 70%|██████▉   | 107/153 [00:28<00:12,  3.73it/s] 71%|███████   | 108/153 [00:29<00:12,  3.66it/s] 71%|███████   | 109/153 [00:29<00:12,  3.64it/s] 72%|███████▏  | 110/153 [00:29<00:11,  3.75it/s] 73%|███████▎  | 111/153 [00:29<00:11,  3.81it/s] 73%|███████▎  | 112/153 [00:30<00:10,  3.73it/s] 74%|███████▍  | 113/153 [00:30<00:10,  3.67it/s] 75%|███████▍  | 114/153 [00:30<00:10,  3.62it/s] 75%|███████▌  | 115/153 [00:30<00:10,  3.77it/s] 76%|███████▌  | 116/153 [00:31<00:09,  3.81it/s] 76%|███████▋  | 117/153 [00:31<00:09,  3.72it/s] 77%|███████▋  | 118/153 [00:31<00:09,  3.68it/s] 78%|███████▊  | 119/153 [00:31<00:09,  3.62it/s] 78%|███████▊  | 120/153 [00:32<00:08,  3.76it/s] 79%|███████▉  | 121/153 [00:32<00:08,  3.80it/s] 80%|███████▉  | 122/153 [00:32<00:08,  3.72it/s] 80%|████████  | 123/153 [00:33<00:08,  3.66it/s] 81%|████████  | 124/153 [00:33<00:07,  3.65it/s] 82%|████████▏ | 125/153 [00:33<00:07,  3.74it/s] 82%|████████▏ | 126/153 [00:33<00:07,  3.81it/s] 83%|████████▎ | 127/153 [00:34<00:06,  3.74it/s] 84%|████████▎ | 128/153 [00:34<00:06,  3.67it/s] 84%|████████▍ | 129/153 [00:34<00:06,  3.62it/s] 85%|████████▍ | 130/153 [00:34<00:06,  3.76it/s] 86%|████████▌ | 131/153 [00:35<00:05,  3.80it/s] 86%|████████▋ | 132/153 [00:35<00:05,  3.73it/s] 87%|████████▋ | 133/153 [00:35<00:05,  3.67it/s] 88%|████████▊ | 134/153 [00:36<00:05,  3.62it/s] 88%|████████▊ | 135/153 [00:36<00:04,  3.76it/s] 89%|████████▉ | 136/153 [00:36<00:04,  3.78it/s] 90%|████████▉ | 137/153 [00:36<00:04,  3.71it/s] 90%|█████████ | 138/153 [00:37<00:04,  3.66it/s] 91%|█████████ | 139/153 [00:37<00:03,  3.61it/s] 92%|█████████▏| 140/153 [00:37<00:03,  3.75it/s] 92%|█████████▏| 141/153 [00:37<00:03,  3.80it/s] 93%|█████████▎| 142/153 [00:38<00:02,  3.73it/s] 93%|█████████▎| 143/153 [00:38<00:02,  3.67it/s] 94%|█████████▍| 144/153 [00:38<00:02,  3.62it/s] 95%|█████████▍| 145/153 [00:38<00:02,  3.76it/s] 95%|█████████▌| 146/153 [00:39<00:01,  3.79it/s] 96%|█████████▌| 147/153 [00:39<00:01,  3.72it/s] 97%|█████████▋| 148/153 [00:39<00:01,  3.67it/s] 97%|█████████▋| 149/153 [00:40<00:01,  3.61it/s] 98%|█████████▊| 150/153 [00:40<00:00,  3.75it/s] 99%|█████████▊| 151/153 [00:40<00:00,  3.79it/s] 99%|█████████▉| 152/153 [00:40<00:00,  3.71it/s]100%|██████████| 153/153 [00:41<00:00,  3.66it/s]accuracy:  0.6993464052287581
100%|██████████| 153/153 [00:43<00:00,  3.51it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 152
Training dataset size: 384, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 152
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:19:41,592] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2025-03-12 07:19:41,693] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:19:41,972] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<02:45,  2.63s/it]  3%|▎         | 2/64 [00:05<02:33,  2.48s/it]  5%|▍         | 3/64 [00:08<02:48,  2.76s/it]  6%|▋         | 4/64 [00:10<02:33,  2.56s/it]  8%|▊         | 5/64 [00:12<02:30,  2.55s/it]  9%|▉         | 6/64 [00:15<02:22,  2.45s/it] 11%|█         | 7/64 [00:17<02:22,  2.50s/it] 12%|█▎        | 8/64 [00:20<02:19,  2.48s/it] 14%|█▍        | 9/64 [00:22<02:11,  2.40s/it] 16%|█▌        | 10/64 [00:25<02:15,  2.51s/it]                                               {'loss': 1.0994, 'grad_norm': 7.0441508293151855, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:25<02:15,  2.51s/it] 17%|█▋        | 11/64 [00:27<02:16,  2.58s/it] 19%|█▉        | 12/64 [00:30<02:10,  2.51s/it] 20%|██        | 13/64 [00:32<02:07,  2.51s/it] 22%|██▏       | 14/64 [00:34<01:57,  2.35s/it] 23%|██▎       | 15/64 [00:36<01:51,  2.27s/it] 25%|██▌       | 16/64 [00:39<01:52,  2.35s/it] 27%|██▋       | 17/64 [00:41<01:53,  2.41s/it] 28%|██▊       | 18/64 [00:44<01:48,  2.35s/it] 30%|██▉       | 19/64 [00:46<01:42,  2.28s/it] 31%|███▏      | 20/64 [00:48<01:39,  2.27s/it]                                               {'loss': 0.9514, 'grad_norm': 9.144801139831543, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:48<01:39,  2.27s/it] 33%|███▎      | 21/64 [00:50<01:34,  2.20s/it] 34%|███▍      | 22/64 [00:53<01:45,  2.52s/it] 36%|███▌      | 23/64 [00:56<01:41,  2.48s/it] 38%|███▊      | 24/64 [00:58<01:38,  2.46s/it] 39%|███▉      | 25/64 [01:00<01:32,  2.38s/it] 41%|████      | 26/64 [01:03<01:36,  2.54s/it] 42%|████▏     | 27/64 [01:05<01:31,  2.47s/it] 44%|████▍     | 28/64 [01:08<01:25,  2.39s/it] 45%|████▌     | 29/64 [01:10<01:23,  2.40s/it] 47%|████▋     | 30/64 [01:13<01:22,  2.42s/it]                                               {'loss': 0.7081, 'grad_norm': 5.694999694824219, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:13<01:22,  2.42s/it] 48%|████▊     | 31/64 [01:14<01:12,  2.21s/it] 50%|█████     | 32/64 [01:16<01:09,  2.18s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:19<01:11,  2.30s/it] 53%|█████▎    | 34/64 [01:22<01:11,  2.39s/it] 55%|█████▍    | 35/64 [01:24<01:06,  2.29s/it] 56%|█████▋    | 36/64 [01:26<01:04,  2.29s/it] 58%|█████▊    | 37/64 [01:29<01:04,  2.38s/it] 59%|█████▉    | 38/64 [01:31<00:59,  2.30s/it] 61%|██████    | 39/64 [01:33<00:56,  2.26s/it] 62%|██████▎   | 40/64 [01:35<00:53,  2.22s/it]                                               {'loss': 0.6291, 'grad_norm': 7.3257622718811035, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:35<00:53,  2.22s/it] 64%|██████▍   | 41/64 [01:37<00:51,  2.24s/it] 66%|██████▌   | 42/64 [01:39<00:48,  2.22s/it] 67%|██████▋   | 43/64 [01:42<00:50,  2.38s/it] 69%|██████▉   | 44/64 [01:44<00:46,  2.35s/it] 70%|███████   | 45/64 [01:47<00:45,  2.41s/it] 72%|███████▏  | 46/64 [01:50<00:44,  2.46s/it] 73%|███████▎  | 47/64 [01:52<00:43,  2.53s/it] 75%|███████▌  | 48/64 [01:54<00:38,  2.41s/it] 77%|███████▋  | 49/64 [01:56<00:34,  2.30s/it] 78%|███████▊  | 50/64 [01:58<00:31,  2.23s/it]                                               {'loss': 0.6547, 'grad_norm': 6.141257286071777, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:58<00:31,  2.23s/it] 80%|███████▉  | 51/64 [02:00<00:27,  2.15s/it] 81%|████████▏ | 52/64 [02:03<00:26,  2.20s/it] 83%|████████▎ | 53/64 [02:05<00:24,  2.24s/it] 84%|████████▍ | 54/64 [02:08<00:22,  2.30s/it] 86%|████████▌ | 55/64 [02:10<00:20,  2.33s/it] 88%|████████▊ | 56/64 [02:12<00:18,  2.26s/it] 89%|████████▉ | 57/64 [02:14<00:16,  2.29s/it] 91%|█████████ | 58/64 [02:17<00:14,  2.36s/it] 92%|█████████▏| 59/64 [02:19<00:11,  2.37s/it] 94%|█████████▍| 60/64 [02:21<00:09,  2.26s/it]                                               {'loss': 0.7067, 'grad_norm': 7.208560943603516, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:21<00:09,  2.26s/it] 95%|█████████▌| 61/64 [02:23<00:06,  2.22s/it] 97%|█████████▋| 62/64 [02:26<00:04,  2.29s/it] 98%|█████████▊| 63/64 [02:28<00:02,  2.24s/it]100%|██████████| 64/64 [02:30<00:00,  2.25s/it]                                               {'train_runtime': 151.4046, 'train_samples_per_second': 5.072, 'train_steps_per_second': 0.423, 'train_loss': 0.7746273018419743, 'epoch': 2.0}
100%|██████████| 64/64 [02:31<00:00,  2.25s/it]100%|██████████| 64/64 [02:31<00:00,  2.36s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:04,  3.25it/s]  1%|          | 2/212 [00:00<00:56,  3.74it/s]  1%|▏         | 3/212 [00:00<00:53,  3.94it/s]  2%|▏         | 4/212 [00:01<00:51,  4.03it/s]  2%|▏         | 5/212 [00:01<00:50,  4.08it/s]  3%|▎         | 6/212 [00:01<00:50,  4.12it/s]  3%|▎         | 7/212 [00:01<00:49,  4.11it/s]  4%|▍         | 8/212 [00:02<00:51,  3.98it/s]  4%|▍         | 9/212 [00:02<00:51,  3.91it/s]  5%|▍         | 10/212 [00:02<00:51,  3.90it/s]  5%|▌         | 11/212 [00:02<00:51,  3.91it/s]  6%|▌         | 12/212 [00:03<00:52,  3.79it/s]  6%|▌         | 13/212 [00:03<00:51,  3.83it/s]  7%|▋         | 14/212 [00:03<00:52,  3.76it/s]  7%|▋         | 15/212 [00:03<00:51,  3.84it/s]  8%|▊         | 16/212 [00:04<00:52,  3.75it/s]  8%|▊         | 17/212 [00:04<00:51,  3.79it/s]  8%|▊         | 18/212 [00:04<00:51,  3.73it/s]  9%|▉         | 19/212 [00:04<00:50,  3.81it/s]  9%|▉         | 20/212 [00:05<00:51,  3.72it/s] 10%|▉         | 21/212 [00:05<00:50,  3.78it/s] 10%|█         | 22/212 [00:05<00:51,  3.72it/s] 11%|█         | 23/212 [00:05<00:50,  3.78it/s] 11%|█▏        | 24/212 [00:06<00:50,  3.70it/s] 12%|█▏        | 25/212 [00:06<00:49,  3.77it/s] 12%|█▏        | 26/212 [00:06<00:50,  3.71it/s] 13%|█▎        | 27/212 [00:07<00:48,  3.78it/s] 13%|█▎        | 28/212 [00:07<00:49,  3.70it/s] 14%|█▎        | 29/212 [00:07<00:48,  3.76it/s] 14%|█▍        | 30/212 [00:07<00:49,  3.71it/s] 15%|█▍        | 31/212 [00:08<00:48,  3.77it/s] 15%|█▌        | 32/212 [00:08<00:48,  3.69it/s] 16%|█▌        | 33/212 [00:08<00:47,  3.75it/s] 16%|█▌        | 34/212 [00:08<00:48,  3.70it/s] 17%|█▋        | 35/212 [00:09<00:46,  3.78it/s] 17%|█▋        | 36/212 [00:09<00:47,  3.71it/s] 17%|█▋        | 37/212 [00:09<00:46,  3.77it/s] 18%|█▊        | 38/212 [00:10<00:46,  3.72it/s] 18%|█▊        | 39/212 [00:10<00:45,  3.79it/s] 19%|█▉        | 40/212 [00:10<00:46,  3.71it/s] 19%|█▉        | 41/212 [00:10<00:45,  3.77it/s] 20%|█▉        | 42/212 [00:11<00:45,  3.72it/s] 20%|██        | 43/212 [00:11<00:44,  3.79it/s] 21%|██        | 44/212 [00:11<00:45,  3.70it/s] 21%|██        | 45/212 [00:11<00:44,  3.76it/s] 22%|██▏       | 46/212 [00:12<00:44,  3.71it/s] 22%|██▏       | 47/212 [00:12<00:43,  3.79it/s] 23%|██▎       | 48/212 [00:12<00:44,  3.70it/s] 23%|██▎       | 49/212 [00:12<00:43,  3.76it/s] 24%|██▎       | 50/212 [00:13<00:43,  3.70it/s] 24%|██▍       | 51/212 [00:13<00:42,  3.77it/s] 25%|██▍       | 52/212 [00:13<00:43,  3.70it/s] 25%|██▌       | 53/212 [00:14<00:42,  3.76it/s] 25%|██▌       | 54/212 [00:14<00:42,  3.71it/s] 26%|██▌       | 55/212 [00:14<00:41,  3.78it/s] 26%|██▋       | 56/212 [00:14<00:42,  3.69it/s] 27%|██▋       | 57/212 [00:15<00:41,  3.75it/s] 27%|██▋       | 58/212 [00:15<00:41,  3.70it/s] 28%|██▊       | 59/212 [00:15<00:40,  3.76it/s] 28%|██▊       | 60/212 [00:15<00:41,  3.68it/s] 29%|██▉       | 61/212 [00:16<00:40,  3.75it/s] 29%|██▉       | 62/212 [00:16<00:40,  3.70it/s] 30%|██▉       | 63/212 [00:16<00:39,  3.76it/s] 30%|███       | 64/212 [00:16<00:40,  3.69it/s] 31%|███       | 65/212 [00:17<00:39,  3.76it/s] 31%|███       | 66/212 [00:17<00:39,  3.71it/s] 32%|███▏      | 67/212 [00:17<00:38,  3.78it/s] 32%|███▏      | 68/212 [00:18<00:38,  3.70it/s] 33%|███▎      | 69/212 [00:18<00:38,  3.75it/s] 33%|███▎      | 70/212 [00:18<00:38,  3.69it/s] 33%|███▎      | 71/212 [00:18<00:37,  3.75it/s] 34%|███▍      | 72/212 [00:19<00:38,  3.67it/s] 34%|███▍      | 73/212 [00:19<00:37,  3.74it/s] 35%|███▍      | 74/212 [00:19<00:37,  3.68it/s] 35%|███▌      | 75/212 [00:19<00:36,  3.77it/s] 36%|███▌      | 76/212 [00:20<00:36,  3.70it/s] 36%|███▋      | 77/212 [00:20<00:35,  3.77it/s] 37%|███▋      | 78/212 [00:20<00:36,  3.72it/s] 37%|███▋      | 79/212 [00:20<00:35,  3.79it/s] 38%|███▊      | 80/212 [00:21<00:35,  3.70it/s] 38%|███▊      | 81/212 [00:21<00:34,  3.75it/s] 39%|███▊      | 82/212 [00:21<00:35,  3.69it/s] 39%|███▉      | 83/212 [00:22<00:34,  3.75it/s] 40%|███▉      | 84/212 [00:22<00:34,  3.67it/s] 40%|████      | 85/212 [00:22<00:34,  3.73it/s] 41%|████      | 86/212 [00:22<00:34,  3.68it/s] 41%|████      | 87/212 [00:23<00:33,  3.75it/s] 42%|████▏     | 88/212 [00:23<00:33,  3.66it/s] 42%|████▏     | 89/212 [00:23<00:33,  3.72it/s] 42%|████▏     | 90/212 [00:23<00:33,  3.68it/s] 43%|████▎     | 91/212 [00:24<00:32,  3.73it/s] 43%|████▎     | 92/212 [00:24<00:32,  3.66it/s] 44%|████▍     | 93/212 [00:24<00:32,  3.69it/s] 44%|████▍     | 94/212 [00:25<00:31,  3.70it/s] 45%|████▍     | 95/212 [00:25<00:31,  3.75it/s] 45%|████▌     | 96/212 [00:25<00:31,  3.67it/s] 46%|████▌     | 97/212 [00:25<00:31,  3.70it/s] 46%|████▌     | 98/212 [00:26<00:30,  3.69it/s] 47%|████▋     | 99/212 [00:26<00:30,  3.75it/s] 47%|████▋     | 100/212 [00:26<00:30,  3.67it/s] 48%|████▊     | 101/212 [00:26<00:29,  3.70it/s] 48%|████▊     | 102/212 [00:27<00:29,  3.69it/s] 49%|████▊     | 103/212 [00:27<00:29,  3.75it/s] 49%|████▉     | 104/212 [00:27<00:29,  3.68it/s] 50%|████▉     | 105/212 [00:28<00:28,  3.74it/s] 50%|█████     | 106/212 [00:28<00:28,  3.68it/s] 50%|█████     | 107/212 [00:28<00:27,  3.76it/s] 51%|█████     | 108/212 [00:28<00:28,  3.68it/s] 51%|█████▏    | 109/212 [00:29<00:27,  3.70it/s] 52%|█████▏    | 110/212 [00:29<00:27,  3.69it/s] 52%|█████▏    | 111/212 [00:29<00:26,  3.75it/s] 53%|█████▎    | 112/212 [00:29<00:27,  3.67it/s] 53%|█████▎    | 113/212 [00:30<00:26,  3.71it/s] 54%|█████▍    | 114/212 [00:30<00:26,  3.69it/s] 54%|█████▍    | 115/212 [00:30<00:25,  3.74it/s] 55%|█████▍    | 116/212 [00:30<00:26,  3.66it/s] 55%|█████▌    | 117/212 [00:31<00:25,  3.69it/s] 56%|█████▌    | 118/212 [00:31<00:25,  3.69it/s] 56%|█████▌    | 119/212 [00:31<00:24,  3.74it/s] 57%|█████▋    | 120/212 [00:32<00:25,  3.67it/s] 57%|█████▋    | 121/212 [00:32<00:24,  3.69it/s] 58%|█████▊    | 122/212 [00:32<00:24,  3.70it/s] 58%|█████▊    | 123/212 [00:32<00:23,  3.75it/s] 58%|█████▊    | 124/212 [00:33<00:23,  3.68it/s] 59%|█████▉    | 125/212 [00:33<00:23,  3.67it/s] 59%|█████▉    | 126/212 [00:33<00:22,  3.79it/s] 60%|█████▉    | 127/212 [00:33<00:21,  3.89it/s] 60%|██████    | 128/212 [00:34<00:21,  3.96it/s] 61%|██████    | 129/212 [00:34<00:20,  4.00it/s] 61%|██████▏   | 130/212 [00:34<00:20,  4.03it/s] 62%|██████▏   | 131/212 [00:34<00:20,  4.05it/s] 62%|██████▏   | 132/212 [00:35<00:19,  4.07it/s] 63%|██████▎   | 133/212 [00:35<00:19,  4.08it/s] 63%|██████▎   | 134/212 [00:35<00:19,  4.10it/s] 64%|██████▎   | 135/212 [00:35<00:18,  4.11it/s] 64%|██████▍   | 136/212 [00:36<00:18,  4.11it/s] 65%|██████▍   | 137/212 [00:36<00:18,  4.10it/s] 65%|██████▌   | 138/212 [00:36<00:18,  4.10it/s] 66%|██████▌   | 139/212 [00:36<00:17,  4.11it/s] 66%|██████▌   | 140/212 [00:37<00:17,  4.11it/s] 67%|██████▋   | 141/212 [00:37<00:17,  4.12it/s] 67%|██████▋   | 142/212 [00:37<00:16,  4.13it/s] 67%|██████▋   | 143/212 [00:37<00:16,  4.12it/s] 68%|██████▊   | 144/212 [00:38<00:16,  4.11it/s] 68%|██████▊   | 145/212 [00:38<00:16,  4.11it/s] 69%|██████▉   | 146/212 [00:38<00:16,  4.11it/s] 69%|██████▉   | 147/212 [00:38<00:16,  4.06it/s] 70%|██████▉   | 148/212 [00:39<00:16,  3.93it/s] 70%|███████   | 149/212 [00:39<00:16,  3.81it/s] 71%|███████   | 150/212 [00:39<00:15,  3.90it/s] 71%|███████   | 151/212 [00:39<00:15,  3.94it/s] 72%|███████▏  | 152/212 [00:40<00:15,  3.82it/s] 72%|███████▏  | 153/212 [00:40<00:15,  3.74it/s] 73%|███████▎  | 154/212 [00:40<00:15,  3.72it/s] 73%|███████▎  | 155/212 [00:40<00:15,  3.72it/s] 74%|███████▎  | 156/212 [00:41<00:14,  3.83it/s] 74%|███████▍  | 157/212 [00:41<00:14,  3.75it/s] 75%|███████▍  | 158/212 [00:41<00:14,  3.69it/s] 75%|███████▌  | 159/212 [00:42<00:14,  3.71it/s] 75%|███████▌  | 160/212 [00:42<00:14,  3.64it/s] 76%|███████▌  | 161/212 [00:42<00:13,  3.77it/s] 76%|███████▋  | 162/212 [00:42<00:12,  3.86it/s] 77%|███████▋  | 163/212 [00:43<00:13,  3.77it/s] 77%|███████▋  | 164/212 [00:43<00:12,  3.71it/s] 78%|███████▊  | 165/212 [00:43<00:12,  3.71it/s] 78%|███████▊  | 166/212 [00:43<00:12,  3.73it/s] 79%|███████▉  | 167/212 [00:44<00:11,  3.84it/s] 79%|███████▉  | 168/212 [00:44<00:11,  3.74it/s] 80%|███████▉  | 169/212 [00:44<00:11,  3.70it/s] 80%|████████  | 170/212 [00:44<00:11,  3.71it/s] 81%|████████  | 171/212 [00:45<00:11,  3.63it/s] 81%|████████  | 172/212 [00:45<00:10,  3.77it/s] 82%|████████▏ | 173/212 [00:45<00:10,  3.85it/s] 82%|████████▏ | 174/212 [00:46<00:10,  3.76it/s] 83%|████████▎ | 175/212 [00:46<00:10,  3.70it/s] 83%|████████▎ | 176/212 [00:46<00:09,  3.67it/s] 83%|████████▎ | 177/212 [00:46<00:09,  3.73it/s] 84%|████████▍ | 178/212 [00:47<00:08,  3.82it/s] 84%|████████▍ | 179/212 [00:47<00:08,  3.79it/s] 85%|████████▍ | 180/212 [00:47<00:08,  3.68it/s] 85%|████████▌ | 181/212 [00:47<00:08,  3.71it/s] 86%|████████▌ | 182/212 [00:48<00:08,  3.63it/s] 86%|████████▋ | 183/212 [00:48<00:07,  3.76it/s] 87%|████████▋ | 184/212 [00:48<00:07,  3.81it/s] 87%|████████▋ | 185/212 [00:48<00:07,  3.74it/s] 88%|████████▊ | 186/212 [00:49<00:07,  3.69it/s] 88%|████████▊ | 187/212 [00:49<00:06,  3.61it/s] 89%|████████▊ | 188/212 [00:49<00:06,  3.75it/s] 89%|████████▉ | 189/212 [00:50<00:06,  3.83it/s] 90%|████████▉ | 190/212 [00:50<00:05,  3.75it/s] 90%|█████████ | 191/212 [00:50<00:05,  3.70it/s] 91%|█████████ | 192/212 [00:50<00:05,  3.69it/s] 91%|█████████ | 193/212 [00:51<00:05,  3.72it/s] 92%|█████████▏| 194/212 [00:51<00:04,  3.83it/s] 92%|█████████▏| 195/212 [00:51<00:04,  3.79it/s] 92%|█████████▏| 196/212 [00:51<00:04,  3.68it/s] 93%|█████████▎| 197/212 [00:52<00:04,  3.70it/s] 93%|█████████▎| 198/212 [00:52<00:03,  3.62it/s] 94%|█████████▍| 199/212 [00:52<00:03,  3.76it/s] 94%|█████████▍| 200/212 [00:52<00:03,  3.84it/s] 95%|█████████▍| 201/212 [00:53<00:02,  3.76it/s] 95%|█████████▌| 202/212 [00:53<00:02,  3.70it/s] 96%|█████████▌| 203/212 [00:53<00:02,  3.70it/s] 96%|█████████▌| 204/212 [00:54<00:02,  3.70it/s] 97%|█████████▋| 205/212 [00:54<00:01,  3.80it/s] 97%|█████████▋| 206/212 [00:54<00:01,  3.71it/s] 98%|█████████▊| 207/212 [00:54<00:01,  3.69it/s] 98%|█████████▊| 208/212 [00:55<00:01,  3.70it/s] 99%|█████████▊| 209/212 [00:55<00:00,  3.63it/s] 99%|█████████▉| 210/212 [00:55<00:00,  3.75it/s]100%|█████████▉| 211/212 [00:55<00:00,  3.83it/s]100%|██████████| 212/212 [00:56<00:00,  3.75it/s]accuracy:  0.6839622641509434
100%|██████████| 212/212 [00:59<00:00,  3.56it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 135
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:23:41,505] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:23:41,851] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2025-03-12 07:23:42,342] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<02:56,  2.79s/it]  3%|▎         | 2/64 [00:05<02:40,  2.60s/it]  5%|▍         | 3/64 [00:07<02:23,  2.35s/it]  6%|▋         | 4/64 [00:09<02:22,  2.37s/it]  8%|▊         | 5/64 [00:11<02:16,  2.31s/it]  9%|▉         | 6/64 [00:14<02:11,  2.26s/it] 11%|█         | 7/64 [00:16<02:02,  2.16s/it] 12%|█▎        | 8/64 [00:18<02:05,  2.25s/it] 14%|█▍        | 9/64 [00:20<02:04,  2.27s/it] 16%|█▌        | 10/64 [00:23<02:09,  2.39s/it]                                               {'loss': 0.3979, 'grad_norm': 2.6679649353027344, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:23<02:09,  2.39s/it] 17%|█▋        | 11/64 [00:25<02:02,  2.31s/it] 19%|█▉        | 12/64 [00:27<02:00,  2.32s/it] 20%|██        | 13/64 [00:30<01:54,  2.25s/it] 22%|██▏       | 14/64 [00:32<01:50,  2.21s/it] 23%|██▎       | 15/64 [00:34<01:56,  2.37s/it] 25%|██▌       | 16/64 [00:36<01:49,  2.29s/it] 27%|██▋       | 17/64 [00:39<01:46,  2.26s/it] 28%|██▊       | 18/64 [00:41<01:49,  2.37s/it] 30%|██▉       | 19/64 [00:44<01:44,  2.33s/it] 31%|███▏      | 20/64 [00:46<01:42,  2.32s/it]                                               {'loss': 0.3744, 'grad_norm': 7.562658309936523, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:46<01:42,  2.32s/it] 33%|███▎      | 21/64 [00:48<01:41,  2.36s/it] 34%|███▍      | 22/64 [00:51<01:43,  2.46s/it] 36%|███▌      | 23/64 [00:53<01:40,  2.45s/it] 38%|███▊      | 24/64 [00:56<01:34,  2.35s/it] 39%|███▉      | 25/64 [00:58<01:31,  2.35s/it] 41%|████      | 26/64 [01:00<01:31,  2.40s/it] 42%|████▏     | 27/64 [01:03<01:26,  2.34s/it] 44%|████▍     | 28/64 [01:05<01:20,  2.24s/it] 45%|████▌     | 29/64 [01:07<01:18,  2.23s/it] 47%|████▋     | 30/64 [01:09<01:20,  2.36s/it]                                               {'loss': 0.6247, 'grad_norm': 7.8657073974609375, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:09<01:20,  2.36s/it] 48%|████▊     | 31/64 [01:12<01:15,  2.30s/it] 50%|█████     | 32/64 [01:14<01:16,  2.39s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:17<01:13,  2.38s/it] 53%|█████▎    | 34/64 [01:19<01:09,  2.31s/it] 55%|█████▍    | 35/64 [01:21<01:05,  2.25s/it] 56%|█████▋    | 36/64 [01:23<01:02,  2.22s/it] 58%|█████▊    | 37/64 [01:25<01:01,  2.27s/it] 59%|█████▉    | 38/64 [01:28<00:58,  2.26s/it] 61%|██████    | 39/64 [01:30<00:56,  2.27s/it] 62%|██████▎   | 40/64 [01:33<00:58,  2.44s/it]                                               {'loss': 0.4089, 'grad_norm': 5.552337646484375, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:33<00:58,  2.44s/it] 64%|██████▍   | 41/64 [01:35<00:57,  2.49s/it] 66%|██████▌   | 42/64 [01:37<00:52,  2.38s/it] 67%|██████▋   | 43/64 [01:40<00:48,  2.32s/it] 69%|██████▉   | 44/64 [01:42<00:47,  2.38s/it] 70%|███████   | 45/64 [01:45<00:45,  2.38s/it] 72%|███████▏  | 46/64 [01:47<00:42,  2.37s/it] 73%|███████▎  | 47/64 [01:49<00:41,  2.43s/it] 75%|███████▌  | 48/64 [01:52<00:38,  2.39s/it] 77%|███████▋  | 49/64 [01:54<00:34,  2.29s/it] 78%|███████▊  | 50/64 [01:56<00:31,  2.24s/it]                                               {'loss': 0.4133, 'grad_norm': 5.185201168060303, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:56<00:31,  2.24s/it] 80%|███████▉  | 51/64 [01:58<00:29,  2.26s/it] 81%|████████▏ | 52/64 [02:01<00:27,  2.27s/it] 83%|████████▎ | 53/64 [02:03<00:24,  2.21s/it] 84%|████████▍ | 54/64 [02:05<00:23,  2.36s/it] 86%|████████▌ | 55/64 [02:08<00:21,  2.36s/it] 88%|████████▊ | 56/64 [02:10<00:19,  2.46s/it] 89%|████████▉ | 57/64 [02:13<00:17,  2.47s/it] 91%|█████████ | 58/64 [02:15<00:14,  2.45s/it] 92%|█████████▏| 59/64 [02:18<00:12,  2.45s/it] 94%|█████████▍| 60/64 [02:20<00:09,  2.38s/it]                                               {'loss': 0.3528, 'grad_norm': 0.28018325567245483, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:20<00:09,  2.38s/it] 95%|█████████▌| 61/64 [02:22<00:07,  2.38s/it] 97%|█████████▋| 62/64 [02:25<00:04,  2.41s/it] 98%|█████████▊| 63/64 [02:27<00:02,  2.44s/it]100%|██████████| 64/64 [02:30<00:00,  2.55s/it]                                               {'train_runtime': 151.2249, 'train_samples_per_second': 5.079, 'train_steps_per_second': 0.423, 'train_loss': 0.4229089841246605, 'epoch': 2.0}
100%|██████████| 64/64 [02:31<00:00,  2.55s/it]100%|██████████| 64/64 [02:31<00:00,  2.36s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<00:56,  2.85it/s]  1%|          | 2/163 [00:00<00:48,  3.34it/s]  2%|▏         | 3/163 [00:00<00:43,  3.68it/s]  2%|▏         | 4/163 [00:01<00:41,  3.86it/s]  3%|▎         | 5/163 [00:01<00:39,  3.97it/s]  4%|▎         | 6/163 [00:01<00:38,  4.04it/s]  4%|▍         | 7/163 [00:01<00:38,  4.08it/s]  5%|▍         | 8/163 [00:02<00:37,  4.11it/s]  6%|▌         | 9/163 [00:02<00:37,  4.13it/s]  6%|▌         | 10/163 [00:02<00:36,  4.14it/s]  7%|▋         | 11/163 [00:02<00:36,  4.15it/s]  7%|▋         | 12/163 [00:03<00:36,  4.15it/s]  8%|▊         | 13/163 [00:03<00:36,  4.16it/s]  9%|▊         | 14/163 [00:03<00:35,  4.17it/s]  9%|▉         | 15/163 [00:03<00:35,  4.17it/s] 10%|▉         | 16/163 [00:03<00:35,  4.17it/s] 10%|█         | 17/163 [00:04<00:35,  4.17it/s] 11%|█         | 18/163 [00:04<00:34,  4.17it/s] 12%|█▏        | 19/163 [00:04<00:34,  4.17it/s] 12%|█▏        | 20/163 [00:04<00:34,  4.17it/s] 13%|█▎        | 21/163 [00:05<00:34,  4.17it/s] 13%|█▎        | 22/163 [00:05<00:33,  4.17it/s] 14%|█▍        | 23/163 [00:05<00:33,  4.17it/s] 15%|█▍        | 24/163 [00:05<00:33,  4.16it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.16it/s] 16%|█▌        | 26/163 [00:06<00:32,  4.16it/s] 17%|█▋        | 27/163 [00:06<00:32,  4.17it/s] 17%|█▋        | 28/163 [00:06<00:32,  4.17it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.17it/s] 18%|█▊        | 30/163 [00:07<00:31,  4.17it/s] 19%|█▉        | 31/163 [00:07<00:31,  4.17it/s] 20%|█▉        | 32/163 [00:07<00:31,  4.16it/s] 20%|██        | 33/163 [00:08<00:32,  4.02it/s] 21%|██        | 34/163 [00:08<00:32,  3.93it/s] 21%|██▏       | 35/163 [00:08<00:32,  3.91it/s] 22%|██▏       | 36/163 [00:08<00:32,  3.95it/s] 23%|██▎       | 37/163 [00:09<00:32,  3.83it/s] 23%|██▎       | 38/163 [00:09<00:33,  3.74it/s] 24%|██▍       | 39/163 [00:09<00:33,  3.67it/s] 25%|██▍       | 40/163 [00:09<00:32,  3.80it/s] 25%|██▌       | 41/163 [00:10<00:31,  3.84it/s] 26%|██▌       | 42/163 [00:10<00:32,  3.75it/s] 26%|██▋       | 43/163 [00:10<00:32,  3.70it/s] 27%|██▋       | 44/163 [00:11<00:32,  3.65it/s] 28%|██▊       | 45/163 [00:11<00:31,  3.79it/s] 28%|██▊       | 46/163 [00:11<00:30,  3.81it/s] 29%|██▉       | 47/163 [00:11<00:31,  3.68it/s] 29%|██▉       | 48/163 [00:12<00:30,  3.74it/s] 30%|███       | 49/163 [00:12<00:30,  3.69it/s] 31%|███       | 50/163 [00:12<00:29,  3.80it/s] 31%|███▏      | 51/163 [00:12<00:30,  3.72it/s] 32%|███▏      | 52/163 [00:13<00:30,  3.67it/s] 33%|███▎      | 53/163 [00:13<00:30,  3.62it/s] 33%|███▎      | 54/163 [00:13<00:29,  3.76it/s] 34%|███▎      | 55/163 [00:13<00:28,  3.78it/s] 34%|███▍      | 56/163 [00:14<00:29,  3.67it/s] 35%|███▍      | 57/163 [00:14<00:28,  3.72it/s] 36%|███▌      | 58/163 [00:14<00:28,  3.68it/s] 36%|███▌      | 59/163 [00:15<00:27,  3.81it/s] 37%|███▋      | 60/163 [00:15<00:27,  3.73it/s] 37%|███▋      | 61/163 [00:15<00:27,  3.67it/s] 38%|███▊      | 62/163 [00:15<00:27,  3.62it/s] 39%|███▊      | 63/163 [00:16<00:26,  3.77it/s] 39%|███▉      | 64/163 [00:16<00:26,  3.79it/s] 40%|███▉      | 65/163 [00:16<00:26,  3.67it/s] 40%|████      | 66/163 [00:16<00:26,  3.73it/s] 41%|████      | 67/163 [00:17<00:26,  3.68it/s] 42%|████▏     | 68/163 [00:17<00:24,  3.80it/s] 42%|████▏     | 69/163 [00:17<00:25,  3.71it/s] 43%|████▎     | 70/163 [00:18<00:25,  3.65it/s] 44%|████▎     | 71/163 [00:18<00:25,  3.61it/s] 44%|████▍     | 72/163 [00:18<00:24,  3.76it/s] 45%|████▍     | 73/163 [00:18<00:23,  3.79it/s] 45%|████▌     | 74/163 [00:19<00:24,  3.66it/s] 46%|████▌     | 75/163 [00:19<00:23,  3.71it/s] 47%|████▋     | 76/163 [00:19<00:23,  3.67it/s] 47%|████▋     | 77/163 [00:19<00:22,  3.78it/s] 48%|████▊     | 78/163 [00:20<00:22,  3.71it/s] 48%|████▊     | 79/163 [00:20<00:23,  3.65it/s] 49%|████▉     | 80/163 [00:20<00:23,  3.61it/s] 50%|████▉     | 81/163 [00:20<00:21,  3.75it/s] 50%|█████     | 82/163 [00:21<00:21,  3.80it/s] 51%|█████     | 83/163 [00:21<00:21,  3.72it/s] 52%|█████▏    | 84/163 [00:21<00:21,  3.67it/s] 52%|█████▏    | 85/163 [00:22<00:21,  3.62it/s] 53%|█████▎    | 86/163 [00:22<00:20,  3.77it/s] 53%|█████▎    | 87/163 [00:22<00:19,  3.81it/s] 54%|█████▍    | 88/163 [00:22<00:20,  3.72it/s] 55%|█████▍    | 89/163 [00:23<00:20,  3.67it/s] 55%|█████▌    | 90/163 [00:23<00:20,  3.63it/s] 56%|█████▌    | 91/163 [00:23<00:19,  3.77it/s] 56%|█████▋    | 92/163 [00:23<00:18,  3.81it/s] 57%|█████▋    | 93/163 [00:24<00:18,  3.73it/s] 58%|█████▊    | 94/163 [00:24<00:18,  3.67it/s] 58%|█████▊    | 95/163 [00:24<00:18,  3.62it/s] 59%|█████▉    | 96/163 [00:25<00:17,  3.75it/s] 60%|█████▉    | 97/163 [00:25<00:17,  3.79it/s] 60%|██████    | 98/163 [00:25<00:17,  3.71it/s] 61%|██████    | 99/163 [00:25<00:17,  3.66it/s] 61%|██████▏   | 100/163 [00:26<00:17,  3.61it/s] 62%|██████▏   | 101/163 [00:26<00:16,  3.76it/s] 63%|██████▎   | 102/163 [00:26<00:16,  3.77it/s] 63%|██████▎   | 103/163 [00:26<00:16,  3.65it/s] 64%|██████▍   | 104/163 [00:27<00:15,  3.71it/s] 64%|██████▍   | 105/163 [00:27<00:15,  3.67it/s] 65%|██████▌   | 106/163 [00:27<00:15,  3.77it/s] 66%|██████▌   | 107/163 [00:27<00:15,  3.69it/s] 66%|██████▋   | 108/163 [00:28<00:15,  3.64it/s] 67%|██████▋   | 109/163 [00:28<00:15,  3.60it/s] 67%|██████▋   | 110/163 [00:28<00:14,  3.75it/s] 68%|██████▊   | 111/163 [00:29<00:13,  3.79it/s] 69%|██████▊   | 112/163 [00:29<00:13,  3.71it/s] 69%|██████▉   | 113/163 [00:29<00:13,  3.66it/s] 70%|██████▉   | 114/163 [00:29<00:13,  3.61it/s] 71%|███████   | 115/163 [00:30<00:12,  3.74it/s] 71%|███████   | 116/163 [00:30<00:12,  3.77it/s] 72%|███████▏  | 117/163 [00:30<00:12,  3.65it/s] 72%|███████▏  | 118/163 [00:30<00:12,  3.70it/s] 73%|███████▎  | 119/163 [00:31<00:12,  3.66it/s] 74%|███████▎  | 120/163 [00:31<00:11,  3.77it/s] 74%|███████▍  | 121/163 [00:31<00:11,  3.69it/s] 75%|███████▍  | 122/163 [00:32<00:11,  3.64it/s] 75%|███████▌  | 123/163 [00:32<00:11,  3.59it/s] 76%|███████▌  | 124/163 [00:32<00:10,  3.73it/s] 77%|███████▋  | 125/163 [00:32<00:10,  3.77it/s] 77%|███████▋  | 126/163 [00:33<00:10,  3.65it/s] 78%|███████▊  | 127/163 [00:33<00:09,  3.70it/s] 79%|███████▊  | 128/163 [00:33<00:09,  3.65it/s] 79%|███████▉  | 129/163 [00:33<00:09,  3.76it/s] 80%|███████▉  | 130/163 [00:34<00:08,  3.69it/s] 80%|████████  | 131/163 [00:34<00:08,  3.63it/s] 81%|████████  | 132/163 [00:34<00:08,  3.59it/s] 82%|████████▏ | 133/163 [00:35<00:08,  3.74it/s] 82%|████████▏ | 134/163 [00:35<00:07,  3.78it/s] 83%|████████▎ | 135/163 [00:35<00:07,  3.71it/s] 83%|████████▎ | 136/163 [00:35<00:07,  3.65it/s] 84%|████████▍ | 137/163 [00:36<00:07,  3.61it/s] 85%|████████▍ | 138/163 [00:36<00:06,  3.74it/s] 85%|████████▌ | 139/163 [00:36<00:06,  3.76it/s] 86%|████████▌ | 140/163 [00:36<00:06,  3.64it/s] 87%|████████▋ | 141/163 [00:37<00:05,  3.69it/s] 87%|████████▋ | 142/163 [00:37<00:05,  3.65it/s] 88%|████████▊ | 143/163 [00:37<00:05,  3.76it/s] 88%|████████▊ | 144/163 [00:38<00:05,  3.68it/s] 89%|████████▉ | 145/163 [00:38<00:04,  3.63it/s] 90%|████████▉ | 146/163 [00:38<00:04,  3.58it/s] 90%|█████████ | 147/163 [00:38<00:04,  3.73it/s] 91%|█████████ | 148/163 [00:39<00:03,  3.78it/s] 91%|█████████▏| 149/163 [00:39<00:03,  3.65it/s] 92%|█████████▏| 150/163 [00:39<00:03,  3.70it/s] 93%|█████████▎| 151/163 [00:39<00:03,  3.72it/s] 93%|█████████▎| 152/163 [00:40<00:02,  3.83it/s] 94%|█████████▍| 153/163 [00:40<00:02,  3.91it/s] 94%|█████████▍| 154/163 [00:40<00:02,  3.96it/s] 95%|█████████▌| 155/163 [00:40<00:02,  3.99it/s] 96%|█████████▌| 156/163 [00:41<00:01,  4.03it/s] 96%|█████████▋| 157/163 [00:41<00:01,  4.06it/s] 97%|█████████▋| 158/163 [00:41<00:01,  4.08it/s] 98%|█████████▊| 159/163 [00:41<00:00,  4.08it/s] 98%|█████████▊| 160/163 [00:42<00:00,  4.08it/s] 99%|█████████▉| 161/163 [00:42<00:00,  4.09it/s] 99%|█████████▉| 162/163 [00:42<00:00,  4.10it/s]100%|██████████| 163/163 [00:42<00:00,  4.11it/s]accuracy:  0.8773006134969326
100%|██████████| 163/163 [00:45<00:00,  3.59it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 198
Training dataset size: 384, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:27:26,453] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Training dataset size: 384, validation dataset size: 198
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
training start
[2025-03-12 07:27:33,235] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:27:33,391] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<03:05,  2.94s/it]  3%|▎         | 2/64 [00:05<02:49,  2.73s/it]  5%|▍         | 3/64 [00:08<02:42,  2.67s/it]  6%|▋         | 4/64 [00:10<02:33,  2.55s/it]  8%|▊         | 5/64 [00:13<02:33,  2.60s/it]  9%|▉         | 6/64 [00:15<02:32,  2.63s/it] 11%|█         | 7/64 [00:18<02:30,  2.65s/it] 12%|█▎        | 8/64 [00:21<02:26,  2.62s/it] 14%|█▍        | 9/64 [00:23<02:20,  2.55s/it] 16%|█▌        | 10/64 [00:26<02:18,  2.56s/it]                                               {'loss': 0.6352, 'grad_norm': 7.183201313018799, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:26<02:18,  2.56s/it] 17%|█▋        | 11/64 [00:28<02:11,  2.48s/it] 19%|█▉        | 12/64 [00:30<02:06,  2.43s/it] 20%|██        | 13/64 [00:33<02:06,  2.48s/it] 22%|██▏       | 14/64 [00:35<02:03,  2.47s/it] 23%|██▎       | 15/64 [00:38<02:04,  2.54s/it] 25%|██▌       | 16/64 [00:41<02:03,  2.58s/it] 27%|██▋       | 17/64 [00:43<02:01,  2.58s/it] 28%|██▊       | 18/64 [00:46<02:01,  2.63s/it] 30%|██▉       | 19/64 [00:49<01:57,  2.62s/it] 31%|███▏      | 20/64 [00:51<01:53,  2.59s/it]                                               {'loss': 0.6375, 'grad_norm': 1.7102051973342896, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:51<01:53,  2.59s/it] 33%|███▎      | 21/64 [00:53<01:45,  2.46s/it] 34%|███▍      | 22/64 [00:56<01:44,  2.48s/it] 36%|███▌      | 23/64 [00:58<01:41,  2.48s/it] 38%|███▊      | 24/64 [01:01<01:37,  2.43s/it] 39%|███▉      | 25/64 [01:03<01:34,  2.44s/it] 41%|████      | 26/64 [01:06<01:40,  2.64s/it] 42%|████▏     | 27/64 [01:09<01:44,  2.83s/it] 44%|████▍     | 28/64 [01:12<01:39,  2.75s/it] 45%|████▌     | 29/64 [01:14<01:32,  2.66s/it] 47%|████▋     | 30/64 [01:17<01:31,  2.70s/it]                                               {'loss': 0.5852, 'grad_norm': 9.042339324951172, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:17<01:31,  2.70s/it] 48%|████▊     | 31/64 [01:19<01:24,  2.56s/it] 50%|█████     | 32/64 [01:22<01:23,  2.61s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:25<01:23,  2.68s/it] 53%|█████▎    | 34/64 [01:27<01:18,  2.62s/it] 55%|█████▍    | 35/64 [01:30<01:19,  2.74s/it] 56%|█████▋    | 36/64 [01:34<01:23,  2.97s/it] 58%|█████▊    | 37/64 [01:37<01:17,  2.87s/it] 59%|█████▉    | 38/64 [01:39<01:12,  2.79s/it] 61%|██████    | 39/64 [01:41<01:04,  2.58s/it] 62%|██████▎   | 40/64 [01:44<01:03,  2.66s/it]                                               {'loss': 0.461, 'grad_norm': 1.5905466079711914, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:44<01:03,  2.66s/it] 64%|██████▍   | 41/64 [01:47<01:00,  2.61s/it] 66%|██████▌   | 42/64 [01:49<00:56,  2.56s/it] 67%|██████▋   | 43/64 [01:51<00:52,  2.48s/it] 69%|██████▉   | 44/64 [01:54<00:49,  2.47s/it] 70%|███████   | 45/64 [01:56<00:46,  2.44s/it] 72%|███████▏  | 46/64 [01:59<00:43,  2.40s/it] 73%|███████▎  | 47/64 [02:02<00:44,  2.60s/it] 75%|███████▌  | 48/64 [02:04<00:42,  2.67s/it] 77%|███████▋  | 49/64 [02:07<00:40,  2.73s/it] 78%|███████▊  | 50/64 [02:10<00:37,  2.71s/it]                                               {'loss': 0.4504, 'grad_norm': 2.0807697772979736, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [02:10<00:37,  2.71s/it] 80%|███████▉  | 51/64 [02:12<00:34,  2.62s/it] 81%|████████▏ | 52/64 [02:15<00:30,  2.51s/it] 83%|████████▎ | 53/64 [02:18<00:29,  2.67s/it] 84%|████████▍ | 54/64 [02:20<00:25,  2.60s/it] 86%|████████▌ | 55/64 [02:23<00:23,  2.66s/it] 88%|████████▊ | 56/64 [02:26<00:22,  2.76s/it] 89%|████████▉ | 57/64 [02:28<00:17,  2.54s/it] 91%|█████████ | 58/64 [02:30<00:14,  2.42s/it] 92%|█████████▏| 59/64 [02:33<00:12,  2.50s/it] 94%|█████████▍| 60/64 [02:36<00:10,  2.60s/it]                                               {'loss': 0.4379, 'grad_norm': 1.9584630727767944, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:36<00:10,  2.60s/it] 95%|█████████▌| 61/64 [02:38<00:07,  2.57s/it] 97%|█████████▋| 62/64 [02:41<00:05,  2.65s/it] 98%|█████████▊| 63/64 [02:44<00:02,  2.68s/it]100%|██████████| 64/64 [02:46<00:00,  2.48s/it]                                               {'train_runtime': 166.8518, 'train_samples_per_second': 4.603, 'train_steps_per_second': 0.384, 'train_loss': 0.5638634413480759, 'epoch': 2.0}
100%|██████████| 64/64 [02:46<00:00,  2.48s/it]100%|██████████| 64/64 [02:46<00:00,  2.60s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  232
  0%|          | 0/232 [00:00<?, ?it/s]  0%|          | 1/232 [00:00<01:15,  3.05it/s]  1%|          | 2/232 [00:00<01:08,  3.37it/s]  1%|▏         | 3/232 [00:00<01:01,  3.70it/s]  2%|▏         | 4/232 [00:01<01:00,  3.76it/s]  2%|▏         | 5/232 [00:01<01:00,  3.78it/s]  3%|▎         | 6/232 [00:01<00:59,  3.82it/s]  3%|▎         | 7/232 [00:01<00:59,  3.77it/s]  3%|▎         | 8/232 [00:02<00:57,  3.89it/s]  4%|▍         | 9/232 [00:02<00:58,  3.84it/s]  4%|▍         | 10/232 [00:02<00:58,  3.82it/s]  5%|▍         | 11/232 [00:02<00:57,  3.83it/s]  5%|▌         | 12/232 [00:03<00:55,  3.94it/s]  6%|▌         | 13/232 [00:03<00:54,  4.02it/s]  6%|▌         | 14/232 [00:03<00:53,  4.07it/s]  6%|▋         | 15/232 [00:03<00:52,  4.10it/s]  7%|▋         | 16/232 [00:04<00:52,  4.13it/s]  7%|▋         | 17/232 [00:04<00:51,  4.14it/s]  8%|▊         | 18/232 [00:04<00:51,  4.15it/s]  8%|▊         | 19/232 [00:04<00:51,  4.16it/s]  9%|▊         | 20/232 [00:05<00:52,  4.04it/s]  9%|▉         | 21/232 [00:05<00:53,  3.94it/s]  9%|▉         | 22/232 [00:05<00:53,  3.93it/s] 10%|▉         | 23/232 [00:05<00:52,  3.98it/s] 10%|█         | 24/232 [00:06<00:53,  3.90it/s] 11%|█         | 25/232 [00:06<00:53,  3.86it/s] 11%|█         | 26/232 [00:06<00:54,  3.80it/s] 12%|█▏        | 27/232 [00:06<00:52,  3.90it/s] 12%|█▏        | 28/232 [00:07<00:52,  3.90it/s] 12%|█▎        | 29/232 [00:07<00:52,  3.86it/s] 13%|█▎        | 30/232 [00:07<00:51,  3.89it/s] 13%|█▎        | 31/232 [00:07<00:52,  3.79it/s] 14%|█▍        | 32/232 [00:08<00:51,  3.87it/s] 14%|█▍        | 33/232 [00:08<00:52,  3.82it/s] 15%|█▍        | 34/232 [00:08<00:51,  3.81it/s] 15%|█▌        | 35/232 [00:09<00:52,  3.75it/s] 16%|█▌        | 36/232 [00:09<00:50,  3.87it/s] 16%|█▌        | 37/232 [00:09<00:50,  3.89it/s] 16%|█▋        | 38/232 [00:09<00:50,  3.84it/s] 17%|█▋        | 39/232 [00:10<00:49,  3.87it/s] 17%|█▋        | 40/232 [00:10<00:50,  3.79it/s] 18%|█▊        | 41/232 [00:10<00:49,  3.89it/s] 18%|█▊        | 42/232 [00:10<00:49,  3.84it/s] 19%|█▊        | 43/232 [00:11<00:49,  3.81it/s] 19%|█▉        | 44/232 [00:11<00:50,  3.75it/s] 19%|█▉        | 45/232 [00:11<00:48,  3.86it/s] 20%|█▉        | 46/232 [00:11<00:48,  3.86it/s] 20%|██        | 47/232 [00:12<00:48,  3.82it/s] 21%|██        | 48/232 [00:12<00:48,  3.82it/s] 21%|██        | 49/232 [00:12<00:48,  3.79it/s] 22%|██▏       | 50/232 [00:12<00:46,  3.88it/s] 22%|██▏       | 51/232 [00:13<00:47,  3.82it/s] 22%|██▏       | 52/232 [00:13<00:46,  3.86it/s] 23%|██▎       | 53/232 [00:13<00:46,  3.84it/s] 23%|██▎       | 54/232 [00:13<00:45,  3.93it/s] 24%|██▎       | 55/232 [00:14<00:44,  4.01it/s] 24%|██▍       | 56/232 [00:14<00:43,  4.06it/s] 25%|██▍       | 57/232 [00:14<00:42,  4.09it/s] 25%|██▌       | 58/232 [00:14<00:42,  4.12it/s] 25%|██▌       | 59/232 [00:15<00:41,  4.13it/s] 26%|██▌       | 60/232 [00:15<00:41,  4.14it/s] 26%|██▋       | 61/232 [00:15<00:41,  4.15it/s] 27%|██▋       | 62/232 [00:15<00:40,  4.16it/s] 27%|██▋       | 63/232 [00:16<00:40,  4.16it/s] 28%|██▊       | 64/232 [00:16<00:40,  4.16it/s] 28%|██▊       | 65/232 [00:16<00:40,  4.16it/s] 28%|██▊       | 66/232 [00:16<00:39,  4.16it/s] 29%|██▉       | 67/232 [00:17<00:39,  4.16it/s] 29%|██▉       | 68/232 [00:17<00:39,  4.16it/s] 30%|██▉       | 69/232 [00:17<00:39,  4.16it/s] 30%|███       | 70/232 [00:17<00:38,  4.16it/s] 31%|███       | 71/232 [00:18<00:38,  4.17it/s] 31%|███       | 72/232 [00:18<00:38,  4.17it/s] 31%|███▏      | 73/232 [00:18<00:38,  4.17it/s] 32%|███▏      | 74/232 [00:18<00:37,  4.17it/s] 32%|███▏      | 75/232 [00:18<00:37,  4.17it/s] 33%|███▎      | 76/232 [00:19<00:37,  4.17it/s] 33%|███▎      | 77/232 [00:19<00:37,  4.17it/s] 34%|███▎      | 78/232 [00:19<00:36,  4.17it/s] 34%|███▍      | 79/232 [00:19<00:36,  4.17it/s] 34%|███▍      | 80/232 [00:20<00:36,  4.17it/s] 35%|███▍      | 81/232 [00:20<00:36,  4.17it/s] 35%|███▌      | 82/232 [00:20<00:36,  4.17it/s] 36%|███▌      | 83/232 [00:20<00:35,  4.17it/s] 36%|███▌      | 84/232 [00:21<00:35,  4.17it/s] 37%|███▋      | 85/232 [00:21<00:35,  4.17it/s] 37%|███▋      | 86/232 [00:21<00:35,  4.07it/s] 38%|███▊      | 87/232 [00:21<00:36,  3.94it/s] 38%|███▊      | 88/232 [00:22<00:37,  3.87it/s] 38%|███▊      | 89/232 [00:22<00:36,  3.95it/s] 39%|███▉      | 90/232 [00:22<00:36,  3.85it/s] 39%|███▉      | 91/232 [00:22<00:37,  3.79it/s] 40%|███▉      | 92/232 [00:23<00:37,  3.70it/s] 40%|████      | 93/232 [00:23<00:36,  3.83it/s] 41%|████      | 94/232 [00:23<00:36,  3.83it/s] 41%|████      | 95/232 [00:24<00:36,  3.73it/s] 41%|████▏     | 96/232 [00:24<00:37,  3.67it/s] 42%|████▏     | 97/232 [00:24<00:35,  3.81it/s] 42%|████▏     | 98/232 [00:24<00:35,  3.81it/s] 43%|████▎     | 99/232 [00:25<00:35,  3.72it/s] 43%|████▎     | 100/232 [00:25<00:36,  3.65it/s] 44%|████▎     | 101/232 [00:25<00:34,  3.79it/s] 44%|████▍     | 102/232 [00:25<00:34,  3.80it/s] 44%|████▍     | 103/232 [00:26<00:34,  3.71it/s] 45%|████▍     | 104/232 [00:26<00:34,  3.74it/s] 45%|████▌     | 105/232 [00:26<00:33,  3.85it/s] 46%|████▌     | 106/232 [00:26<00:32,  3.92it/s] 46%|████▌     | 107/232 [00:27<00:31,  3.98it/s] 47%|████▋     | 108/232 [00:27<00:30,  4.02it/s] 47%|████▋     | 109/232 [00:27<00:30,  4.05it/s] 47%|████▋     | 110/232 [00:27<00:29,  4.08it/s] 48%|████▊     | 111/232 [00:28<00:29,  4.10it/s] 48%|████▊     | 112/232 [00:28<00:29,  4.12it/s] 49%|████▊     | 113/232 [00:28<00:28,  4.13it/s] 49%|████▉     | 114/232 [00:28<00:28,  4.13it/s] 50%|████▉     | 115/232 [00:29<00:28,  4.14it/s] 50%|█████     | 116/232 [00:29<00:28,  4.14it/s] 50%|█████     | 117/232 [00:29<00:27,  4.14it/s] 51%|█████     | 118/232 [00:29<00:27,  4.14it/s] 51%|█████▏    | 119/232 [00:30<00:27,  4.13it/s] 52%|█████▏    | 120/232 [00:30<00:27,  4.13it/s] 52%|█████▏    | 121/232 [00:30<00:26,  4.13it/s] 53%|█████▎    | 122/232 [00:30<00:26,  4.13it/s] 53%|█████▎    | 123/232 [00:31<00:26,  4.14it/s] 53%|█████▎    | 124/232 [00:31<00:26,  4.14it/s] 54%|█████▍    | 125/232 [00:31<00:25,  4.15it/s] 54%|█████▍    | 126/232 [00:31<00:25,  4.15it/s] 55%|█████▍    | 127/232 [00:31<00:25,  4.15it/s] 55%|█████▌    | 128/232 [00:32<00:25,  4.15it/s] 56%|█████▌    | 129/232 [00:32<00:24,  4.15it/s] 56%|█████▌    | 130/232 [00:32<00:24,  4.15it/s] 56%|█████▋    | 131/232 [00:32<00:25,  4.03it/s] 57%|█████▋    | 132/232 [00:33<00:25,  3.95it/s] 57%|█████▋    | 133/232 [00:33<00:25,  3.92it/s] 58%|█████▊    | 134/232 [00:33<00:25,  3.90it/s] 58%|█████▊    | 135/232 [00:34<00:24,  3.94it/s] 59%|█████▊    | 136/232 [00:34<00:25,  3.83it/s] 59%|█████▉    | 137/232 [00:34<00:25,  3.74it/s] 59%|█████▉    | 138/232 [00:34<00:24,  3.79it/s] 60%|█████▉    | 139/232 [00:35<00:24,  3.73it/s] 60%|██████    | 140/232 [00:35<00:23,  3.84it/s] 61%|██████    | 141/232 [00:35<00:23,  3.82it/s] 61%|██████    | 142/232 [00:35<00:24,  3.69it/s] 62%|██████▏   | 143/232 [00:36<00:23,  3.73it/s] 62%|██████▏   | 144/232 [00:36<00:23,  3.69it/s] 62%|██████▎   | 145/232 [00:36<00:22,  3.81it/s] 63%|██████▎   | 146/232 [00:36<00:22,  3.83it/s] 63%|██████▎   | 147/232 [00:37<00:23,  3.69it/s] 64%|██████▍   | 148/232 [00:37<00:22,  3.71it/s] 64%|██████▍   | 149/232 [00:37<00:22,  3.66it/s] 65%|██████▍   | 150/232 [00:38<00:21,  3.79it/s] 65%|██████▌   | 151/232 [00:38<00:21,  3.82it/s] 66%|██████▌   | 152/232 [00:38<00:21,  3.77it/s] 66%|██████▌   | 153/232 [00:38<00:21,  3.69it/s] 66%|██████▋   | 154/232 [00:39<00:20,  3.74it/s] 67%|██████▋   | 155/232 [00:39<00:20,  3.74it/s] 67%|██████▋   | 156/232 [00:39<00:19,  3.82it/s] 68%|██████▊   | 157/232 [00:39<00:19,  3.75it/s] 68%|██████▊   | 158/232 [00:40<00:20,  3.68it/s] 69%|██████▊   | 159/232 [00:40<00:19,  3.72it/s] 69%|██████▉   | 160/232 [00:40<00:19,  3.75it/s] 69%|██████▉   | 161/232 [00:40<00:18,  3.81it/s] 70%|██████▉   | 162/232 [00:41<00:18,  3.75it/s] 70%|███████   | 163/232 [00:41<00:18,  3.68it/s] 71%|███████   | 164/232 [00:41<00:18,  3.70it/s] 71%|███████   | 165/232 [00:42<00:17,  3.74it/s] 72%|███████▏  | 166/232 [00:42<00:17,  3.81it/s] 72%|███████▏  | 167/232 [00:42<00:17,  3.75it/s] 72%|███████▏  | 168/232 [00:42<00:17,  3.68it/s] 73%|███████▎  | 169/232 [00:43<00:16,  3.72it/s] 73%|███████▎  | 170/232 [00:43<00:16,  3.73it/s] 74%|███████▎  | 171/232 [00:43<00:16,  3.81it/s] 74%|███████▍  | 172/232 [00:43<00:16,  3.75it/s] 75%|███████▍  | 173/232 [00:44<00:16,  3.67it/s] 75%|███████▌  | 174/232 [00:44<00:15,  3.68it/s] 75%|███████▌  | 175/232 [00:44<00:15,  3.74it/s] 76%|███████▌  | 176/232 [00:44<00:14,  3.81it/s] 76%|███████▋  | 177/232 [00:45<00:14,  3.75it/s] 77%|███████▋  | 178/232 [00:45<00:14,  3.67it/s] 77%|███████▋  | 179/232 [00:45<00:14,  3.67it/s] 78%|███████▊  | 180/232 [00:46<00:13,  3.74it/s] 78%|███████▊  | 181/232 [00:46<00:13,  3.79it/s] 78%|███████▊  | 182/232 [00:46<00:13,  3.74it/s] 79%|███████▉  | 183/232 [00:46<00:13,  3.67it/s] 79%|███████▉  | 184/232 [00:47<00:13,  3.68it/s] 80%|███████▉  | 185/232 [00:47<00:12,  3.74it/s] 80%|████████  | 186/232 [00:47<00:12,  3.79it/s] 81%|████████  | 187/232 [00:47<00:12,  3.73it/s] 81%|████████  | 188/232 [00:48<00:11,  3.67it/s] 81%|████████▏ | 189/232 [00:48<00:11,  3.67it/s] 82%|████████▏ | 190/232 [00:48<00:11,  3.74it/s] 82%|████████▏ | 191/232 [00:49<00:10,  3.81it/s] 83%|████████▎ | 192/232 [00:49<00:10,  3.69it/s] 83%|████████▎ | 193/232 [00:49<00:10,  3.73it/s] 84%|████████▎ | 194/232 [00:49<00:10,  3.73it/s] 84%|████████▍ | 195/232 [00:50<00:09,  3.84it/s] 84%|████████▍ | 196/232 [00:50<00:09,  3.92it/s] 85%|████████▍ | 197/232 [00:50<00:08,  3.98it/s] 85%|████████▌ | 198/232 [00:50<00:08,  4.02it/s] 86%|████████▌ | 199/232 [00:51<00:08,  4.04it/s] 86%|████████▌ | 200/232 [00:51<00:07,  4.06it/s] 87%|████████▋ | 201/232 [00:51<00:07,  4.08it/s] 87%|████████▋ | 202/232 [00:51<00:07,  4.09it/s] 88%|████████▊ | 203/232 [00:52<00:07,  4.10it/s] 88%|████████▊ | 204/232 [00:52<00:06,  4.11it/s] 88%|████████▊ | 205/232 [00:52<00:06,  4.10it/s] 89%|████████▉ | 206/232 [00:52<00:06,  4.09it/s] 89%|████████▉ | 207/232 [00:52<00:06,  4.10it/s] 90%|████████▉ | 208/232 [00:53<00:05,  4.10it/s] 90%|█████████ | 209/232 [00:53<00:05,  4.11it/s] 91%|█████████ | 210/232 [00:53<00:05,  4.10it/s] 91%|█████████ | 211/232 [00:53<00:05,  4.10it/s] 91%|█████████▏| 212/232 [00:54<00:04,  4.10it/s] 92%|█████████▏| 213/232 [00:54<00:04,  4.11it/s] 92%|█████████▏| 214/232 [00:54<00:04,  4.12it/s] 93%|█████████▎| 215/232 [00:54<00:04,  4.11it/s] 93%|█████████▎| 216/232 [00:55<00:03,  4.10it/s] 94%|█████████▎| 217/232 [00:55<00:03,  4.09it/s] 94%|█████████▍| 218/232 [00:55<00:03,  4.10it/s] 94%|█████████▍| 219/232 [00:55<00:03,  4.10it/s] 95%|█████████▍| 220/232 [00:56<00:02,  4.10it/s] 95%|█████████▌| 221/232 [00:56<00:02,  4.09it/s] 96%|█████████▌| 222/232 [00:56<00:02,  4.09it/s] 96%|█████████▌| 223/232 [00:56<00:02,  4.10it/s] 97%|█████████▋| 224/232 [00:57<00:01,  4.11it/s] 97%|█████████▋| 225/232 [00:57<00:01,  4.10it/s] 97%|█████████▋| 226/232 [00:57<00:01,  4.09it/s] 98%|█████████▊| 227/232 [00:57<00:01,  4.07it/s] 98%|█████████▊| 228/232 [00:58<00:01,  3.95it/s] 99%|█████████▊| 229/232 [00:58<00:00,  3.86it/s] 99%|█████████▉| 230/232 [00:58<00:00,  3.83it/s]100%|█████████▉| 231/232 [00:58<00:00,  3.84it/s]100%|██████████| 232/232 [00:59<00:00,  3.73it/s]accuracy:  0.8879310344827587
100%|██████████| 232/232 [01:02<00:00,  3.70it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 164
Training dataset size: 384, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 164
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.34s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:31:53,825] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:31:55,073] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:32:05,378] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<02:46,  2.64s/it]  3%|▎         | 2/64 [00:05<02:51,  2.77s/it]  5%|▍         | 3/64 [00:07<02:36,  2.57s/it]  6%|▋         | 4/64 [00:09<02:23,  2.38s/it]  8%|▊         | 5/64 [00:12<02:23,  2.44s/it]  9%|▉         | 6/64 [00:14<02:22,  2.45s/it] 11%|█         | 7/64 [00:17<02:14,  2.37s/it] 12%|█▎        | 8/64 [00:19<02:12,  2.36s/it] 14%|█▍        | 9/64 [00:22<02:13,  2.42s/it] 16%|█▌        | 10/64 [00:24<02:12,  2.45s/it]                                               {'loss': 1.2856, 'grad_norm': 8.19374942779541, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:24<02:12,  2.45s/it] 17%|█▋        | 11/64 [00:26<02:06,  2.39s/it] 19%|█▉        | 12/64 [00:29<02:02,  2.35s/it] 20%|██        | 13/64 [00:31<01:55,  2.26s/it] 22%|██▏       | 14/64 [00:33<02:01,  2.42s/it] 23%|██▎       | 15/64 [00:36<02:00,  2.45s/it] 25%|██▌       | 16/64 [00:38<01:56,  2.44s/it] 27%|██▋       | 17/64 [00:41<01:54,  2.43s/it] 28%|██▊       | 18/64 [00:43<01:51,  2.43s/it] 30%|██▉       | 19/64 [00:46<01:48,  2.42s/it] 31%|███▏      | 20/64 [00:48<01:46,  2.42s/it]                                               {'loss': 0.925, 'grad_norm': 6.831404685974121, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:48<01:46,  2.42s/it] 33%|███▎      | 21/64 [00:51<01:47,  2.49s/it] 34%|███▍      | 22/64 [00:53<01:41,  2.42s/it] 36%|███▌      | 23/64 [00:55<01:34,  2.31s/it] 38%|███▊      | 24/64 [00:57<01:34,  2.36s/it] 39%|███▉      | 25/64 [01:00<01:30,  2.33s/it] 41%|████      | 26/64 [01:03<01:35,  2.50s/it] 42%|████▏     | 27/64 [01:05<01:29,  2.43s/it] 44%|████▍     | 28/64 [01:07<01:25,  2.38s/it] 45%|████▌     | 29/64 [01:10<01:23,  2.38s/it] 47%|████▋     | 30/64 [01:12<01:23,  2.47s/it]                                               {'loss': 0.8909, 'grad_norm': 7.127037525177002, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:12<01:23,  2.47s/it] 48%|████▊     | 31/64 [01:15<01:20,  2.42s/it] 50%|█████     | 32/64 [01:17<01:17,  2.41s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:19<01:15,  2.43s/it] 53%|█████▎    | 34/64 [01:22<01:13,  2.43s/it] 55%|█████▍    | 35/64 [01:24<01:09,  2.41s/it] 56%|█████▋    | 36/64 [01:26<01:04,  2.31s/it] 58%|█████▊    | 37/64 [01:28<01:01,  2.27s/it] 59%|█████▉    | 38/64 [01:31<01:01,  2.35s/it] 61%|██████    | 39/64 [01:34<01:00,  2.43s/it] 62%|██████▎   | 40/64 [01:36<00:59,  2.50s/it]                                               {'loss': 0.8756, 'grad_norm': 7.427877426147461, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:36<00:59,  2.50s/it] 64%|██████▍   | 41/64 [01:38<00:55,  2.39s/it] 66%|██████▌   | 42/64 [01:41<00:53,  2.43s/it] 67%|██████▋   | 43/64 [01:43<00:49,  2.34s/it] 69%|██████▉   | 44/64 [01:45<00:45,  2.29s/it] 70%|███████   | 45/64 [01:48<00:44,  2.35s/it] 72%|███████▏  | 46/64 [01:51<00:46,  2.57s/it] 73%|███████▎  | 47/64 [01:53<00:41,  2.46s/it] 75%|███████▌  | 48/64 [01:56<00:39,  2.49s/it] 77%|███████▋  | 49/64 [01:58<00:37,  2.49s/it] 78%|███████▊  | 50/64 [02:00<00:34,  2.48s/it]                                               {'loss': 0.7252, 'grad_norm': 4.283470630645752, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [02:00<00:34,  2.48s/it] 80%|███████▉  | 51/64 [02:03<00:32,  2.52s/it] 81%|████████▏ | 52/64 [02:05<00:28,  2.41s/it] 83%|████████▎ | 53/64 [02:08<00:26,  2.40s/it] 84%|████████▍ | 54/64 [02:10<00:24,  2.46s/it] 86%|████████▌ | 55/64 [02:13<00:22,  2.54s/it] 88%|████████▊ | 56/64 [02:16<00:20,  2.57s/it] 89%|████████▉ | 57/64 [02:18<00:17,  2.54s/it] 91%|█████████ | 58/64 [02:20<00:14,  2.42s/it] 92%|█████████▏| 59/64 [02:23<00:12,  2.41s/it] 94%|█████████▍| 60/64 [02:25<00:09,  2.33s/it]                                               {'loss': 0.8139, 'grad_norm': 4.302075386047363, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:25<00:09,  2.33s/it] 95%|█████████▌| 61/64 [02:27<00:07,  2.39s/it] 97%|█████████▋| 62/64 [02:29<00:04,  2.31s/it] 98%|█████████▊| 63/64 [02:31<00:02,  2.22s/it]100%|██████████| 64/64 [02:34<00:00,  2.28s/it]                                               {'train_runtime': 154.9646, 'train_samples_per_second': 4.956, 'train_steps_per_second': 0.413, 'train_loss': 0.9005033634603024, 'epoch': 2.0}
100%|██████████| 64/64 [02:34<00:00,  2.28s/it]100%|██████████| 64/64 [02:34<00:00,  2.42s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  248
  0%|          | 0/248 [00:00<?, ?it/s]  0%|          | 1/248 [00:00<01:16,  3.22it/s]  1%|          | 2/248 [00:00<01:06,  3.71it/s]  1%|          | 3/248 [00:00<01:02,  3.90it/s]  2%|▏         | 4/248 [00:01<01:00,  4.00it/s]  2%|▏         | 5/248 [00:01<01:00,  4.05it/s]  2%|▏         | 6/248 [00:01<00:59,  4.08it/s]  3%|▎         | 7/248 [00:01<00:58,  4.10it/s]  3%|▎         | 8/248 [00:01<00:58,  4.11it/s]  4%|▎         | 9/248 [00:02<00:57,  4.12it/s]  4%|▍         | 10/248 [00:02<00:57,  4.14it/s]  4%|▍         | 11/248 [00:02<00:57,  4.14it/s]  5%|▍         | 12/248 [00:02<00:56,  4.15it/s]  5%|▌         | 13/248 [00:03<00:56,  4.16it/s]  6%|▌         | 14/248 [00:03<00:56,  4.16it/s]  6%|▌         | 15/248 [00:03<00:55,  4.16it/s]  6%|▋         | 16/248 [00:03<00:55,  4.16it/s]  7%|▋         | 17/248 [00:04<00:55,  4.16it/s]  7%|▋         | 18/248 [00:04<00:56,  4.06it/s]  8%|▊         | 19/248 [00:04<00:58,  3.95it/s]  8%|▊         | 20/248 [00:04<00:58,  3.93it/s]  8%|▊         | 21/248 [00:05<00:57,  3.95it/s]  9%|▉         | 22/248 [00:05<00:58,  3.87it/s]  9%|▉         | 23/248 [00:05<00:57,  3.90it/s] 10%|▉         | 24/248 [00:05<00:58,  3.81it/s] 10%|█         | 25/248 [00:06<00:57,  3.88it/s] 10%|█         | 26/248 [00:06<00:58,  3.82it/s] 11%|█         | 27/248 [00:06<00:57,  3.85it/s] 11%|█▏        | 28/248 [00:07<00:58,  3.77it/s] 12%|█▏        | 29/248 [00:07<00:57,  3.81it/s] 12%|█▏        | 30/248 [00:07<00:57,  3.78it/s] 12%|█▎        | 31/248 [00:07<00:56,  3.83it/s] 13%|█▎        | 32/248 [00:08<00:57,  3.75it/s] 13%|█▎        | 33/248 [00:08<00:56,  3.84it/s] 14%|█▎        | 34/248 [00:08<00:56,  3.78it/s] 14%|█▍        | 35/248 [00:08<00:55,  3.83it/s] 15%|█▍        | 36/248 [00:09<00:56,  3.76it/s] 15%|█▍        | 37/248 [00:09<00:55,  3.83it/s] 15%|█▌        | 38/248 [00:09<00:55,  3.79it/s] 16%|█▌        | 39/248 [00:09<00:54,  3.83it/s] 16%|█▌        | 40/248 [00:10<00:55,  3.75it/s] 17%|█▋        | 41/248 [00:10<00:54,  3.80it/s] 17%|█▋        | 42/248 [00:10<00:54,  3.78it/s] 17%|█▋        | 43/248 [00:10<00:53,  3.81it/s] 18%|█▊        | 44/248 [00:11<00:54,  3.74it/s] 18%|█▊        | 45/248 [00:11<00:53,  3.79it/s] 19%|█▊        | 46/248 [00:11<00:53,  3.77it/s] 19%|█▉        | 47/248 [00:12<00:52,  3.81it/s] 19%|█▉        | 48/248 [00:12<00:53,  3.73it/s] 20%|█▉        | 49/248 [00:12<00:52,  3.80it/s] 20%|██        | 50/248 [00:12<00:52,  3.77it/s] 21%|██        | 51/248 [00:13<00:51,  3.81it/s] 21%|██        | 52/248 [00:13<00:52,  3.74it/s] 21%|██▏       | 53/248 [00:13<00:51,  3.81it/s] 22%|██▏       | 54/248 [00:13<00:51,  3.77it/s] 22%|██▏       | 55/248 [00:14<00:50,  3.81it/s] 23%|██▎       | 56/248 [00:14<00:51,  3.76it/s] 23%|██▎       | 57/248 [00:14<00:49,  3.83it/s] 23%|██▎       | 58/248 [00:14<00:50,  3.78it/s] 24%|██▍       | 59/248 [00:15<00:49,  3.83it/s] 24%|██▍       | 60/248 [00:15<00:50,  3.75it/s] 25%|██▍       | 61/248 [00:15<00:48,  3.85it/s] 25%|██▌       | 62/248 [00:16<00:48,  3.80it/s] 25%|██▌       | 63/248 [00:16<00:48,  3.78it/s] 26%|██▌       | 64/248 [00:16<00:49,  3.72it/s] 26%|██▌       | 65/248 [00:16<00:47,  3.84it/s] 27%|██▋       | 66/248 [00:17<00:47,  3.80it/s] 27%|██▋       | 67/248 [00:17<00:47,  3.78it/s] 27%|██▋       | 68/248 [00:17<00:48,  3.72it/s] 28%|██▊       | 69/248 [00:17<00:46,  3.84it/s] 28%|██▊       | 70/248 [00:18<00:46,  3.81it/s] 29%|██▊       | 71/248 [00:18<00:46,  3.78it/s] 29%|██▉       | 72/248 [00:18<00:47,  3.72it/s] 29%|██▉       | 73/248 [00:18<00:45,  3.83it/s] 30%|██▉       | 74/248 [00:19<00:45,  3.80it/s] 30%|███       | 75/248 [00:19<00:45,  3.78it/s] 31%|███       | 76/248 [00:19<00:46,  3.71it/s] 31%|███       | 77/248 [00:19<00:44,  3.82it/s] 31%|███▏      | 78/248 [00:20<00:44,  3.82it/s] 32%|███▏      | 79/248 [00:20<00:44,  3.77it/s] 32%|███▏      | 80/248 [00:20<00:45,  3.73it/s] 33%|███▎      | 81/248 [00:21<00:43,  3.81it/s] 33%|███▎      | 82/248 [00:21<00:43,  3.84it/s] 33%|███▎      | 83/248 [00:21<00:43,  3.79it/s] 34%|███▍      | 84/248 [00:21<00:43,  3.79it/s] 34%|███▍      | 85/248 [00:22<00:43,  3.76it/s] 35%|███▍      | 86/248 [00:22<00:42,  3.81it/s] 35%|███▌      | 87/248 [00:22<00:42,  3.78it/s] 35%|███▌      | 88/248 [00:22<00:42,  3.76it/s] 36%|███▌      | 89/248 [00:23<00:42,  3.77it/s] 36%|███▋      | 90/248 [00:23<00:41,  3.80it/s] 37%|███▋      | 91/248 [00:23<00:41,  3.76it/s] 37%|███▋      | 92/248 [00:23<00:42,  3.71it/s] 38%|███▊      | 93/248 [00:24<00:41,  3.78it/s] 38%|███▊      | 94/248 [00:24<00:40,  3.82it/s] 38%|███▊      | 95/248 [00:24<00:40,  3.77it/s] 39%|███▊      | 96/248 [00:25<00:40,  3.71it/s] 39%|███▉      | 97/248 [00:25<00:39,  3.78it/s] 40%|███▉      | 98/248 [00:25<00:39,  3.80it/s] 40%|███▉      | 99/248 [00:25<00:39,  3.76it/s] 40%|████      | 100/248 [00:26<00:40,  3.67it/s] 41%|████      | 101/248 [00:26<00:38,  3.80it/s] 41%|████      | 102/248 [00:26<00:38,  3.80it/s] 42%|████▏     | 103/248 [00:26<00:38,  3.76it/s] 42%|████▏     | 104/248 [00:27<00:39,  3.68it/s] 42%|████▏     | 105/248 [00:27<00:37,  3.81it/s] 43%|████▎     | 106/248 [00:27<00:37,  3.80it/s] 43%|████▎     | 107/248 [00:27<00:37,  3.77it/s] 44%|████▎     | 108/248 [00:28<00:37,  3.69it/s] 44%|████▍     | 109/248 [00:28<00:36,  3.81it/s] 44%|████▍     | 110/248 [00:28<00:36,  3.80it/s] 45%|████▍     | 111/248 [00:28<00:36,  3.76it/s] 45%|████▌     | 112/248 [00:29<00:36,  3.69it/s] 46%|████▌     | 113/248 [00:29<00:35,  3.81it/s] 46%|████▌     | 114/248 [00:29<00:35,  3.78it/s] 46%|████▋     | 115/248 [00:30<00:35,  3.74it/s] 47%|████▋     | 116/248 [00:30<00:35,  3.69it/s] 47%|████▋     | 117/248 [00:30<00:34,  3.80it/s] 48%|████▊     | 118/248 [00:30<00:34,  3.77it/s] 48%|████▊     | 119/248 [00:31<00:34,  3.76it/s] 48%|████▊     | 120/248 [00:31<00:34,  3.70it/s] 49%|████▉     | 121/248 [00:31<00:33,  3.81it/s] 49%|████▉     | 122/248 [00:31<00:33,  3.76it/s] 50%|████▉     | 123/248 [00:32<00:33,  3.74it/s] 50%|█████     | 124/248 [00:32<00:33,  3.69it/s] 50%|█████     | 125/248 [00:32<00:32,  3.81it/s] 51%|█████     | 126/248 [00:32<00:32,  3.77it/s] 51%|█████     | 127/248 [00:33<00:32,  3.75it/s] 52%|█████▏    | 128/248 [00:33<00:32,  3.69it/s] 52%|█████▏    | 129/248 [00:33<00:31,  3.81it/s] 52%|█████▏    | 130/248 [00:34<00:31,  3.77it/s] 53%|█████▎    | 131/248 [00:34<00:31,  3.75it/s] 53%|█████▎    | 132/248 [00:34<00:31,  3.70it/s] 54%|█████▎    | 133/248 [00:34<00:30,  3.81it/s] 54%|█████▍    | 134/248 [00:35<00:30,  3.77it/s] 54%|█████▍    | 135/248 [00:35<00:30,  3.75it/s] 55%|█████▍    | 136/248 [00:35<00:30,  3.70it/s] 55%|█████▌    | 137/248 [00:35<00:29,  3.81it/s] 56%|█████▌    | 138/248 [00:36<00:29,  3.77it/s] 56%|█████▌    | 139/248 [00:36<00:29,  3.75it/s] 56%|█████▋    | 140/248 [00:36<00:29,  3.70it/s] 57%|█████▋    | 141/248 [00:36<00:28,  3.81it/s] 57%|█████▋    | 142/248 [00:37<00:28,  3.79it/s] 58%|█████▊    | 143/248 [00:37<00:28,  3.75it/s] 58%|█████▊    | 144/248 [00:37<00:28,  3.69it/s] 58%|█████▊    | 145/248 [00:38<00:27,  3.80it/s] 59%|█████▉    | 146/248 [00:38<00:27,  3.76it/s] 59%|█████▉    | 147/248 [00:38<00:26,  3.74it/s] 60%|█████▉    | 148/248 [00:38<00:27,  3.69it/s] 60%|██████    | 149/248 [00:39<00:26,  3.80it/s] 60%|██████    | 150/248 [00:39<00:26,  3.76it/s] 61%|██████    | 151/248 [00:39<00:26,  3.73it/s] 61%|██████▏   | 152/248 [00:39<00:26,  3.67it/s] 62%|██████▏   | 153/248 [00:40<00:25,  3.79it/s] 62%|██████▏   | 154/248 [00:40<00:25,  3.74it/s] 62%|██████▎   | 155/248 [00:40<00:24,  3.73it/s] 63%|██████▎   | 156/248 [00:41<00:24,  3.68it/s] 63%|██████▎   | 157/248 [00:41<00:23,  3.80it/s] 64%|██████▎   | 158/248 [00:41<00:23,  3.76it/s] 64%|██████▍   | 159/248 [00:41<00:23,  3.73it/s] 65%|██████▍   | 160/248 [00:42<00:23,  3.68it/s] 65%|██████▍   | 161/248 [00:42<00:22,  3.79it/s] 65%|██████▌   | 162/248 [00:42<00:22,  3.76it/s] 66%|██████▌   | 163/248 [00:42<00:22,  3.74it/s] 66%|██████▌   | 164/248 [00:43<00:22,  3.69it/s] 67%|██████▋   | 165/248 [00:43<00:21,  3.80it/s] 67%|██████▋   | 166/248 [00:43<00:21,  3.76it/s] 67%|██████▋   | 167/248 [00:43<00:21,  3.73it/s] 68%|██████▊   | 168/248 [00:44<00:21,  3.69it/s] 68%|██████▊   | 169/248 [00:44<00:20,  3.81it/s] 69%|██████▊   | 170/248 [00:44<00:20,  3.76it/s] 69%|██████▉   | 171/248 [00:44<00:20,  3.74it/s] 69%|██████▉   | 172/248 [00:45<00:20,  3.69it/s] 70%|██████▉   | 173/248 [00:45<00:19,  3.78it/s] 70%|███████   | 174/248 [00:45<00:19,  3.74it/s] 71%|███████   | 175/248 [00:46<00:19,  3.73it/s] 71%|███████   | 176/248 [00:46<00:19,  3.68it/s] 71%|███████▏  | 177/248 [00:46<00:18,  3.78it/s] 72%|███████▏  | 178/248 [00:46<00:18,  3.74it/s] 72%|███████▏  | 179/248 [00:47<00:18,  3.74it/s] 73%|███████▎  | 180/248 [00:47<00:18,  3.69it/s] 73%|███████▎  | 181/248 [00:47<00:17,  3.79it/s] 73%|███████▎  | 182/248 [00:47<00:17,  3.74it/s] 74%|███████▍  | 183/248 [00:48<00:17,  3.73it/s] 74%|███████▍  | 184/248 [00:48<00:17,  3.68it/s] 75%|███████▍  | 185/248 [00:48<00:16,  3.77it/s] 75%|███████▌  | 186/248 [00:49<00:16,  3.73it/s] 75%|███████▌  | 187/248 [00:49<00:16,  3.71it/s] 76%|███████▌  | 188/248 [00:49<00:16,  3.72it/s] 76%|███████▌  | 189/248 [00:49<00:15,  3.77it/s] 77%|███████▋  | 190/248 [00:50<00:15,  3.73it/s] 77%|███████▋  | 191/248 [00:50<00:15,  3.68it/s] 77%|███████▋  | 192/248 [00:50<00:14,  3.75it/s] 78%|███████▊  | 193/248 [00:50<00:14,  3.79it/s] 78%|███████▊  | 194/248 [00:51<00:14,  3.74it/s] 79%|███████▊  | 195/248 [00:51<00:14,  3.67it/s] 79%|███████▉  | 196/248 [00:51<00:13,  3.74it/s] 79%|███████▉  | 197/248 [00:51<00:13,  3.76it/s] 80%|███████▉  | 198/248 [00:52<00:13,  3.72it/s] 80%|████████  | 199/248 [00:52<00:13,  3.65it/s] 81%|████████  | 200/248 [00:52<00:12,  3.77it/s] 81%|████████  | 201/248 [00:53<00:12,  3.78it/s] 81%|████████▏ | 202/248 [00:53<00:12,  3.74it/s] 82%|████████▏ | 203/248 [00:53<00:12,  3.66it/s] 82%|████████▏ | 204/248 [00:53<00:11,  3.77it/s] 83%|████████▎ | 205/248 [00:54<00:11,  3.77it/s] 83%|████████▎ | 206/248 [00:54<00:11,  3.73it/s] 83%|████████▎ | 207/248 [00:54<00:11,  3.65it/s] 84%|████████▍ | 208/248 [00:54<00:10,  3.78it/s] 84%|████████▍ | 209/248 [00:55<00:10,  3.76it/s] 85%|████████▍ | 210/248 [00:55<00:10,  3.74it/s] 85%|████████▌ | 211/248 [00:55<00:10,  3.66it/s] 85%|████████▌ | 212/248 [00:55<00:09,  3.77it/s] 86%|████████▌ | 213/248 [00:56<00:09,  3.77it/s] 86%|████████▋ | 214/248 [00:56<00:09,  3.74it/s] 87%|████████▋ | 215/248 [00:56<00:09,  3.65it/s] 87%|████████▋ | 216/248 [00:57<00:08,  3.77it/s] 88%|████████▊ | 217/248 [00:57<00:08,  3.76it/s] 88%|████████▊ | 218/248 [00:57<00:08,  3.72it/s] 88%|████████▊ | 219/248 [00:57<00:07,  3.65it/s] 89%|████████▊ | 220/248 [00:58<00:07,  3.77it/s] 89%|████████▉ | 221/248 [00:58<00:07,  3.76it/s] 90%|████████▉ | 222/248 [00:58<00:06,  3.72it/s] 90%|████████▉ | 223/248 [00:58<00:06,  3.67it/s] 90%|█████████ | 224/248 [00:59<00:06,  3.77it/s] 91%|█████████ | 225/248 [00:59<00:06,  3.72it/s] 91%|█████████ | 226/248 [00:59<00:05,  3.71it/s] 92%|█████████▏| 227/248 [01:00<00:05,  3.66it/s] 92%|█████████▏| 228/248 [01:00<00:05,  3.76it/s] 92%|█████████▏| 229/248 [01:00<00:05,  3.71it/s] 93%|█████████▎| 230/248 [01:00<00:04,  3.69it/s] 93%|█████████▎| 231/248 [01:01<00:04,  3.65it/s] 94%|█████████▎| 232/248 [01:01<00:04,  3.76it/s] 94%|█████████▍| 233/248 [01:01<00:04,  3.71it/s] 94%|█████████▍| 234/248 [01:01<00:03,  3.70it/s] 95%|█████████▍| 235/248 [01:02<00:03,  3.65it/s] 95%|█████████▌| 236/248 [01:02<00:03,  3.75it/s] 96%|█████████▌| 237/248 [01:02<00:02,  3.71it/s] 96%|█████████▌| 238/248 [01:02<00:02,  3.70it/s] 96%|█████████▋| 239/248 [01:03<00:02,  3.64it/s] 97%|█████████▋| 240/248 [01:03<00:02,  3.76it/s] 97%|█████████▋| 241/248 [01:03<00:01,  3.72it/s] 98%|█████████▊| 242/248 [01:04<00:01,  3.70it/s] 98%|█████████▊| 243/248 [01:04<00:01,  3.65it/s] 98%|█████████▊| 244/248 [01:04<00:01,  3.76it/s] 99%|█████████▉| 245/248 [01:04<00:00,  3.73it/s] 99%|█████████▉| 246/248 [01:05<00:00,  3.70it/s]100%|█████████▉| 247/248 [01:05<00:00,  3.65it/s]100%|██████████| 248/248 [01:05<00:00,  3.77it/s]accuracy:  0.6532258064516129
100%|██████████| 248/248 [01:09<00:00,  3.57it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 236
Training dataset size: 384, validation dataset size: 236
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.82s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:36:19,466] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:36:19,882] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:36:20,941] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<02:51,  2.72s/it]  3%|▎         | 2/64 [00:05<02:41,  2.60s/it]  5%|▍         | 3/64 [00:07<02:33,  2.51s/it]  6%|▋         | 4/64 [00:09<02:20,  2.34s/it]  8%|▊         | 5/64 [00:11<02:09,  2.19s/it]  9%|▉         | 6/64 [00:14<02:10,  2.25s/it] 11%|█         | 7/64 [00:16<02:10,  2.29s/it] 12%|█▎        | 8/64 [00:18<02:09,  2.31s/it] 14%|█▍        | 9/64 [00:20<02:02,  2.22s/it] 16%|█▌        | 10/64 [00:22<01:59,  2.22s/it]                                               {'loss': 1.055, 'grad_norm': 7.790726661682129, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:22<01:59,  2.22s/it] 17%|█▋        | 11/64 [00:25<02:01,  2.29s/it] 19%|█▉        | 12/64 [00:27<02:03,  2.37s/it] 20%|██        | 13/64 [00:29<01:55,  2.26s/it] 22%|██▏       | 14/64 [00:32<01:58,  2.36s/it] 23%|██▎       | 15/64 [00:34<01:55,  2.35s/it] 25%|██▌       | 16/64 [00:36<01:46,  2.23s/it] 27%|██▋       | 17/64 [00:39<01:46,  2.26s/it] 28%|██▊       | 18/64 [00:41<01:42,  2.23s/it] 30%|██▉       | 19/64 [00:44<01:47,  2.39s/it] 31%|███▏      | 20/64 [00:46<01:42,  2.34s/it]                                               {'loss': 0.8545, 'grad_norm': 8.890902519226074, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:46<01:42,  2.34s/it] 33%|███▎      | 21/64 [00:48<01:42,  2.37s/it] 34%|███▍      | 22/64 [00:51<01:39,  2.36s/it] 36%|███▌      | 23/64 [00:53<01:35,  2.34s/it] 38%|███▊      | 24/64 [00:55<01:29,  2.25s/it] 39%|███▉      | 25/64 [00:58<01:32,  2.38s/it] 41%|████      | 26/64 [01:00<01:33,  2.45s/it] 42%|████▏     | 27/64 [01:03<01:31,  2.47s/it] 44%|████▍     | 28/64 [01:05<01:27,  2.43s/it] 45%|████▌     | 29/64 [01:07<01:23,  2.38s/it] 47%|████▋     | 30/64 [01:09<01:18,  2.31s/it]                                               {'loss': 0.8772, 'grad_norm': 6.681375980377197, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:09<01:18,  2.31s/it] 48%|████▊     | 31/64 [01:12<01:16,  2.31s/it] 50%|█████     | 32/64 [01:14<01:16,  2.38s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:17<01:13,  2.37s/it] 53%|█████▎    | 34/64 [01:19<01:11,  2.40s/it] 55%|█████▍    | 35/64 [01:22<01:13,  2.52s/it] 56%|█████▋    | 36/64 [01:24<01:08,  2.43s/it] 58%|█████▊    | 37/64 [01:26<01:02,  2.32s/it] 59%|█████▉    | 38/64 [01:29<01:02,  2.40s/it] 61%|██████    | 39/64 [01:31<01:01,  2.46s/it] 62%|██████▎   | 40/64 [01:33<00:56,  2.34s/it]                                               {'loss': 0.6447, 'grad_norm': 5.3515238761901855, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:33<00:56,  2.34s/it] 64%|██████▍   | 41/64 [01:36<00:54,  2.39s/it] 66%|██████▌   | 42/64 [01:38<00:52,  2.36s/it] 67%|██████▋   | 43/64 [01:41<00:50,  2.40s/it] 69%|██████▉   | 44/64 [01:43<00:48,  2.41s/it] 70%|███████   | 45/64 [01:45<00:44,  2.33s/it] 72%|███████▏  | 46/64 [01:48<00:41,  2.29s/it] 73%|███████▎  | 47/64 [01:50<00:38,  2.28s/it] 75%|███████▌  | 48/64 [01:52<00:36,  2.27s/it] 77%|███████▋  | 49/64 [01:54<00:33,  2.21s/it] 78%|███████▊  | 50/64 [01:56<00:31,  2.26s/it]                                               {'loss': 0.7354, 'grad_norm': 4.742506980895996, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:56<00:31,  2.26s/it] 80%|███████▉  | 51/64 [01:59<00:30,  2.33s/it] 81%|████████▏ | 52/64 [02:01<00:27,  2.28s/it] 83%|████████▎ | 53/64 [02:03<00:24,  2.26s/it] 84%|████████▍ | 54/64 [02:06<00:23,  2.39s/it] 86%|████████▌ | 55/64 [02:08<00:21,  2.40s/it] 88%|████████▊ | 56/64 [02:11<00:18,  2.36s/it] 89%|████████▉ | 57/64 [02:13<00:15,  2.27s/it] 91%|█████████ | 58/64 [02:15<00:13,  2.31s/it] 92%|█████████▏| 59/64 [02:17<00:11,  2.29s/it] 94%|█████████▍| 60/64 [02:20<00:09,  2.46s/it]                                               {'loss': 0.6909, 'grad_norm': 7.69789981842041, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:20<00:09,  2.46s/it] 95%|█████████▌| 61/64 [02:23<00:07,  2.46s/it] 97%|█████████▋| 62/64 [02:25<00:04,  2.47s/it] 98%|█████████▊| 63/64 [02:28<00:02,  2.42s/it]100%|██████████| 64/64 [02:30<00:00,  2.28s/it]                                               {'train_runtime': 150.6744, 'train_samples_per_second': 5.097, 'train_steps_per_second': 0.425, 'train_loss': 0.7956962063908577, 'epoch': 2.0}
100%|██████████| 64/64 [02:30<00:00,  2.28s/it]100%|██████████| 64/64 [02:30<00:00,  2.35s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  361
  0%|          | 0/361 [00:00<?, ?it/s]  0%|          | 1/361 [00:00<02:13,  2.70it/s]  1%|          | 2/361 [00:00<01:45,  3.40it/s]  1%|          | 3/361 [00:00<01:41,  3.54it/s]  1%|          | 4/361 [00:01<01:39,  3.60it/s]  1%|▏         | 5/361 [00:01<01:38,  3.61it/s]  2%|▏         | 6/361 [00:01<01:33,  3.80it/s]  2%|▏         | 7/361 [00:01<01:32,  3.83it/s]  2%|▏         | 8/361 [00:02<01:33,  3.78it/s]  2%|▏         | 9/361 [00:02<01:31,  3.84it/s]  3%|▎         | 10/361 [00:02<01:33,  3.76it/s]  3%|▎         | 11/361 [00:02<01:30,  3.85it/s]  3%|▎         | 12/361 [00:03<01:31,  3.79it/s]  4%|▎         | 13/361 [00:03<01:31,  3.80it/s]  4%|▍         | 14/361 [00:03<01:32,  3.75it/s]  4%|▍         | 15/361 [00:04<01:29,  3.86it/s]  4%|▍         | 16/361 [00:04<01:29,  3.83it/s]  5%|▍         | 17/361 [00:04<01:30,  3.78it/s]  5%|▍         | 18/361 [00:04<01:31,  3.73it/s]  5%|▌         | 19/361 [00:05<01:28,  3.85it/s]  6%|▌         | 20/361 [00:05<01:28,  3.86it/s]  6%|▌         | 21/361 [00:05<01:29,  3.81it/s]  6%|▌         | 22/361 [00:05<01:28,  3.85it/s]  6%|▋         | 23/361 [00:06<01:29,  3.77it/s]  7%|▋         | 24/361 [00:06<01:27,  3.85it/s]  7%|▋         | 25/361 [00:06<01:28,  3.79it/s]  7%|▋         | 26/361 [00:06<01:27,  3.84it/s]  7%|▋         | 27/361 [00:07<01:28,  3.77it/s]  8%|▊         | 28/361 [00:07<01:26,  3.86it/s]  8%|▊         | 29/361 [00:07<01:27,  3.81it/s]  8%|▊         | 30/361 [00:07<01:27,  3.77it/s]  9%|▊         | 31/361 [00:08<01:28,  3.73it/s]  9%|▉         | 32/361 [00:08<01:25,  3.85it/s]  9%|▉         | 33/361 [00:08<01:25,  3.86it/s]  9%|▉         | 34/361 [00:09<01:26,  3.80it/s] 10%|▉         | 35/361 [00:09<01:24,  3.84it/s] 10%|▉         | 36/361 [00:09<01:26,  3.77it/s] 10%|█         | 37/361 [00:09<01:23,  3.86it/s] 11%|█         | 38/361 [00:10<01:24,  3.81it/s] 11%|█         | 39/361 [00:10<01:24,  3.79it/s] 11%|█         | 40/361 [00:10<01:25,  3.74it/s] 11%|█▏        | 41/361 [00:10<01:22,  3.86it/s] 12%|█▏        | 42/361 [00:11<01:23,  3.84it/s] 12%|█▏        | 43/361 [00:11<01:23,  3.81it/s] 12%|█▏        | 44/361 [00:11<01:22,  3.85it/s] 12%|█▏        | 45/361 [00:11<01:23,  3.78it/s] 13%|█▎        | 46/361 [00:12<01:21,  3.86it/s] 13%|█▎        | 47/361 [00:12<01:22,  3.80it/s] 13%|█▎        | 48/361 [00:12<01:22,  3.79it/s] 14%|█▎        | 49/361 [00:12<01:23,  3.74it/s] 14%|█▍        | 50/361 [00:13<01:20,  3.86it/s] 14%|█▍        | 51/361 [00:13<01:20,  3.86it/s] 14%|█▍        | 52/361 [00:13<01:21,  3.80it/s] 15%|█▍        | 53/361 [00:13<01:20,  3.84it/s] 15%|█▍        | 54/361 [00:14<01:21,  3.77it/s] 15%|█▌        | 55/361 [00:14<01:19,  3.86it/s] 16%|█▌        | 56/361 [00:14<01:20,  3.80it/s] 16%|█▌        | 57/361 [00:15<01:20,  3.79it/s] 16%|█▌        | 58/361 [00:15<01:21,  3.74it/s] 16%|█▋        | 59/361 [00:15<01:18,  3.86it/s] 17%|█▋        | 60/361 [00:15<01:17,  3.87it/s] 17%|█▋        | 61/361 [00:16<01:18,  3.80it/s] 17%|█▋        | 62/361 [00:16<01:17,  3.84it/s] 17%|█▋        | 63/361 [00:16<01:19,  3.77it/s] 18%|█▊        | 64/361 [00:16<01:17,  3.85it/s] 18%|█▊        | 65/361 [00:17<01:18,  3.79it/s] 18%|█▊        | 66/361 [00:17<01:18,  3.74it/s] 19%|█▊        | 67/361 [00:17<01:17,  3.79it/s] 19%|█▉        | 68/361 [00:17<01:16,  3.83it/s] 19%|█▉        | 69/361 [00:18<01:17,  3.78it/s] 19%|█▉        | 70/361 [00:18<01:17,  3.74it/s] 20%|█▉        | 71/361 [00:18<01:16,  3.77it/s] 20%|█▉        | 72/361 [00:18<01:15,  3.83it/s] 20%|██        | 73/361 [00:19<01:16,  3.77it/s] 20%|██        | 74/361 [00:19<01:17,  3.72it/s] 21%|██        | 75/361 [00:19<01:15,  3.77it/s] 21%|██        | 76/361 [00:20<01:14,  3.80it/s] 21%|██▏       | 77/361 [00:20<01:15,  3.75it/s] 22%|██▏       | 78/361 [00:20<01:16,  3.70it/s] 22%|██▏       | 79/361 [00:20<01:14,  3.77it/s] 22%|██▏       | 80/361 [00:21<01:13,  3.82it/s] 22%|██▏       | 81/361 [00:21<01:14,  3.76it/s] 23%|██▎       | 82/361 [00:21<01:15,  3.72it/s] 23%|██▎       | 83/361 [00:21<01:13,  3.78it/s] 23%|██▎       | 84/361 [00:22<01:12,  3.82it/s] 24%|██▎       | 85/361 [00:22<01:13,  3.76it/s] 24%|██▍       | 86/361 [00:22<01:13,  3.72it/s] 24%|██▍       | 87/361 [00:22<01:12,  3.78it/s] 24%|██▍       | 88/361 [00:23<01:11,  3.84it/s] 25%|██▍       | 89/361 [00:23<01:12,  3.77it/s] 25%|██▍       | 90/361 [00:23<01:12,  3.74it/s] 25%|██▌       | 91/361 [00:24<01:10,  3.86it/s] 25%|██▌       | 92/361 [00:24<01:08,  3.94it/s] 26%|██▌       | 93/361 [00:24<01:06,  4.01it/s] 26%|██▌       | 94/361 [00:24<01:05,  4.05it/s] 26%|██▋       | 95/361 [00:24<01:05,  4.09it/s] 27%|██▋       | 96/361 [00:25<01:04,  4.11it/s] 27%|██▋       | 97/361 [00:25<01:03,  4.13it/s] 27%|██▋       | 98/361 [00:25<01:03,  4.14it/s] 27%|██▋       | 99/361 [00:25<01:03,  4.15it/s] 28%|██▊       | 100/361 [00:26<01:02,  4.15it/s] 28%|██▊       | 101/361 [00:26<01:02,  4.15it/s] 28%|██▊       | 102/361 [00:26<01:02,  4.15it/s] 29%|██▊       | 103/361 [00:26<01:02,  4.15it/s] 29%|██▉       | 104/361 [00:27<01:01,  4.15it/s] 29%|██▉       | 105/361 [00:27<01:01,  4.14it/s] 29%|██▉       | 106/361 [00:27<01:01,  4.14it/s] 30%|██▉       | 107/361 [00:27<01:01,  4.14it/s] 30%|██▉       | 108/361 [00:28<01:01,  4.14it/s] 30%|███       | 109/361 [00:28<01:00,  4.15it/s] 30%|███       | 110/361 [00:28<01:00,  4.15it/s] 31%|███       | 111/361 [00:28<01:01,  4.07it/s] 31%|███       | 112/361 [00:29<01:03,  3.94it/s] 31%|███▏      | 113/361 [00:29<01:03,  3.89it/s] 32%|███▏      | 114/361 [00:29<01:02,  3.95it/s] 32%|███▏      | 115/361 [00:29<01:02,  3.91it/s] 32%|███▏      | 116/361 [00:30<01:03,  3.83it/s] 32%|███▏      | 117/361 [00:30<01:05,  3.75it/s] 33%|███▎      | 118/361 [00:30<01:03,  3.80it/s] 33%|███▎      | 119/361 [00:30<01:02,  3.86it/s] 33%|███▎      | 120/361 [00:31<01:03,  3.80it/s] 34%|███▎      | 121/361 [00:31<01:03,  3.77it/s] 34%|███▍      | 122/361 [00:31<01:04,  3.73it/s] 34%|███▍      | 123/361 [00:32<01:01,  3.84it/s] 34%|███▍      | 124/361 [00:32<01:02,  3.81it/s] 35%|███▍      | 125/361 [00:32<01:02,  3.77it/s] 35%|███▍      | 126/361 [00:32<01:03,  3.70it/s] 35%|███▌      | 127/361 [00:33<01:01,  3.82it/s] 35%|███▌      | 128/361 [00:33<01:00,  3.83it/s] 36%|███▌      | 129/361 [00:33<01:01,  3.78it/s] 36%|███▌      | 130/361 [00:33<01:02,  3.72it/s] 36%|███▋      | 131/361 [00:34<01:00,  3.79it/s] 37%|███▋      | 132/361 [00:34<00:59,  3.85it/s] 37%|███▋      | 133/361 [00:34<01:00,  3.79it/s] 37%|███▋      | 134/361 [00:34<01:00,  3.78it/s] 37%|███▋      | 135/361 [00:35<01:00,  3.72it/s] 38%|███▊      | 136/361 [00:35<00:58,  3.83it/s] 38%|███▊      | 137/361 [00:35<00:59,  3.77it/s] 38%|███▊      | 138/361 [00:35<00:59,  3.74it/s] 39%|███▊      | 139/361 [00:36<00:59,  3.70it/s] 39%|███▉      | 140/361 [00:36<00:57,  3.83it/s] 39%|███▉      | 141/361 [00:36<00:57,  3.81it/s] 39%|███▉      | 142/361 [00:37<00:57,  3.79it/s] 40%|███▉      | 143/361 [00:37<00:58,  3.72it/s] 40%|███▉      | 144/361 [00:37<00:57,  3.78it/s] 40%|████      | 145/361 [00:37<00:56,  3.83it/s] 40%|████      | 146/361 [00:38<00:56,  3.79it/s] 41%|████      | 147/361 [00:38<00:56,  3.78it/s] 41%|████      | 148/361 [00:38<00:57,  3.72it/s] 41%|████▏     | 149/361 [00:38<00:55,  3.84it/s] 42%|████▏     | 150/361 [00:39<00:55,  3.79it/s] 42%|████▏     | 151/361 [00:39<00:55,  3.75it/s] 42%|████▏     | 152/361 [00:39<00:56,  3.71it/s] 42%|████▏     | 153/361 [00:39<00:54,  3.83it/s] 43%|████▎     | 154/361 [00:40<00:53,  3.84it/s] 43%|████▎     | 155/361 [00:40<00:54,  3.79it/s] 43%|████▎     | 156/361 [00:40<00:55,  3.72it/s] 43%|████▎     | 157/361 [00:41<00:53,  3.78it/s] 44%|████▍     | 158/361 [00:41<00:52,  3.83it/s] 44%|████▍     | 159/361 [00:41<00:53,  3.78it/s] 44%|████▍     | 160/361 [00:41<00:53,  3.77it/s] 45%|████▍     | 161/361 [00:42<00:53,  3.71it/s] 45%|████▍     | 162/361 [00:42<00:52,  3.83it/s] 45%|████▌     | 163/361 [00:42<00:52,  3.78it/s] 45%|████▌     | 164/361 [00:42<00:52,  3.74it/s] 46%|████▌     | 165/361 [00:43<00:53,  3.70it/s] 46%|████▌     | 166/361 [00:43<00:51,  3.82it/s] 46%|████▋     | 167/361 [00:43<00:51,  3.79it/s] 47%|████▋     | 168/361 [00:43<00:51,  3.76it/s] 47%|████▋     | 169/361 [00:44<00:51,  3.70it/s] 47%|████▋     | 170/361 [00:44<00:50,  3.77it/s] 47%|████▋     | 171/361 [00:44<00:49,  3.84it/s] 48%|████▊     | 172/361 [00:44<00:49,  3.79it/s] 48%|████▊     | 173/361 [00:45<00:50,  3.75it/s] 48%|████▊     | 174/361 [00:45<00:50,  3.70it/s] 48%|████▊     | 175/361 [00:45<00:48,  3.81it/s] 49%|████▉     | 176/361 [00:46<00:48,  3.80it/s] 49%|████▉     | 177/361 [00:46<00:48,  3.77it/s] 49%|████▉     | 178/361 [00:46<00:49,  3.70it/s] 50%|████▉     | 179/361 [00:46<00:48,  3.75it/s] 50%|████▉     | 180/361 [00:47<00:47,  3.80it/s] 50%|█████     | 181/361 [00:47<00:47,  3.76it/s] 50%|█████     | 182/361 [00:47<00:47,  3.74it/s] 51%|█████     | 183/361 [00:47<00:47,  3.75it/s] 51%|█████     | 184/361 [00:48<00:46,  3.83it/s] 51%|█████     | 185/361 [00:48<00:46,  3.78it/s] 52%|█████▏    | 186/361 [00:48<00:46,  3.74it/s] 52%|█████▏    | 187/361 [00:48<00:47,  3.70it/s] 52%|█████▏    | 188/361 [00:49<00:45,  3.81it/s] 52%|█████▏    | 189/361 [00:49<00:45,  3.79it/s] 53%|█████▎    | 190/361 [00:49<00:45,  3.76it/s] 53%|█████▎    | 191/361 [00:50<00:46,  3.69it/s] 53%|█████▎    | 192/361 [00:50<00:45,  3.75it/s] 53%|█████▎    | 193/361 [00:50<00:44,  3.81it/s] 54%|█████▎    | 194/361 [00:50<00:44,  3.76it/s] 54%|█████▍    | 195/361 [00:51<00:44,  3.74it/s] 54%|█████▍    | 196/361 [00:51<00:44,  3.70it/s] 55%|█████▍    | 197/361 [00:51<00:43,  3.81it/s] 55%|█████▍    | 198/361 [00:51<00:42,  3.80it/s] 55%|█████▌    | 199/361 [00:52<00:43,  3.75it/s] 55%|█████▌    | 200/361 [00:52<00:43,  3.70it/s] 56%|█████▌    | 201/361 [00:52<00:42,  3.76it/s] 56%|█████▌    | 202/361 [00:52<00:41,  3.82it/s] 56%|█████▌    | 203/361 [00:53<00:42,  3.76it/s] 57%|█████▋    | 204/361 [00:53<00:42,  3.74it/s] 57%|█████▋    | 205/361 [00:53<00:42,  3.69it/s] 57%|█████▋    | 206/361 [00:54<00:40,  3.81it/s] 57%|█████▋    | 207/361 [00:54<00:40,  3.79it/s] 58%|█████▊    | 208/361 [00:54<00:40,  3.74it/s] 58%|█████▊    | 209/361 [00:54<00:40,  3.75it/s] 58%|█████▊    | 210/361 [00:55<00:39,  3.84it/s] 58%|█████▊    | 211/361 [00:55<00:38,  3.92it/s] 59%|█████▊    | 212/361 [00:55<00:37,  3.98it/s] 59%|█████▉    | 213/361 [00:55<00:36,  4.02it/s] 59%|█████▉    | 214/361 [00:56<00:36,  4.04it/s] 60%|█████▉    | 215/361 [00:56<00:36,  4.05it/s] 60%|█████▉    | 216/361 [00:56<00:35,  4.07it/s] 60%|██████    | 217/361 [00:56<00:35,  4.08it/s] 60%|██████    | 218/361 [00:57<00:35,  4.08it/s] 61%|██████    | 219/361 [00:57<00:34,  4.08it/s] 61%|██████    | 220/361 [00:57<00:34,  4.08it/s] 61%|██████    | 221/361 [00:57<00:34,  4.09it/s] 61%|██████▏   | 222/361 [00:58<00:33,  4.09it/s] 62%|██████▏   | 223/361 [00:58<00:33,  4.09it/s] 62%|██████▏   | 224/361 [00:58<00:33,  4.09it/s] 62%|██████▏   | 225/361 [00:58<00:33,  4.09it/s] 63%|██████▎   | 226/361 [00:58<00:32,  4.10it/s] 63%|██████▎   | 227/361 [00:59<00:32,  4.09it/s] 63%|██████▎   | 228/361 [00:59<00:32,  4.09it/s] 63%|██████▎   | 229/361 [00:59<00:32,  4.09it/s] 64%|██████▎   | 230/361 [00:59<00:32,  3.98it/s] 64%|██████▍   | 231/361 [01:00<00:33,  3.87it/s] 64%|██████▍   | 232/361 [01:00<00:33,  3.87it/s] 65%|██████▍   | 233/361 [01:00<00:32,  3.91it/s] 65%|██████▍   | 234/361 [01:01<00:33,  3.81it/s] 65%|██████▌   | 235/361 [01:01<00:33,  3.79it/s] 65%|██████▌   | 236/361 [01:01<00:33,  3.74it/s] 66%|██████▌   | 237/361 [01:01<00:32,  3.79it/s] 66%|██████▌   | 238/361 [01:02<00:32,  3.75it/s] 66%|██████▌   | 239/361 [01:02<00:32,  3.72it/s] 66%|██████▋   | 240/361 [01:02<00:32,  3.72it/s] 67%|██████▋   | 241/361 [01:02<00:31,  3.79it/s] 67%|██████▋   | 242/361 [01:03<00:31,  3.74it/s] 67%|██████▋   | 243/361 [01:03<00:31,  3.70it/s] 68%|██████▊   | 244/361 [01:03<00:31,  3.71it/s] 68%|██████▊   | 245/361 [01:03<00:30,  3.77it/s] 68%|██████▊   | 246/361 [01:04<00:30,  3.73it/s] 68%|██████▊   | 247/361 [01:04<00:30,  3.68it/s] 69%|██████▊   | 248/361 [01:04<00:30,  3.73it/s] 69%|██████▉   | 249/361 [01:05<00:29,  3.77it/s] 69%|██████▉   | 250/361 [01:05<00:29,  3.73it/s] 70%|██████▉   | 251/361 [01:05<00:29,  3.69it/s] 70%|██████▉   | 252/361 [01:05<00:29,  3.74it/s] 70%|███████   | 253/361 [01:06<00:28,  3.80it/s] 70%|███████   | 254/361 [01:06<00:28,  3.74it/s] 71%|███████   | 255/361 [01:06<00:28,  3.71it/s] 71%|███████   | 256/361 [01:06<00:28,  3.73it/s] 71%|███████   | 257/361 [01:07<00:27,  3.78it/s] 71%|███████▏  | 258/361 [01:07<00:27,  3.74it/s] 72%|███████▏  | 259/361 [01:07<00:27,  3.70it/s] 72%|███████▏  | 260/361 [01:08<00:27,  3.73it/s] 72%|███████▏  | 261/361 [01:08<00:26,  3.80it/s] 73%|███████▎  | 262/361 [01:08<00:26,  3.75it/s] 73%|███████▎  | 263/361 [01:08<00:26,  3.73it/s] 73%|███████▎  | 264/361 [01:09<00:26,  3.72it/s] 73%|███████▎  | 265/361 [01:09<00:25,  3.77it/s] 74%|███████▎  | 266/361 [01:09<00:25,  3.74it/s] 74%|███████▍  | 267/361 [01:09<00:25,  3.70it/s] 74%|███████▍  | 268/361 [01:10<00:24,  3.73it/s] 75%|███████▍  | 269/361 [01:10<00:24,  3.79it/s] 75%|███████▍  | 270/361 [01:10<00:24,  3.74it/s] 75%|███████▌  | 271/361 [01:10<00:23,  3.76it/s] 75%|███████▌  | 272/361 [01:11<00:24,  3.69it/s] 76%|███████▌  | 273/361 [01:11<00:23,  3.77it/s] 76%|███████▌  | 274/361 [01:11<00:23,  3.72it/s] 76%|███████▌  | 275/361 [01:12<00:23,  3.71it/s] 76%|███████▋  | 276/361 [01:12<00:23,  3.67it/s] 77%|███████▋  | 277/361 [01:12<00:22,  3.79it/s] 77%|███████▋  | 278/361 [01:12<00:22,  3.75it/s] 77%|███████▋  | 279/361 [01:13<00:22,  3.71it/s] 78%|███████▊  | 280/361 [01:13<00:22,  3.66it/s] 78%|███████▊  | 281/361 [01:13<00:21,  3.78it/s] 78%|███████▊  | 282/361 [01:13<00:21,  3.76it/s] 78%|███████▊  | 283/361 [01:14<00:20,  3.74it/s] 79%|███████▊  | 284/361 [01:14<00:20,  3.68it/s] 79%|███████▉  | 285/361 [01:14<00:20,  3.74it/s] 79%|███████▉  | 286/361 [01:14<00:19,  3.78it/s] 80%|███████▉  | 287/361 [01:15<00:19,  3.73it/s] 80%|███████▉  | 288/361 [01:15<00:19,  3.69it/s] 80%|████████  | 289/361 [01:15<00:19,  3.74it/s] 80%|████████  | 290/361 [01:16<00:18,  3.78it/s] 81%|████████  | 291/361 [01:16<00:18,  3.74it/s] 81%|████████  | 292/361 [01:16<00:18,  3.67it/s] 81%|████████  | 293/361 [01:16<00:18,  3.73it/s] 81%|████████▏ | 294/361 [01:17<00:17,  3.79it/s] 82%|████████▏ | 295/361 [01:17<00:17,  3.73it/s] 82%|████████▏ | 296/361 [01:17<00:17,  3.68it/s] 82%|████████▏ | 297/361 [01:17<00:17,  3.73it/s] 83%|████████▎ | 298/361 [01:18<00:16,  3.78it/s] 83%|████████▎ | 299/361 [01:18<00:16,  3.74it/s] 83%|████████▎ | 300/361 [01:18<00:16,  3.71it/s] 83%|████████▎ | 301/361 [01:19<00:16,  3.73it/s] 84%|████████▎ | 302/361 [01:19<00:15,  3.80it/s] 84%|████████▍ | 303/361 [01:19<00:15,  3.73it/s] 84%|████████▍ | 304/361 [01:19<00:15,  3.68it/s] 84%|████████▍ | 305/361 [01:20<00:15,  3.73it/s] 85%|████████▍ | 306/361 [01:20<00:14,  3.78it/s] 85%|████████▌ | 307/361 [01:20<00:14,  3.73it/s] 85%|████████▌ | 308/361 [01:20<00:14,  3.69it/s] 86%|████████▌ | 309/361 [01:21<00:13,  3.73it/s] 86%|████████▌ | 310/361 [01:21<00:13,  3.78it/s] 86%|████████▌ | 311/361 [01:21<00:13,  3.74it/s] 86%|████████▋ | 312/361 [01:21<00:13,  3.69it/s] 87%|████████▋ | 313/361 [01:22<00:12,  3.73it/s] 87%|████████▋ | 314/361 [01:22<00:12,  3.78it/s] 87%|████████▋ | 315/361 [01:22<00:12,  3.73it/s] 88%|████████▊ | 316/361 [01:23<00:12,  3.69it/s] 88%|████████▊ | 317/361 [01:23<00:11,  3.73it/s] 88%|████████▊ | 318/361 [01:23<00:11,  3.79it/s] 88%|████████▊ | 319/361 [01:23<00:11,  3.75it/s] 89%|████████▊ | 320/361 [01:24<00:11,  3.72it/s] 89%|████████▉ | 321/361 [01:24<00:10,  3.72it/s] 89%|████████▉ | 322/361 [01:24<00:10,  3.78it/s] 89%|████████▉ | 323/361 [01:24<00:10,  3.73it/s] 90%|████████▉ | 324/361 [01:25<00:10,  3.69it/s] 90%|█████████ | 325/361 [01:25<00:09,  3.72it/s] 90%|█████████ | 326/361 [01:25<00:09,  3.75it/s] 91%|█████████ | 327/361 [01:25<00:09,  3.71it/s] 91%|█████████ | 328/361 [01:26<00:09,  3.66it/s] 91%|█████████ | 329/361 [01:26<00:08,  3.73it/s] 91%|█████████▏| 330/361 [01:26<00:08,  3.76it/s] 92%|█████████▏| 331/361 [01:27<00:08,  3.72it/s] 92%|█████████▏| 332/361 [01:27<00:07,  3.68it/s] 92%|█████████▏| 333/361 [01:27<00:07,  3.73it/s] 93%|█████████▎| 334/361 [01:27<00:07,  3.77it/s] 93%|█████████▎| 335/361 [01:28<00:06,  3.73it/s] 93%|█████████▎| 336/361 [01:28<00:06,  3.68it/s] 93%|█████████▎| 337/361 [01:28<00:06,  3.73it/s] 94%|█████████▎| 338/361 [01:28<00:06,  3.79it/s] 94%|█████████▍| 339/361 [01:29<00:05,  3.73it/s] 94%|█████████▍| 340/361 [01:29<00:05,  3.73it/s] 94%|█████████▍| 341/361 [01:29<00:05,  3.67it/s] 95%|█████████▍| 342/361 [01:29<00:05,  3.79it/s] 95%|█████████▌| 343/361 [01:30<00:04,  3.74it/s] 95%|█████████▌| 344/361 [01:30<00:04,  3.71it/s] 96%|█████████▌| 345/361 [01:30<00:04,  3.66it/s] 96%|█████████▌| 346/361 [01:31<00:03,  3.78it/s] 96%|█████████▌| 347/361 [01:31<00:03,  3.75it/s] 96%|█████████▋| 348/361 [01:31<00:03,  3.71it/s] 97%|█████████▋| 349/361 [01:31<00:03,  3.66it/s] 97%|█████████▋| 350/361 [01:32<00:02,  3.77it/s] 97%|█████████▋| 351/361 [01:32<00:02,  3.74it/s] 98%|█████████▊| 352/361 [01:32<00:02,  3.70it/s] 98%|█████████▊| 353/361 [01:32<00:02,  3.66it/s] 98%|█████████▊| 354/361 [01:33<00:01,  3.78it/s] 98%|█████████▊| 355/361 [01:33<00:01,  3.73it/s] 99%|█████████▊| 356/361 [01:33<00:01,  3.70it/s] 99%|█████████▉| 357/361 [01:34<00:01,  3.66it/s] 99%|█████████▉| 358/361 [01:34<00:00,  3.78it/s] 99%|█████████▉| 359/361 [01:34<00:00,  3.75it/s]100%|█████████▉| 360/361 [01:34<00:00,  3.71it/s]100%|██████████| 361/361 [01:35<00:00,  3.67it/s]accuracy:  0.6260387811634349
100%|██████████| 361/361 [01:40<00:00,  3.60it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 188
Training dataset size: 384, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 188
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.03s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:41:01,329] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:41:01,741] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.49s/it][93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 07:41:03,132] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<02:51,  2.72s/it]  3%|▎         | 2/64 [00:05<02:54,  2.81s/it]  5%|▍         | 3/64 [00:08<02:43,  2.69s/it]  6%|▋         | 4/64 [00:10<02:31,  2.52s/it]  8%|▊         | 5/64 [00:13<02:37,  2.66s/it]  9%|▉         | 6/64 [00:15<02:32,  2.63s/it] 11%|█         | 7/64 [00:18<02:30,  2.64s/it] 12%|█▎        | 8/64 [00:21<02:29,  2.66s/it] 14%|█▍        | 9/64 [00:23<02:26,  2.66s/it] 16%|█▌        | 10/64 [00:26<02:24,  2.68s/it]                                               {'loss': 0.7365, 'grad_norm': 8.613594055175781, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:26<02:24,  2.68s/it] 17%|█▋        | 11/64 [00:29<02:26,  2.77s/it] 19%|█▉        | 12/64 [00:32<02:31,  2.92s/it] 20%|██        | 13/64 [00:35<02:23,  2.81s/it] 22%|██▏       | 14/64 [00:37<02:15,  2.71s/it] 23%|██▎       | 15/64 [00:40<02:11,  2.68s/it] 25%|██▌       | 16/64 [00:43<02:12,  2.77s/it] 27%|██▋       | 17/64 [00:45<02:05,  2.67s/it] 28%|██▊       | 18/64 [00:48<02:03,  2.68s/it] 30%|██▉       | 19/64 [00:51<02:02,  2.73s/it] 31%|███▏      | 20/64 [00:54<01:58,  2.70s/it]                                               {'loss': 0.5404, 'grad_norm': 9.94523811340332, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:54<01:58,  2.70s/it] 33%|███▎      | 21/64 [00:56<01:56,  2.72s/it] 34%|███▍      | 22/64 [00:59<01:54,  2.73s/it] 36%|███▌      | 23/64 [01:01<01:44,  2.55s/it] 38%|███▊      | 24/64 [01:04<01:46,  2.66s/it] 39%|███▉      | 25/64 [01:07<01:45,  2.71s/it] 41%|████      | 26/64 [01:10<01:43,  2.71s/it] 42%|████▏     | 27/64 [01:13<01:47,  2.90s/it] 44%|████▍     | 28/64 [01:16<01:45,  2.94s/it] 45%|████▌     | 29/64 [01:19<01:46,  3.03s/it] 47%|████▋     | 30/64 [01:22<01:40,  2.97s/it]                                               {'loss': 0.5371, 'grad_norm': 6.967282295227051, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:22<01:40,  2.97s/it] 48%|████▊     | 31/64 [01:25<01:40,  3.03s/it] 50%|█████     | 32/64 [01:28<01:31,  2.86s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Traceback (most recent call last):
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
run_grm_reward_train.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-12_13:35:30
  host      : gpu28.int.autonlab.org
  rank      : 1 (local_rank: 1)
  exitcode  : -7 (pid: 887725)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 887725
======================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  212
  0%|          | 0/212 [00:00<?, ?it/s]  0%|          | 1/212 [00:00<01:38,  2.14it/s]  1%|          | 2/212 [00:00<01:09,  3.02it/s]  1%|▏         | 3/212 [00:00<01:00,  3.46it/s]  2%|▏         | 4/212 [00:01<00:55,  3.72it/s]  2%|▏         | 5/212 [00:01<00:53,  3.88it/s]  3%|▎         | 6/212 [00:01<00:51,  3.99it/s]  3%|▎         | 7/212 [00:01<00:50,  4.06it/s]  4%|▍         | 8/212 [00:02<00:49,  4.12it/s]  4%|▍         | 9/212 [00:02<00:48,  4.16it/s]  5%|▍         | 10/212 [00:02<00:48,  4.18it/s]  5%|▌         | 11/212 [00:02<00:48,  4.18it/s]  6%|▌         | 12/212 [00:03<00:47,  4.19it/s]  6%|▌         | 13/212 [00:03<00:47,  4.19it/s]  7%|▋         | 14/212 [00:03<00:47,  4.19it/s]  7%|▋         | 15/212 [00:03<00:46,  4.20it/s]  8%|▊         | 16/212 [00:04<00:46,  4.21it/s]  8%|▊         | 17/212 [00:04<00:46,  4.22it/s]  8%|▊         | 18/212 [00:04<00:46,  4.21it/s]  9%|▉         | 19/212 [00:04<00:45,  4.21it/s]  9%|▉         | 20/212 [00:04<00:45,  4.20it/s] 10%|▉         | 21/212 [00:05<00:45,  4.20it/s] 10%|█         | 22/212 [00:05<00:45,  4.20it/s] 11%|█         | 23/212 [00:05<00:44,  4.20it/s] 11%|█▏        | 24/212 [00:05<00:44,  4.21it/s] 12%|█▏        | 25/212 [00:06<00:44,  4.23it/s] 12%|█▏        | 26/212 [00:06<00:43,  4.23it/s] 13%|█▎        | 27/212 [00:06<00:43,  4.22it/s] 13%|█▎        | 28/212 [00:06<00:43,  4.22it/s] 14%|█▎        | 29/212 [00:07<00:43,  4.23it/s] 14%|█▍        | 30/212 [00:07<00:42,  4.24it/s] 15%|█▍        | 31/212 [00:07<00:42,  4.24it/s] 15%|█▌        | 32/212 [00:07<00:42,  4.22it/s] 16%|█▌        | 33/212 [00:08<00:42,  4.22it/s] 16%|█▌        | 34/212 [00:08<00:42,  4.23it/s] 17%|█▋        | 35/212 [00:08<00:41,  4.23it/s] 17%|█▋        | 36/212 [00:08<00:41,  4.24it/s] 17%|█▋        | 37/212 [00:08<00:41,  4.24it/s] 18%|█▊        | 38/212 [00:09<00:41,  4.23it/s] 18%|█▊        | 39/212 [00:09<00:40,  4.23it/s] 19%|█▉        | 40/212 [00:09<00:40,  4.24it/s] 19%|█▉        | 41/212 [00:09<00:40,  4.25it/s] 20%|█▉        | 42/212 [00:10<00:40,  4.24it/s] 20%|██        | 43/212 [00:10<00:39,  4.24it/s] 21%|██        | 44/212 [00:10<00:39,  4.23it/s] 21%|██        | 45/212 [00:10<00:39,  4.24it/s] 22%|██▏       | 46/212 [00:11<00:39,  4.25it/s] 22%|██▏       | 47/212 [00:11<00:38,  4.24it/s] 23%|██▎       | 48/212 [00:11<00:38,  4.23it/s] 23%|██▎       | 49/212 [00:11<00:38,  4.23it/s] 24%|██▎       | 50/212 [00:12<00:38,  4.24it/s] 24%|██▍       | 51/212 [00:12<00:37,  4.24it/s] 25%|██▍       | 52/212 [00:12<00:37,  4.23it/s] 25%|██▌       | 53/212 [00:12<00:37,  4.23it/s] 25%|██▌       | 54/212 [00:13<00:37,  4.23it/s] 26%|██▌       | 55/212 [00:13<00:37,  4.23it/s] 26%|██▋       | 56/212 [00:13<00:36,  4.24it/s] 27%|██▋       | 57/212 [00:13<00:36,  4.24it/s] 27%|██▋       | 58/212 [00:13<00:36,  4.23it/s] 28%|██▊       | 59/212 [00:14<00:36,  4.22it/s] 28%|██▊       | 60/212 [00:14<00:36,  4.22it/s] 29%|██▉       | 61/212 [00:14<00:35,  4.22it/s] 29%|██▉       | 62/212 [00:14<00:35,  4.23it/s] 30%|██▉       | 63/212 [00:15<00:35,  4.23it/s] 30%|███       | 64/212 [00:15<00:34,  4.23it/s] 31%|███       | 65/212 [00:15<00:34,  4.22it/s] 31%|███       | 66/212 [00:15<00:34,  4.22it/s] 32%|███▏      | 67/212 [00:16<00:34,  4.22it/s] 32%|███▏      | 68/212 [00:16<00:34,  4.22it/s] 33%|███▎      | 69/212 [00:16<00:33,  4.23it/s] 33%|███▎      | 70/212 [00:16<00:33,  4.23it/s] 33%|███▎      | 71/212 [00:17<00:33,  4.22it/s] 34%|███▍      | 72/212 [00:17<00:33,  4.21it/s] 34%|███▍      | 73/212 [00:17<00:33,  4.21it/s] 35%|███▍      | 74/212 [00:17<00:32,  4.20it/s] 35%|███▌      | 75/212 [00:17<00:32,  4.21it/s] 36%|███▌      | 76/212 [00:18<00:32,  4.21it/s] 36%|███▋      | 77/212 [00:18<00:31,  4.22it/s] 37%|███▋      | 78/212 [00:18<00:31,  4.22it/s] 37%|███▋      | 79/212 [00:18<00:31,  4.21it/s] 38%|███▊      | 80/212 [00:19<00:31,  4.20it/s] 38%|███▊      | 81/212 [00:19<00:31,  4.20it/s] 39%|███▊      | 82/212 [00:19<00:31,  4.19it/s] 39%|███▉      | 83/212 [00:19<00:30,  4.19it/s] 40%|███▉      | 84/212 [00:20<00:30,  4.19it/s] 40%|████      | 85/212 [00:20<00:30,  4.20it/s] 41%|████      | 86/212 [00:20<00:29,  4.20it/s] 41%|████      | 87/212 [00:20<00:29,  4.21it/s] 42%|████▏     | 88/212 [00:21<00:29,  4.22it/s] 42%|████▏     | 89/212 [00:21<00:29,  4.21it/s] 42%|████▏     | 90/212 [00:21<00:29,  4.20it/s] 43%|████▎     | 91/212 [00:21<00:28,  4.20it/s] 43%|████▎     | 92/212 [00:22<00:28,  4.19it/s] 44%|████▍     | 93/212 [00:22<00:28,  4.19it/s] 44%|████▍     | 94/212 [00:22<00:28,  4.19it/s] 45%|████▍     | 95/212 [00:22<00:27,  4.19it/s] 45%|████▌     | 96/212 [00:22<00:27,  4.19it/s] 46%|████▌     | 97/212 [00:23<00:27,  4.19it/s] 46%|████▌     | 98/212 [00:23<00:27,  4.19it/s] 47%|████▋     | 99/212 [00:23<00:26,  4.20it/s] 47%|████▋     | 100/212 [00:23<00:26,  4.20it/s] 48%|████▊     | 101/212 [00:24<00:26,  4.20it/s] 48%|████▊     | 102/212 [00:24<00:26,  4.20it/s] 49%|████▊     | 103/212 [00:24<00:25,  4.20it/s] 49%|████▉     | 104/212 [00:24<00:25,  4.19it/s] 50%|████▉     | 105/212 [00:25<00:25,  4.19it/s] 50%|█████     | 106/212 [00:25<00:25,  4.18it/s] 50%|█████     | 107/212 [00:25<00:25,  4.18it/s] 51%|█████     | 108/212 [00:25<00:24,  4.17it/s] 51%|█████▏    | 109/212 [00:26<00:24,  4.18it/s] 52%|█████▏    | 110/212 [00:26<00:24,  4.18it/s] 52%|█████▏    | 111/212 [00:26<00:24,  4.18it/s] 53%|█████▎    | 112/212 [00:26<00:23,  4.18it/s] 53%|█████▎    | 113/212 [00:27<00:23,  4.18it/s] 54%|█████▍    | 114/212 [00:27<00:23,  4.18it/s] 54%|█████▍    | 115/212 [00:27<00:23,  4.19it/s] 55%|█████▍    | 116/212 [00:27<00:22,  4.19it/s] 55%|█████▌    | 117/212 [00:28<00:22,  4.19it/s] 56%|█████▌    | 118/212 [00:28<00:22,  4.20it/s] 56%|█████▌    | 119/212 [00:28<00:22,  4.21it/s] 57%|█████▋    | 120/212 [00:28<00:21,  4.21it/s] 57%|█████▋    | 121/212 [00:28<00:21,  4.21it/s] 58%|█████▊    | 122/212 [00:29<00:21,  4.20it/s] 58%|█████▊    | 123/212 [00:29<00:21,  4.19it/s] 58%|█████▊    | 124/212 [00:29<00:21,  4.19it/s] 59%|█████▉    | 125/212 [00:29<00:20,  4.18it/s] 59%|█████▉    | 126/212 [00:30<00:20,  4.17it/s] 60%|█████▉    | 127/212 [00:30<00:20,  4.17it/s] 60%|██████    | 128/212 [00:30<00:20,  4.17it/s] 61%|██████    | 129/212 [00:30<00:19,  4.17it/s] 61%|██████▏   | 130/212 [00:31<00:19,  4.17it/s] 62%|██████▏   | 131/212 [00:31<00:19,  4.17it/s] 62%|██████▏   | 132/212 [00:31<00:19,  4.17it/s] 63%|██████▎   | 133/212 [00:31<00:18,  4.17it/s] 63%|██████▎   | 134/212 [00:32<00:18,  4.17it/s] 64%|██████▎   | 135/212 [00:32<00:18,  4.17it/s] 64%|██████▍   | 136/212 [00:32<00:18,  4.17it/s] 65%|██████▍   | 137/212 [00:32<00:18,  4.17it/s] 65%|██████▌   | 138/212 [00:33<00:17,  4.17it/s] 66%|██████▌   | 139/212 [00:33<00:17,  4.17it/s] 66%|██████▌   | 140/212 [00:33<00:17,  4.17it/s] 67%|██████▋   | 141/212 [00:33<00:17,  4.17it/s] 67%|██████▋   | 142/212 [00:33<00:16,  4.17it/s] 67%|██████▋   | 143/212 [00:34<00:16,  4.17it/s] 68%|██████▊   | 144/212 [00:34<00:16,  4.16it/s] 68%|██████▊   | 145/212 [00:34<00:16,  4.17it/s] 69%|██████▉   | 146/212 [00:34<00:15,  4.17it/s] 69%|██████▉   | 147/212 [00:35<00:15,  4.17it/s] 70%|██████▉   | 148/212 [00:35<00:15,  4.17it/s] 70%|███████   | 149/212 [00:35<00:15,  4.17it/s] 71%|███████   | 150/212 [00:35<00:14,  4.17it/s] 71%|███████   | 151/212 [00:36<00:14,  4.16it/s] 72%|███████▏  | 152/212 [00:36<00:14,  4.16it/s] 72%|███████▏  | 153/212 [00:36<00:14,  4.16it/s] 73%|███████▎  | 154/212 [00:36<00:13,  4.17it/s] 73%|███████▎  | 155/212 [00:37<00:13,  4.16it/s] 74%|███████▎  | 156/212 [00:37<00:13,  4.17it/s] 74%|███████▍  | 157/212 [00:37<00:13,  4.17it/s] 75%|███████▍  | 158/212 [00:37<00:12,  4.17it/s] 75%|███████▌  | 159/212 [00:38<00:12,  4.17it/s] 75%|███████▌  | 160/212 [00:38<00:12,  4.17it/s] 76%|███████▌  | 161/212 [00:38<00:12,  4.16it/s] 76%|███████▋  | 162/212 [00:38<00:11,  4.17it/s] 77%|███████▋  | 163/212 [00:39<00:11,  4.17it/s] 77%|███████▋  | 164/212 [00:39<00:11,  4.17it/s] 78%|███████▊  | 165/212 [00:39<00:11,  4.17it/s] 78%|███████▊  | 166/212 [00:39<00:11,  4.17it/s] 79%|███████▉  | 167/212 [00:39<00:10,  4.17it/s] 79%|███████▉  | 168/212 [00:40<00:10,  4.17it/s] 80%|███████▉  | 169/212 [00:40<00:10,  4.17it/s] 80%|████████  | 170/212 [00:40<00:10,  4.17it/s] 81%|████████  | 171/212 [00:40<00:09,  4.16it/s] 81%|████████  | 172/212 [00:41<00:09,  4.16it/s] 82%|████████▏ | 173/212 [00:41<00:09,  4.16it/s] 82%|████████▏ | 174/212 [00:41<00:09,  4.16it/s] 83%|████████▎ | 175/212 [00:41<00:08,  4.17it/s] 83%|████████▎ | 176/212 [00:42<00:08,  4.17it/s] 83%|████████▎ | 177/212 [00:42<00:08,  4.17it/s] 84%|████████▍ | 178/212 [00:42<00:08,  4.17it/s] 84%|████████▍ | 179/212 [00:42<00:07,  4.17it/s] 85%|████████▍ | 180/212 [00:43<00:07,  4.17it/s] 85%|████████▌ | 181/212 [00:43<00:07,  4.17it/s] 86%|████████▌ | 182/212 [00:43<00:07,  4.17it/s] 86%|████████▋ | 183/212 [00:43<00:06,  4.17it/s] 87%|████████▋ | 184/212 [00:44<00:06,  4.16it/s] 87%|████████▋ | 185/212 [00:44<00:06,  4.17it/s] 88%|████████▊ | 186/212 [00:44<00:06,  4.17it/s] 88%|████████▊ | 187/212 [00:44<00:06,  4.17it/s] 89%|████████▊ | 188/212 [00:45<00:05,  4.17it/s] 89%|████████▉ | 189/212 [00:45<00:05,  4.17it/s] 90%|████████▉ | 190/212 [00:45<00:05,  4.17it/s] 90%|█████████ | 191/212 [00:45<00:05,  4.17it/s] 91%|█████████ | 192/212 [00:45<00:04,  4.17it/s] 91%|█████████ | 193/212 [00:46<00:04,  4.17it/s] 92%|█████████▏| 194/212 [00:46<00:04,  4.17it/s] 92%|█████████▏| 195/212 [00:46<00:04,  4.17it/s] 92%|█████████▏| 196/212 [00:46<00:03,  4.17it/s] 93%|█████████▎| 197/212 [00:47<00:03,  4.17it/s] 93%|█████████▎| 198/212 [00:47<00:03,  4.17it/s] 94%|█████████▍| 199/212 [00:47<00:03,  4.16it/s] 94%|█████████▍| 200/212 [00:47<00:02,  4.16it/s] 95%|█████████▍| 201/212 [00:48<00:02,  4.16it/s] 95%|█████████▌| 202/212 [00:48<00:02,  4.17it/s] 96%|█████████▌| 203/212 [00:48<00:02,  4.16it/s] 96%|█████████▌| 204/212 [00:48<00:01,  4.17it/s] 97%|█████████▋| 205/212 [00:49<00:01,  4.17it/s] 97%|█████████▋| 206/212 [00:49<00:01,  4.17it/s] 98%|█████████▊| 207/212 [00:49<00:01,  4.17it/s] 98%|█████████▊| 208/212 [00:49<00:00,  4.17it/s] 99%|█████████▊| 209/212 [00:50<00:00,  4.17it/s] 99%|█████████▉| 210/212 [00:50<00:00,  4.17it/s]100%|█████████▉| 211/212 [00:50<00:00,  4.17it/s]100%|██████████| 212/212 [00:50<00:00,  4.17it/s]accuracy:  0.8584905660377359
100%|██████████| 212/212 [00:53<00:00,  3.93it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 134
Training dataset size: 384, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 134
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:40:30,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 13:40:30,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:40:32,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:22,  3.21s/it]  3%|▎         | 2/64 [00:05<02:57,  2.86s/it]  5%|▍         | 3/64 [00:08<02:41,  2.64s/it]  6%|▋         | 4/64 [00:10<02:35,  2.60s/it]  8%|▊         | 5/64 [00:13<02:29,  2.54s/it]  9%|▉         | 6/64 [00:15<02:23,  2.47s/it] 11%|█         | 7/64 [00:17<02:19,  2.46s/it] 12%|█▎        | 8/64 [00:20<02:18,  2.47s/it] 14%|█▍        | 9/64 [00:22<02:10,  2.38s/it] 16%|█▌        | 10/64 [00:25<02:09,  2.39s/it]                                               {'loss': 0.6163, 'grad_norm': 6.823370456695557, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:25<02:09,  2.39s/it] 17%|█▋        | 11/64 [00:27<02:04,  2.35s/it] 19%|█▉        | 12/64 [00:29<02:04,  2.40s/it] 20%|██        | 13/64 [00:31<01:58,  2.32s/it] 22%|██▏       | 14/64 [00:34<02:03,  2.48s/it] 23%|██▎       | 15/64 [00:37<02:03,  2.52s/it] 25%|██▌       | 16/64 [00:39<01:57,  2.44s/it] 27%|██▋       | 17/64 [00:42<02:00,  2.55s/it] 28%|██▊       | 18/64 [00:44<01:56,  2.54s/it] 30%|██▉       | 19/64 [00:47<01:53,  2.52s/it] 31%|███▏      | 20/64 [00:50<01:51,  2.54s/it]                                               {'loss': 0.692, 'grad_norm': 11.330827713012695, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:50<01:51,  2.54s/it] 33%|███▎      | 21/64 [00:52<01:47,  2.50s/it] 34%|███▍      | 22/64 [00:55<01:48,  2.59s/it] 36%|███▌      | 23/64 [00:57<01:40,  2.45s/it] 38%|███▊      | 24/64 [00:59<01:39,  2.48s/it] 39%|███▉      | 25/64 [01:02<01:37,  2.51s/it] 41%|████      | 26/64 [01:05<01:36,  2.53s/it] 42%|████▏     | 27/64 [01:07<01:35,  2.58s/it] 44%|████▍     | 28/64 [01:10<01:35,  2.66s/it] 45%|████▌     | 29/64 [01:12<01:27,  2.51s/it] 47%|████▋     | 30/64 [01:15<01:24,  2.48s/it]                                               {'loss': 0.6548, 'grad_norm': 4.515817165374756, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:15<01:24,  2.48s/it] 48%|████▊     | 31/64 [01:17<01:18,  2.37s/it] 50%|█████     | 32/64 [01:19<01:16,  2.40s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:22<01:16,  2.47s/it] 53%|█████▎    | 34/64 [01:25<01:16,  2.56s/it] 55%|█████▍    | 35/64 [01:27<01:11,  2.47s/it] 56%|█████▋    | 36/64 [01:29<01:06,  2.39s/it] 58%|█████▊    | 37/64 [01:32<01:07,  2.52s/it] 59%|█████▉    | 38/64 [01:35<01:05,  2.53s/it] 61%|██████    | 39/64 [01:37<01:00,  2.44s/it] 62%|██████▎   | 40/64 [01:39<00:56,  2.35s/it]                                               {'loss': 0.5355, 'grad_norm': 2.5962352752685547, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:39<00:56,  2.35s/it] 64%|██████▍   | 41/64 [01:41<00:52,  2.29s/it] 66%|██████▌   | 42/64 [01:44<00:52,  2.39s/it] 67%|██████▋   | 43/64 [01:46<00:50,  2.41s/it] 69%|██████▉   | 44/64 [01:48<00:47,  2.35s/it] 70%|███████   | 45/64 [01:51<00:47,  2.49s/it] 72%|███████▏  | 46/64 [01:54<00:44,  2.46s/it] 73%|███████▎  | 47/64 [01:56<00:41,  2.45s/it] 75%|███████▌  | 48/64 [01:59<00:40,  2.50s/it] 77%|███████▋  | 49/64 [02:01<00:38,  2.54s/it] 78%|███████▊  | 50/64 [02:04<00:35,  2.53s/it]                                               {'loss': 0.4296, 'grad_norm': 5.912809371948242, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [02:04<00:35,  2.53s/it] 80%|███████▉  | 51/64 [02:06<00:33,  2.55s/it] 81%|████████▏ | 52/64 [02:09<00:29,  2.45s/it] 83%|████████▎ | 53/64 [02:11<00:25,  2.34s/it] 84%|████████▍ | 54/64 [02:13<00:23,  2.35s/it] 86%|████████▌ | 55/64 [02:16<00:22,  2.50s/it] 88%|████████▊ | 56/64 [02:19<00:20,  2.60s/it] 89%|████████▉ | 57/64 [02:21<00:18,  2.61s/it] 91%|█████████ | 58/64 [02:24<00:15,  2.58s/it] 92%|█████████▏| 59/64 [02:27<00:13,  2.66s/it] 94%|█████████▍| 60/64 [02:29<00:09,  2.49s/it]                                               {'loss': 0.401, 'grad_norm': 4.4507975578308105, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:29<00:09,  2.49s/it] 95%|█████████▌| 61/64 [02:32<00:07,  2.58s/it] 97%|█████████▋| 62/64 [02:34<00:05,  2.56s/it] 98%|█████████▊| 63/64 [02:36<00:02,  2.47s/it]100%|██████████| 64/64 [02:39<00:00,  2.48s/it]                                               {'train_runtime': 160.1486, 'train_samples_per_second': 4.796, 'train_steps_per_second': 0.4, 'train_loss': 0.5497975572943687, 'epoch': 2.0}
100%|██████████| 64/64 [02:39<00:00,  2.48s/it]100%|██████████| 64/64 [02:39<00:00,  2.50s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.31s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  163
  0%|          | 0/163 [00:00<?, ?it/s]  1%|          | 1/163 [00:00<01:16,  2.12it/s]  1%|          | 2/163 [00:00<00:54,  2.98it/s]  2%|▏         | 3/163 [00:00<00:46,  3.42it/s]  2%|▏         | 4/163 [00:01<00:43,  3.68it/s]  3%|▎         | 5/163 [00:01<00:41,  3.84it/s]  4%|▎         | 6/163 [00:01<00:39,  3.95it/s]  4%|▍         | 7/163 [00:01<00:38,  4.01it/s]  5%|▍         | 8/163 [00:02<00:38,  4.05it/s]  6%|▌         | 9/163 [00:02<00:37,  4.08it/s]  6%|▌         | 10/163 [00:02<00:37,  4.10it/s]  7%|▋         | 11/163 [00:02<00:36,  4.11it/s]  7%|▋         | 12/163 [00:03<00:36,  4.12it/s]  8%|▊         | 13/163 [00:03<00:36,  4.13it/s]  9%|▊         | 14/163 [00:03<00:36,  4.14it/s]  9%|▉         | 15/163 [00:03<00:35,  4.15it/s] 10%|▉         | 16/163 [00:04<00:35,  4.15it/s] 10%|█         | 17/163 [00:04<00:35,  4.16it/s] 11%|█         | 18/163 [00:04<00:34,  4.16it/s] 12%|█▏        | 19/163 [00:04<00:34,  4.17it/s] 12%|█▏        | 20/163 [00:05<00:34,  4.17it/s] 13%|█▎        | 21/163 [00:05<00:34,  4.17it/s] 13%|█▎        | 22/163 [00:05<00:33,  4.17it/s] 14%|█▍        | 23/163 [00:05<00:33,  4.17it/s] 15%|█▍        | 24/163 [00:06<00:33,  4.17it/s] 15%|█▌        | 25/163 [00:06<00:33,  4.17it/s] 16%|█▌        | 26/163 [00:06<00:32,  4.17it/s] 17%|█▋        | 27/163 [00:06<00:32,  4.16it/s] 17%|█▋        | 28/163 [00:06<00:32,  4.16it/s] 18%|█▊        | 29/163 [00:07<00:32,  4.16it/s] 18%|█▊        | 30/163 [00:07<00:31,  4.16it/s] 19%|█▉        | 31/163 [00:07<00:31,  4.17it/s] 20%|█▉        | 32/163 [00:07<00:31,  4.16it/s] 20%|██        | 33/163 [00:08<00:31,  4.16it/s] 21%|██        | 34/163 [00:08<00:30,  4.16it/s] 21%|██▏       | 35/163 [00:08<00:30,  4.17it/s] 22%|██▏       | 36/163 [00:08<00:30,  4.17it/s] 23%|██▎       | 37/163 [00:09<00:30,  4.17it/s] 23%|██▎       | 38/163 [00:09<00:29,  4.17it/s] 24%|██▍       | 39/163 [00:09<00:29,  4.17it/s] 25%|██▍       | 40/163 [00:09<00:29,  4.16it/s] 25%|██▌       | 41/163 [00:10<00:29,  4.16it/s] 26%|██▌       | 42/163 [00:10<00:29,  4.17it/s] 26%|██▋       | 43/163 [00:10<00:28,  4.17it/s] 27%|██▋       | 44/163 [00:10<00:28,  4.17it/s] 28%|██▊       | 45/163 [00:11<00:28,  4.17it/s] 28%|██▊       | 46/163 [00:11<00:28,  4.17it/s] 29%|██▉       | 47/163 [00:11<00:27,  4.17it/s] 29%|██▉       | 48/163 [00:11<00:27,  4.17it/s] 30%|███       | 49/163 [00:12<00:27,  4.16it/s] 31%|███       | 50/163 [00:12<00:27,  4.17it/s] 31%|███▏      | 51/163 [00:12<00:26,  4.16it/s] 32%|███▏      | 52/163 [00:12<00:26,  4.16it/s] 33%|███▎      | 53/163 [00:12<00:26,  4.16it/s] 33%|███▎      | 54/163 [00:13<00:26,  4.17it/s] 34%|███▎      | 55/163 [00:13<00:25,  4.17it/s] 34%|███▍      | 56/163 [00:13<00:25,  4.17it/s] 35%|███▍      | 57/163 [00:13<00:25,  4.17it/s] 36%|███▌      | 58/163 [00:14<00:25,  4.16it/s] 36%|███▌      | 59/163 [00:14<00:24,  4.16it/s] 37%|███▋      | 60/163 [00:14<00:24,  4.16it/s] 37%|███▋      | 61/163 [00:14<00:24,  4.17it/s] 38%|███▊      | 62/163 [00:15<00:24,  4.17it/s] 39%|███▊      | 63/163 [00:15<00:23,  4.17it/s] 39%|███▉      | 64/163 [00:15<00:23,  4.17it/s] 40%|███▉      | 65/163 [00:15<00:23,  4.17it/s] 40%|████      | 66/163 [00:16<00:23,  4.17it/s] 41%|████      | 67/163 [00:16<00:23,  4.17it/s] 42%|████▏     | 68/163 [00:16<00:22,  4.16it/s] 42%|████▏     | 69/163 [00:16<00:22,  4.16it/s] 43%|████▎     | 70/163 [00:17<00:22,  4.17it/s] 44%|████▎     | 71/163 [00:17<00:22,  4.16it/s] 44%|████▍     | 72/163 [00:17<00:21,  4.16it/s] 45%|████▍     | 73/163 [00:17<00:21,  4.16it/s] 45%|████▌     | 74/163 [00:18<00:21,  4.16it/s] 46%|████▌     | 75/163 [00:18<00:21,  4.16it/s] 47%|████▋     | 76/163 [00:18<00:20,  4.15it/s] 47%|████▋     | 77/163 [00:18<00:20,  4.15it/s] 48%|████▊     | 78/163 [00:18<00:20,  4.14it/s] 48%|████▊     | 79/163 [00:19<00:20,  4.13it/s] 49%|████▉     | 80/163 [00:19<00:20,  4.12it/s] 50%|████▉     | 81/163 [00:19<00:19,  4.13it/s] 50%|█████     | 82/163 [00:19<00:19,  4.13it/s] 51%|█████     | 83/163 [00:20<00:19,  4.14it/s] 52%|█████▏    | 84/163 [00:20<00:19,  4.13it/s] 52%|█████▏    | 85/163 [00:20<00:18,  4.13it/s] 53%|█████▎    | 86/163 [00:20<00:18,  4.13it/s] 53%|█████▎    | 87/163 [00:21<00:18,  4.12it/s] 54%|█████▍    | 88/163 [00:21<00:18,  4.12it/s] 55%|█████▍    | 89/163 [00:21<00:17,  4.12it/s] 55%|█████▌    | 90/163 [00:21<00:17,  4.13it/s] 56%|█████▌    | 91/163 [00:22<00:17,  4.13it/s] 56%|█████▋    | 92/163 [00:22<00:17,  4.13it/s] 57%|█████▋    | 93/163 [00:22<00:16,  4.14it/s] 58%|█████▊    | 94/163 [00:22<00:16,  4.13it/s] 58%|█████▊    | 95/163 [00:23<00:16,  4.13it/s] 59%|█████▉    | 96/163 [00:23<00:16,  4.12it/s] 60%|█████▉    | 97/163 [00:23<00:16,  4.12it/s] 60%|██████    | 98/163 [00:23<00:15,  4.12it/s] 61%|██████    | 99/163 [00:24<00:15,  4.13it/s] 61%|██████▏   | 100/163 [00:24<00:15,  4.14it/s] 62%|██████▏   | 101/163 [00:24<00:14,  4.14it/s] 63%|██████▎   | 102/163 [00:24<00:14,  4.14it/s] 63%|██████▎   | 103/163 [00:25<00:14,  4.14it/s] 64%|██████▍   | 104/163 [00:25<00:14,  4.13it/s] 64%|██████▍   | 105/163 [00:25<00:14,  4.13it/s] 65%|██████▌   | 106/163 [00:25<00:13,  4.12it/s] 66%|██████▌   | 107/163 [00:25<00:13,  4.12it/s] 66%|██████▋   | 108/163 [00:26<00:13,  4.13it/s] 67%|██████▋   | 109/163 [00:26<00:13,  4.13it/s] 67%|██████▋   | 110/163 [00:26<00:12,  4.14it/s] 68%|██████▊   | 111/163 [00:26<00:12,  4.14it/s] 69%|██████▊   | 112/163 [00:27<00:12,  4.14it/s] 69%|██████▉   | 113/163 [00:27<00:12,  4.13it/s] 70%|██████▉   | 114/163 [00:27<00:11,  4.12it/s] 71%|███████   | 115/163 [00:27<00:11,  4.11it/s] 71%|███████   | 116/163 [00:28<00:11,  4.12it/s] 72%|███████▏  | 117/163 [00:28<00:11,  4.12it/s] 72%|███████▏  | 118/163 [00:28<00:10,  4.13it/s] 73%|███████▎  | 119/163 [00:28<00:10,  4.12it/s] 74%|███████▎  | 120/163 [00:29<00:10,  4.12it/s] 74%|███████▍  | 121/163 [00:29<00:10,  4.11it/s] 75%|███████▍  | 122/163 [00:29<00:09,  4.11it/s] 75%|███████▌  | 123/163 [00:29<00:09,  4.11it/s] 76%|███████▌  | 124/163 [00:30<00:09,  4.12it/s] 77%|███████▋  | 125/163 [00:30<00:09,  4.12it/s] 77%|███████▋  | 126/163 [00:30<00:09,  4.11it/s] 78%|███████▊  | 127/163 [00:30<00:08,  4.10it/s] 79%|███████▊  | 128/163 [00:31<00:08,  4.10it/s] 79%|███████▉  | 129/163 [00:31<00:08,  4.11it/s] 80%|███████▉  | 130/163 [00:31<00:08,  4.11it/s] 80%|████████  | 131/163 [00:31<00:07,  4.11it/s] 81%|████████  | 132/163 [00:32<00:07,  4.10it/s] 82%|████████▏ | 133/163 [00:32<00:07,  4.10it/s] 82%|████████▏ | 134/163 [00:32<00:07,  4.11it/s] 83%|████████▎ | 135/163 [00:32<00:06,  4.11it/s] 83%|████████▎ | 136/163 [00:33<00:06,  4.12it/s] 84%|████████▍ | 137/163 [00:33<00:06,  4.11it/s] 85%|████████▍ | 138/163 [00:33<00:06,  4.10it/s] 85%|████████▌ | 139/163 [00:33<00:05,  4.10it/s] 86%|████████▌ | 140/163 [00:34<00:05,  4.11it/s] 87%|████████▋ | 141/163 [00:34<00:05,  4.11it/s] 87%|████████▋ | 142/163 [00:34<00:05,  4.12it/s] 88%|████████▊ | 143/163 [00:34<00:04,  4.11it/s] 88%|████████▊ | 144/163 [00:34<00:04,  4.11it/s] 89%|████████▉ | 145/163 [00:35<00:04,  4.11it/s] 90%|████████▉ | 146/163 [00:35<00:04,  4.11it/s] 90%|█████████ | 147/163 [00:35<00:03,  4.11it/s] 91%|█████████ | 148/163 [00:35<00:03,  4.10it/s] 91%|█████████▏| 149/163 [00:36<00:03,  4.09it/s] 92%|█████████▏| 150/163 [00:36<00:03,  4.09it/s] 93%|█████████▎| 151/163 [00:36<00:02,  4.10it/s] 93%|█████████▎| 152/163 [00:36<00:02,  4.10it/s] 94%|█████████▍| 153/163 [00:37<00:02,  4.10it/s] 94%|█████████▍| 154/163 [00:37<00:02,  4.09it/s] 95%|█████████▌| 155/163 [00:37<00:01,  4.09it/s] 96%|█████████▌| 156/163 [00:37<00:01,  4.10it/s] 96%|█████████▋| 157/163 [00:38<00:01,  4.10it/s] 97%|█████████▋| 158/163 [00:38<00:01,  4.10it/s] 98%|█████████▊| 159/163 [00:38<00:00,  4.09it/s] 98%|█████████▊| 160/163 [00:38<00:00,  4.10it/s] 99%|█████████▉| 161/163 [00:39<00:00,  4.10it/s] 99%|█████████▉| 162/163 [00:39<00:00,  4.10it/s]100%|██████████| 163/163 [00:39<00:00,  4.09it/s]accuracy:  0.8220858895705522
100%|██████████| 163/163 [00:42<00:00,  3.85it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 171
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:44:27,685] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:44:28,833] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:44:31,223] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:30,  3.35s/it]  3%|▎         | 2/64 [00:06<03:11,  3.09s/it]  5%|▍         | 3/64 [00:08<02:47,  2.75s/it]  6%|▋         | 4/64 [00:11<02:42,  2.71s/it]  8%|▊         | 5/64 [00:13<02:37,  2.67s/it]  9%|▉         | 6/64 [00:16<02:33,  2.65s/it] 11%|█         | 7/64 [00:18<02:16,  2.39s/it] 12%|█▎        | 8/64 [00:20<02:06,  2.25s/it] 14%|█▍        | 9/64 [00:22<02:09,  2.35s/it] 16%|█▌        | 10/64 [00:25<02:08,  2.38s/it]                                               {'loss': 0.5156, 'grad_norm': 3.35235333442688, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:25<02:08,  2.38s/it] 17%|█▋        | 11/64 [00:27<02:08,  2.42s/it] 19%|█▉        | 12/64 [00:29<01:56,  2.23s/it] 20%|██        | 13/64 [00:31<01:51,  2.19s/it] 22%|██▏       | 14/64 [00:33<01:50,  2.21s/it] 23%|██▎       | 15/64 [00:36<01:55,  2.37s/it] 25%|██▌       | 16/64 [00:39<01:57,  2.44s/it] 27%|██▋       | 17/64 [00:42<01:59,  2.54s/it] 28%|██▊       | 18/64 [00:44<01:57,  2.55s/it] 30%|██▉       | 19/64 [00:46<01:51,  2.47s/it] 31%|███▏      | 20/64 [00:49<01:48,  2.47s/it]                                               {'loss': 0.5127, 'grad_norm': 4.6765666007995605, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:49<01:48,  2.47s/it] 33%|███▎      | 21/64 [00:51<01:45,  2.45s/it] 34%|███▍      | 22/64 [00:53<01:39,  2.38s/it] 36%|███▌      | 23/64 [00:56<01:41,  2.48s/it] 38%|███▊      | 24/64 [00:58<01:36,  2.40s/it] 39%|███▉      | 25/64 [01:01<01:34,  2.43s/it] 41%|████      | 26/64 [01:03<01:32,  2.43s/it] 42%|████▏     | 27/64 [01:06<01:31,  2.48s/it] 44%|████▍     | 28/64 [01:08<01:29,  2.48s/it] 45%|████▌     | 29/64 [01:11<01:24,  2.42s/it] 47%|████▋     | 30/64 [01:13<01:22,  2.41s/it]                                               {'loss': 0.3534, 'grad_norm': 3.921823740005493, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:13<01:22,  2.41s/it] 48%|████▊     | 31/64 [01:16<01:19,  2.42s/it] 50%|█████     | 32/64 [01:18<01:18,  2.46s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:21<01:17,  2.49s/it] 53%|█████▎    | 34/64 [01:23<01:17,  2.57s/it] 55%|█████▍    | 35/64 [01:26<01:16,  2.65s/it] 56%|█████▋    | 36/64 [01:29<01:10,  2.54s/it] 58%|█████▊    | 37/64 [01:31<01:07,  2.51s/it] 59%|█████▉    | 38/64 [01:33<01:02,  2.40s/it] 61%|██████    | 39/64 [01:35<00:56,  2.27s/it] 62%|██████▎   | 40/64 [01:37<00:55,  2.31s/it]                                               {'loss': 0.251, 'grad_norm': 7.084991455078125, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:37<00:55,  2.31s/it] 64%|██████▍   | 41/64 [01:40<00:54,  2.38s/it] 66%|██████▌   | 42/64 [01:42<00:52,  2.40s/it] 67%|██████▋   | 43/64 [01:45<00:48,  2.32s/it] 69%|██████▉   | 44/64 [01:47<00:47,  2.39s/it] 70%|███████   | 45/64 [01:50<00:45,  2.38s/it] 72%|███████▏  | 46/64 [01:52<00:41,  2.31s/it] 73%|███████▎  | 47/64 [01:54<00:39,  2.34s/it] 75%|███████▌  | 48/64 [01:57<00:39,  2.48s/it] 77%|███████▋  | 49/64 [02:00<00:38,  2.54s/it] 78%|███████▊  | 50/64 [02:02<00:34,  2.43s/it]                                               {'loss': 0.313, 'grad_norm': 4.915910243988037, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [02:02<00:34,  2.43s/it] 80%|███████▉  | 51/64 [02:04<00:31,  2.43s/it] 81%|████████▏ | 52/64 [02:07<00:28,  2.41s/it] 83%|████████▎ | 53/64 [02:08<00:24,  2.25s/it] 84%|████████▍ | 54/64 [02:11<00:23,  2.32s/it] 86%|████████▌ | 55/64 [02:13<00:20,  2.25s/it] 88%|████████▊ | 56/64 [02:15<00:18,  2.29s/it] 89%|████████▉ | 57/64 [02:18<00:15,  2.28s/it] 91%|█████████ | 58/64 [02:20<00:13,  2.26s/it] 92%|█████████▏| 59/64 [02:22<00:11,  2.25s/it] 94%|█████████▍| 60/64 [02:24<00:08,  2.22s/it]                                               {'loss': 0.361, 'grad_norm': 6.355282306671143, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:24<00:08,  2.22s/it] 95%|█████████▌| 61/64 [02:27<00:06,  2.31s/it] 97%|█████████▋| 62/64 [02:29<00:04,  2.28s/it] 98%|█████████▊| 63/64 [02:31<00:02,  2.30s/it]100%|██████████| 64/64 [02:34<00:00,  2.44s/it]                                               {'train_runtime': 155.4217, 'train_samples_per_second': 4.941, 'train_steps_per_second': 0.412, 'train_loss': 0.391743540763855, 'epoch': 2.0}
100%|██████████| 64/64 [02:35<00:00,  2.44s/it]100%|██████████| 64/64 [02:35<00:00,  2.43s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  217
  0%|          | 0/217 [00:00<?, ?it/s]  0%|          | 1/217 [00:00<03:27,  1.04it/s]  1%|          | 2/217 [00:01<01:55,  1.86it/s]  1%|▏         | 3/217 [00:01<01:25,  2.50it/s]  2%|▏         | 4/217 [00:01<01:11,  2.97it/s]  2%|▏         | 5/217 [00:01<01:03,  3.32it/s]  3%|▎         | 6/217 [00:02<00:58,  3.58it/s]  3%|▎         | 7/217 [00:02<00:55,  3.77it/s]  4%|▎         | 8/217 [00:02<00:53,  3.91it/s]  4%|▍         | 9/217 [00:02<00:52,  4.00it/s]  5%|▍         | 10/217 [00:03<00:51,  4.06it/s]  5%|▌         | 11/217 [00:03<00:50,  4.10it/s]  6%|▌         | 12/217 [00:03<00:49,  4.12it/s]  6%|▌         | 13/217 [00:03<00:49,  4.14it/s]  6%|▋         | 14/217 [00:04<00:48,  4.15it/s]  7%|▋         | 15/217 [00:04<00:48,  4.16it/s]  7%|▋         | 16/217 [00:04<00:48,  4.17it/s]  8%|▊         | 17/217 [00:04<00:47,  4.19it/s]  8%|▊         | 18/217 [00:05<00:47,  4.20it/s]  9%|▉         | 19/217 [00:05<00:47,  4.21it/s]  9%|▉         | 20/217 [00:05<00:46,  4.21it/s] 10%|▉         | 21/217 [00:05<00:46,  4.20it/s] 10%|█         | 22/217 [00:05<00:46,  4.20it/s] 11%|█         | 23/217 [00:06<00:46,  4.19it/s] 11%|█         | 24/217 [00:06<00:46,  4.19it/s] 12%|█▏        | 25/217 [00:06<00:45,  4.18it/s] 12%|█▏        | 26/217 [00:06<00:45,  4.18it/s] 12%|█▏        | 27/217 [00:07<00:45,  4.18it/s] 13%|█▎        | 28/217 [00:07<00:45,  4.18it/s] 13%|█▎        | 29/217 [00:07<00:44,  4.19it/s] 14%|█▍        | 30/217 [00:07<00:44,  4.19it/s] 14%|█▍        | 31/217 [00:08<00:44,  4.20it/s] 15%|█▍        | 32/217 [00:08<00:43,  4.21it/s] 15%|█▌        | 33/217 [00:08<00:43,  4.21it/s] 16%|█▌        | 34/217 [00:08<00:43,  4.20it/s] 16%|█▌        | 35/217 [00:09<00:43,  4.19it/s] 17%|█▋        | 36/217 [00:09<00:43,  4.19it/s] 17%|█▋        | 37/217 [00:09<00:43,  4.18it/s] 18%|█▊        | 38/217 [00:09<00:42,  4.18it/s] 18%|█▊        | 39/217 [00:10<00:42,  4.18it/s] 18%|█▊        | 40/217 [00:10<00:42,  4.17it/s] 19%|█▉        | 41/217 [00:10<00:42,  4.17it/s] 19%|█▉        | 42/217 [00:10<00:42,  4.17it/s] 20%|█▉        | 43/217 [00:10<00:41,  4.16it/s] 20%|██        | 44/217 [00:11<00:41,  4.17it/s] 21%|██        | 45/217 [00:11<00:41,  4.17it/s] 21%|██        | 46/217 [00:11<00:41,  4.17it/s] 22%|██▏       | 47/217 [00:11<00:40,  4.17it/s] 22%|██▏       | 48/217 [00:12<00:40,  4.17it/s] 23%|██▎       | 49/217 [00:12<00:40,  4.17it/s] 23%|██▎       | 50/217 [00:12<00:40,  4.17it/s] 24%|██▎       | 51/217 [00:12<00:39,  4.17it/s] 24%|██▍       | 52/217 [00:13<00:39,  4.17it/s] 24%|██▍       | 53/217 [00:13<00:39,  4.17it/s] 25%|██▍       | 54/217 [00:13<00:39,  4.17it/s] 25%|██▌       | 55/217 [00:13<00:38,  4.17it/s] 26%|██▌       | 56/217 [00:14<00:38,  4.17it/s] 26%|██▋       | 57/217 [00:14<00:38,  4.17it/s] 27%|██▋       | 58/217 [00:14<00:38,  4.17it/s] 27%|██▋       | 59/217 [00:14<00:37,  4.17it/s] 28%|██▊       | 60/217 [00:15<00:37,  4.17it/s] 28%|██▊       | 61/217 [00:15<00:37,  4.17it/s] 29%|██▊       | 62/217 [00:15<00:37,  4.17it/s] 29%|██▉       | 63/217 [00:15<00:36,  4.17it/s] 29%|██▉       | 64/217 [00:16<00:36,  4.17it/s] 30%|██▉       | 65/217 [00:16<00:36,  4.17it/s] 30%|███       | 66/217 [00:16<00:36,  4.17it/s] 31%|███       | 67/217 [00:16<00:35,  4.17it/s] 31%|███▏      | 68/217 [00:16<00:35,  4.17it/s] 32%|███▏      | 69/217 [00:17<00:35,  4.17it/s] 32%|███▏      | 70/217 [00:17<00:35,  4.17it/s] 33%|███▎      | 71/217 [00:17<00:35,  4.17it/s] 33%|███▎      | 72/217 [00:17<00:34,  4.17it/s] 34%|███▎      | 73/217 [00:18<00:34,  4.17it/s] 34%|███▍      | 74/217 [00:18<00:34,  4.17it/s] 35%|███▍      | 75/217 [00:18<00:34,  4.17it/s] 35%|███▌      | 76/217 [00:18<00:33,  4.17it/s] 35%|███▌      | 77/217 [00:19<00:33,  4.17it/s] 36%|███▌      | 78/217 [00:19<00:33,  4.17it/s] 36%|███▋      | 79/217 [00:19<00:33,  4.17it/s] 37%|███▋      | 80/217 [00:19<00:32,  4.17it/s] 37%|███▋      | 81/217 [00:20<00:32,  4.17it/s] 38%|███▊      | 82/217 [00:20<00:32,  4.17it/s] 38%|███▊      | 83/217 [00:20<00:32,  4.17it/s] 39%|███▊      | 84/217 [00:20<00:31,  4.17it/s] 39%|███▉      | 85/217 [00:21<00:31,  4.16it/s] 40%|███▉      | 86/217 [00:21<00:31,  4.16it/s] 40%|████      | 87/217 [00:21<00:31,  4.17it/s] 41%|████      | 88/217 [00:21<00:30,  4.17it/s] 41%|████      | 89/217 [00:22<00:30,  4.17it/s] 41%|████▏     | 90/217 [00:22<00:30,  4.17it/s] 42%|████▏     | 91/217 [00:22<00:30,  4.17it/s] 42%|████▏     | 92/217 [00:22<00:29,  4.17it/s] 43%|████▎     | 93/217 [00:22<00:29,  4.17it/s] 43%|████▎     | 94/217 [00:23<00:29,  4.17it/s] 44%|████▍     | 95/217 [00:23<00:29,  4.17it/s] 44%|████▍     | 96/217 [00:23<00:29,  4.17it/s] 45%|████▍     | 97/217 [00:23<00:28,  4.17it/s] 45%|████▌     | 98/217 [00:24<00:28,  4.17it/s] 46%|████▌     | 99/217 [00:24<00:28,  4.17it/s] 46%|████▌     | 100/217 [00:24<00:28,  4.17it/s] 47%|████▋     | 101/217 [00:24<00:27,  4.17it/s] 47%|████▋     | 102/217 [00:25<00:27,  4.17it/s] 47%|████▋     | 103/217 [00:25<00:27,  4.17it/s] 48%|████▊     | 104/217 [00:25<00:27,  4.17it/s] 48%|████▊     | 105/217 [00:25<00:26,  4.17it/s] 49%|████▉     | 106/217 [00:26<00:26,  4.17it/s] 49%|████▉     | 107/217 [00:26<00:26,  4.17it/s] 50%|████▉     | 108/217 [00:26<00:26,  4.17it/s] 50%|█████     | 109/217 [00:26<00:25,  4.17it/s] 51%|█████     | 110/217 [00:27<00:25,  4.17it/s] 51%|█████     | 111/217 [00:27<00:25,  4.17it/s] 52%|█████▏    | 112/217 [00:27<00:25,  4.17it/s] 52%|█████▏    | 113/217 [00:27<00:24,  4.17it/s] 53%|█████▎    | 114/217 [00:28<00:24,  4.17it/s] 53%|█████▎    | 115/217 [00:28<00:24,  4.17it/s] 53%|█████▎    | 116/217 [00:28<00:24,  4.17it/s] 54%|█████▍    | 117/217 [00:28<00:23,  4.17it/s] 54%|█████▍    | 118/217 [00:28<00:23,  4.17it/s] 55%|█████▍    | 119/217 [00:29<00:23,  4.17it/s] 55%|█████▌    | 120/217 [00:29<00:23,  4.16it/s] 56%|█████▌    | 121/217 [00:29<00:23,  4.16it/s] 56%|█████▌    | 122/217 [00:29<00:22,  4.16it/s] 57%|█████▋    | 123/217 [00:30<00:22,  4.16it/s] 57%|█████▋    | 124/217 [00:30<00:22,  4.16it/s] 58%|█████▊    | 125/217 [00:30<00:22,  4.17it/s] 58%|█████▊    | 126/217 [00:30<00:21,  4.17it/s] 59%|█████▊    | 127/217 [00:31<00:21,  4.17it/s] 59%|█████▉    | 128/217 [00:31<00:21,  4.17it/s] 59%|█████▉    | 129/217 [00:31<00:21,  4.16it/s] 60%|█████▉    | 130/217 [00:31<00:20,  4.17it/s] 60%|██████    | 131/217 [00:32<00:20,  4.17it/s] 61%|██████    | 132/217 [00:32<00:20,  4.17it/s] 61%|██████▏   | 133/217 [00:32<00:20,  4.17it/s] 62%|██████▏   | 134/217 [00:32<00:19,  4.17it/s] 62%|██████▏   | 135/217 [00:33<00:19,  4.17it/s] 63%|██████▎   | 136/217 [00:33<00:19,  4.17it/s] 63%|██████▎   | 137/217 [00:33<00:19,  4.17it/s] 64%|██████▎   | 138/217 [00:33<00:18,  4.17it/s] 64%|██████▍   | 139/217 [00:34<00:18,  4.17it/s] 65%|██████▍   | 140/217 [00:34<00:18,  4.17it/s] 65%|██████▍   | 141/217 [00:34<00:18,  4.17it/s] 65%|██████▌   | 142/217 [00:34<00:18,  4.17it/s] 66%|██████▌   | 143/217 [00:34<00:17,  4.16it/s] 66%|██████▋   | 144/217 [00:35<00:17,  4.16it/s] 67%|██████▋   | 145/217 [00:35<00:17,  4.16it/s] 67%|██████▋   | 146/217 [00:35<00:17,  4.16it/s] 68%|██████▊   | 147/217 [00:35<00:16,  4.16it/s] 68%|██████▊   | 148/217 [00:36<00:16,  4.16it/s] 69%|██████▊   | 149/217 [00:36<00:16,  4.16it/s] 69%|██████▉   | 150/217 [00:36<00:16,  4.16it/s] 70%|██████▉   | 151/217 [00:36<00:15,  4.16it/s] 70%|███████   | 152/217 [00:37<00:15,  4.16it/s] 71%|███████   | 153/217 [00:37<00:15,  4.16it/s] 71%|███████   | 154/217 [00:37<00:15,  4.15it/s] 71%|███████▏  | 155/217 [00:37<00:14,  4.15it/s] 72%|███████▏  | 156/217 [00:38<00:14,  4.14it/s] 72%|███████▏  | 157/217 [00:38<00:14,  4.13it/s] 73%|███████▎  | 158/217 [00:38<00:14,  4.13it/s] 73%|███████▎  | 159/217 [00:38<00:14,  4.13it/s] 74%|███████▎  | 160/217 [00:39<00:13,  4.13it/s] 74%|███████▍  | 161/217 [00:39<00:13,  4.14it/s] 75%|███████▍  | 162/217 [00:39<00:13,  4.14it/s] 75%|███████▌  | 163/217 [00:39<00:13,  4.15it/s] 76%|███████▌  | 164/217 [00:40<00:12,  4.15it/s] 76%|███████▌  | 165/217 [00:40<00:12,  4.15it/s] 76%|███████▋  | 166/217 [00:40<00:12,  4.15it/s] 77%|███████▋  | 167/217 [00:40<00:12,  4.14it/s] 77%|███████▋  | 168/217 [00:41<00:11,  4.14it/s] 78%|███████▊  | 169/217 [00:41<00:11,  4.13it/s] 78%|███████▊  | 170/217 [00:41<00:11,  4.12it/s] 79%|███████▉  | 171/217 [00:41<00:11,  4.13it/s] 79%|███████▉  | 172/217 [00:41<00:10,  4.14it/s] 80%|███████▉  | 173/217 [00:42<00:10,  4.14it/s] 80%|████████  | 174/217 [00:42<00:10,  4.15it/s] 81%|████████  | 175/217 [00:42<00:10,  4.15it/s] 81%|████████  | 176/217 [00:42<00:09,  4.15it/s] 82%|████████▏ | 177/217 [00:43<00:09,  4.15it/s] 82%|████████▏ | 178/217 [00:43<00:09,  4.15it/s] 82%|████████▏ | 179/217 [00:43<00:09,  4.15it/s] 83%|████████▎ | 180/217 [00:43<00:08,  4.15it/s] 83%|████████▎ | 181/217 [00:44<00:08,  4.14it/s] 84%|████████▍ | 182/217 [00:44<00:08,  4.14it/s] 84%|████████▍ | 183/217 [00:44<00:08,  4.13it/s] 85%|████████▍ | 184/217 [00:44<00:07,  4.13it/s] 85%|████████▌ | 185/217 [00:45<00:07,  4.13it/s] 86%|████████▌ | 186/217 [00:45<00:07,  4.13it/s] 86%|████████▌ | 187/217 [00:45<00:07,  4.13it/s] 87%|████████▋ | 188/217 [00:45<00:07,  4.14it/s] 87%|████████▋ | 189/217 [00:46<00:06,  4.14it/s] 88%|████████▊ | 190/217 [00:46<00:06,  4.14it/s] 88%|████████▊ | 191/217 [00:46<00:06,  4.13it/s] 88%|████████▊ | 192/217 [00:46<00:06,  4.13it/s] 89%|████████▉ | 193/217 [00:47<00:05,  4.12it/s] 89%|████████▉ | 194/217 [00:47<00:05,  4.12it/s] 90%|████████▉ | 195/217 [00:47<00:05,  4.13it/s] 90%|█████████ | 196/217 [00:47<00:05,  4.13it/s] 91%|█████████ | 197/217 [00:48<00:04,  4.14it/s] 91%|█████████ | 198/217 [00:48<00:04,  4.14it/s] 92%|█████████▏| 199/217 [00:48<00:04,  4.14it/s] 92%|█████████▏| 200/217 [00:48<00:04,  4.14it/s] 93%|█████████▎| 201/217 [00:48<00:03,  4.14it/s] 93%|█████████▎| 202/217 [00:49<00:03,  4.13it/s] 94%|█████████▎| 203/217 [00:49<00:03,  4.12it/s] 94%|█████████▍| 204/217 [00:49<00:03,  4.12it/s] 94%|█████████▍| 205/217 [00:49<00:02,  4.13it/s] 95%|█████████▍| 206/217 [00:50<00:02,  4.13it/s] 95%|█████████▌| 207/217 [00:50<00:02,  4.14it/s] 96%|█████████▌| 208/217 [00:50<00:02,  4.14it/s] 96%|█████████▋| 209/217 [00:50<00:01,  4.14it/s] 97%|█████████▋| 210/217 [00:51<00:01,  4.13it/s] 97%|█████████▋| 211/217 [00:51<00:01,  4.13it/s] 98%|█████████▊| 212/217 [00:51<00:01,  4.12it/s] 98%|█████████▊| 213/217 [00:51<00:00,  4.12it/s] 99%|█████████▊| 214/217 [00:52<00:00,  4.12it/s] 99%|█████████▉| 215/217 [00:52<00:00,  4.13it/s]100%|█████████▉| 216/217 [00:52<00:00,  4.14it/s]100%|██████████| 217/217 [00:52<00:00,  4.15it/s]accuracy:  0.8387096774193549
100%|██████████| 217/217 [00:56<00:00,  3.87it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 103
Training dataset size: 384, validation dataset size: 103
Training dataset size: 384, validation dataset size: 103
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:48:34,314] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:48:34,657] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:48:36,014] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:02<02:20,  2.24s/it]  3%|▎         | 2/64 [00:05<02:46,  2.68s/it]  5%|▍         | 3/64 [00:07<02:37,  2.59s/it]  6%|▋         | 4/64 [00:09<02:21,  2.35s/it]  8%|▊         | 5/64 [00:12<02:19,  2.37s/it]  9%|▉         | 6/64 [00:14<02:19,  2.40s/it] 11%|█         | 7/64 [00:16<02:16,  2.39s/it] 12%|█▎        | 8/64 [00:18<02:05,  2.24s/it] 14%|█▍        | 9/64 [00:20<01:57,  2.13s/it] 16%|█▌        | 10/64 [00:22<01:52,  2.09s/it]                                               {'loss': 0.7987, 'grad_norm': 6.103691101074219, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:22<01:52,  2.09s/it] 17%|█▋        | 11/64 [00:24<01:50,  2.08s/it] 19%|█▉        | 12/64 [00:26<01:47,  2.07s/it] 20%|██        | 13/64 [00:28<01:44,  2.04s/it] 22%|██▏       | 14/64 [00:31<01:48,  2.17s/it] 23%|██▎       | 15/64 [00:33<01:48,  2.22s/it] 25%|██▌       | 16/64 [00:36<01:50,  2.30s/it] 27%|██▋       | 17/64 [00:38<01:51,  2.37s/it] 28%|██▊       | 18/64 [00:40<01:47,  2.33s/it] 30%|██▉       | 19/64 [00:42<01:41,  2.26s/it] 31%|███▏      | 20/64 [00:45<01:41,  2.31s/it]                                               {'loss': 0.5584, 'grad_norm': 5.265355587005615, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:45<01:41,  2.31s/it] 33%|███▎      | 21/64 [00:47<01:39,  2.32s/it] 34%|███▍      | 22/64 [00:49<01:36,  2.30s/it] 36%|███▌      | 23/64 [00:51<01:28,  2.16s/it] 38%|███▊      | 24/64 [00:53<01:21,  2.04s/it] 39%|███▉      | 25/64 [00:55<01:18,  2.01s/it] 41%|████      | 26/64 [00:57<01:18,  2.05s/it] 42%|████▏     | 27/64 [00:59<01:17,  2.08s/it] 44%|████▍     | 28/64 [01:01<01:11,  1.99s/it] 45%|████▌     | 29/64 [01:03<01:13,  2.09s/it] 47%|████▋     | 30/64 [01:05<01:09,  2.05s/it]                                               {'loss': 0.6256, 'grad_norm': 3.961165189743042, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:05<01:09,  2.05s/it] 48%|████▊     | 31/64 [01:08<01:10,  2.14s/it] 50%|█████     | 32/64 [01:10<01:06,  2.09s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:13<01:11,  2.32s/it] 53%|█████▎    | 34/64 [01:15<01:08,  2.28s/it] 55%|█████▍    | 35/64 [01:17<01:03,  2.18s/it] 56%|█████▋    | 36/64 [01:19<01:01,  2.20s/it] 58%|█████▊    | 37/64 [01:21<00:59,  2.20s/it] 59%|█████▉    | 38/64 [01:24<00:58,  2.26s/it] 61%|██████    | 39/64 [01:26<00:54,  2.19s/it] 62%|██████▎   | 40/64 [01:28<00:51,  2.15s/it]                                               {'loss': 0.4697, 'grad_norm': 5.312895774841309, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:28<00:51,  2.15s/it] 64%|██████▍   | 41/64 [01:30<00:49,  2.16s/it] 66%|██████▌   | 42/64 [01:32<00:46,  2.10s/it] 67%|██████▋   | 43/64 [01:33<00:39,  1.90s/it] 69%|██████▉   | 44/64 [01:36<00:41,  2.07s/it] 70%|███████   | 45/64 [01:38<00:38,  2.01s/it] 72%|███████▏  | 46/64 [01:40<00:38,  2.13s/it] 73%|███████▎  | 47/64 [01:42<00:34,  2.04s/it] 75%|███████▌  | 48/64 [01:44<00:32,  2.04s/it] 77%|███████▋  | 49/64 [01:46<00:31,  2.12s/it] 78%|███████▊  | 50/64 [01:48<00:29,  2.13s/it]                                               {'loss': 0.4368, 'grad_norm': 4.965967178344727, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:48<00:29,  2.13s/it] 80%|███████▉  | 51/64 [01:51<00:28,  2.16s/it] 81%|████████▏ | 52/64 [01:53<00:26,  2.21s/it] 83%|████████▎ | 53/64 [01:56<00:25,  2.34s/it] 84%|████████▍ | 54/64 [01:57<00:20,  2.09s/it] 86%|████████▌ | 55/64 [01:59<00:18,  2.07s/it] 88%|████████▊ | 56/64 [02:01<00:16,  2.05s/it] 89%|████████▉ | 57/64 [02:03<00:14,  2.14s/it] 91%|█████████ | 58/64 [02:05<00:12,  2.13s/it] 92%|█████████▏| 59/64 [02:08<00:11,  2.21s/it] 94%|█████████▍| 60/64 [02:10<00:08,  2.24s/it]                                               {'loss': 0.5179, 'grad_norm': 8.208345413208008, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:10<00:08,  2.24s/it] 95%|█████████▌| 61/64 [02:12<00:06,  2.20s/it] 97%|█████████▋| 62/64 [02:14<00:04,  2.13s/it] 98%|█████████▊| 63/64 [02:17<00:02,  2.21s/it]100%|██████████| 64/64 [02:18<00:00,  1.99s/it]                                               {'train_runtime': 139.4452, 'train_samples_per_second': 5.508, 'train_steps_per_second': 0.459, 'train_loss': 0.5650426261126995, 'epoch': 2.0}
100%|██████████| 64/64 [02:19<00:00,  1.99s/it]100%|██████████| 64/64 [02:19<00:00,  2.18s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  150
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:00<00:45,  3.27it/s]  1%|▏         | 2/150 [00:00<00:39,  3.77it/s]  2%|▏         | 3/150 [00:00<00:37,  3.96it/s]  3%|▎         | 4/150 [00:01<00:36,  4.05it/s]  3%|▎         | 5/150 [00:01<00:35,  4.10it/s]  4%|▍         | 6/150 [00:01<00:34,  4.13it/s]  5%|▍         | 7/150 [00:01<00:34,  4.14it/s]  5%|▌         | 8/150 [00:01<00:34,  4.16it/s]  6%|▌         | 9/150 [00:02<00:33,  4.16it/s]  7%|▋         | 10/150 [00:02<00:33,  4.17it/s]  7%|▋         | 11/150 [00:02<00:33,  4.17it/s]  8%|▊         | 12/150 [00:02<00:33,  4.17it/s]  9%|▊         | 13/150 [00:03<00:32,  4.17it/s]  9%|▉         | 14/150 [00:03<00:32,  4.17it/s] 10%|█         | 15/150 [00:03<00:32,  4.18it/s] 11%|█         | 16/150 [00:03<00:32,  4.18it/s] 11%|█▏        | 17/150 [00:04<00:31,  4.19it/s] 12%|█▏        | 18/150 [00:04<00:31,  4.19it/s] 13%|█▎        | 19/150 [00:04<00:31,  4.21it/s] 13%|█▎        | 20/150 [00:04<00:30,  4.21it/s] 14%|█▍        | 21/150 [00:05<00:30,  4.20it/s] 15%|█▍        | 22/150 [00:05<00:30,  4.20it/s] 15%|█▌        | 23/150 [00:05<00:30,  4.20it/s] 16%|█▌        | 24/150 [00:05<00:30,  4.19it/s] 17%|█▋        | 25/150 [00:06<00:29,  4.19it/s] 17%|█▋        | 26/150 [00:06<00:29,  4.20it/s] 18%|█▊        | 27/150 [00:06<00:29,  4.20it/s] 19%|█▊        | 28/150 [00:06<00:29,  4.20it/s] 19%|█▉        | 29/150 [00:06<00:28,  4.21it/s] 20%|██        | 30/150 [00:07<00:28,  4.21it/s] 21%|██        | 31/150 [00:07<00:28,  4.21it/s] 21%|██▏       | 32/150 [00:07<00:28,  4.20it/s] 22%|██▏       | 33/150 [00:07<00:27,  4.20it/s] 23%|██▎       | 34/150 [00:08<00:27,  4.19it/s] 23%|██▎       | 35/150 [00:08<00:27,  4.19it/s] 24%|██▍       | 36/150 [00:08<00:27,  4.18it/s] 25%|██▍       | 37/150 [00:08<00:27,  4.18it/s] 25%|██▌       | 38/150 [00:09<00:26,  4.18it/s] 26%|██▌       | 39/150 [00:09<00:26,  4.18it/s] 27%|██▋       | 40/150 [00:09<00:26,  4.18it/s] 27%|██▋       | 41/150 [00:09<00:26,  4.18it/s] 28%|██▊       | 42/150 [00:10<00:25,  4.18it/s] 29%|██▊       | 43/150 [00:10<00:25,  4.18it/s] 29%|██▉       | 44/150 [00:10<00:25,  4.18it/s] 30%|███       | 45/150 [00:10<00:25,  4.19it/s] 31%|███       | 46/150 [00:11<00:24,  4.19it/s] 31%|███▏      | 47/150 [00:11<00:24,  4.19it/s] 32%|███▏      | 48/150 [00:11<00:24,  4.20it/s] 33%|███▎      | 49/150 [00:11<00:24,  4.20it/s] 33%|███▎      | 50/150 [00:11<00:23,  4.20it/s] 34%|███▍      | 51/150 [00:12<00:23,  4.20it/s] 35%|███▍      | 52/150 [00:12<00:23,  4.19it/s] 35%|███▌      | 53/150 [00:12<00:23,  4.18it/s] 36%|███▌      | 54/150 [00:12<00:22,  4.18it/s] 37%|███▋      | 55/150 [00:13<00:22,  4.18it/s] 37%|███▋      | 56/150 [00:13<00:22,  4.17it/s] 38%|███▊      | 57/150 [00:13<00:22,  4.17it/s] 39%|███▊      | 58/150 [00:13<00:22,  4.17it/s] 39%|███▉      | 59/150 [00:14<00:21,  4.17it/s] 40%|████      | 60/150 [00:14<00:21,  4.17it/s] 41%|████      | 61/150 [00:14<00:21,  4.17it/s] 41%|████▏     | 62/150 [00:14<00:21,  4.17it/s] 42%|████▏     | 63/150 [00:15<00:20,  4.17it/s] 43%|████▎     | 64/150 [00:15<00:20,  4.17it/s] 43%|████▎     | 65/150 [00:15<00:20,  4.17it/s] 44%|████▍     | 66/150 [00:15<00:20,  4.17it/s] 45%|████▍     | 67/150 [00:16<00:19,  4.17it/s] 45%|████▌     | 68/150 [00:16<00:19,  4.17it/s] 46%|████▌     | 69/150 [00:16<00:19,  4.17it/s] 47%|████▋     | 70/150 [00:16<00:19,  4.17it/s] 47%|████▋     | 71/150 [00:17<00:18,  4.17it/s] 48%|████▊     | 72/150 [00:17<00:18,  4.17it/s] 49%|████▊     | 73/150 [00:17<00:18,  4.17it/s] 49%|████▉     | 74/150 [00:17<00:18,  4.17it/s] 50%|█████     | 75/150 [00:17<00:18,  4.17it/s] 51%|█████     | 76/150 [00:18<00:17,  4.17it/s] 51%|█████▏    | 77/150 [00:18<00:17,  4.17it/s] 52%|█████▏    | 78/150 [00:18<00:17,  4.17it/s] 53%|█████▎    | 79/150 [00:18<00:17,  4.17it/s] 53%|█████▎    | 80/150 [00:19<00:16,  4.16it/s] 54%|█████▍    | 81/150 [00:19<00:16,  4.16it/s] 55%|█████▍    | 82/150 [00:19<00:16,  4.16it/s] 55%|█████▌    | 83/150 [00:19<00:16,  4.16it/s] 56%|█████▌    | 84/150 [00:20<00:15,  4.17it/s] 57%|█████▋    | 85/150 [00:20<00:15,  4.16it/s] 57%|█████▋    | 86/150 [00:20<00:15,  4.17it/s] 58%|█████▊    | 87/150 [00:20<00:15,  4.17it/s] 59%|█████▊    | 88/150 [00:21<00:14,  4.17it/s] 59%|█████▉    | 89/150 [00:21<00:14,  4.17it/s] 60%|██████    | 90/150 [00:21<00:14,  4.17it/s] 61%|██████    | 91/150 [00:21<00:14,  4.17it/s] 61%|██████▏   | 92/150 [00:22<00:13,  4.17it/s] 62%|██████▏   | 93/150 [00:22<00:13,  4.16it/s] 63%|██████▎   | 94/150 [00:22<00:13,  4.17it/s] 63%|██████▎   | 95/150 [00:22<00:13,  4.17it/s] 64%|██████▍   | 96/150 [00:23<00:12,  4.17it/s] 65%|██████▍   | 97/150 [00:23<00:12,  4.16it/s] 65%|██████▌   | 98/150 [00:23<00:12,  4.16it/s] 66%|██████▌   | 99/150 [00:23<00:12,  4.16it/s] 67%|██████▋   | 100/150 [00:23<00:12,  4.16it/s] 67%|██████▋   | 101/150 [00:24<00:11,  4.16it/s] 68%|██████▊   | 102/150 [00:24<00:11,  4.16it/s] 69%|██████▊   | 103/150 [00:24<00:11,  4.17it/s] 69%|██████▉   | 104/150 [00:24<00:11,  4.17it/s] 70%|███████   | 105/150 [00:25<00:10,  4.17it/s] 71%|███████   | 106/150 [00:25<00:10,  4.17it/s] 71%|███████▏  | 107/150 [00:25<00:10,  4.17it/s] 72%|███████▏  | 108/150 [00:25<00:10,  4.17it/s] 73%|███████▎  | 109/150 [00:26<00:09,  4.17it/s] 73%|███████▎  | 110/150 [00:26<00:09,  4.16it/s] 74%|███████▍  | 111/150 [00:26<00:09,  4.16it/s] 75%|███████▍  | 112/150 [00:26<00:09,  4.17it/s] 75%|███████▌  | 113/150 [00:27<00:08,  4.17it/s] 76%|███████▌  | 114/150 [00:27<00:08,  4.17it/s] 77%|███████▋  | 115/150 [00:27<00:08,  4.17it/s] 77%|███████▋  | 116/150 [00:27<00:08,  4.17it/s] 78%|███████▊  | 117/150 [00:28<00:07,  4.17it/s] 79%|███████▊  | 118/150 [00:28<00:07,  4.17it/s] 79%|███████▉  | 119/150 [00:28<00:07,  4.17it/s] 80%|████████  | 120/150 [00:28<00:07,  4.17it/s] 81%|████████  | 121/150 [00:29<00:06,  4.17it/s] 81%|████████▏ | 122/150 [00:29<00:06,  4.17it/s] 82%|████████▏ | 123/150 [00:29<00:06,  4.17it/s] 83%|████████▎ | 124/150 [00:29<00:06,  4.17it/s] 83%|████████▎ | 125/150 [00:29<00:06,  4.16it/s] 84%|████████▍ | 126/150 [00:30<00:05,  4.16it/s] 85%|████████▍ | 127/150 [00:30<00:05,  4.17it/s] 85%|████████▌ | 128/150 [00:30<00:05,  4.17it/s] 86%|████████▌ | 129/150 [00:30<00:05,  4.17it/s] 87%|████████▋ | 130/150 [00:31<00:04,  4.17it/s] 87%|████████▋ | 131/150 [00:31<00:04,  4.17it/s] 88%|████████▊ | 132/150 [00:31<00:04,  4.17it/s] 89%|████████▊ | 133/150 [00:31<00:04,  4.16it/s] 89%|████████▉ | 134/150 [00:32<00:03,  4.16it/s] 90%|█████████ | 135/150 [00:32<00:03,  4.17it/s] 91%|█████████ | 136/150 [00:32<00:03,  4.17it/s] 91%|█████████▏| 137/150 [00:32<00:03,  4.16it/s] 92%|█████████▏| 138/150 [00:33<00:02,  4.16it/s] 93%|█████████▎| 139/150 [00:33<00:02,  4.16it/s] 93%|█████████▎| 140/150 [00:33<00:02,  4.16it/s] 94%|█████████▍| 141/150 [00:33<00:02,  4.16it/s] 95%|█████████▍| 142/150 [00:34<00:01,  4.16it/s] 95%|█████████▌| 143/150 [00:34<00:01,  4.16it/s] 96%|█████████▌| 144/150 [00:34<00:01,  4.16it/s] 97%|█████████▋| 145/150 [00:34<00:01,  4.16it/s] 97%|█████████▋| 146/150 [00:35<00:00,  4.16it/s] 98%|█████████▊| 147/150 [00:35<00:00,  4.16it/s] 99%|█████████▊| 148/150 [00:35<00:00,  4.16it/s] 99%|█████████▉| 149/150 [00:35<00:00,  4.15it/s]100%|██████████| 150/150 [00:35<00:00,  4.15it/s]accuracy:  0.6333333333333333
100%|██████████| 150/150 [00:38<00:00,  3.92it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 145
Training dataset size: 384, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 145
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:52:04,428] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 13:52:04,428] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:52:06,235] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:14,  3.09s/it]  3%|▎         | 2/64 [00:05<02:47,  2.70s/it]  5%|▍         | 3/64 [00:08<02:47,  2.75s/it]  6%|▋         | 4/64 [00:10<02:26,  2.45s/it]  8%|▊         | 5/64 [00:12<02:22,  2.42s/it]  9%|▉         | 6/64 [00:14<02:12,  2.29s/it] 11%|█         | 7/64 [00:17<02:15,  2.37s/it] 12%|█▎        | 8/64 [00:18<02:00,  2.16s/it] 14%|█▍        | 9/64 [00:21<02:05,  2.29s/it] 16%|█▌        | 10/64 [00:23<02:05,  2.32s/it]                                               {'loss': 1.495, 'grad_norm': 8.49174690246582, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:23<02:05,  2.32s/it] 17%|█▋        | 11/64 [00:26<02:00,  2.28s/it] 19%|█▉        | 12/64 [00:28<02:00,  2.32s/it] 20%|██        | 13/64 [00:30<01:55,  2.26s/it] 22%|██▏       | 14/64 [00:33<02:02,  2.46s/it] 23%|██▎       | 15/64 [00:35<01:59,  2.43s/it] 25%|██▌       | 16/64 [00:38<01:52,  2.34s/it] 27%|██▋       | 17/64 [00:40<01:44,  2.23s/it] 28%|██▊       | 18/64 [00:42<01:48,  2.35s/it] 30%|██▉       | 19/64 [00:44<01:41,  2.26s/it] 31%|███▏      | 20/64 [00:46<01:36,  2.20s/it]                                               {'loss': 1.2868, 'grad_norm': 13.73344898223877, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:46<01:36,  2.20s/it] 33%|███▎      | 21/64 [00:48<01:33,  2.18s/it] 34%|███▍      | 22/64 [00:51<01:32,  2.19s/it] 36%|███▌      | 23/64 [00:53<01:31,  2.23s/it] 38%|███▊      | 24/64 [00:55<01:26,  2.17s/it] 39%|███▉      | 25/64 [00:57<01:26,  2.22s/it] 41%|████      | 26/64 [01:00<01:25,  2.24s/it] 42%|████▏     | 27/64 [01:02<01:25,  2.30s/it] 44%|████▍     | 28/64 [01:04<01:19,  2.22s/it] 45%|████▌     | 29/64 [01:06<01:16,  2.18s/it] 47%|████▋     | 30/64 [01:08<01:15,  2.23s/it]                                               {'loss': 1.0094, 'grad_norm': 6.080299377441406, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:08<01:15,  2.23s/it] 48%|████▊     | 31/64 [01:11<01:11,  2.17s/it] 50%|█████     | 32/64 [01:13<01:11,  2.23s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:15<01:09,  2.25s/it] 53%|█████▎    | 34/64 [01:18<01:08,  2.27s/it] 55%|█████▍    | 35/64 [01:20<01:03,  2.20s/it] 56%|█████▋    | 36/64 [01:22<01:04,  2.29s/it] 58%|█████▊    | 37/64 [01:24<01:00,  2.23s/it] 59%|█████▉    | 38/64 [01:27<01:00,  2.34s/it] 61%|██████    | 39/64 [01:29<00:58,  2.33s/it] 62%|██████▎   | 40/64 [01:31<00:55,  2.32s/it]                                               {'loss': 0.8706, 'grad_norm': 7.208657264709473, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:31<00:55,  2.32s/it] 64%|██████▍   | 41/64 [01:33<00:51,  2.24s/it] 66%|██████▌   | 42/64 [01:36<00:49,  2.26s/it] 67%|██████▋   | 43/64 [01:38<00:45,  2.17s/it] 69%|██████▉   | 44/64 [01:40<00:43,  2.16s/it] 70%|███████   | 45/64 [01:42<00:39,  2.10s/it] 72%|███████▏  | 46/64 [01:44<00:38,  2.11s/it] 73%|███████▎  | 47/64 [01:46<00:38,  2.24s/it] 75%|███████▌  | 48/64 [01:49<00:35,  2.24s/it] 77%|███████▋  | 49/64 [01:51<00:33,  2.24s/it] 78%|███████▊  | 50/64 [01:53<00:31,  2.26s/it]                                               {'loss': 0.7948, 'grad_norm': 5.430015563964844, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:53<00:31,  2.26s/it] 80%|███████▉  | 51/64 [01:55<00:29,  2.25s/it] 81%|████████▏ | 52/64 [01:58<00:27,  2.31s/it] 83%|████████▎ | 53/64 [02:00<00:24,  2.21s/it] 84%|████████▍ | 54/64 [02:02<00:21,  2.12s/it] 86%|████████▌ | 55/64 [02:04<00:19,  2.20s/it] 88%|████████▊ | 56/64 [02:07<00:18,  2.30s/it] 89%|████████▉ | 57/64 [02:09<00:15,  2.23s/it] 91%|█████████ | 58/64 [02:11<00:13,  2.17s/it] 92%|█████████▏| 59/64 [02:13<00:10,  2.19s/it] 94%|█████████▍| 60/64 [02:15<00:08,  2.24s/it]                                               {'loss': 0.789, 'grad_norm': 6.069930553436279, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:15<00:08,  2.24s/it] 95%|█████████▌| 61/64 [02:18<00:07,  2.36s/it] 97%|█████████▋| 62/64 [02:20<00:04,  2.33s/it] 98%|█████████▊| 63/64 [02:22<00:02,  2.12s/it]100%|██████████| 64/64 [02:24<00:00,  2.22s/it]                                               {'train_runtime': 146.0083, 'train_samples_per_second': 5.26, 'train_steps_per_second': 0.438, 'train_loss': 1.0153508186340332, 'epoch': 2.0}
100%|██████████| 64/64 [02:25<00:00,  2.22s/it]100%|██████████| 64/64 [02:25<00:00,  2.28s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  200
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:00,  3.29it/s]  1%|          | 2/200 [00:00<00:52,  3.79it/s]  2%|▏         | 3/200 [00:00<00:49,  3.97it/s]  2%|▏         | 4/200 [00:01<00:48,  4.06it/s]  2%|▎         | 5/200 [00:01<00:47,  4.10it/s]  3%|▎         | 6/200 [00:01<00:46,  4.13it/s]  4%|▎         | 7/200 [00:01<00:46,  4.15it/s]  4%|▍         | 8/200 [00:01<00:46,  4.16it/s]  4%|▍         | 9/200 [00:02<00:45,  4.17it/s]  5%|▌         | 10/200 [00:02<00:45,  4.17it/s]  6%|▌         | 11/200 [00:02<00:45,  4.18it/s]  6%|▌         | 12/200 [00:02<00:44,  4.18it/s]  6%|▋         | 13/200 [00:03<00:44,  4.19it/s]  7%|▋         | 14/200 [00:03<00:44,  4.20it/s]  8%|▊         | 15/200 [00:03<00:43,  4.21it/s]  8%|▊         | 16/200 [00:03<00:43,  4.21it/s]  8%|▊         | 17/200 [00:04<00:43,  4.20it/s]  9%|▉         | 18/200 [00:04<00:43,  4.19it/s] 10%|▉         | 19/200 [00:04<00:43,  4.19it/s] 10%|█         | 20/200 [00:04<00:43,  4.18it/s] 10%|█         | 21/200 [00:05<00:42,  4.18it/s] 11%|█         | 22/200 [00:05<00:42,  4.17it/s] 12%|█▏        | 23/200 [00:05<00:42,  4.17it/s] 12%|█▏        | 24/200 [00:05<00:42,  4.17it/s] 12%|█▎        | 25/200 [00:06<00:41,  4.18it/s] 13%|█▎        | 26/200 [00:06<00:41,  4.17it/s] 14%|█▎        | 27/200 [00:06<00:41,  4.17it/s] 14%|█▍        | 28/200 [00:06<00:41,  4.17it/s] 14%|█▍        | 29/200 [00:06<00:40,  4.17it/s] 15%|█▌        | 30/200 [00:07<00:40,  4.17it/s] 16%|█▌        | 31/200 [00:07<00:40,  4.16it/s] 16%|█▌        | 32/200 [00:07<00:40,  4.17it/s] 16%|█▋        | 33/200 [00:07<00:40,  4.17it/s] 17%|█▋        | 34/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 35/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 36/200 [00:08<00:39,  4.17it/s] 18%|█▊        | 37/200 [00:08<00:39,  4.17it/s] 19%|█▉        | 38/200 [00:09<00:38,  4.16it/s] 20%|█▉        | 39/200 [00:09<00:38,  4.16it/s] 20%|██        | 40/200 [00:09<00:38,  4.16it/s] 20%|██        | 41/200 [00:09<00:38,  4.16it/s] 21%|██        | 42/200 [00:10<00:37,  4.16it/s] 22%|██▏       | 43/200 [00:10<00:37,  4.17it/s] 22%|██▏       | 44/200 [00:10<00:37,  4.17it/s] 22%|██▎       | 45/200 [00:10<00:37,  4.17it/s] 23%|██▎       | 46/200 [00:11<00:36,  4.17it/s] 24%|██▎       | 47/200 [00:11<00:36,  4.17it/s] 24%|██▍       | 48/200 [00:11<00:36,  4.16it/s] 24%|██▍       | 49/200 [00:11<00:36,  4.17it/s] 25%|██▌       | 50/200 [00:12<00:35,  4.17it/s] 26%|██▌       | 51/200 [00:12<00:35,  4.17it/s] 26%|██▌       | 52/200 [00:12<00:35,  4.18it/s] 26%|██▋       | 53/200 [00:12<00:35,  4.17it/s] 27%|██▋       | 54/200 [00:12<00:34,  4.17it/s] 28%|██▊       | 55/200 [00:13<00:34,  4.17it/s] 28%|██▊       | 56/200 [00:13<00:34,  4.17it/s] 28%|██▊       | 57/200 [00:13<00:34,  4.17it/s] 29%|██▉       | 58/200 [00:13<00:34,  4.17it/s] 30%|██▉       | 59/200 [00:14<00:33,  4.17it/s] 30%|███       | 60/200 [00:14<00:33,  4.17it/s] 30%|███       | 61/200 [00:14<00:33,  4.17it/s] 31%|███       | 62/200 [00:14<00:33,  4.17it/s] 32%|███▏      | 63/200 [00:15<00:32,  4.17it/s] 32%|███▏      | 64/200 [00:15<00:32,  4.17it/s] 32%|███▎      | 65/200 [00:15<00:32,  4.16it/s] 33%|███▎      | 66/200 [00:15<00:32,  4.17it/s] 34%|███▎      | 67/200 [00:16<00:31,  4.17it/s] 34%|███▍      | 68/200 [00:16<00:31,  4.17it/s] 34%|███▍      | 69/200 [00:16<00:31,  4.17it/s] 35%|███▌      | 70/200 [00:16<00:31,  4.17it/s] 36%|███▌      | 71/200 [00:17<00:30,  4.17it/s] 36%|███▌      | 72/200 [00:17<00:30,  4.17it/s] 36%|███▋      | 73/200 [00:17<00:30,  4.16it/s] 37%|███▋      | 74/200 [00:17<00:30,  4.16it/s] 38%|███▊      | 75/200 [00:18<00:30,  4.17it/s] 38%|███▊      | 76/200 [00:18<00:29,  4.17it/s] 38%|███▊      | 77/200 [00:18<00:29,  4.17it/s] 39%|███▉      | 78/200 [00:18<00:29,  4.17it/s] 40%|███▉      | 79/200 [00:18<00:29,  4.17it/s] 40%|████      | 80/200 [00:19<00:28,  4.17it/s] 40%|████      | 81/200 [00:19<00:28,  4.16it/s] 41%|████      | 82/200 [00:19<00:28,  4.16it/s] 42%|████▏     | 83/200 [00:19<00:28,  4.16it/s] 42%|████▏     | 84/200 [00:20<00:27,  4.16it/s] 42%|████▎     | 85/200 [00:20<00:27,  4.16it/s] 43%|████▎     | 86/200 [00:20<00:27,  4.16it/s] 44%|████▎     | 87/200 [00:20<00:27,  4.17it/s] 44%|████▍     | 88/200 [00:21<00:26,  4.17it/s] 44%|████▍     | 89/200 [00:21<00:26,  4.17it/s] 45%|████▌     | 90/200 [00:21<00:26,  4.17it/s] 46%|████▌     | 91/200 [00:21<00:26,  4.17it/s] 46%|████▌     | 92/200 [00:22<00:25,  4.17it/s] 46%|████▋     | 93/200 [00:22<00:25,  4.17it/s] 47%|████▋     | 94/200 [00:22<00:25,  4.17it/s] 48%|████▊     | 95/200 [00:22<00:25,  4.17it/s] 48%|████▊     | 96/200 [00:23<00:24,  4.17it/s] 48%|████▊     | 97/200 [00:23<00:24,  4.16it/s] 49%|████▉     | 98/200 [00:23<00:24,  4.17it/s] 50%|████▉     | 99/200 [00:23<00:24,  4.17it/s] 50%|█████     | 100/200 [00:24<00:24,  4.17it/s] 50%|█████     | 101/200 [00:24<00:23,  4.17it/s] 51%|█████     | 102/200 [00:24<00:23,  4.17it/s] 52%|█████▏    | 103/200 [00:24<00:23,  4.17it/s] 52%|█████▏    | 104/200 [00:24<00:23,  4.16it/s] 52%|█████▎    | 105/200 [00:25<00:22,  4.17it/s] 53%|█████▎    | 106/200 [00:25<00:22,  4.17it/s] 54%|█████▎    | 107/200 [00:25<00:22,  4.17it/s] 54%|█████▍    | 108/200 [00:25<00:22,  4.17it/s] 55%|█████▍    | 109/200 [00:26<00:21,  4.17it/s] 55%|█████▌    | 110/200 [00:26<00:21,  4.17it/s] 56%|█████▌    | 111/200 [00:26<00:21,  4.17it/s] 56%|█████▌    | 112/200 [00:26<00:21,  4.16it/s] 56%|█████▋    | 113/200 [00:27<00:20,  4.16it/s] 57%|█████▋    | 114/200 [00:27<00:20,  4.16it/s] 57%|█████▊    | 115/200 [00:27<00:20,  4.16it/s] 58%|█████▊    | 116/200 [00:27<00:20,  4.16it/s] 58%|█████▊    | 117/200 [00:28<00:19,  4.16it/s] 59%|█████▉    | 118/200 [00:28<00:19,  4.17it/s] 60%|█████▉    | 119/200 [00:28<00:19,  4.16it/s] 60%|██████    | 120/200 [00:28<00:19,  4.16it/s] 60%|██████    | 121/200 [00:29<00:18,  4.17it/s] 61%|██████    | 122/200 [00:29<00:18,  4.17it/s] 62%|██████▏   | 123/200 [00:29<00:18,  4.17it/s] 62%|██████▏   | 124/200 [00:29<00:18,  4.17it/s] 62%|██████▎   | 125/200 [00:30<00:18,  4.16it/s] 63%|██████▎   | 126/200 [00:30<00:17,  4.16it/s] 64%|██████▎   | 127/200 [00:30<00:17,  4.16it/s] 64%|██████▍   | 128/200 [00:30<00:17,  4.16it/s] 64%|██████▍   | 129/200 [00:30<00:17,  4.16it/s] 65%|██████▌   | 130/200 [00:31<00:16,  4.16it/s] 66%|██████▌   | 131/200 [00:31<00:16,  4.17it/s] 66%|██████▌   | 132/200 [00:31<00:16,  4.17it/s] 66%|██████▋   | 133/200 [00:31<00:16,  4.17it/s] 67%|██████▋   | 134/200 [00:32<00:15,  4.17it/s] 68%|██████▊   | 135/200 [00:32<00:15,  4.17it/s] 68%|██████▊   | 136/200 [00:32<00:15,  4.17it/s] 68%|██████▊   | 137/200 [00:32<00:15,  4.17it/s] 69%|██████▉   | 138/200 [00:33<00:14,  4.17it/s] 70%|██████▉   | 139/200 [00:33<00:14,  4.17it/s] 70%|███████   | 140/200 [00:33<00:14,  4.16it/s] 70%|███████   | 141/200 [00:33<00:14,  4.16it/s] 71%|███████   | 142/200 [00:34<00:13,  4.16it/s] 72%|███████▏  | 143/200 [00:34<00:13,  4.16it/s] 72%|███████▏  | 144/200 [00:34<00:13,  4.16it/s] 72%|███████▎  | 145/200 [00:34<00:13,  4.16it/s] 73%|███████▎  | 146/200 [00:35<00:12,  4.16it/s] 74%|███████▎  | 147/200 [00:35<00:12,  4.16it/s] 74%|███████▍  | 148/200 [00:35<00:12,  4.16it/s] 74%|███████▍  | 149/200 [00:35<00:12,  4.16it/s] 75%|███████▌  | 150/200 [00:36<00:12,  4.15it/s] 76%|███████▌  | 151/200 [00:36<00:11,  4.15it/s] 76%|███████▌  | 152/200 [00:36<00:11,  4.14it/s] 76%|███████▋  | 153/200 [00:36<00:11,  4.13it/s] 77%|███████▋  | 154/200 [00:37<00:11,  4.13it/s] 78%|███████▊  | 155/200 [00:37<00:10,  4.14it/s] 78%|███████▊  | 156/200 [00:37<00:10,  4.14it/s] 78%|███████▊  | 157/200 [00:37<00:10,  4.14it/s] 79%|███████▉  | 158/200 [00:37<00:10,  4.15it/s] 80%|███████▉  | 159/200 [00:38<00:09,  4.15it/s] 80%|████████  | 160/200 [00:38<00:09,  4.16it/s] 80%|████████  | 161/200 [00:38<00:09,  4.16it/s] 81%|████████  | 162/200 [00:38<00:09,  4.16it/s] 82%|████████▏ | 163/200 [00:39<00:08,  4.16it/s] 82%|████████▏ | 164/200 [00:39<00:08,  4.15it/s] 82%|████████▎ | 165/200 [00:39<00:08,  4.15it/s] 83%|████████▎ | 166/200 [00:39<00:08,  4.15it/s] 84%|████████▎ | 167/200 [00:40<00:07,  4.14it/s] 84%|████████▍ | 168/200 [00:40<00:07,  4.13it/s] 84%|████████▍ | 169/200 [00:40<00:07,  4.13it/s] 85%|████████▌ | 170/200 [00:40<00:07,  4.13it/s] 86%|████████▌ | 171/200 [00:41<00:07,  4.13it/s] 86%|████████▌ | 172/200 [00:41<00:06,  4.14it/s] 86%|████████▋ | 173/200 [00:41<00:06,  4.14it/s] 87%|████████▋ | 174/200 [00:41<00:06,  4.15it/s] 88%|████████▊ | 175/200 [00:42<00:06,  4.15it/s] 88%|████████▊ | 176/200 [00:42<00:05,  4.15it/s] 88%|████████▊ | 177/200 [00:42<00:05,  4.15it/s] 89%|████████▉ | 178/200 [00:42<00:05,  4.14it/s] 90%|████████▉ | 179/200 [00:43<00:05,  4.14it/s] 90%|█████████ | 180/200 [00:43<00:04,  4.13it/s] 90%|█████████ | 181/200 [00:43<00:04,  4.12it/s] 91%|█████████ | 182/200 [00:43<00:04,  4.12it/s] 92%|█████████▏| 183/200 [00:44<00:04,  4.13it/s] 92%|█████████▏| 184/200 [00:44<00:03,  4.14it/s] 92%|█████████▎| 185/200 [00:44<00:03,  4.14it/s] 93%|█████████▎| 186/200 [00:44<00:03,  4.14it/s] 94%|█████████▎| 187/200 [00:44<00:03,  4.14it/s] 94%|█████████▍| 188/200 [00:45<00:02,  4.13it/s] 94%|█████████▍| 189/200 [00:45<00:02,  4.13it/s] 95%|█████████▌| 190/200 [00:45<00:02,  4.12it/s] 96%|█████████▌| 191/200 [00:45<00:02,  4.12it/s] 96%|█████████▌| 192/200 [00:46<00:01,  4.13it/s] 96%|█████████▋| 193/200 [00:46<00:01,  4.13it/s] 97%|█████████▋| 194/200 [00:46<00:01,  4.14it/s] 98%|█████████▊| 195/200 [00:46<00:01,  4.14it/s] 98%|█████████▊| 196/200 [00:47<00:00,  4.14it/s] 98%|█████████▊| 197/200 [00:47<00:00,  4.14it/s] 99%|█████████▉| 198/200 [00:47<00:00,  4.13it/s]100%|█████████▉| 199/200 [00:47<00:00,  4.12it/s]100%|██████████| 200/200 [00:48<00:00,  4.12it/s]accuracy:  0.605
100%|██████████| 200/200 [00:51<00:00,  3.91it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 191
Training dataset size: 384, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 191
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.03s/it]trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:56:03,043] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 13:56:03,043] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 13:56:04,590] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:24,  3.24s/it]  3%|▎         | 2/64 [00:05<02:50,  2.75s/it]  5%|▍         | 3/64 [00:08<02:43,  2.68s/it]  6%|▋         | 4/64 [00:11<02:47,  2.80s/it]  8%|▊         | 5/64 [00:13<02:34,  2.63s/it]  9%|▉         | 6/64 [00:16<02:43,  2.81s/it] 11%|█         | 7/64 [00:19<02:35,  2.73s/it] 12%|█▎        | 8/64 [00:21<02:32,  2.72s/it] 14%|█▍        | 9/64 [00:24<02:23,  2.60s/it] 16%|█▌        | 10/64 [00:27<02:23,  2.65s/it]                                               {'loss': 1.7571, 'grad_norm': 16.773475646972656, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:27<02:23,  2.65s/it] 17%|█▋        | 11/64 [00:29<02:17,  2.60s/it] 19%|█▉        | 12/64 [00:31<02:09,  2.49s/it] 20%|██        | 13/64 [00:34<02:08,  2.52s/it] 22%|██▏       | 14/64 [00:37<02:09,  2.58s/it] 23%|██▎       | 15/64 [00:39<02:00,  2.47s/it] 25%|██▌       | 16/64 [00:41<01:56,  2.43s/it] 27%|██▋       | 17/64 [00:44<01:55,  2.45s/it] 28%|██▊       | 18/64 [00:46<01:54,  2.48s/it] 30%|██▉       | 19/64 [00:49<01:50,  2.45s/it] 31%|███▏      | 20/64 [00:51<01:47,  2.45s/it]                                               {'loss': 1.0499, 'grad_norm': 12.594833374023438, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:51<01:47,  2.45s/it] 33%|███▎      | 21/64 [00:53<01:42,  2.39s/it] 34%|███▍      | 22/64 [00:56<01:42,  2.45s/it] 36%|███▌      | 23/64 [00:58<01:35,  2.32s/it] 38%|███▊      | 24/64 [01:01<01:38,  2.47s/it] 39%|███▉      | 25/64 [01:03<01:34,  2.41s/it] 41%|████      | 26/64 [01:05<01:28,  2.33s/it] 42%|████▏     | 27/64 [01:07<01:23,  2.25s/it] 44%|████▍     | 28/64 [01:10<01:23,  2.31s/it] 45%|████▌     | 29/64 [01:12<01:21,  2.33s/it] 47%|████▋     | 30/64 [01:15<01:21,  2.39s/it]                                               {'loss': 0.8669, 'grad_norm': 7.636939525604248, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:15<01:21,  2.39s/it] 48%|████▊     | 31/64 [01:17<01:20,  2.43s/it] 50%|█████     | 32/64 [01:20<01:18,  2.45s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:22<01:18,  2.55s/it] 53%|█████▎    | 34/64 [01:25<01:14,  2.49s/it] 55%|█████▍    | 35/64 [01:27<01:08,  2.38s/it] 56%|█████▋    | 36/64 [01:29<01:08,  2.44s/it] 58%|█████▊    | 37/64 [01:32<01:04,  2.37s/it] 59%|█████▉    | 38/64 [01:34<01:01,  2.37s/it] 61%|██████    | 39/64 [01:37<01:02,  2.50s/it] 62%|██████▎   | 40/64 [01:39<01:00,  2.51s/it]                                               {'loss': 0.7766, 'grad_norm': 5.985191345214844, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:39<01:00,  2.51s/it] 64%|██████▍   | 41/64 [01:42<00:57,  2.51s/it] 66%|██████▌   | 42/64 [01:44<00:54,  2.46s/it] 67%|██████▋   | 43/64 [01:46<00:50,  2.42s/it] 69%|██████▉   | 44/64 [01:49<00:46,  2.35s/it] 70%|███████   | 45/64 [01:51<00:44,  2.35s/it] 72%|███████▏  | 46/64 [01:54<00:44,  2.45s/it] 73%|███████▎  | 47/64 [01:57<00:43,  2.56s/it] 75%|███████▌  | 48/64 [01:59<00:40,  2.56s/it] 77%|███████▋  | 49/64 [02:01<00:37,  2.51s/it] 78%|███████▊  | 50/64 [02:04<00:33,  2.42s/it]                                               {'loss': 0.7044, 'grad_norm': 6.096829891204834, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [02:04<00:33,  2.42s/it] 80%|███████▉  | 51/64 [02:06<00:31,  2.40s/it] 81%|████████▏ | 52/64 [02:09<00:29,  2.44s/it] 83%|████████▎ | 53/64 [02:11<00:25,  2.32s/it] 84%|████████▍ | 54/64 [02:13<00:22,  2.21s/it] 86%|████████▌ | 55/64 [02:15<00:20,  2.25s/it] 88%|████████▊ | 56/64 [02:18<00:19,  2.43s/it] 89%|████████▉ | 57/64 [02:20<00:16,  2.33s/it] 91%|█████████ | 58/64 [02:23<00:15,  2.52s/it] 92%|█████████▏| 59/64 [02:26<00:12,  2.58s/it] 94%|█████████▍| 60/64 [02:28<00:10,  2.62s/it]                                               {'loss': 0.8192, 'grad_norm': 4.3592529296875, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:28<00:10,  2.62s/it] 95%|█████████▌| 61/64 [02:31<00:07,  2.54s/it] 97%|█████████▋| 62/64 [02:33<00:04,  2.47s/it] 98%|█████████▊| 63/64 [02:35<00:02,  2.37s/it]100%|██████████| 64/64 [02:38<00:00,  2.42s/it]                                               {'train_runtime': 158.9914, 'train_samples_per_second': 4.83, 'train_steps_per_second': 0.403, 'train_loss': 0.9845665134489536, 'epoch': 2.0}
100%|██████████| 64/64 [02:38<00:00,  2.42s/it]100%|██████████| 64/64 [02:38<00:00,  2.48s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  279
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<04:21,  1.06it/s]  1%|          | 2/279 [00:01<02:25,  1.90it/s]  1%|          | 3/279 [00:01<01:49,  2.53it/s]  1%|▏         | 4/279 [00:01<01:31,  3.00it/s]  2%|▏         | 5/279 [00:01<01:21,  3.34it/s]  2%|▏         | 6/279 [00:02<01:15,  3.60it/s]  3%|▎         | 7/279 [00:02<01:12,  3.78it/s]  3%|▎         | 8/279 [00:02<01:09,  3.91it/s]  3%|▎         | 9/279 [00:02<01:07,  4.00it/s]  4%|▎         | 10/279 [00:03<01:06,  4.06it/s]  4%|▍         | 11/279 [00:03<01:05,  4.10it/s]  4%|▍         | 12/279 [00:03<01:04,  4.12it/s]  5%|▍         | 13/279 [00:03<01:04,  4.14it/s]  5%|▌         | 14/279 [00:04<01:03,  4.16it/s]  5%|▌         | 15/279 [00:04<01:03,  4.16it/s]  6%|▌         | 16/279 [00:04<01:03,  4.17it/s]  6%|▌         | 17/279 [00:04<01:02,  4.17it/s]  6%|▋         | 18/279 [00:04<01:02,  4.18it/s]  7%|▋         | 19/279 [00:05<01:02,  4.19it/s]  7%|▋         | 20/279 [00:05<01:01,  4.19it/s]  8%|▊         | 21/279 [00:05<01:01,  4.20it/s]  8%|▊         | 22/279 [00:05<01:01,  4.20it/s]  8%|▊         | 23/279 [00:06<01:00,  4.21it/s]  9%|▊         | 24/279 [00:06<01:00,  4.20it/s]  9%|▉         | 25/279 [00:06<01:00,  4.19it/s]  9%|▉         | 26/279 [00:06<01:00,  4.19it/s] 10%|▉         | 27/279 [00:07<01:00,  4.18it/s] 10%|█         | 28/279 [00:07<01:00,  4.18it/s] 10%|█         | 29/279 [00:07<00:59,  4.17it/s] 11%|█         | 30/279 [00:07<00:59,  4.17it/s] 11%|█         | 31/279 [00:08<00:59,  4.17it/s] 11%|█▏        | 32/279 [00:08<00:59,  4.17it/s] 12%|█▏        | 33/279 [00:08<00:59,  4.16it/s] 12%|█▏        | 34/279 [00:08<00:58,  4.17it/s] 13%|█▎        | 35/279 [00:09<00:58,  4.17it/s] 13%|█▎        | 36/279 [00:09<00:58,  4.17it/s] 13%|█▎        | 37/279 [00:09<00:58,  4.17it/s] 14%|█▎        | 38/279 [00:09<00:57,  4.17it/s] 14%|█▍        | 39/279 [00:10<00:57,  4.17it/s] 14%|█▍        | 40/279 [00:10<00:57,  4.17it/s] 15%|█▍        | 41/279 [00:10<00:57,  4.17it/s] 15%|█▌        | 42/279 [00:10<00:56,  4.18it/s] 15%|█▌        | 43/279 [00:10<00:56,  4.18it/s] 16%|█▌        | 44/279 [00:11<00:56,  4.19it/s] 16%|█▌        | 45/279 [00:11<00:55,  4.19it/s] 16%|█▋        | 46/279 [00:11<00:55,  4.20it/s] 17%|█▋        | 47/279 [00:11<00:55,  4.21it/s] 17%|█▋        | 48/279 [00:12<00:54,  4.21it/s] 18%|█▊        | 49/279 [00:12<00:54,  4.20it/s] 18%|█▊        | 50/279 [00:12<00:54,  4.19it/s] 18%|█▊        | 51/279 [00:12<00:54,  4.17it/s] 19%|█▊        | 52/279 [00:13<00:54,  4.18it/s] 19%|█▉        | 53/279 [00:13<00:54,  4.18it/s] 19%|█▉        | 54/279 [00:13<00:53,  4.18it/s] 20%|█▉        | 55/279 [00:13<00:53,  4.18it/s] 20%|██        | 56/279 [00:14<00:53,  4.17it/s] 20%|██        | 57/279 [00:14<00:53,  4.18it/s] 21%|██        | 58/279 [00:14<00:52,  4.17it/s] 21%|██        | 59/279 [00:14<00:52,  4.17it/s] 22%|██▏       | 60/279 [00:15<00:52,  4.17it/s] 22%|██▏       | 61/279 [00:15<00:52,  4.17it/s] 22%|██▏       | 62/279 [00:15<00:52,  4.17it/s] 23%|██▎       | 63/279 [00:15<00:51,  4.17it/s] 23%|██▎       | 64/279 [00:15<00:51,  4.17it/s] 23%|██▎       | 65/279 [00:16<00:51,  4.17it/s] 24%|██▎       | 66/279 [00:16<00:51,  4.17it/s] 24%|██▍       | 67/279 [00:16<00:50,  4.17it/s] 24%|██▍       | 68/279 [00:16<00:50,  4.17it/s] 25%|██▍       | 69/279 [00:17<00:50,  4.16it/s] 25%|██▌       | 70/279 [00:17<00:50,  4.17it/s] 25%|██▌       | 71/279 [00:17<00:49,  4.17it/s] 26%|██▌       | 72/279 [00:17<00:49,  4.17it/s] 26%|██▌       | 73/279 [00:18<00:49,  4.17it/s] 27%|██▋       | 74/279 [00:18<00:49,  4.17it/s] 27%|██▋       | 75/279 [00:18<00:48,  4.17it/s] 27%|██▋       | 76/279 [00:18<00:48,  4.17it/s] 28%|██▊       | 77/279 [00:19<00:48,  4.17it/s] 28%|██▊       | 78/279 [00:19<00:48,  4.17it/s] 28%|██▊       | 79/279 [00:19<00:47,  4.17it/s] 29%|██▊       | 80/279 [00:19<00:47,  4.17it/s] 29%|██▉       | 81/279 [00:20<00:47,  4.16it/s] 29%|██▉       | 82/279 [00:20<00:47,  4.17it/s] 30%|██▉       | 83/279 [00:20<00:47,  4.16it/s] 30%|███       | 84/279 [00:20<00:46,  4.16it/s] 30%|███       | 85/279 [00:21<00:46,  4.17it/s] 31%|███       | 86/279 [00:21<00:46,  4.17it/s] 31%|███       | 87/279 [00:21<00:46,  4.17it/s] 32%|███▏      | 88/279 [00:21<00:45,  4.17it/s] 32%|███▏      | 89/279 [00:21<00:45,  4.17it/s] 32%|███▏      | 90/279 [00:22<00:45,  4.17it/s] 33%|███▎      | 91/279 [00:22<00:45,  4.17it/s] 33%|███▎      | 92/279 [00:22<00:44,  4.17it/s] 33%|███▎      | 93/279 [00:22<00:44,  4.17it/s] 34%|███▎      | 94/279 [00:23<00:44,  4.17it/s] 34%|███▍      | 95/279 [00:23<00:44,  4.16it/s] 34%|███▍      | 96/279 [00:23<00:43,  4.17it/s] 35%|███▍      | 97/279 [00:23<00:43,  4.17it/s] 35%|███▌      | 98/279 [00:24<00:43,  4.16it/s] 35%|███▌      | 99/279 [00:24<00:43,  4.16it/s] 36%|███▌      | 100/279 [00:24<00:42,  4.16it/s] 36%|███▌      | 101/279 [00:24<00:42,  4.17it/s] 37%|███▋      | 102/279 [00:25<00:42,  4.16it/s] 37%|███▋      | 103/279 [00:25<00:42,  4.17it/s] 37%|███▋      | 104/279 [00:25<00:42,  4.16it/s] 38%|███▊      | 105/279 [00:25<00:41,  4.17it/s] 38%|███▊      | 106/279 [00:26<00:41,  4.17it/s] 38%|███▊      | 107/279 [00:26<00:41,  4.16it/s] 39%|███▊      | 108/279 [00:26<00:41,  4.17it/s] 39%|███▉      | 109/279 [00:26<00:40,  4.17it/s] 39%|███▉      | 110/279 [00:27<00:40,  4.17it/s] 40%|███▉      | 111/279 [00:27<00:40,  4.17it/s] 40%|████      | 112/279 [00:27<00:40,  4.17it/s] 41%|████      | 113/279 [00:27<00:39,  4.17it/s] 41%|████      | 114/279 [00:27<00:39,  4.17it/s] 41%|████      | 115/279 [00:28<00:39,  4.16it/s] 42%|████▏     | 116/279 [00:28<00:39,  4.16it/s] 42%|████▏     | 117/279 [00:28<00:38,  4.16it/s] 42%|████▏     | 118/279 [00:28<00:38,  4.17it/s] 43%|████▎     | 119/279 [00:29<00:38,  4.17it/s] 43%|████▎     | 120/279 [00:29<00:38,  4.17it/s] 43%|████▎     | 121/279 [00:29<00:37,  4.17it/s] 44%|████▎     | 122/279 [00:29<00:37,  4.17it/s] 44%|████▍     | 123/279 [00:30<00:37,  4.16it/s] 44%|████▍     | 124/279 [00:30<00:37,  4.16it/s] 45%|████▍     | 125/279 [00:30<00:36,  4.17it/s] 45%|████▌     | 126/279 [00:30<00:36,  4.17it/s] 46%|████▌     | 127/279 [00:31<00:36,  4.17it/s] 46%|████▌     | 128/279 [00:31<00:36,  4.17it/s] 46%|████▌     | 129/279 [00:31<00:35,  4.17it/s] 47%|████▋     | 130/279 [00:31<00:35,  4.17it/s] 47%|████▋     | 131/279 [00:32<00:35,  4.17it/s] 47%|████▋     | 132/279 [00:32<00:35,  4.17it/s] 48%|████▊     | 133/279 [00:32<00:35,  4.17it/s] 48%|████▊     | 134/279 [00:32<00:34,  4.17it/s] 48%|████▊     | 135/279 [00:33<00:34,  4.17it/s] 49%|████▊     | 136/279 [00:33<00:34,  4.17it/s] 49%|████▉     | 137/279 [00:33<00:34,  4.17it/s] 49%|████▉     | 138/279 [00:33<00:33,  4.16it/s] 50%|████▉     | 139/279 [00:33<00:33,  4.16it/s] 50%|█████     | 140/279 [00:34<00:33,  4.16it/s] 51%|█████     | 141/279 [00:34<00:33,  4.16it/s] 51%|█████     | 142/279 [00:34<00:32,  4.16it/s] 51%|█████▏    | 143/279 [00:34<00:32,  4.16it/s] 52%|█████▏    | 144/279 [00:35<00:32,  4.16it/s] 52%|█████▏    | 145/279 [00:35<00:32,  4.16it/s] 52%|█████▏    | 146/279 [00:35<00:32,  4.15it/s] 53%|█████▎    | 147/279 [00:35<00:31,  4.15it/s] 53%|█████▎    | 148/279 [00:36<00:31,  4.15it/s] 53%|█████▎    | 149/279 [00:36<00:31,  4.14it/s] 54%|█████▍    | 150/279 [00:36<00:31,  4.13it/s] 54%|█████▍    | 151/279 [00:36<00:30,  4.13it/s] 54%|█████▍    | 152/279 [00:37<00:30,  4.14it/s] 55%|█████▍    | 153/279 [00:37<00:30,  4.14it/s] 55%|█████▌    | 154/279 [00:37<00:30,  4.15it/s] 56%|█████▌    | 155/279 [00:37<00:29,  4.15it/s] 56%|█████▌    | 156/279 [00:38<00:29,  4.15it/s] 56%|█████▋    | 157/279 [00:38<00:29,  4.16it/s] 57%|█████▋    | 158/279 [00:38<00:29,  4.15it/s] 57%|█████▋    | 159/279 [00:38<00:28,  4.15it/s] 57%|█████▋    | 160/279 [00:39<00:28,  4.15it/s] 58%|█████▊    | 161/279 [00:39<00:28,  4.15it/s] 58%|█████▊    | 162/279 [00:39<00:28,  4.15it/s] 58%|█████▊    | 163/279 [00:39<00:27,  4.14it/s] 59%|█████▉    | 164/279 [00:40<00:27,  4.14it/s] 59%|█████▉    | 165/279 [00:40<00:27,  4.13it/s] 59%|█████▉    | 166/279 [00:40<00:27,  4.13it/s] 60%|█████▉    | 167/279 [00:40<00:27,  4.13it/s] 60%|██████    | 168/279 [00:40<00:26,  4.14it/s] 61%|██████    | 169/279 [00:41<00:26,  4.14it/s] 61%|██████    | 170/279 [00:41<00:26,  4.15it/s] 61%|██████▏   | 171/279 [00:41<00:26,  4.15it/s] 62%|██████▏   | 172/279 [00:41<00:25,  4.15it/s] 62%|██████▏   | 173/279 [00:42<00:25,  4.15it/s] 62%|██████▏   | 174/279 [00:42<00:25,  4.15it/s] 63%|██████▎   | 175/279 [00:42<00:25,  4.15it/s] 63%|██████▎   | 176/279 [00:42<00:24,  4.15it/s] 63%|██████▎   | 177/279 [00:43<00:24,  4.15it/s] 64%|██████▍   | 178/279 [00:43<00:24,  4.14it/s] 64%|██████▍   | 179/279 [00:43<00:24,  4.14it/s] 65%|██████▍   | 180/279 [00:43<00:23,  4.13it/s] 65%|██████▍   | 181/279 [00:44<00:23,  4.14it/s] 65%|██████▌   | 182/279 [00:44<00:23,  4.14it/s] 66%|██████▌   | 183/279 [00:44<00:23,  4.15it/s] 66%|██████▌   | 184/279 [00:44<00:22,  4.15it/s] 66%|██████▋   | 185/279 [00:45<00:22,  4.15it/s] 67%|██████▋   | 186/279 [00:45<00:22,  4.15it/s] 67%|██████▋   | 187/279 [00:45<00:22,  4.16it/s] 67%|██████▋   | 188/279 [00:45<00:21,  4.16it/s] 68%|██████▊   | 189/279 [00:46<00:21,  4.16it/s] 68%|██████▊   | 190/279 [00:46<00:21,  4.16it/s] 68%|██████▊   | 191/279 [00:46<00:21,  4.16it/s] 69%|██████▉   | 192/279 [00:46<00:20,  4.16it/s] 69%|██████▉   | 193/279 [00:47<00:20,  4.16it/s] 70%|██████▉   | 194/279 [00:47<00:20,  4.16it/s] 70%|██████▉   | 195/279 [00:47<00:20,  4.16it/s] 70%|███████   | 196/279 [00:47<00:19,  4.16it/s] 71%|███████   | 197/279 [00:47<00:19,  4.15it/s] 71%|███████   | 198/279 [00:48<00:19,  4.15it/s] 71%|███████▏  | 199/279 [00:48<00:19,  4.15it/s] 72%|███████▏  | 200/279 [00:48<00:19,  4.14it/s] 72%|███████▏  | 201/279 [00:48<00:18,  4.13it/s] 72%|███████▏  | 202/279 [00:49<00:18,  4.13it/s] 73%|███████▎  | 203/279 [00:49<00:18,  4.13it/s] 73%|███████▎  | 204/279 [00:49<00:18,  4.14it/s] 73%|███████▎  | 205/279 [00:49<00:17,  4.15it/s] 74%|███████▍  | 206/279 [00:50<00:17,  4.15it/s] 74%|███████▍  | 207/279 [00:50<00:17,  4.15it/s] 75%|███████▍  | 208/279 [00:50<00:17,  4.15it/s] 75%|███████▍  | 209/279 [00:50<00:16,  4.15it/s] 75%|███████▌  | 210/279 [00:51<00:16,  4.14it/s] 76%|███████▌  | 211/279 [00:51<00:16,  4.14it/s] 76%|███████▌  | 212/279 [00:51<00:16,  4.14it/s] 76%|███████▋  | 213/279 [00:51<00:15,  4.13it/s] 77%|███████▋  | 214/279 [00:52<00:15,  4.13it/s] 77%|███████▋  | 215/279 [00:52<00:15,  4.13it/s] 77%|███████▋  | 216/279 [00:52<00:15,  4.13it/s] 78%|███████▊  | 217/279 [00:52<00:14,  4.14it/s] 78%|███████▊  | 218/279 [00:53<00:14,  4.14it/s] 78%|███████▊  | 219/279 [00:53<00:14,  4.14it/s] 79%|███████▉  | 220/279 [00:53<00:14,  4.14it/s] 79%|███████▉  | 221/279 [00:53<00:14,  4.13it/s] 80%|███████▉  | 222/279 [00:54<00:13,  4.13it/s] 80%|███████▉  | 223/279 [00:54<00:13,  4.12it/s] 80%|████████  | 224/279 [00:54<00:13,  4.12it/s] 81%|████████  | 225/279 [00:54<00:13,  4.13it/s] 81%|████████  | 226/279 [00:54<00:12,  4.13it/s] 81%|████████▏ | 227/279 [00:55<00:12,  4.14it/s] 82%|████████▏ | 228/279 [00:55<00:12,  4.14it/s] 82%|████████▏ | 229/279 [00:55<00:12,  4.14it/s] 82%|████████▏ | 230/279 [00:55<00:11,  4.15it/s] 83%|████████▎ | 231/279 [00:56<00:11,  4.15it/s] 83%|████████▎ | 232/279 [00:56<00:11,  4.15it/s] 84%|████████▎ | 233/279 [00:56<00:11,  4.14it/s] 84%|████████▍ | 234/279 [00:56<00:10,  4.13it/s] 84%|████████▍ | 235/279 [00:57<00:10,  4.13it/s] 85%|████████▍ | 236/279 [00:57<00:10,  4.13it/s] 85%|████████▍ | 237/279 [00:57<00:10,  4.14it/s] 85%|████████▌ | 238/279 [00:57<00:09,  4.14it/s] 86%|████████▌ | 239/279 [00:58<00:09,  4.14it/s] 86%|████████▌ | 240/279 [00:58<00:09,  4.14it/s] 86%|████████▋ | 241/279 [00:58<00:09,  4.14it/s] 87%|████████▋ | 242/279 [00:58<00:08,  4.14it/s] 87%|████████▋ | 243/279 [00:59<00:08,  4.13it/s] 87%|████████▋ | 244/279 [00:59<00:08,  4.13it/s] 88%|████████▊ | 245/279 [00:59<00:08,  4.12it/s] 88%|████████▊ | 246/279 [00:59<00:07,  4.13it/s] 89%|████████▊ | 247/279 [01:00<00:07,  4.13it/s] 89%|████████▉ | 248/279 [01:00<00:07,  4.13it/s] 89%|████████▉ | 249/279 [01:00<00:07,  4.14it/s] 90%|████████▉ | 250/279 [01:00<00:07,  4.14it/s] 90%|████████▉ | 251/279 [01:01<00:06,  4.14it/s] 90%|█████████ | 252/279 [01:01<00:06,  4.14it/s] 91%|█████████ | 253/279 [01:01<00:06,  4.13it/s] 91%|█████████ | 254/279 [01:01<00:06,  4.13it/s] 91%|█████████▏| 255/279 [01:02<00:05,  4.12it/s] 92%|█████████▏| 256/279 [01:02<00:05,  4.13it/s] 92%|█████████▏| 257/279 [01:02<00:05,  4.13it/s] 92%|█████████▏| 258/279 [01:02<00:05,  4.14it/s] 93%|█████████▎| 259/279 [01:02<00:04,  4.14it/s] 93%|█████████▎| 260/279 [01:03<00:04,  4.14it/s] 94%|█████████▎| 261/279 [01:03<00:04,  4.13it/s] 94%|█████████▍| 262/279 [01:03<00:04,  4.13it/s] 94%|█████████▍| 263/279 [01:03<00:03,  4.12it/s] 95%|█████████▍| 264/279 [01:04<00:03,  4.12it/s] 95%|█████████▍| 265/279 [01:04<00:03,  4.12it/s] 95%|█████████▌| 266/279 [01:04<00:03,  4.13it/s] 96%|█████████▌| 267/279 [01:04<00:02,  4.13it/s] 96%|█████████▌| 268/279 [01:05<00:02,  4.12it/s] 96%|█████████▋| 269/279 [01:05<00:02,  4.11it/s] 97%|█████████▋| 270/279 [01:05<00:02,  4.11it/s] 97%|█████████▋| 271/279 [01:05<00:01,  4.11it/s] 97%|█████████▋| 272/279 [01:06<00:01,  4.12it/s] 98%|█████████▊| 273/279 [01:06<00:01,  4.12it/s] 98%|█████████▊| 274/279 [01:06<00:01,  4.13it/s] 99%|█████████▊| 275/279 [01:06<00:00,  4.12it/s] 99%|█████████▉| 276/279 [01:07<00:00,  4.12it/s] 99%|█████████▉| 277/279 [01:07<00:00,  4.11it/s]100%|█████████▉| 278/279 [01:07<00:00,  4.12it/s]100%|██████████| 279/279 [01:07<00:00,  4.12it/s]accuracy:  0.6272401433691757
100%|██████████| 279/279 [01:11<00:00,  3.88it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 241
Training dataset size: 384, validation dataset size: 241
Training dataset size: 384, validation dataset size: 241
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.24s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 14:00:39,520] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 14:00:39,531] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 14:00:41,034] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:31,  3.35s/it]  3%|▎         | 2/64 [00:06<03:02,  2.94s/it]  5%|▍         | 3/64 [00:08<02:47,  2.75s/it]  6%|▋         | 4/64 [00:10<02:34,  2.58s/it]  8%|▊         | 5/64 [00:12<02:18,  2.35s/it]  9%|▉         | 6/64 [00:15<02:17,  2.37s/it] 11%|█         | 7/64 [00:17<02:15,  2.38s/it] 12%|█▎        | 8/64 [00:19<02:12,  2.36s/it] 14%|█▍        | 9/64 [00:21<02:03,  2.25s/it] 16%|█▌        | 10/64 [00:24<02:08,  2.39s/it]                                               {'loss': 0.421, 'grad_norm': 3.795212984085083, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:24<02:08,  2.39s/it] 17%|█▋        | 11/64 [00:26<02:05,  2.37s/it] 19%|█▉        | 12/64 [00:29<02:01,  2.33s/it] 20%|██        | 13/64 [00:31<02:05,  2.46s/it] 22%|██▏       | 14/64 [00:34<01:56,  2.34s/it] 23%|██▎       | 15/64 [00:36<01:56,  2.37s/it] 25%|██▌       | 16/64 [00:38<01:43,  2.16s/it] 27%|██▋       | 17/64 [00:40<01:41,  2.15s/it] 28%|██▊       | 18/64 [00:42<01:38,  2.15s/it] 30%|██▉       | 19/64 [00:44<01:34,  2.09s/it] 31%|███▏      | 20/64 [00:46<01:26,  1.98s/it]                                               {'loss': 0.2731, 'grad_norm': 7.136384963989258, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:46<01:26,  1.98s/it] 33%|███▎      | 21/64 [00:48<01:26,  2.02s/it] 34%|███▍      | 22/64 [00:49<01:21,  1.95s/it] 36%|███▌      | 23/64 [00:52<01:22,  2.00s/it] 38%|███▊      | 24/64 [00:54<01:22,  2.07s/it] 39%|███▉      | 25/64 [00:56<01:21,  2.10s/it] 41%|████      | 26/64 [00:59<01:28,  2.34s/it] 42%|████▏     | 27/64 [01:01<01:22,  2.23s/it] 44%|████▍     | 28/64 [01:03<01:16,  2.13s/it] 45%|████▌     | 29/64 [01:05<01:20,  2.29s/it] 47%|████▋     | 30/64 [01:07<01:13,  2.17s/it]                                               {'loss': 0.2352, 'grad_norm': 7.447174549102783, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:07<01:13,  2.17s/it] 48%|████▊     | 31/64 [01:09<01:11,  2.17s/it] 50%|█████     | 32/64 [01:12<01:11,  2.23s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:16<01:22,  2.68s/it] 53%|█████▎    | 34/64 [01:18<01:16,  2.55s/it] 55%|█████▍    | 35/64 [01:20<01:11,  2.47s/it] 56%|█████▋    | 36/64 [01:23<01:11,  2.57s/it] 58%|█████▊    | 37/64 [01:25<01:06,  2.45s/it] 59%|█████▉    | 38/64 [01:28<01:04,  2.47s/it] 61%|██████    | 39/64 [01:30<01:01,  2.44s/it] 62%|██████▎   | 40/64 [01:32<00:54,  2.29s/it]                                               {'loss': 0.2198, 'grad_norm': 2.4175734519958496, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:32<00:54,  2.29s/it] 64%|██████▍   | 41/64 [01:34<00:51,  2.24s/it] 66%|██████▌   | 42/64 [01:36<00:49,  2.24s/it] 67%|██████▋   | 43/64 [01:38<00:43,  2.09s/it] 69%|██████▉   | 44/64 [01:40<00:41,  2.05s/it] 70%|███████   | 45/64 [01:42<00:39,  2.08s/it] 72%|███████▏  | 46/64 [01:45<00:41,  2.30s/it] 73%|███████▎  | 47/64 [01:47<00:38,  2.27s/it] 75%|███████▌  | 48/64 [01:49<00:34,  2.14s/it] 77%|███████▋  | 49/64 [01:51<00:33,  2.25s/it] 78%|███████▊  | 50/64 [01:54<00:31,  2.21s/it]                                               {'loss': 0.3515, 'grad_norm': 8.1664457321167, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:54<00:31,  2.21s/it] 80%|███████▉  | 51/64 [01:55<00:26,  2.08s/it] 81%|████████▏ | 52/64 [01:57<00:23,  1.97s/it] 83%|████████▎ | 53/64 [01:59<00:21,  1.96s/it] 84%|████████▍ | 54/64 [02:02<00:21,  2.12s/it] 86%|████████▌ | 55/64 [02:03<00:18,  2.06s/it] 88%|████████▊ | 56/64 [02:06<00:16,  2.08s/it] 89%|████████▉ | 57/64 [02:08<00:15,  2.16s/it] 91%|█████████ | 58/64 [02:10<00:12,  2.08s/it] 92%|█████████▏| 59/64 [02:12<00:10,  2.09s/it] 94%|█████████▍| 60/64 [02:14<00:08,  2.05s/it]                                               {'loss': 0.2449, 'grad_norm': 6.951296806335449, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:14<00:08,  2.05s/it] 95%|█████████▌| 61/64 [02:16<00:06,  2.07s/it] 97%|█████████▋| 62/64 [02:18<00:04,  2.11s/it] 98%|█████████▊| 63/64 [02:20<00:02,  2.00s/it]100%|██████████| 64/64 [02:22<00:00,  1.96s/it]                                               {'train_runtime': 143.2106, 'train_samples_per_second': 5.363, 'train_steps_per_second': 0.447, 'train_loss': 0.27958265226334333, 'epoch': 2.0}
100%|██████████| 64/64 [02:23<00:00,  1.96s/it]100%|██████████| 64/64 [02:23<00:00,  2.23s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.29s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  272
  0%|          | 0/272 [00:00<?, ?it/s]  0%|          | 1/272 [00:00<01:24,  3.21it/s]  1%|          | 2/272 [00:00<01:12,  3.72it/s]  1%|          | 3/272 [00:00<01:08,  3.91it/s]  1%|▏         | 4/272 [00:01<01:06,  4.00it/s]  2%|▏         | 5/272 [00:01<01:05,  4.06it/s]  2%|▏         | 6/272 [00:01<01:04,  4.10it/s]  3%|▎         | 7/272 [00:01<01:04,  4.12it/s]  3%|▎         | 8/272 [00:01<01:03,  4.13it/s]  3%|▎         | 9/272 [00:02<01:03,  4.14it/s]  4%|▎         | 10/272 [00:02<01:03,  4.15it/s]  4%|▍         | 11/272 [00:02<01:02,  4.15it/s]  4%|▍         | 12/272 [00:02<01:02,  4.16it/s]  5%|▍         | 13/272 [00:03<01:02,  4.16it/s]  5%|▌         | 14/272 [00:03<01:01,  4.16it/s]  6%|▌         | 15/272 [00:03<01:01,  4.17it/s]  6%|▌         | 16/272 [00:03<01:01,  4.17it/s]  6%|▋         | 17/272 [00:04<01:01,  4.17it/s]  7%|▋         | 18/272 [00:04<01:00,  4.17it/s]  7%|▋         | 19/272 [00:04<01:00,  4.17it/s]  7%|▋         | 20/272 [00:04<01:00,  4.17it/s]  8%|▊         | 21/272 [00:05<01:00,  4.17it/s]  8%|▊         | 22/272 [00:05<00:59,  4.17it/s]  8%|▊         | 23/272 [00:05<00:59,  4.17it/s]  9%|▉         | 24/272 [00:05<00:59,  4.17it/s]  9%|▉         | 25/272 [00:06<00:59,  4.17it/s] 10%|▉         | 26/272 [00:06<00:59,  4.17it/s] 10%|▉         | 27/272 [00:06<00:58,  4.17it/s] 10%|█         | 28/272 [00:06<00:58,  4.17it/s] 11%|█         | 29/272 [00:07<00:58,  4.17it/s] 11%|█         | 30/272 [00:07<00:58,  4.17it/s] 11%|█▏        | 31/272 [00:07<00:57,  4.17it/s] 12%|█▏        | 32/272 [00:07<00:57,  4.17it/s] 12%|█▏        | 33/272 [00:07<00:57,  4.17it/s] 12%|█▎        | 34/272 [00:08<00:57,  4.17it/s] 13%|█▎        | 35/272 [00:08<00:56,  4.17it/s] 13%|█▎        | 36/272 [00:08<00:56,  4.17it/s] 14%|█▎        | 37/272 [00:08<00:56,  4.17it/s] 14%|█▍        | 38/272 [00:09<00:56,  4.17it/s] 14%|█▍        | 39/272 [00:09<00:55,  4.17it/s] 15%|█▍        | 40/272 [00:09<00:55,  4.17it/s] 15%|█▌        | 41/272 [00:09<00:55,  4.17it/s] 15%|█▌        | 42/272 [00:10<00:55,  4.16it/s] 16%|█▌        | 43/272 [00:10<00:54,  4.17it/s] 16%|█▌        | 44/272 [00:10<00:54,  4.17it/s] 17%|█▋        | 45/272 [00:10<00:54,  4.17it/s] 17%|█▋        | 46/272 [00:11<00:54,  4.17it/s] 17%|█▋        | 47/272 [00:11<00:54,  4.16it/s] 18%|█▊        | 48/272 [00:11<00:53,  4.16it/s] 18%|█▊        | 49/272 [00:11<00:53,  4.17it/s] 18%|█▊        | 50/272 [00:12<00:53,  4.17it/s] 19%|█▉        | 51/272 [00:12<00:53,  4.17it/s] 19%|█▉        | 52/272 [00:12<00:52,  4.17it/s] 19%|█▉        | 53/272 [00:12<00:52,  4.17it/s] 20%|█▉        | 54/272 [00:13<00:52,  4.17it/s] 20%|██        | 55/272 [00:13<00:52,  4.17it/s] 21%|██        | 56/272 [00:13<00:51,  4.17it/s] 21%|██        | 57/272 [00:13<00:51,  4.17it/s] 21%|██▏       | 58/272 [00:13<00:51,  4.17it/s] 22%|██▏       | 59/272 [00:14<00:51,  4.17it/s] 22%|██▏       | 60/272 [00:14<00:50,  4.17it/s] 22%|██▏       | 61/272 [00:14<00:50,  4.16it/s] 23%|██▎       | 62/272 [00:14<00:50,  4.17it/s] 23%|██▎       | 63/272 [00:15<00:50,  4.17it/s] 24%|██▎       | 64/272 [00:15<00:49,  4.17it/s] 24%|██▍       | 65/272 [00:15<00:49,  4.17it/s] 24%|██▍       | 66/272 [00:15<00:49,  4.17it/s] 25%|██▍       | 67/272 [00:16<00:49,  4.17it/s] 25%|██▌       | 68/272 [00:16<00:48,  4.17it/s] 25%|██▌       | 69/272 [00:16<00:48,  4.17it/s] 26%|██▌       | 70/272 [00:16<00:48,  4.17it/s] 26%|██▌       | 71/272 [00:17<00:48,  4.17it/s] 26%|██▋       | 72/272 [00:17<00:47,  4.17it/s] 27%|██▋       | 73/272 [00:17<00:47,  4.17it/s] 27%|██▋       | 74/272 [00:17<00:47,  4.17it/s] 28%|██▊       | 75/272 [00:18<00:47,  4.17it/s] 28%|██▊       | 76/272 [00:18<00:47,  4.17it/s] 28%|██▊       | 77/272 [00:18<00:46,  4.17it/s] 29%|██▊       | 78/272 [00:18<00:46,  4.17it/s] 29%|██▉       | 79/272 [00:19<00:46,  4.17it/s] 29%|██▉       | 80/272 [00:19<00:46,  4.17it/s] 30%|██▉       | 81/272 [00:19<00:45,  4.17it/s] 30%|███       | 82/272 [00:19<00:45,  4.17it/s] 31%|███       | 83/272 [00:19<00:45,  4.16it/s] 31%|███       | 84/272 [00:20<00:45,  4.16it/s] 31%|███▏      | 85/272 [00:20<00:44,  4.16it/s] 32%|███▏      | 86/272 [00:20<00:44,  4.16it/s] 32%|███▏      | 87/272 [00:20<00:44,  4.16it/s] 32%|███▏      | 88/272 [00:21<00:44,  4.17it/s] 33%|███▎      | 89/272 [00:21<00:43,  4.17it/s] 33%|███▎      | 90/272 [00:21<00:43,  4.17it/s] 33%|███▎      | 91/272 [00:21<00:43,  4.17it/s] 34%|███▍      | 92/272 [00:22<00:43,  4.17it/s] 34%|███▍      | 93/272 [00:22<00:42,  4.17it/s] 35%|███▍      | 94/272 [00:22<00:42,  4.16it/s] 35%|███▍      | 95/272 [00:22<00:42,  4.16it/s] 35%|███▌      | 96/272 [00:23<00:42,  4.16it/s] 36%|███▌      | 97/272 [00:23<00:42,  4.16it/s] 36%|███▌      | 98/272 [00:23<00:41,  4.16it/s] 36%|███▋      | 99/272 [00:23<00:41,  4.16it/s] 37%|███▋      | 100/272 [00:24<00:41,  4.16it/s] 37%|███▋      | 101/272 [00:24<00:41,  4.16it/s] 38%|███▊      | 102/272 [00:24<00:40,  4.16it/s] 38%|███▊      | 103/272 [00:24<00:40,  4.16it/s] 38%|███▊      | 104/272 [00:25<00:40,  4.16it/s] 39%|███▊      | 105/272 [00:25<00:40,  4.15it/s] 39%|███▉      | 106/272 [00:25<00:40,  4.15it/s] 39%|███▉      | 107/272 [00:25<00:39,  4.14it/s] 40%|███▉      | 108/272 [00:26<00:39,  4.13it/s] 40%|████      | 109/272 [00:26<00:39,  4.13it/s] 40%|████      | 110/272 [00:26<00:39,  4.13it/s] 41%|████      | 111/272 [00:26<00:38,  4.13it/s] 41%|████      | 112/272 [00:26<00:38,  4.14it/s] 42%|████▏     | 113/272 [00:27<00:38,  4.15it/s] 42%|████▏     | 114/272 [00:27<00:38,  4.15it/s] 42%|████▏     | 115/272 [00:27<00:37,  4.15it/s] 43%|████▎     | 116/272 [00:27<00:37,  4.16it/s] 43%|████▎     | 117/272 [00:28<00:37,  4.16it/s] 43%|████▎     | 118/272 [00:28<00:37,  4.16it/s] 44%|████▍     | 119/272 [00:28<00:36,  4.16it/s] 44%|████▍     | 120/272 [00:28<00:36,  4.16it/s] 44%|████▍     | 121/272 [00:29<00:36,  4.16it/s] 45%|████▍     | 122/272 [00:29<00:36,  4.15it/s] 45%|████▌     | 123/272 [00:29<00:35,  4.14it/s] 46%|████▌     | 124/272 [00:29<00:35,  4.13it/s] 46%|████▌     | 125/272 [00:30<00:35,  4.13it/s] 46%|████▋     | 126/272 [00:30<00:35,  4.13it/s] 47%|████▋     | 127/272 [00:30<00:35,  4.13it/s] 47%|████▋     | 128/272 [00:30<00:34,  4.14it/s] 47%|████▋     | 129/272 [00:31<00:34,  4.14it/s] 48%|████▊     | 130/272 [00:31<00:34,  4.15it/s] 48%|████▊     | 131/272 [00:31<00:33,  4.15it/s] 49%|████▊     | 132/272 [00:31<00:33,  4.15it/s] 49%|████▉     | 133/272 [00:32<00:33,  4.15it/s] 49%|████▉     | 134/272 [00:32<00:33,  4.15it/s] 50%|████▉     | 135/272 [00:32<00:33,  4.15it/s] 50%|█████     | 136/272 [00:32<00:32,  4.14it/s] 50%|█████     | 137/272 [00:32<00:32,  4.13it/s] 51%|█████     | 138/272 [00:33<00:32,  4.12it/s] 51%|█████     | 139/272 [00:33<00:32,  4.12it/s] 51%|█████▏    | 140/272 [00:33<00:31,  4.13it/s] 52%|█████▏    | 141/272 [00:33<00:31,  4.14it/s] 52%|█████▏    | 142/272 [00:34<00:31,  4.14it/s] 53%|█████▎    | 143/272 [00:34<00:31,  4.14it/s] 53%|█████▎    | 144/272 [00:34<00:30,  4.14it/s] 53%|█████▎    | 145/272 [00:34<00:30,  4.13it/s] 54%|█████▎    | 146/272 [00:35<00:30,  4.12it/s] 54%|█████▍    | 147/272 [00:35<00:30,  4.11it/s] 54%|█████▍    | 148/272 [00:35<00:30,  4.12it/s] 55%|█████▍    | 149/272 [00:35<00:29,  4.13it/s] 55%|█████▌    | 150/272 [00:36<00:29,  4.13it/s] 56%|█████▌    | 151/272 [00:36<00:29,  4.14it/s] 56%|█████▌    | 152/272 [00:36<00:28,  4.14it/s] 56%|█████▋    | 153/272 [00:36<00:28,  4.14it/s] 57%|█████▋    | 154/272 [00:37<00:28,  4.13it/s] 57%|█████▋    | 155/272 [00:37<00:28,  4.12it/s] 57%|█████▋    | 156/272 [00:37<00:28,  4.12it/s] 58%|█████▊    | 157/272 [00:37<00:27,  4.12it/s] 58%|█████▊    | 158/272 [00:38<00:27,  4.12it/s] 58%|█████▊    | 159/272 [00:38<00:27,  4.13it/s] 59%|█████▉    | 160/272 [00:38<00:27,  4.13it/s] 59%|█████▉    | 161/272 [00:38<00:26,  4.13it/s] 60%|█████▉    | 162/272 [00:39<00:26,  4.12it/s] 60%|█████▉    | 163/272 [00:39<00:26,  4.11it/s] 60%|██████    | 164/272 [00:39<00:26,  4.12it/s] 61%|██████    | 165/272 [00:39<00:25,  4.12it/s] 61%|██████    | 166/272 [00:40<00:25,  4.13it/s] 61%|██████▏   | 167/272 [00:40<00:25,  4.13it/s] 62%|██████▏   | 168/272 [00:40<00:25,  4.14it/s] 62%|██████▏   | 169/272 [00:40<00:24,  4.14it/s] 62%|██████▎   | 170/272 [00:40<00:24,  4.13it/s] 63%|██████▎   | 171/272 [00:41<00:24,  4.12it/s] 63%|██████▎   | 172/272 [00:41<00:24,  4.11it/s] 64%|██████▎   | 173/272 [00:41<00:24,  4.12it/s] 64%|██████▍   | 174/272 [00:41<00:23,  4.12it/s] 64%|██████▍   | 175/272 [00:42<00:23,  4.13it/s] 65%|██████▍   | 176/272 [00:42<00:23,  4.13it/s] 65%|██████▌   | 177/272 [00:42<00:23,  4.13it/s] 65%|██████▌   | 178/272 [00:42<00:22,  4.12it/s] 66%|██████▌   | 179/272 [00:43<00:22,  4.11it/s] 66%|██████▌   | 180/272 [00:43<00:22,  4.11it/s] 67%|██████▋   | 181/272 [00:43<00:22,  4.12it/s] 67%|██████▋   | 182/272 [00:43<00:21,  4.12it/s] 67%|██████▋   | 183/272 [00:44<00:21,  4.12it/s] 68%|██████▊   | 184/272 [00:44<00:21,  4.12it/s] 68%|██████▊   | 185/272 [00:44<00:21,  4.11it/s] 68%|██████▊   | 186/272 [00:44<00:20,  4.11it/s] 69%|██████▉   | 187/272 [00:45<00:20,  4.12it/s] 69%|██████▉   | 188/272 [00:45<00:20,  4.13it/s] 69%|██████▉   | 189/272 [00:45<00:20,  4.13it/s] 70%|██████▉   | 190/272 [00:45<00:19,  4.13it/s] 70%|███████   | 191/272 [00:46<00:19,  4.12it/s] 71%|███████   | 192/272 [00:46<00:19,  4.11it/s] 71%|███████   | 193/272 [00:46<00:19,  4.11it/s] 71%|███████▏  | 194/272 [00:46<00:18,  4.11it/s] 72%|███████▏  | 195/272 [00:47<00:18,  4.12it/s] 72%|███████▏  | 196/272 [00:47<00:18,  4.12it/s] 72%|███████▏  | 197/272 [00:47<00:18,  4.11it/s] 73%|███████▎  | 198/272 [00:47<00:18,  4.11it/s] 73%|███████▎  | 199/272 [00:48<00:17,  4.11it/s] 74%|███████▎  | 200/272 [00:48<00:17,  4.11it/s] 74%|███████▍  | 201/272 [00:48<00:17,  4.11it/s] 74%|███████▍  | 202/272 [00:48<00:17,  4.11it/s] 75%|███████▍  | 203/272 [00:49<00:16,  4.11it/s] 75%|███████▌  | 204/272 [00:49<00:16,  4.10it/s] 75%|███████▌  | 205/272 [00:49<00:16,  4.10it/s] 76%|███████▌  | 206/272 [00:49<00:16,  4.11it/s] 76%|███████▌  | 207/272 [00:49<00:15,  4.12it/s] 76%|███████▋  | 208/272 [00:50<00:15,  4.12it/s] 77%|███████▋  | 209/272 [00:50<00:15,  4.12it/s] 77%|███████▋  | 210/272 [00:50<00:15,  4.11it/s] 78%|███████▊  | 211/272 [00:50<00:14,  4.11it/s] 78%|███████▊  | 212/272 [00:51<00:14,  4.11it/s] 78%|███████▊  | 213/272 [00:51<00:14,  4.12it/s] 79%|███████▊  | 214/272 [00:51<00:14,  4.12it/s] 79%|███████▉  | 215/272 [00:51<00:13,  4.12it/s] 79%|███████▉  | 216/272 [00:52<00:13,  4.11it/s] 80%|███████▉  | 217/272 [00:52<00:13,  4.10it/s] 80%|████████  | 218/272 [00:52<00:13,  4.10it/s] 81%|████████  | 219/272 [00:52<00:12,  4.11it/s] 81%|████████  | 220/272 [00:53<00:12,  4.12it/s] 81%|████████▏ | 221/272 [00:53<00:12,  4.12it/s] 82%|████████▏ | 222/272 [00:53<00:12,  4.11it/s] 82%|████████▏ | 223/272 [00:53<00:11,  4.10it/s] 82%|████████▏ | 224/272 [00:54<00:11,  4.10it/s] 83%|████████▎ | 225/272 [00:54<00:11,  4.11it/s] 83%|████████▎ | 226/272 [00:54<00:11,  4.12it/s] 83%|████████▎ | 227/272 [00:54<00:10,  4.11it/s] 84%|████████▍ | 228/272 [00:55<00:10,  4.10it/s] 84%|████████▍ | 229/272 [00:55<00:10,  4.10it/s] 85%|████████▍ | 230/272 [00:55<00:10,  4.11it/s] 85%|████████▍ | 231/272 [00:55<00:09,  4.11it/s] 85%|████████▌ | 232/272 [00:56<00:09,  4.11it/s] 86%|████████▌ | 233/272 [00:56<00:09,  4.11it/s] 86%|████████▌ | 234/272 [00:56<00:09,  4.10it/s] 86%|████████▋ | 235/272 [00:56<00:09,  4.10it/s] 87%|████████▋ | 236/272 [00:57<00:08,  4.11it/s] 87%|████████▋ | 237/272 [00:57<00:08,  4.11it/s] 88%|████████▊ | 238/272 [00:57<00:08,  4.11it/s] 88%|████████▊ | 239/272 [00:57<00:08,  4.10it/s] 88%|████████▊ | 240/272 [00:58<00:07,  4.09it/s] 89%|████████▊ | 241/272 [00:58<00:07,  4.10it/s] 89%|████████▉ | 242/272 [00:58<00:07,  4.11it/s] 89%|████████▉ | 243/272 [00:58<00:07,  4.11it/s] 90%|████████▉ | 244/272 [00:58<00:06,  4.11it/s] 90%|█████████ | 245/272 [00:59<00:06,  4.10it/s] 90%|█████████ | 246/272 [00:59<00:06,  4.11it/s] 91%|█████████ | 247/272 [00:59<00:06,  4.12it/s] 91%|█████████ | 248/272 [00:59<00:05,  4.12it/s] 92%|█████████▏| 249/272 [01:00<00:05,  4.13it/s] 92%|█████████▏| 250/272 [01:00<00:05,  4.12it/s] 92%|█████████▏| 251/272 [01:00<00:05,  4.10it/s] 93%|█████████▎| 252/272 [01:00<00:04,  4.10it/s] 93%|█████████▎| 253/272 [01:01<00:04,  4.10it/s] 93%|█████████▎| 254/272 [01:01<00:04,  4.11it/s] 94%|█████████▍| 255/272 [01:01<00:04,  4.11it/s] 94%|█████████▍| 256/272 [01:01<00:03,  4.10it/s] 94%|█████████▍| 257/272 [01:02<00:03,  4.10it/s] 95%|█████████▍| 258/272 [01:02<00:03,  4.11it/s] 95%|█████████▌| 259/272 [01:02<00:03,  4.11it/s] 96%|█████████▌| 260/272 [01:02<00:02,  4.11it/s] 96%|█████████▌| 261/272 [01:03<00:02,  4.11it/s] 96%|█████████▋| 262/272 [01:03<00:02,  4.09it/s] 97%|█████████▋| 263/272 [01:03<00:02,  4.10it/s] 97%|█████████▋| 264/272 [01:03<00:01,  4.11it/s] 97%|█████████▋| 265/272 [01:04<00:01,  4.11it/s] 98%|█████████▊| 266/272 [01:04<00:01,  4.11it/s] 98%|█████████▊| 267/272 [01:04<00:01,  4.10it/s] 99%|█████████▊| 268/272 [01:04<00:00,  4.10it/s] 99%|█████████▉| 269/272 [01:05<00:00,  4.10it/s] 99%|█████████▉| 270/272 [01:05<00:00,  4.11it/s]100%|█████████▉| 271/272 [01:05<00:00,  4.11it/s]100%|██████████| 272/272 [01:05<00:00,  4.11it/s]accuracy:  0.9007352941176471
100%|██████████| 272/272 [01:09<00:00,  3.90it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 149
Training dataset size: 384, validation dataset size: 149
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 14:04:48,416] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 14:04:49,313] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-03-12 14:04:49,419] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:19,  3.16s/it]  3%|▎         | 2/64 [00:05<03:02,  2.95s/it]  5%|▍         | 3/64 [00:08<02:45,  2.72s/it]  6%|▋         | 4/64 [00:10<02:32,  2.53s/it]  8%|▊         | 5/64 [00:12<02:20,  2.38s/it]  9%|▉         | 6/64 [00:14<02:15,  2.33s/it] 11%|█         | 7/64 [00:17<02:22,  2.50s/it] 12%|█▎        | 8/64 [00:19<02:11,  2.35s/it] 14%|█▍        | 9/64 [00:22<02:08,  2.34s/it] 16%|█▌        | 10/64 [00:24<02:08,  2.38s/it]                                               {'loss': 0.8961, 'grad_norm': 9.180986404418945, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:24<02:08,  2.38s/it] 17%|█▋        | 11/64 [00:26<02:02,  2.32s/it] 19%|█▉        | 12/64 [00:28<01:56,  2.24s/it] 20%|██        | 13/64 [00:31<01:52,  2.20s/it] 22%|██▏       | 14/64 [00:33<01:47,  2.14s/it] 23%|██▎       | 15/64 [00:35<01:52,  2.30s/it] 25%|██▌       | 16/64 [00:37<01:48,  2.26s/it] 27%|██▋       | 17/64 [00:39<01:44,  2.22s/it] 28%|██▊       | 18/64 [00:42<01:49,  2.37s/it] 30%|██▉       | 19/64 [00:44<01:45,  2.35s/it] 31%|███▏      | 20/64 [00:47<01:44,  2.38s/it]                                               {'loss': 0.6851, 'grad_norm': 6.881891250610352, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:47<01:44,  2.38s/it] 33%|███▎      | 21/64 [00:49<01:34,  2.20s/it] 34%|███▍      | 22/64 [00:51<01:34,  2.26s/it] 36%|███▌      | 23/64 [00:53<01:29,  2.19s/it] 38%|███▊      | 24/64 [00:56<01:32,  2.32s/it] 39%|███▉      | 25/64 [00:58<01:32,  2.37s/it] 41%|████      | 26/64 [01:01<01:35,  2.52s/it] 42%|████▏     | 27/64 [01:03<01:25,  2.31s/it] 44%|████▍     | 28/64 [01:05<01:23,  2.33s/it] 45%|████▌     | 29/64 [01:08<01:22,  2.37s/it] 47%|████▋     | 30/64 [01:10<01:17,  2.27s/it]                                               {'loss': 0.4105, 'grad_norm': 3.8140716552734375, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:10<01:17,  2.27s/it] 48%|████▊     | 31/64 [01:12<01:12,  2.20s/it] 50%|█████     | 32/64 [01:14<01:08,  2.14s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:16<01:07,  2.17s/it] 53%|█████▎    | 34/64 [01:19<01:09,  2.31s/it] 55%|█████▍    | 35/64 [01:20<01:01,  2.13s/it] 56%|█████▋    | 36/64 [01:23<01:00,  2.15s/it] 58%|█████▊    | 37/64 [01:25<00:58,  2.18s/it] 59%|█████▉    | 38/64 [01:27<00:55,  2.12s/it] 61%|██████    | 39/64 [01:29<00:53,  2.16s/it] 62%|██████▎   | 40/64 [01:31<00:52,  2.20s/it]                                               {'loss': 0.4449, 'grad_norm': 6.730396270751953, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:31<00:52,  2.20s/it] 64%|██████▍   | 41/64 [01:34<00:53,  2.33s/it] 66%|██████▌   | 42/64 [01:36<00:51,  2.34s/it] 67%|██████▋   | 43/64 [01:39<00:47,  2.28s/it] 69%|██████▉   | 44/64 [01:41<00:46,  2.31s/it] 70%|███████   | 45/64 [01:43<00:42,  2.26s/it] 72%|███████▏  | 46/64 [01:45<00:40,  2.24s/it] 73%|███████▎  | 47/64 [01:48<00:39,  2.32s/it] 75%|███████▌  | 48/64 [01:50<00:34,  2.18s/it] 77%|███████▋  | 49/64 [01:51<00:30,  2.06s/it] 78%|███████▊  | 50/64 [01:54<00:30,  2.15s/it]                                               {'loss': 0.379, 'grad_norm': 3.980900764465332, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [01:54<00:30,  2.15s/it] 80%|███████▉  | 51/64 [01:56<00:28,  2.19s/it] 81%|████████▏ | 52/64 [01:58<00:26,  2.20s/it] 83%|████████▎ | 53/64 [02:00<00:24,  2.18s/it] 84%|████████▍ | 54/64 [02:03<00:21,  2.16s/it] 86%|████████▌ | 55/64 [02:05<00:19,  2.21s/it] 88%|████████▊ | 56/64 [02:07<00:17,  2.22s/it] 89%|████████▉ | 57/64 [02:09<00:15,  2.21s/it] 91%|█████████ | 58/64 [02:12<00:13,  2.23s/it] 92%|█████████▏| 59/64 [02:14<00:11,  2.32s/it] 94%|█████████▍| 60/64 [02:16<00:08,  2.16s/it]                                               {'loss': 0.4349, 'grad_norm': 4.049511432647705, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:16<00:08,  2.16s/it] 95%|█████████▌| 61/64 [02:18<00:06,  2.17s/it] 97%|█████████▋| 62/64 [02:20<00:04,  2.24s/it] 98%|█████████▊| 63/64 [02:23<00:02,  2.34s/it]100%|██████████| 64/64 [02:25<00:00,  2.25s/it]                                               {'train_runtime': 146.2171, 'train_samples_per_second': 5.252, 'train_steps_per_second': 0.438, 'train_loss': 0.549963116645813, 'epoch': 2.0}
100%|██████████| 64/64 [02:26<00:00,  2.25s/it]100%|██████████| 64/64 [02:26<00:00,  2.28s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  175
  0%|          | 0/175 [00:00<?, ?it/s]  1%|          | 1/175 [00:00<00:54,  3.22it/s]  1%|          | 2/175 [00:00<00:46,  3.72it/s]  2%|▏         | 3/175 [00:00<00:43,  3.92it/s]  2%|▏         | 4/175 [00:01<00:42,  4.01it/s]  3%|▎         | 5/175 [00:01<00:41,  4.06it/s]  3%|▎         | 6/175 [00:01<00:41,  4.10it/s]  4%|▍         | 7/175 [00:01<00:40,  4.12it/s]  5%|▍         | 8/175 [00:01<00:40,  4.13it/s]  5%|▌         | 9/175 [00:02<00:40,  4.14it/s]  6%|▌         | 10/175 [00:02<00:39,  4.15it/s]  6%|▋         | 11/175 [00:02<00:39,  4.15it/s]  7%|▋         | 12/175 [00:02<00:39,  4.16it/s]  7%|▋         | 13/175 [00:03<00:38,  4.16it/s]  8%|▊         | 14/175 [00:03<00:38,  4.15it/s]  9%|▊         | 15/175 [00:03<00:38,  4.15it/s]  9%|▉         | 16/175 [00:03<00:38,  4.15it/s] 10%|▉         | 17/175 [00:04<00:38,  4.16it/s] 10%|█         | 18/175 [00:04<00:37,  4.16it/s] 11%|█         | 19/175 [00:04<00:37,  4.16it/s] 11%|█▏        | 20/175 [00:04<00:37,  4.16it/s] 12%|█▏        | 21/175 [00:05<00:37,  4.16it/s] 13%|█▎        | 22/175 [00:05<00:36,  4.16it/s] 13%|█▎        | 23/175 [00:05<00:36,  4.17it/s] 14%|█▎        | 24/175 [00:05<00:36,  4.17it/s] 14%|█▍        | 25/175 [00:06<00:35,  4.17it/s] 15%|█▍        | 26/175 [00:06<00:35,  4.17it/s] 15%|█▌        | 27/175 [00:06<00:35,  4.17it/s] 16%|█▌        | 28/175 [00:06<00:35,  4.17it/s] 17%|█▋        | 29/175 [00:07<00:35,  4.16it/s] 17%|█▋        | 30/175 [00:07<00:34,  4.16it/s] 18%|█▊        | 31/175 [00:07<00:34,  4.16it/s] 18%|█▊        | 32/175 [00:07<00:34,  4.17it/s] 19%|█▉        | 33/175 [00:07<00:34,  4.17it/s] 19%|█▉        | 34/175 [00:08<00:33,  4.17it/s] 20%|██        | 35/175 [00:08<00:33,  4.17it/s] 21%|██        | 36/175 [00:08<00:33,  4.17it/s] 21%|██        | 37/175 [00:08<00:33,  4.16it/s] 22%|██▏       | 38/175 [00:09<00:32,  4.16it/s] 22%|██▏       | 39/175 [00:09<00:32,  4.16it/s] 23%|██▎       | 40/175 [00:09<00:32,  4.16it/s] 23%|██▎       | 41/175 [00:09<00:32,  4.17it/s] 24%|██▍       | 42/175 [00:10<00:31,  4.17it/s] 25%|██▍       | 43/175 [00:10<00:31,  4.17it/s] 25%|██▌       | 44/175 [00:10<00:31,  4.17it/s] 26%|██▌       | 45/175 [00:10<00:31,  4.17it/s] 26%|██▋       | 46/175 [00:11<00:30,  4.17it/s] 27%|██▋       | 47/175 [00:11<00:30,  4.17it/s] 27%|██▋       | 48/175 [00:11<00:30,  4.16it/s] 28%|██▊       | 49/175 [00:11<00:30,  4.17it/s] 29%|██▊       | 50/175 [00:12<00:29,  4.17it/s] 29%|██▉       | 51/175 [00:12<00:29,  4.17it/s] 30%|██▉       | 52/175 [00:12<00:29,  4.17it/s] 30%|███       | 53/175 [00:12<00:29,  4.17it/s] 31%|███       | 54/175 [00:13<00:29,  4.17it/s] 31%|███▏      | 55/175 [00:13<00:28,  4.16it/s] 32%|███▏      | 56/175 [00:13<00:28,  4.16it/s] 33%|███▎      | 57/175 [00:13<00:28,  4.16it/s] 33%|███▎      | 58/175 [00:13<00:28,  4.17it/s] 34%|███▎      | 59/175 [00:14<00:27,  4.17it/s] 34%|███▍      | 60/175 [00:14<00:27,  4.17it/s] 35%|███▍      | 61/175 [00:14<00:27,  4.17it/s] 35%|███▌      | 62/175 [00:14<00:27,  4.17it/s] 36%|███▌      | 63/175 [00:15<00:26,  4.17it/s] 37%|███▋      | 64/175 [00:15<00:26,  4.17it/s] 37%|███▋      | 65/175 [00:15<00:26,  4.17it/s] 38%|███▊      | 66/175 [00:15<00:26,  4.17it/s] 38%|███▊      | 67/175 [00:16<00:25,  4.17it/s] 39%|███▉      | 68/175 [00:16<00:25,  4.17it/s] 39%|███▉      | 69/175 [00:16<00:25,  4.17it/s] 40%|████      | 70/175 [00:16<00:25,  4.16it/s] 41%|████      | 71/175 [00:17<00:24,  4.16it/s] 41%|████      | 72/175 [00:17<00:24,  4.17it/s] 42%|████▏     | 73/175 [00:17<00:24,  4.17it/s] 42%|████▏     | 74/175 [00:17<00:24,  4.17it/s] 43%|████▎     | 75/175 [00:18<00:23,  4.17it/s] 43%|████▎     | 76/175 [00:18<00:23,  4.17it/s] 44%|████▍     | 77/175 [00:18<00:23,  4.17it/s] 45%|████▍     | 78/175 [00:18<00:23,  4.17it/s] 45%|████▌     | 79/175 [00:19<00:23,  4.17it/s] 46%|████▌     | 80/175 [00:19<00:22,  4.17it/s] 46%|████▋     | 81/175 [00:19<00:22,  4.16it/s] 47%|████▋     | 82/175 [00:19<00:22,  4.17it/s] 47%|████▋     | 83/175 [00:19<00:22,  4.16it/s] 48%|████▊     | 84/175 [00:20<00:21,  4.17it/s] 49%|████▊     | 85/175 [00:20<00:21,  4.17it/s] 49%|████▉     | 86/175 [00:20<00:21,  4.17it/s] 50%|████▉     | 87/175 [00:20<00:21,  4.17it/s] 50%|█████     | 88/175 [00:21<00:20,  4.17it/s] 51%|█████     | 89/175 [00:21<00:20,  4.16it/s] 51%|█████▏    | 90/175 [00:21<00:20,  4.17it/s] 52%|█████▏    | 91/175 [00:21<00:20,  4.17it/s] 53%|█████▎    | 92/175 [00:22<00:19,  4.16it/s] 53%|█████▎    | 93/175 [00:22<00:19,  4.16it/s] 54%|█████▎    | 94/175 [00:22<00:19,  4.16it/s] 54%|█████▍    | 95/175 [00:22<00:19,  4.16it/s] 55%|█████▍    | 96/175 [00:23<00:18,  4.16it/s] 55%|█████▌    | 97/175 [00:23<00:18,  4.16it/s] 56%|█████▌    | 98/175 [00:23<00:18,  4.16it/s] 57%|█████▋    | 99/175 [00:23<00:18,  4.16it/s] 57%|█████▋    | 100/175 [00:24<00:18,  4.16it/s] 58%|█████▊    | 101/175 [00:24<00:17,  4.16it/s] 58%|█████▊    | 102/175 [00:24<00:17,  4.17it/s] 59%|█████▉    | 103/175 [00:24<00:17,  4.16it/s] 59%|█████▉    | 104/175 [00:25<00:17,  4.16it/s] 60%|██████    | 105/175 [00:25<00:16,  4.17it/s] 61%|██████    | 106/175 [00:25<00:16,  4.16it/s] 61%|██████    | 107/175 [00:25<00:16,  4.16it/s] 62%|██████▏   | 108/175 [00:25<00:16,  4.16it/s] 62%|██████▏   | 109/175 [00:26<00:15,  4.15it/s] 63%|██████▎   | 110/175 [00:26<00:15,  4.14it/s] 63%|██████▎   | 111/175 [00:26<00:15,  4.13it/s] 64%|██████▍   | 112/175 [00:26<00:15,  4.13it/s] 65%|██████▍   | 113/175 [00:27<00:15,  4.13it/s] 65%|██████▌   | 114/175 [00:27<00:14,  4.13it/s] 66%|██████▌   | 115/175 [00:27<00:14,  4.14it/s] 66%|██████▋   | 116/175 [00:27<00:14,  4.14it/s] 67%|██████▋   | 117/175 [00:28<00:13,  4.15it/s] 67%|██████▋   | 118/175 [00:28<00:13,  4.15it/s] 68%|██████▊   | 119/175 [00:28<00:13,  4.15it/s] 69%|██████▊   | 120/175 [00:28<00:13,  4.15it/s] 69%|██████▉   | 121/175 [00:29<00:13,  4.14it/s] 70%|██████▉   | 122/175 [00:29<00:12,  4.14it/s] 70%|███████   | 123/175 [00:29<00:12,  4.13it/s] 71%|███████   | 124/175 [00:29<00:12,  4.12it/s] 71%|███████▏  | 125/175 [00:30<00:12,  4.13it/s] 72%|███████▏  | 126/175 [00:30<00:11,  4.13it/s] 73%|███████▎  | 127/175 [00:30<00:11,  4.14it/s] 73%|███████▎  | 128/175 [00:30<00:11,  4.14it/s] 74%|███████▎  | 129/175 [00:31<00:11,  4.15it/s] 74%|███████▍  | 130/175 [00:31<00:10,  4.15it/s] 75%|███████▍  | 131/175 [00:31<00:10,  4.15it/s] 75%|███████▌  | 132/175 [00:31<00:10,  4.15it/s] 76%|███████▌  | 133/175 [00:32<00:10,  4.14it/s] 77%|███████▋  | 134/175 [00:32<00:09,  4.13it/s] 77%|███████▋  | 135/175 [00:32<00:09,  4.12it/s] 78%|███████▊  | 136/175 [00:32<00:09,  4.12it/s] 78%|███████▊  | 137/175 [00:33<00:09,  4.13it/s] 79%|███████▉  | 138/175 [00:33<00:08,  4.14it/s] 79%|███████▉  | 139/175 [00:33<00:08,  4.14it/s] 80%|████████  | 140/175 [00:33<00:08,  4.15it/s] 81%|████████  | 141/175 [00:33<00:08,  4.15it/s] 81%|████████  | 142/175 [00:34<00:07,  4.15it/s] 82%|████████▏ | 143/175 [00:34<00:07,  4.15it/s] 82%|████████▏ | 144/175 [00:34<00:07,  4.14it/s] 83%|████████▎ | 145/175 [00:34<00:07,  4.13it/s] 83%|████████▎ | 146/175 [00:35<00:07,  4.12it/s] 84%|████████▍ | 147/175 [00:35<00:06,  4.12it/s] 85%|████████▍ | 148/175 [00:35<00:06,  4.12it/s] 85%|████████▌ | 149/175 [00:35<00:06,  4.13it/s] 86%|████████▌ | 150/175 [00:36<00:06,  4.13it/s] 86%|████████▋ | 151/175 [00:36<00:05,  4.14it/s] 87%|████████▋ | 152/175 [00:36<00:05,  4.14it/s] 87%|████████▋ | 153/175 [00:36<00:05,  4.14it/s] 88%|████████▊ | 154/175 [00:37<00:05,  4.13it/s] 89%|████████▊ | 155/175 [00:37<00:04,  4.12it/s] 89%|████████▉ | 156/175 [00:37<00:04,  4.12it/s] 90%|████████▉ | 157/175 [00:37<00:04,  4.12it/s] 90%|█████████ | 158/175 [00:38<00:04,  4.13it/s] 91%|█████████ | 159/175 [00:38<00:03,  4.13it/s] 91%|█████████▏| 160/175 [00:38<00:03,  4.13it/s] 92%|█████████▏| 161/175 [00:38<00:03,  4.12it/s] 93%|█████████▎| 162/175 [00:39<00:03,  4.11it/s] 93%|█████████▎| 163/175 [00:39<00:02,  4.11it/s] 94%|█████████▎| 164/175 [00:39<00:02,  4.12it/s] 94%|█████████▍| 165/175 [00:39<00:02,  4.12it/s] 95%|█████████▍| 166/175 [00:40<00:02,  4.13it/s] 95%|█████████▌| 167/175 [00:40<00:01,  4.13it/s] 96%|█████████▌| 168/175 [00:40<00:01,  4.12it/s] 97%|█████████▋| 169/175 [00:40<00:01,  4.12it/s] 97%|█████████▋| 170/175 [00:41<00:01,  4.11it/s] 98%|█████████▊| 171/175 [00:41<00:00,  4.11it/s] 98%|█████████▊| 172/175 [00:41<00:00,  4.12it/s] 99%|█████████▉| 173/175 [00:41<00:00,  4.13it/s] 99%|█████████▉| 174/175 [00:41<00:00,  4.13it/s]100%|██████████| 175/175 [00:42<00:00,  4.13it/s]accuracy:  0.8457142857142858
100%|██████████| 175/175 [00:44<00:00,  3.90it/s]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Training dataset size: 384, validation dataset size: 119
Training dataset size: 384, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training dataset size: 384, validation dataset size: 119
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.02s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 14:08:32,534] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-03-12 14:08:32,605] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
trainable params: 2616703233 || all params: 2616703233 || trainable%: 100.0
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
trainable params: 15140865 || all params: 2629482753 || trainable%: 0.5758115349007578
/zfsauton2/home/kzaidi/10707/Generalizable-Reward-Model/reward_models/base_trainer.py:110: FutureWarning: Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.
  warnings.warn(
training start
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2025-03-12 14:08:33,818] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /zfsauton2/home/kzaidi/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
  0%|          | 0/64 [00:00<?, ?it/s]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/64 [00:03<03:55,  3.73s/it]  3%|▎         | 2/64 [00:06<03:16,  3.17s/it]  5%|▍         | 3/64 [00:08<02:52,  2.84s/it]  6%|▋         | 4/64 [00:11<02:43,  2.72s/it]  8%|▊         | 5/64 [00:14<02:36,  2.66s/it]  9%|▉         | 6/64 [00:16<02:28,  2.57s/it] 11%|█         | 7/64 [00:18<02:22,  2.51s/it] 12%|█▎        | 8/64 [00:20<02:13,  2.39s/it] 14%|█▍        | 9/64 [00:23<02:12,  2.42s/it] 16%|█▌        | 10/64 [00:25<02:08,  2.37s/it]                                               {'loss': 1.2472, 'grad_norm': 14.008123397827148, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.31}
 16%|█▌        | 10/64 [00:25<02:08,  2.37s/it] 17%|█▋        | 11/64 [00:28<02:06,  2.39s/it] 19%|█▉        | 12/64 [00:30<02:01,  2.33s/it] 20%|██        | 13/64 [00:32<01:57,  2.30s/it] 22%|██▏       | 14/64 [00:34<01:54,  2.29s/it] 23%|██▎       | 15/64 [00:37<01:54,  2.33s/it] 25%|██▌       | 16/64 [00:39<01:50,  2.31s/it] 27%|██▋       | 17/64 [00:41<01:47,  2.29s/it] 28%|██▊       | 18/64 [00:44<01:47,  2.33s/it] 30%|██▉       | 19/64 [00:46<01:47,  2.40s/it] 31%|███▏      | 20/64 [00:49<01:44,  2.37s/it]                                               {'loss': 1.0387, 'grad_norm': 11.601146697998047, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.62}
 31%|███▏      | 20/64 [00:49<01:44,  2.37s/it] 33%|███▎      | 21/64 [00:51<01:40,  2.35s/it] 34%|███▍      | 22/64 [00:53<01:42,  2.44s/it] 36%|███▌      | 23/64 [00:56<01:36,  2.34s/it] 38%|███▊      | 24/64 [00:58<01:33,  2.33s/it] 39%|███▉      | 25/64 [01:01<01:34,  2.42s/it] 41%|████      | 26/64 [01:03<01:34,  2.48s/it] 42%|████▏     | 27/64 [01:06<01:32,  2.49s/it] 44%|████▍     | 28/64 [01:08<01:26,  2.42s/it] 45%|████▌     | 29/64 [01:10<01:24,  2.41s/it] 47%|████▋     | 30/64 [01:13<01:22,  2.42s/it]                                               {'loss': 0.8792, 'grad_norm': 3.150951862335205, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.94}
 47%|████▋     | 30/64 [01:13<01:22,  2.42s/it] 48%|████▊     | 31/64 [01:15<01:18,  2.38s/it] 50%|█████     | 32/64 [01:17<01:12,  2.26s/it]/zfsauton2/home/kzaidi/miniconda3/envs/grm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 52%|█████▏    | 33/64 [01:20<01:14,  2.41s/it] 53%|█████▎    | 34/64 [01:22<01:09,  2.32s/it] 55%|█████▍    | 35/64 [01:24<01:04,  2.22s/it] 56%|█████▋    | 36/64 [01:26<01:01,  2.19s/it] 58%|█████▊    | 37/64 [01:28<01:00,  2.23s/it] 59%|█████▉    | 38/64 [01:31<00:59,  2.28s/it] 61%|██████    | 39/64 [01:33<00:56,  2.26s/it] 62%|██████▎   | 40/64 [01:35<00:55,  2.32s/it]                                               {'loss': 0.6949, 'grad_norm': 3.9304018020629883, 'learning_rate': 3.2634737357758994e-06, 'epoch': 1.25}
 62%|██████▎   | 40/64 [01:35<00:55,  2.32s/it] 64%|██████▍   | 41/64 [01:38<00:52,  2.28s/it] 66%|██████▌   | 42/64 [01:40<00:52,  2.38s/it] 67%|██████▋   | 43/64 [01:43<00:51,  2.43s/it] 69%|██████▉   | 44/64 [01:45<00:48,  2.40s/it] 70%|███████   | 45/64 [01:47<00:45,  2.39s/it] 72%|███████▏  | 46/64 [01:50<00:44,  2.48s/it] 73%|███████▎  | 47/64 [01:53<00:41,  2.46s/it] 75%|███████▌  | 48/64 [01:55<00:38,  2.42s/it] 77%|███████▋  | 49/64 [01:57<00:36,  2.40s/it] 78%|███████▊  | 50/64 [02:00<00:35,  2.55s/it]                                               {'loss': 0.8245, 'grad_norm': 9.806671142578125, 'learning_rate': 1.2062093865360458e-06, 'epoch': 1.56}
 78%|███████▊  | 50/64 [02:00<00:35,  2.55s/it] 80%|███████▉  | 51/64 [02:03<00:33,  2.59s/it] 81%|████████▏ | 52/64 [02:06<00:31,  2.62s/it] 83%|████████▎ | 53/64 [02:07<00:26,  2.38s/it] 84%|████████▍ | 54/64 [02:10<00:24,  2.41s/it] 86%|████████▌ | 55/64 [02:12<00:20,  2.31s/it] 88%|████████▊ | 56/64 [02:14<00:17,  2.23s/it] 89%|████████▉ | 57/64 [02:16<00:15,  2.23s/it] 91%|█████████ | 58/64 [02:19<00:13,  2.30s/it] 92%|█████████▏| 59/64 [02:21<00:11,  2.23s/it] 94%|█████████▍| 60/64 [02:23<00:09,  2.30s/it]                                               {'loss': 0.7103, 'grad_norm': 7.415162563323975, 'learning_rate': 1.0235029373752758e-07, 'epoch': 1.88}
 94%|█████████▍| 60/64 [02:23<00:09,  2.30s/it] 95%|█████████▌| 61/64 [02:26<00:07,  2.41s/it] 97%|█████████▋| 62/64 [02:28<00:04,  2.47s/it] 98%|█████████▊| 63/64 [02:31<00:02,  2.51s/it]100%|██████████| 64/64 [02:33<00:00,  2.41s/it]                                               {'train_runtime': 154.3596, 'train_samples_per_second': 4.975, 'train_steps_per_second': 0.415, 'train_loss': 0.8972844034433365, 'epoch': 2.0}
100%|██████████| 64/64 [02:34<00:00,  2.41s/it]100%|██████████| 64/64 [02:34<00:00,  2.41s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.29s/it]
Some weights of the model checkpoint at Ray2333/GRM-Gemma2-2B-sftreg were not used when initializing Gemma2ForCausalLM: ['v_head.summary.0.bias', 'v_head.summary.0.weight', 'v_head.summary.2.bias', 'v_head.summary.2.weight']
- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:root:A <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'> model is loaded from 'Ray2333/GRM-Gemma2-2B-sftreg', and no v_head weight is found. This IS expected if you are not resuming PPO training.
size of test dataset:  153
  0%|          | 0/153 [00:00<?, ?it/s]  1%|          | 1/153 [00:00<01:21,  1.86it/s]  1%|▏         | 2/153 [00:00<00:54,  2.75it/s]  2%|▏         | 3/153 [00:01<00:46,  3.25it/s]  3%|▎         | 4/153 [00:01<00:41,  3.55it/s]  3%|▎         | 5/153 [00:01<00:39,  3.75it/s]  4%|▍         | 6/153 [00:01<00:37,  3.88it/s]  5%|▍         | 7/153 [00:01<00:36,  3.97it/s]  5%|▌         | 8/153 [00:02<00:35,  4.03it/s]  6%|▌         | 9/153 [00:02<00:35,  4.07it/s]  7%|▋         | 10/153 [00:02<00:34,  4.10it/s]  7%|▋         | 11/153 [00:02<00:34,  4.12it/s]  8%|▊         | 12/153 [00:03<00:34,  4.13it/s]  8%|▊         | 13/153 [00:03<00:33,  4.13it/s]  9%|▉         | 14/153 [00:03<00:33,  4.14it/s] 10%|▉         | 15/153 [00:03<00:33,  4.15it/s] 10%|█         | 16/153 [00:04<00:32,  4.16it/s] 11%|█         | 17/153 [00:04<00:32,  4.16it/s] 12%|█▏        | 18/153 [00:04<00:32,  4.16it/s] 12%|█▏        | 19/153 [00:04<00:32,  4.17it/s] 13%|█▎        | 20/153 [00:05<00:31,  4.17it/s] 14%|█▎        | 21/153 [00:05<00:31,  4.17it/s] 14%|█▍        | 22/153 [00:05<00:31,  4.17it/s] 15%|█▌        | 23/153 [00:05<00:31,  4.16it/s] 16%|█▌        | 24/153 [00:06<00:30,  4.16it/s] 16%|█▋        | 25/153 [00:06<00:30,  4.17it/s] 17%|█▋        | 26/153 [00:06<00:30,  4.17it/s] 18%|█▊        | 27/153 [00:06<00:30,  4.17it/s] 18%|█▊        | 28/153 [00:07<00:29,  4.17it/s] 19%|█▉        | 29/153 [00:07<00:29,  4.17it/s] 20%|█▉        | 30/153 [00:07<00:29,  4.17it/s] 20%|██        | 31/153 [00:07<00:29,  4.17it/s] 21%|██        | 32/153 [00:07<00:28,  4.17it/s] 22%|██▏       | 33/153 [00:08<00:28,  4.18it/s] 22%|██▏       | 34/153 [00:08<00:28,  4.17it/s] 23%|██▎       | 35/153 [00:08<00:28,  4.17it/s] 24%|██▎       | 36/153 [00:08<00:28,  4.17it/s] 24%|██▍       | 37/153 [00:09<00:27,  4.17it/s] 25%|██▍       | 38/153 [00:09<00:27,  4.17it/s] 25%|██▌       | 39/153 [00:09<00:27,  4.17it/s] 26%|██▌       | 40/153 [00:09<00:27,  4.16it/s] 27%|██▋       | 41/153 [00:10<00:26,  4.17it/s] 27%|██▋       | 42/153 [00:10<00:26,  4.17it/s] 28%|██▊       | 43/153 [00:10<00:26,  4.17it/s] 29%|██▉       | 44/153 [00:10<00:26,  4.17it/s] 29%|██▉       | 45/153 [00:11<00:25,  4.17it/s] 30%|███       | 46/153 [00:11<00:25,  4.17it/s] 31%|███       | 47/153 [00:11<00:25,  4.16it/s] 31%|███▏      | 48/153 [00:11<00:25,  4.16it/s] 32%|███▏      | 49/153 [00:12<00:24,  4.16it/s] 33%|███▎      | 50/153 [00:12<00:24,  4.16it/s] 33%|███▎      | 51/153 [00:12<00:24,  4.17it/s] 34%|███▍      | 52/153 [00:12<00:24,  4.16it/s] 35%|███▍      | 53/153 [00:13<00:24,  4.17it/s] 35%|███▌      | 54/153 [00:13<00:23,  4.17it/s] 36%|███▌      | 55/153 [00:13<00:23,  4.17it/s] 37%|███▋      | 56/153 [00:13<00:23,  4.17it/s] 37%|███▋      | 57/153 [00:13<00:23,  4.17it/s] 38%|███▊      | 58/153 [00:14<00:22,  4.17it/s] 39%|███▊      | 59/153 [00:14<00:22,  4.17it/s] 39%|███▉      | 60/153 [00:14<00:22,  4.16it/s] 40%|███▉      | 61/153 [00:14<00:22,  4.16it/s] 41%|████      | 62/153 [00:15<00:21,  4.16it/s] 41%|████      | 63/153 [00:15<00:21,  4.16it/s] 42%|████▏     | 64/153 [00:15<00:21,  4.16it/s] 42%|████▏     | 65/153 [00:15<00:21,  4.16it/s] 43%|████▎     | 66/153 [00:16<00:20,  4.16it/s] 44%|████▍     | 67/153 [00:16<00:20,  4.16it/s] 44%|████▍     | 68/153 [00:16<00:20,  4.16it/s] 45%|████▌     | 69/153 [00:16<00:20,  4.16it/s] 46%|████▌     | 70/153 [00:17<00:19,  4.15it/s] 46%|████▋     | 71/153 [00:17<00:19,  4.15it/s] 47%|████▋     | 72/153 [00:17<00:19,  4.14it/s] 48%|████▊     | 73/153 [00:17<00:19,  4.14it/s] 48%|████▊     | 74/153 [00:18<00:19,  4.13it/s] 49%|████▉     | 75/153 [00:18<00:18,  4.13it/s] 50%|████▉     | 76/153 [00:18<00:18,  4.13it/s] 50%|█████     | 77/153 [00:18<00:18,  4.13it/s] 51%|█████     | 78/153 [00:19<00:18,  4.13it/s] 52%|█████▏    | 79/153 [00:19<00:17,  4.14it/s] 52%|█████▏    | 80/153 [00:19<00:17,  4.14it/s] 53%|█████▎    | 81/153 [00:19<00:17,  4.13it/s] 54%|█████▎    | 82/153 [00:20<00:17,  4.12it/s] 54%|█████▍    | 83/153 [00:20<00:16,  4.12it/s] 55%|█████▍    | 84/153 [00:20<00:16,  4.13it/s] 56%|█████▌    | 85/153 [00:20<00:16,  4.13it/s] 56%|█████▌    | 86/153 [00:20<00:16,  4.14it/s] 57%|█████▋    | 87/153 [00:21<00:15,  4.14it/s] 58%|█████▊    | 88/153 [00:21<00:15,  4.15it/s] 58%|█████▊    | 89/153 [00:21<00:15,  4.15it/s] 59%|█████▉    | 90/153 [00:21<00:15,  4.15it/s] 59%|█████▉    | 91/153 [00:22<00:14,  4.15it/s] 60%|██████    | 92/153 [00:22<00:14,  4.15it/s] 61%|██████    | 93/153 [00:22<00:14,  4.14it/s] 61%|██████▏   | 94/153 [00:22<00:14,  4.13it/s] 62%|██████▏   | 95/153 [00:23<00:14,  4.12it/s] 63%|██████▎   | 96/153 [00:23<00:13,  4.12it/s] 63%|██████▎   | 97/153 [00:23<00:13,  4.12it/s] 64%|██████▍   | 98/153 [00:23<00:13,  4.13it/s] 65%|██████▍   | 99/153 [00:24<00:13,  4.13it/s] 65%|██████▌   | 100/153 [00:24<00:12,  4.12it/s] 66%|██████▌   | 101/153 [00:24<00:12,  4.11it/s] 67%|██████▋   | 102/153 [00:24<00:12,  4.11it/s] 67%|██████▋   | 103/153 [00:25<00:12,  4.11it/s] 68%|██████▊   | 104/153 [00:25<00:11,  4.12it/s] 69%|██████▊   | 105/153 [00:25<00:11,  4.13it/s] 69%|██████▉   | 106/153 [00:25<00:11,  4.12it/s] 70%|██████▉   | 107/153 [00:26<00:11,  4.12it/s] 71%|███████   | 108/153 [00:26<00:10,  4.11it/s] 71%|███████   | 109/153 [00:26<00:10,  4.11it/s] 72%|███████▏  | 110/153 [00:26<00:10,  4.11it/s] 73%|███████▎  | 111/153 [00:27<00:10,  4.11it/s] 73%|███████▎  | 112/153 [00:27<00:10,  4.10it/s] 74%|███████▍  | 113/153 [00:27<00:09,  4.09it/s] 75%|███████▍  | 114/153 [00:27<00:09,  4.10it/s] 75%|███████▌  | 115/153 [00:28<00:09,  4.11it/s] 76%|███████▌  | 116/153 [00:28<00:08,  4.12it/s] 76%|███████▋  | 117/153 [00:28<00:08,  4.12it/s] 77%|███████▋  | 118/153 [00:28<00:08,  4.11it/s] 78%|███████▊  | 119/153 [00:28<00:08,  4.10it/s] 78%|███████▊  | 120/153 [00:29<00:08,  4.10it/s] 79%|███████▉  | 121/153 [00:29<00:07,  4.10it/s] 80%|███████▉  | 122/153 [00:29<00:07,  4.11it/s] 80%|████████  | 123/153 [00:29<00:07,  4.10it/s] 81%|████████  | 124/153 [00:30<00:07,  4.10it/s] 82%|████████▏ | 125/153 [00:30<00:06,  4.10it/s] 82%|████████▏ | 126/153 [00:30<00:06,  4.10it/s] 83%|████████▎ | 127/153 [00:30<00:06,  4.11it/s] 84%|████████▎ | 128/153 [00:31<00:06,  4.10it/s] 84%|████████▍ | 129/153 [00:31<00:05,  4.09it/s] 85%|████████▍ | 130/153 [00:31<00:05,  4.09it/s] 86%|████████▌ | 131/153 [00:31<00:05,  4.09it/s] 86%|████████▋ | 132/153 [00:32<00:05,  4.09it/s] 87%|████████▋ | 133/153 [00:32<00:04,  4.08it/s] 88%|████████▊ | 134/153 [00:32<00:04,  4.09it/s] 88%|████████▊ | 135/153 [00:32<00:04,  4.09it/s] 89%|████████▉ | 136/153 [00:33<00:04,  4.10it/s] 90%|████████▉ | 137/153 [00:33<00:03,  4.10it/s] 90%|█████████ | 138/153 [00:33<00:03,  4.09it/s] 91%|█████████ | 139/153 [00:33<00:03,  4.09it/s] 92%|█████████▏| 140/153 [00:34<00:03,  4.10it/s] 92%|█████████▏| 141/153 [00:34<00:02,  4.10it/s] 93%|█████████▎| 142/153 [00:34<00:02,  4.09it/s] 93%|█████████▎| 143/153 [00:34<00:02,  4.09it/s] 94%|█████████▍| 144/153 [00:35<00:02,  4.09it/s] 95%|█████████▍| 145/153 [00:35<00:01,  4.10it/s] 95%|█████████▌| 146/153 [00:35<00:01,  4.10it/s] 96%|█████████▌| 147/153 [00:35<00:01,  4.09it/s] 97%|█████████▋| 148/153 [00:36<00:01,  4.09it/s] 97%|█████████▋| 149/153 [00:36<00:00,  4.10it/s] 98%|█████████▊| 150/153 [00:36<00:00,  4.10it/s] 99%|█████████▊| 151/153 [00:36<00:00,  4.09it/s] 99%|█████████▉| 152/153 [00:37<00:00,  4.09it/s]100%|██████████| 153/153 [00:37<00:00,  4.10it/s]accuracy:  0.6862745098039216
100%|██████████| 153/153 [00:39<00:00,  3.85it/s]
